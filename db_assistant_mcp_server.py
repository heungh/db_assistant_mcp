#!/usr/bin/env python3
"""
DB Assistant Amazon Q CLI MCP ì„œë²„
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import sys

import boto3
from botocore.exceptions import ClientError

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

# ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
ANALYSIS_AVAILABLE = False
CHART_AVAILABLE = False
try:
    import pandas as pd
    import numpy as np
    import sqlparse
    import matplotlib

    matplotlib.use("Agg")  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.impute import SimpleImputer

    ANALYSIS_AVAILABLE = True
    CHART_AVAILABLE = True
except ImportError:
    sqlparse = None

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ import
from utils.logging_utils import create_session_log, debug_log, logger
from utils.constants import (
    CURRENT_DIR,
    OUTPUT_DIR,
    SQL_DIR,
    DATA_DIR,
    LOGS_DIR,
    BACKUP_DIR,
    PERFORMANCE_THRESHOLDS,
    DEFAULT_METRICS,
    DEFAULT_REGION,
    BEDROCK_REGION,
    KNOWLEDGE_BASE_REGION,
    KNOWLEDGE_BASE_ID,
    DATA_SOURCE_ID,
    QUERY_RESULTS_BUCKET,
    QUERY_RESULTS_DEV_BUCKET,
    BEDROCK_AGENT_BUCKET,
)
from utils.parsers import (
    parse_table_name,
    format_file_link,
    convert_kst_to_utc,
    convert_utc_to_local,
    parse_data_type,
    extract_sql_type,
    sanitize_sql,
    is_valid_sql_identifier,
)
from utils.formatters import (
    format_bytes,
    format_number,
    format_percentage,
    format_duration,
    format_timestamp,
    format_sql_for_display,
    format_metric_value,
)

# ëª¨ë“ˆ import (ë¦¬íŒ©í† ë§)
from modules.lambda_client import LambdaClient  # Week 1
from modules.cloudwatch_manager import CloudWatchManager  # Week 2
from modules.report_generator import ReportGenerator  # Week 3
from modules.sql_parser import SQLParser  # Week 4 Phase 2

# ë¡œê¹… ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ëŠ” utils ëª¨ë“ˆì—ì„œ importë¨
# create_session_log, debug_log í•¨ìˆ˜ëŠ” utils/logging_utils.pyì—ì„œ ì œê³µ
# ë””ë ‰í† ë¦¬ ê²½ë¡œ ìƒìˆ˜ë“¤ì€ utils/constants.pyì—ì„œ ì œê³µ


class DBAssistantMCPServer:
    def __init__(self):
        try:
            logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œì‘")
            self.bedrock_client = boto3.client(
                "bedrock-runtime", region_name="us-west-2", verify=False
            )
            logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ - ë¦¬ì „: us-west-2")

            # Bedrock ì ‘ê·¼ ê¶Œí•œ í…ŒìŠ¤íŠ¸
            try:
                # ê°„ë‹¨í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒë¡œ ê¶Œí•œ í…ŒìŠ¤íŠ¸
                bedrock_control = boto3.client(
                    "bedrock", region_name="us-west-2", verify=False
                )
                logger.info("Bedrock ì„œë¹„ìŠ¤ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ ì¤‘...")
                # ì‹¤ì œ ê¶Œí•œ í…ŒìŠ¤íŠ¸ëŠ” ëª¨ë¸ í˜¸ì¶œ ì‹œ ìˆ˜í–‰
                logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
            except Exception as perm_e:
                logger.warning(
                    f"Bedrock ê¶Œí•œ ì‚¬ì „ í™•ì¸ ì‹¤íŒ¨ (ëª¨ë¸ í˜¸ì¶œ ì‹œ ì¬ì‹œë„): {perm_e}"
                )

        except Exception as e:
            logger.error(f"Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # Knowledge Base ID (utils/constants.pyì—ì„œ ê°€ì ¸ì˜´)
        self.knowledge_base_id = KNOWLEDGE_BASE_ID
        self.selected_database = None
        self.current_plan = None

        # ê³µìš© DB ì—°ê²° ë³€ìˆ˜ (ì—°ê²° ì¬ì‚¬ìš©ì„ ìœ„í•´)
        self.shared_connection = None
        self.shared_cursor = None
        self.tunnel_used = False

        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì • (utils/constants.pyì—ì„œ ê°€ì ¸ì˜´)
        self.PERFORMANCE_THRESHOLDS = PERFORMANCE_THRESHOLDS

        # ë¶„ì„ ê´€ë ¨ ì´ˆê¸°í™”
        self.cloudwatch = None
        # CloudWatch ë©”íŠ¸ë¦­ ì„¤ì • (utils/constants.pyì—ì„œ ê°€ì ¸ì˜´)
        self.default_metrics = DEFAULT_METRICS

        # ê¸°ë³¸ ë¦¬ì „ ì„¤ì • (utils/constants.pyì—ì„œ ê°€ì ¸ì˜´)
        self.default_region = self.get_default_region()

        # Knowledge Base í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¦¬ì „ì€ utils/constants.pyì—ì„œ ê°€ì ¸ì˜´)
        self.bedrock_agent_client = boto3.client(
            "bedrock-agent-runtime", region_name=KNOWLEDGE_BASE_REGION, verify=False
        )

        # Lambda í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ìš©)
        # ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆ ì‚¬ìš©
        self.lambda_client = LambdaClient(region=self.default_region)
        logger.info(f"Lambda í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - ë¦¬ì „: {self.default_region}")

        # CloudWatch Manager ì´ˆê¸°í™”
        # ë¦¬íŒ©í† ë§: Week 2 - CloudWatchManager ëª¨ë“ˆ ì‚¬ìš©
        self.cloudwatch_manager = CloudWatchManager(
            region=self.default_region,
            lambda_client=self.lambda_client
        )
        logger.info(f"CloudWatch Manager ì´ˆê¸°í™” ì™„ë£Œ - ë¦¬ì „: {self.default_region}")

        # Report Generator ì´ˆê¸°í™”
        # ë¦¬íŒ©í† ë§: Week 3 - ReportGenerator ëª¨ë“ˆ ì‚¬ìš©
        self.report_generator = ReportGenerator()
        logger.info("Report Generator ì´ˆê¸°í™” ì™„ë£Œ")

        # SQL Parser ì´ˆê¸°í™”
        # ë¦¬íŒ©í† ë§: Week 4 Phase 2 - SQLParser ëª¨ë“ˆ ì‚¬ìš©
        self.sql_parser = SQLParser()
        logger.info("SQL Parser ì´ˆê¸°í™” ì™„ë£Œ")

    def get_default_region(self) -> str:
        """í˜„ì¬ AWS í”„ë¡œíŒŒì¼ì˜ ê¸°ë³¸ ë¦¬ì „ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.Session()
            return session.region_name or DEFAULT_REGION
        except Exception:
            return DEFAULT_REGION

    async def _call_lambda(self, function_name: str, payload: dict) -> dict:
        """
        Lambda í•¨ìˆ˜ í˜¸ì¶œ í—¬í¼ (í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ìš©)

        ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        return await self.lambda_client._call_lambda(function_name, payload)

    def parse_table_name(self, full_table_name: str) -> tuple:
        """í…Œì´ë¸”ëª…ì—ì„œ ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª…ì„ ë¶„ë¦¬ (utils/parsers.py ìœ„ì„)"""
        return parse_table_name(full_table_name)

    def format_file_link(self, file_path: str, display_name: str = None) -> str:
        """íŒŒì¼ ê²½ë¡œë¥¼ HTML ë§í¬ë¡œ ë³€í™˜ (utils/parsers.py ìœ„ì„)"""
        return format_file_link(file_path, display_name)

    async def query_knowledge_base(self, query: str, sql_type: str) -> str:
        """Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ"""
        try:
            # SQL íƒ€ì…ì— ë”°ë¥¸ ì¿¼ë¦¬ ì¡°ì •
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

            if sql_type in ddl_types:
                # DDLì¸ ê²½ìš° ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ ì¡°íšŒ
                kb_query = f"ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ {query}"
            elif sql_type in dql_types:
                # DQLì¸ ê²½ìš° Aurora MySQL ìµœì í™” ê°€ì´ë“œ ì¡°íšŒ
                kb_query = f"Aurora MySQL ìµœì í™” ê°€ì´ë“œ {query}"
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ ì¡°íšŒ
                kb_query = f"ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ {query}"

            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": kb_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": 3}
                },
            )

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            knowledge_content = []
            for result in response.get("retrievalResults", []):
                content = result.get("content", {}).get("text", "")
                if content:
                    knowledge_content.append(content)

            if knowledge_content:
                return "\n\n".join(knowledge_content)
            else:
                return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.warning(f"Knowledge Base ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return "Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def convert_kst_to_utc(self, kst_time_str: str) -> datetime:
        """KST ì‹œê°„ ë¬¸ìì—´ì„ UTC datetime ê°ì²´ë¡œ ë³€í™˜ (utils/parsers.py ìœ„ì„)"""
        return convert_kst_to_utc(kst_time_str)

    def convert_utc(self, utc_dt: datetime, region_name: str = None) -> datetime:
        """UTC datetime ê°ì²´ë¥¼ ì§€ì •ëœ ë¦¬ì „ì˜ ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (utils/parsers.py ìœ„ì„)"""
        if region_name is None:
            region_name = self.default_region
        return convert_utc_to_local(utc_dt, region_name)

    def set_default_region(self, region_name: str) -> str:
        """ê¸°ë³¸ AWS ë¦¬ì „ì„ ë³€ê²½í•©ë‹ˆë‹¤"""
        # ì§€ì›í•˜ëŠ” ë¦¬ì „ ëª©ë¡
        supported_regions = {
            "ap-northeast-1",
            "ap-northeast-2",
            "ap-northeast-3",
            "ap-south-1",
            "ap-southeast-1",
            "ap-southeast-2",
            "ap-east-1",
            "eu-west-1",
            "eu-west-2",
            "eu-west-3",
            "eu-central-1",
            "eu-north-1",
            "us-east-1",
            "us-east-2",
            "us-west-1",
            "us-west-2",
            "ca-central-1",
            "sa-east-1",
            "me-south-1",
            "af-south-1",
        }

        if region_name not in supported_regions:
            return (
                f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ì „ì…ë‹ˆë‹¤: {region_name}\n\nâœ… ì§€ì›í•˜ëŠ” ë¦¬ì „:\n"
                + "\n".join([f"â€¢ {region}" for region in sorted(supported_regions)])
            )

        old_region = self.default_region
        self.default_region = region_name

        # í™˜ê²½ ë³€ìˆ˜ë„ ì—…ë°ì´íŠ¸
        os.environ["AWS_DEFAULT_REGION"] = region_name

        return f"âœ… ê¸°ë³¸ ë¦¬ì „ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!\n\nì´ì „: {old_region}\ní˜„ì¬: {self.default_region}\n\nğŸ’¡ ì´ì œ ëª¨ë“  AWS ì„œë¹„ìŠ¤ í˜¸ì¶œê³¼ ì‹œê°„ ë³€í™˜ì´ ìƒˆ ë¦¬ì „ ê¸°ì¤€ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤."

    async def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Lambda ì‚¬ìš©)"""
        try:
            # Lambda í•¨ìˆ˜ í˜¸ì¶œ
            result = await self._call_lambda('get-secret', {
                'secret_name': secret_name,
                'region': 'ap-northeast-2'
            })

            if result.get('success'):
                return result['secret']
            else:
                error_msg = result.get('error', 'Unknown error')
                logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨ (Lambda): {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    async def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (Lambda ì‚¬ìš©)"""
        try:
            # Lambda í•¨ìˆ˜ í˜¸ì¶œ
            result = await self._call_lambda('list-secrets', {
                'keyword': keyword,
                'region': 'ap-northeast-2'
            })

            if result.get('success'):
                return result.get('secrets', [])
            else:
                error_msg = result.get('error', 'Unknown error')
                logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ (Lambda): {error_msg}")
                return []

        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì • (EC2ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨ - VPC ì§ì ‘ ì—°ê²°)"""
        logger.info("EC2 VPC í™˜ê²½: SSH í„°ë„ë§ ê±´ë„ˆë›°ê¸°")
        return False  # í•­ìƒ False ë°˜í™˜í•˜ì—¬ ì§ì ‘ ì—°ê²° ì‚¬ìš©

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬ (EC2ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)"""
        pass  # ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŒ

    def extract_successful_created_tables(
        self, sql_content: str, issues: List[str]
    ) -> List[str]:
        """ì„±ê³µí•œ CREATE TABLEë§Œ ì¶”ì¶œ (ì‹¤íŒ¨í•œ ê²ƒì€ ì œì™¸)"""
        created_tables = self.extract_created_tables(sql_content)
        successful_tables = []

        for table in created_tables:
            # issuesì—ì„œ í•´ë‹¹ í…Œì´ë¸”ì˜ CREATE TABLE ì‹¤íŒ¨ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            table_failed = any(
                f"í…Œì´ë¸” '{table}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤" in issue for issue in issues
            )
            if not table_failed:
                successful_tables.append(table)

        return successful_tables



    async def execute_explain_with_cursor(self, sql_content: str, cursor, debug_log):
        """EXPLAIN ì‹¤í–‰ (ì»¤ì„œ ì‚¬ìš©)"""
        result = {"issues": [], "explain_data": None}

        try:
            if cursor is None:
                debug_log("ì»¤ì„œê°€ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return result

            # EXPLAIN ì‹¤í–‰
            explain_query = f"EXPLAIN {sql_content.strip().rstrip(';')}"
            debug_log(f"EXPLAIN ì¿¼ë¦¬: {explain_query}")

            cursor.execute(explain_query)
            explain_data = cursor.fetchall()
            result["explain_data"] = explain_data

            # EXPLAIN ê²°ê³¼ëŠ” ë¬¸ìì—´ë¡œë§Œ ì €ì¥

            debug_log("EXPLAIN ì‹¤í–‰ ì™„ë£Œ")
            return result

        except Exception as e:
            debug_log(f"EXPLAIN ì‹¤í–‰ ì˜ˆì™¸: {e}")
            result["issues"].append(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return result

    def check_performance_issues(self, explain_data, query_content, debug_log):
        """EXPLAIN ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬"""
        debug_log("ğŸ”ğŸ”ğŸ” check_performance_issues í•¨ìˆ˜ ì‹œì‘ ğŸ”ğŸ”ğŸ”")
        performance_issues = []

        # ìŠ¹ì¸ëœ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì¿¼ë¦¬ ì²´í¬
        batch_approval_patterns = [
            r"ëŒ€ìš©ëŸ‰\s*ë°°ì¹˜.*ìŠ¹ì¸",
            r"ë°°ì¹˜.*ìŠ¹ì¸.*ë°›ìŒ",
            r"ìŠ¹ì¸.*ëŒ€ìš©ëŸ‰",
            r"approved.*batch",
            r"batch.*approved",
        ]

        is_approved_batch = False
        for pattern in batch_approval_patterns:
            if re.search(pattern, query_content, re.IGNORECASE):
                is_approved_batch = True
                debug_log(f"ìŠ¹ì¸ëœ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì¿¼ë¦¬ë¡œ ì¸ì‹: {pattern}")
                break

        debug_log(f"EXPLAIN ë°ì´í„° í–‰ ìˆ˜: {len(explain_data)}")
        for idx, row in enumerate(explain_data):
            debug_log(f"EXPLAIN í–‰ {idx}: {row}")
            if len(row) >= 10:  # EXPLAIN ê²°ê³¼ êµ¬ì¡° í™•ì¸
                rows_examined = row[9] if row[9] is not None else 0
                debug_log(f"ê²€ì‚¬í•  í–‰ ìˆ˜: {rows_examined}")

                if rows_examined >= self.PERFORMANCE_THRESHOLDS["critical_rows_scan"]:
                    if is_approved_batch:
                        issue = f"âš ï¸ ê²½ê³ : ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰) - ìŠ¹ì¸ëœ ë°°ì¹˜ ì‘ì—…"
                        performance_issues.append(issue)
                        debug_log(f"ìŠ¹ì¸ëœ ë°°ì¹˜ - ê²½ê³  ì¶”ê°€: {issue}")
                    else:
                        issue = f"âŒ ì‹¤íŒ¨: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ì „ì²´ ìŠ¤ìº” ({rows_examined:,}í–‰)"
                        performance_issues.append(issue)
                        debug_log(f"ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ì‹¤íŒ¨ ì¶”ê°€: {issue}")

                elif rows_examined >= self.PERFORMANCE_THRESHOLDS["max_rows_scan"]:
                    if is_approved_batch:
                        issue = f"âš ï¸ ê²½ê³ : ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰) - ìŠ¹ì¸ëœ ë°°ì¹˜ ì‘ì—…"
                        performance_issues.append(issue)
                        debug_log(f"ìŠ¹ì¸ëœ ë°°ì¹˜ - ê²½ê³  ì¶”ê°€: {issue}")
                    else:
                        issue = f"âŒ ì‹¤íŒ¨: ì„±ëŠ¥ ë¬¸ì œ - ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰)"
                        performance_issues.append(issue)
                        debug_log(f"ì„±ëŠ¥ ë¬¸ì œ - ì‹¤íŒ¨ ì¶”ê°€: {issue}")

        debug_log(
            f"ğŸ”ğŸ”ğŸ” check_performance_issues ì™„ë£Œ - ì´ìŠˆ: {performance_issues}, ìŠ¹ì¸: {is_approved_batch} ğŸ”ğŸ”ğŸ”"
        )
        return performance_issues, is_approved_batch

    async def test_individual_query_validation(
        self, database_secret: str, filename: str
    ) -> str:
        """ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (Lambda ê¸°ë°˜)"""
        try:
            # SQL íŒŒì¼ ì½ê¸°
            sql_file_path = os.path.join("sql", filename)
            if not os.path.exists(sql_file_path):
                return f"âŒ SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                sql_content = f.read()

            logger.info(f"ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ ì‹œì‘ (Lambda ê¸°ë°˜): {filename}")

            # SQLì„ ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬
            if sqlparse:
                statements = sqlparse.split(sql_content)
            else:
                statements = [stmt.strip() for stmt in sql_content.split(";") if stmt.strip()]

            logger.info(f"ì´ {len(statements)}ê°œì˜ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬")

            # ê²°ê³¼ ì €ì¥
            success_count = 0
            error_count = 0
            performance_issues = []
            all_issues = []

            # ê° SELECT ì¿¼ë¦¬ì— ëŒ€í•´ Lambda EXPLAIN ì‹¤í–‰
            for i, stmt in enumerate(statements):
                if not stmt.strip():
                    continue

                # ì£¼ì„ ì œê±°
                cleaned_stmt = re.sub(r"--.*$", "", stmt, flags=re.MULTILINE)
                cleaned_stmt = re.sub(r"/\*.*?\*/", "", cleaned_stmt, flags=re.DOTALL)
                cleaned_stmt = cleaned_stmt.strip()

                if not cleaned_stmt:
                    continue

                # DML ì¿¼ë¦¬ë§Œ EXPLAIN ì‹¤í–‰ (SELECT, UPDATE, DELETE, INSERT, REPLACE)
                if not re.match(r"^\s*(SELECT|UPDATE|DELETE|INSERT|REPLACE)", cleaned_stmt, re.IGNORECASE):
                    logger.info(f"ì¿¼ë¦¬ {i+1}: DML ì¿¼ë¦¬ê°€ ì•„ë‹ˆë¯€ë¡œ EXPLAIN ìŠ¤í‚µ")
                    continue

                logger.info(f"ì¿¼ë¦¬ {i+1}: Lambda EXPLAIN ì‹¤í–‰ ì¤‘...")

                # Lambda EXPLAIN í˜¸ì¶œ
                explain_result = await self.explain_query_lambda(
                    database_secret,
                    self.selected_database,
                    cleaned_stmt
                )

                # ê²°ê³¼ ì²˜ë¦¬
                if explain_result.get('success'):
                    success_count += 1
                    logger.info(f"ì¿¼ë¦¬ {i+1}: ì„±ê³µ")

                    # ì„±ëŠ¥ ì´ìŠˆ í™•ì¸
                    if explain_result.get('performance_issues'):
                        for perf_issue in explain_result['performance_issues']:
                            issue_desc = perf_issue.get('description', str(perf_issue))
                            performance_issues.append(f"ì¿¼ë¦¬ {i+1}: {issue_desc}")
                else:
                    error_count += 1
                    error_msg = explain_result.get('error', 'Lambda EXPLAIN ì‹¤íŒ¨')
                    all_issues.append(f"ì¿¼ë¦¬ {i+1}: {error_msg}")
                    logger.error(f"ì¿¼ë¦¬ {i+1}: ì˜¤ë¥˜ - {error_msg}")

            # ê²°ê³¼ ìš”ì•½
            result_summary = f"âœ… ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ ì™„ë£Œ (Lambda ê¸°ë°˜)\n"
            result_summary += f"ì´ ì¿¼ë¦¬: {len(statements)}ê°œ\n"
            result_summary += f"ì„±ê³µ: {success_count}ê°œ\n"
            result_summary += f"ì˜¤ë¥˜: {error_count}ê°œ\n"

            if performance_issues:
                result_summary += f"\nâš ï¸ ì„±ëŠ¥ ì´ìŠˆ ({len(performance_issues)}ê°œ):\n"
                for issue in performance_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    result_summary += f"  - {issue}\n"
                if len(performance_issues) > 5:
                    result_summary += f"  ... ì™¸ {len(performance_issues) - 5}ê°œ\n"

            if all_issues:
                result_summary += f"\nâŒ ì˜¤ë¥˜ ëª©ë¡:\n"
                for issue in all_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    result_summary += f"  - {issue}\n"
                if len(all_issues) > 5:
                    result_summary += f"  ... ì™¸ {len(all_issues) - 5}ê°œ\n"

            return result_summary

        except Exception as e:
            logger.error(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"




    def extract_table_name_from_alter(self, ddl_content: str) -> str:
        """ALTER TABLE êµ¬ë¬¸ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # ALTER TABLE íŒ¨í„´
        alter_pattern = r"ALTER\s+TABLE\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        match = re.search(alter_pattern, sql_clean, re.IGNORECASE)

        if match:
            return match.group(1)
        return None

    def extract_created_tables(self, sql_content: str) -> List[str]:
        """í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE TABLE íŒ¨í„´ - ë” ì •í™•í•œ ë§¤ì¹­
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)

        # ìœ íš¨í•œ í…Œì´ë¸”ëª…ë§Œ í•„í„°ë§ (SQL í‚¤ì›Œë“œ ì œì™¸)
        sql_keywords = {
            "and",
            "or",
            "not",
            "in",
            "on",
            "as",
            "is",
            "if",
            "by",
            "to",
            "from",
            "where",
            "select",
            "insert",
            "update",
            "delete",
        }
        for table in create_matches:
            if table.lower() not in sql_keywords and len(table) > 1:
                tables.add(table)

        return list(tables)

    def extract_created_indexes(self, sql_content: str) -> List[str]:
        """í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” ì¸ë±ìŠ¤ëª… ì¶”ì¶œ"""
        indexes = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE INDEX íŒ¨í„´
        index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+ON"
        )
        index_matches = re.findall(index_pattern, sql_clean, re.IGNORECASE)
        indexes.update(index_matches)

        return list(indexes)

    def extract_cte_tables(self, sql_content: str) -> List[str]:
        """WITHì ˆì˜ CTE(Common Table Expression) í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        cte_tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITH RECURSIVE íŒ¨í„´ (ê°€ì¥ ì¼ë°˜ì )
        recursive_with_pattern = (
            r"WITH\s+(?:RECURSIVE\s+)?([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        )
        recursive_matches = re.findall(recursive_with_pattern, sql_clean, re.IGNORECASE)
        cte_tables.update(recursive_matches)

        # ì¶”ê°€ CTE í…Œì´ë¸”ë“¤ (ì‰¼í‘œ í›„)
        additional_cte_pattern = r",\s*([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        additional_matches = re.findall(
            additional_cte_pattern, sql_clean, re.IGNORECASE
        )
        cte_tables.update(additional_matches)

        return list(cte_tables)

    def extract_foreign_keys(self, ddl_content: str) -> List[Dict[str, str]]:
        """DDLì—ì„œ ì™¸ë˜í‚¤ ì •ë³´ ì¶”ì¶œ"""
        foreign_keys = []

        # ì£¼ì„ ì œê±°
        ddl_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        ddl_clean = re.sub(r"/\*.*?\*/", "", ddl_clean, flags=re.DOTALL)

        # FOREIGN KEY íŒ¨í„´ ë§¤ì¹­
        fk_pattern = r"FOREIGN\s+KEY\s*\(\s*([^)]+)\s*\)\s*REFERENCES\s+([^\s(]+)\s*\(\s*([^)]+)\s*\)"
        matches = re.finditer(fk_pattern, ddl_clean, re.IGNORECASE)

        for match in matches:
            column = match.group(1).strip().strip("`")
            ref_table = match.group(2).strip().strip("`")
            ref_column = match.group(3).strip().strip("`")

            foreign_keys.append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return foreign_keys

    def extract_table_names(self, sql_content: str) -> List[str]:
        """SQLì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ (WITHì ˆ CTE í…Œì´ë¸” ì œì™¸)"""
        tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITHì ˆì˜ CTE í…Œì´ë¸”ë“¤ ì¶”ì¶œ
        cte_tables = set(self.extract_cte_tables(sql_content))

        # MySQL í‚¤ì›Œë“œë“¤ (í…Œì´ë¸”ëª…ì´ ì•„ë‹Œ ê²ƒë“¤)
        mysql_keywords = {
            "CURRENT_TIMESTAMP",
            "NOW",
            "NULL",
            "TRUE",
            "FALSE",
            "DEFAULT",
            "AUTO_INCREMENT",
            "PRIMARY",
            "KEY",
            "UNIQUE",
            "INDEX",
            "FOREIGN",
            "REFERENCES",
            "ON",
            "DELETE",
            "UPDATE",
            "CASCADE",
            "SET",
            "RESTRICT",
            "NO",
            "ACTION",
            "CHECK",
            "CONSTRAINT",
            "ENUM",
            "VARCHAR",
            "INT",
            "DECIMAL",
            "DATETIME",
            "TIMESTAMP",
            "TEXT",
            "BOOLEAN",
            "TINYINT",
            "SMALLINT",
            "MEDIUMINT",
            "BIGINT",
            "FLOAT",
            "DOUBLE",
            "CHAR",
            "BINARY",
            "VARBINARY",
            "BLOB",
            "TINYBLOB",
            "MEDIUMBLOB",
            "LONGBLOB",
            "TINYTEXT",
            "MEDIUMTEXT",
            "LONGTEXT",
            "DATE",
            "TIME",
            "YEAR",
        }

        # CREATE TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)
        for schema, table in create_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # ALTER TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        alter_pattern = r"ALTER\s+TABLE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        alter_matches = re.findall(alter_pattern, sql_clean, re.IGNORECASE)
        for schema, table in alter_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # DROP TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        drop_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?"
        drop_matches = re.findall(drop_pattern, sql_clean, re.IGNORECASE)
        for schema, table in drop_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # FROM íŒ¨í„´ (SELECT, DELETE) - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        from_pattern = r"\bFROM\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|WHERE|ORDER|GROUP|LIMIT|JOIN|INNER|LEFT|RIGHT|FULL|CROSS)"
        from_matches = re.findall(from_pattern, sql_clean, re.IGNORECASE)
        for schema, table in from_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # JOIN íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        join_pattern = r"\b(?:INNER\s+|LEFT\s+|RIGHT\s+|FULL\s+|CROSS\s+)?JOIN\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|ON)"
        join_matches = re.findall(join_pattern, sql_clean, re.IGNORECASE)
        for schema, table in join_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # UPDATE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        update_pattern = r"\bUPDATE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|SET)"
        update_matches = re.findall(update_pattern, sql_clean, re.IGNORECASE)
        for schema, table in update_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # INSERT INTO íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        insert_pattern = r"\bINSERT\s+INTO\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|\()"
        insert_matches = re.findall(insert_pattern, sql_clean, re.IGNORECASE)
        for schema, table in insert_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        return list(tables)

    async def execute_explain(self, sql_content: str, connection, debug_log):
        """EXPLAIN ì‹¤í–‰ ë° ë¶„ì„"""
        result = {"issues": [], "explain_data": None}

        try:
            if connection is None:
                debug_log("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
                return result

            cursor = connection.cursor(dictionary=True)
            if cursor is None:
                debug_log("ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                return result

            # ì£¼ì„ ì œê±°í•˜ê³  ì‹¤ì œ SQLë§Œ ì¶”ì¶œ
            sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
            sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)
            sql_clean = sql_clean.strip()

            if sql_clean.endswith(";"):
                sql_clean = sql_clean[:-1]

            explain_sql = f"EXPLAIN {sql_clean}"
            debug_log(f"EXPLAIN ì‹¤í–‰: {explain_sql}")

            cursor.execute(explain_sql)
            explain_result = cursor.fetchall()
            result["explain_data"] = explain_result

            debug_log(f"EXPLAIN ê²°ê³¼: {explain_result}")

            # EXPLAIN ê²°ê³¼ëŠ” ë¬¸ìì—´ë¡œë§Œ ì €ì¥
            cursor.close()

        except Exception as e:
            debug_log(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            result["issues"].append(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")

        return result

    def get_db_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = False,  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
        db_instance_identifier: str = None,
    ):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        if mysql is None:
            raise Exception(
                "mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )

        # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        session = boto3.session.Session()
        client = session.client(
            service_name="secretsmanager",
            region_name="ap-northeast-2",
            verify=False,
        )
        get_secret_value_response = client.get_secret_value(SecretId=database_secret)
        secret = get_secret_value_response["SecretString"]
        db_config = json.loads(secret)

        connection_config = None
        tunnel_used = False

        # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        database_name = selected_database or db_config.get(
            "dbname", db_config.get("database")
        )
        # database_nameì´ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¬¸ìì—´ë¡œ ë³€í™˜
        if database_name is not None:
            database_name = str(database_name)

        # db_instance_identifierê°€ ì œê³µë˜ë©´ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
        host = db_config.get("host")
        if db_instance_identifier:
            # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€ê²½
            if ".cluster-" in host:
                # aurora-cluster.cluster-xxx.region.rds.amazonaws.com -> instance-id.xxx.region.rds.amazonaws.com
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    # cluster- ë¶€ë¶„ì„ ì œê±°í•˜ê³  ì¸ìŠ¤í„´ìŠ¤ IDë¡œ êµì²´
                    host_parts[1] = host_parts[1].replace("cluster-", "")
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"
            else:
                # ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš° ì¸ìŠ¤í„´ìŠ¤ IDë¡œ êµì²´
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"

        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(host):
                connection_config = {
                    "host": "localhost",
                    "port": 3307,
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "database": database_name,
                    "connection_timeout": 10,
                }
                tunnel_used = True

        if not connection_config:
            connection_config = {
                "host": host,
                "port": db_config.get("port", 3306),
                "user": db_config.get("username"),
                "password": db_config.get("password"),
                "database": database_name,
                "connection_timeout": 10,
            }

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def setup_shared_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = False,  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
        db_instance_identifier: str = None,
    ):
        """ê³µìš© DB ì—°ê²° ì„¤ì • (í•œ ë²ˆë§Œ í˜¸ì¶œ)"""
        try:
            if self.shared_connection and self.shared_connection.is_connected():
                logger.info("ì´ë¯¸ í™œì„±í™”ëœ ê³µìš© ì—°ê²°ì´ ìˆìŠµë‹ˆë‹¤.")
                return True

            self.shared_connection, self.tunnel_used = self.get_db_connection(
                database_secret,
                selected_database,
                use_ssh_tunnel,
                db_instance_identifier,
            )

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_cursor = self.shared_connection.cursor()
                # ì—°ê²°ëœ í˜¸ìŠ¤íŠ¸ ì •ë³´ ë¡œê¹…
                host_info = (
                    f"ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}"
                    if db_instance_identifier
                    else "í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸"
                )
                logger.info(
                    f"ê³µìš© DB ì—°ê²° ì„¤ì • ì™„ë£Œ - {host_info} (í„°ë„: {self.tunnel_used})"
                )
                return True
            else:
                logger.error("ê³µìš© DB ì—°ê²° ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"ê³µìš© DB ì—°ê²° ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def cleanup_shared_connection(self):
        """ê³µìš© DB ì—°ê²° ì •ë¦¬"""
        try:
            if self.shared_cursor:
                self.shared_cursor.close()
                self.shared_cursor = None
                logger.info("ê³µìš© ì»¤ì„œ ë‹«ê¸° ì™„ë£Œ")

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_connection.close()
                self.shared_connection = None
                logger.info("ê³µìš© DB ì—°ê²° ë‹«ê¸° ì™„ë£Œ")

            if self.tunnel_used:
                self.cleanup_ssh_tunnel()
                self.tunnel_used = False
                logger.info("SSH í„°ë„ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ê³µìš© ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def get_shared_cursor(self):
        """ê³µìš© ì»¤ì„œ ë°˜í™˜"""
        if self.shared_cursor is None:
            logger.error(
                "ê³µìš© ì»¤ì„œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup_shared_connection()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
            return None
        return self.shared_cursor

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            secrets = await self.get_secrets_by_keyword(keyword)
            if not secrets:
                return (
                    f"'{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                    if keyword
                    else "ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                )

            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:\n{secret_list}"
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = False  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- ì„œë²„ ë²„ì „: {db_info}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}
- ì—°ê²° ë°©ì‹: {'SSH Tunnel' if tunnel_used else 'Direct'}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**"""
                for db in databases:
                    if db not in [
                        "information_schema",
                        "performance_schema",
                        "mysql",
                        "sys",
                    ]:
                        result += f"\n   - {db}"

                if tables:
                    result += f"\n\n**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**"
                    for table in tables:
                        result += f"\n   - {table}"

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ MySQL ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"

    async def list_databases(
        self, database_secret: str, use_ssh_tunnel: bool = False  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            if mysql is None:
                raise Exception("mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)

            connection_config = None
            tunnel_used = False

            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get("host")):
                    connection_config = {
                        "host": "localhost",
                        "port": 3307,
                        "user": db_config.get("username"),
                        "password": db_config.get("password"),
                        "connection_timeout": 10,
                    }
                    tunnel_used = True

            if not connection_config:
                connection_config = {
                    "host": db_config.get("host"),
                    "port": db_config.get("port", 3306),
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "connection_timeout": 10,
                }

            # ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´ ì—°ê²°
            connection = mysql.connector.connect(**connection_config)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = [
                db[0]
                for db in cursor.fetchall()
                if db[0]
                not in ["information_schema", "performance_schema", "mysql", "sys"]
            ]

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            result = "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:\n\n"
            for i, db in enumerate(databases, 1):
                result += f"{i}. {db}\n"
            result += f"\nì´ {len(databases)}ê°œì˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤."
            result += "\n\nğŸ’¡ íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•˜ë ¤ë©´ ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_database(
        self, database_secret: str, database_selection: str, use_ssh_tunnel: bool = False  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ (USE ëª…ë ¹ì–´ ì‹¤í–‰)"""
        try:
            # ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìœ íš¨ì„± ê²€ì¦
            db_list_result = await self.list_databases(database_secret, use_ssh_tunnel)

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì—ì„œ ì‹¤ì œ DB ì´ë¦„ë“¤ ì¶”ì¶œ
            lines = db_list_result.split("\n")
            databases = []
            for line in lines:
                if line.strip() and line[0].isdigit() and ". " in line:
                    db_name = line.split(". ", 1)[1]
                    databases.append(db_name)

            selected_db = None

            # ë²ˆí˜¸ë¡œ ì„ íƒí•œ ê²½ìš°
            if database_selection.isdigit():
                index = int(database_selection) - 1
                if 0 <= index < len(databases):
                    selected_db = databases[index]
                else:
                    return f"âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(databases)} ë²”ìœ„ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.\n\n{db_list_result}"
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒí•œ ê²½ìš°
                if database_selection in databases:
                    selected_db = database_selection
                else:
                    return f"âŒ '{database_selection}' ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n{db_list_result}"

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ë¡œ USE ëª…ë ¹ì–´ ì‹¤í–‰
            connection, tunnel_used = self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )

            if connection.is_connected():
                cursor = connection.cursor()

                # USE ëª…ë ¹ì–´ ì‹¤í–‰
                cursor.execute(f"USE `{selected_db}`")

                # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                cursor.close()
                connection.close()

                # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                self.selected_database = selected_db

                result = f"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{selected_db}' ì„ íƒ ì™„ë£Œ!\n\n"
                result += f"ğŸ”— í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: {current_db}\n"
                result += f"ğŸ’¡ ì´ì œ ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ìŠ¤í‚¤ë§ˆ ë¶„ì„ì´ë‚˜ SQL ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì˜¤ë¥˜: {e}")
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°í•´ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {current_db})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(tables_info)}ê°œ):"""

            for table_info in tables_info:
                table_name = table_info[0]
                table_type = table_info[1]
                engine = table_info[2]
                rows = table_info[3] or 0
                comment = table_info[6] or ""

                # ì»¬ëŸ¼ ìˆ˜ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                column_count = cursor.fetchone()[0]

                # ì¸ë±ìŠ¤ ìˆ˜ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT COUNT(DISTINCT index_name) FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                index_count = cursor.fetchone()[0]

                summary += f"""
  ğŸ”¹ **{table_name}** ({engine})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {rows:,}"""

                if comment:
                    summary += f"\n     - ì„¤ëª…: {comment}"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return summary

        except Exception as e:
            return f"âŒ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def get_table_schema(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            cursor.execute(
                """
                SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT, 
                       COLUMN_COMMENT, COLUMN_KEY, EXTRA
                FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY ORDINAL_POSITION
            """,
                (table_name,),
            )

            columns = cursor.fetchall()

            result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ìŠ¤í‚¤ë§ˆ ì •ë³´**\n\n"
            result += f"ğŸ“Š **ì»¬ëŸ¼ ëª©ë¡** ({len(columns)}ê°œ):\n"

            for col in columns:
                col_name, data_type, is_nullable, default_val, comment, key, extra = col

                result += f"\nğŸ”¹ **{col_name}**\n"
                result += f"   - íƒ€ì…: {data_type}\n"
                result += (
                    f"   - NULL í—ˆìš©: {'ì˜ˆ' if is_nullable == 'YES' else 'ì•„ë‹ˆì˜¤'}\n"
                )

                if default_val is not None:
                    result += f"   - ê¸°ë³¸ê°’: {default_val}\n"

                if key:
                    key_type = {"PRI": "ê¸°ë³¸í‚¤", "UNI": "ê³ ìœ í‚¤", "MUL": "ì¸ë±ìŠ¤"}.get(
                        key, key
                    )
                    result += f"   - í‚¤ íƒ€ì…: {key_type}\n"

                if extra:
                    result += f"   - ì¶”ê°€ ì†ì„±: {extra}\n"

                if comment:
                    result += f"   - ì„¤ëª…: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def text_to_sql(
        self, database_secret: str, natural_language_query: str
    ) -> str:
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜í•˜ê³  ì‹¤í–‰"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ìˆ˜ì§‘
            cursor.execute("SHOW TABLES")
            tables = [table[0] for table in cursor.fetchall()]

            schema_info = {}
            for table in tables:
                cursor.execute(
                    f"""
                    SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_KEY, EXTRA, COLUMN_COMMENT
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = '{table}'
                    ORDER BY ORDINAL_POSITION
                """
                )
                columns = cursor.fetchall()
                schema_info[table] = columns

            # Claudeì—ê²Œ SQL ìƒì„± ìš”ì²­
            sql_query = await self.generate_sql_with_claude(
                natural_language_query, schema_info
            )

            if not sql_query or sql_query.startswith("âŒ"):
                return sql_query

            # ìƒì„±ëœ SQL ì‹¤í–‰
            cursor.execute(sql_query)

            if sql_query.strip().upper().startswith("SELECT"):
                results = cursor.fetchall()
                column_names = [desc[0] for desc in cursor.description]

                result = f"âœ… SQL ìƒì„± ë° ì‹¤í–‰ ì™„ë£Œ!\n\n"
                result += f"ğŸ” **ìƒì„±ëœ SQL:**\n```sql\n{sql_query}\n```\n\n"
                result += f"ğŸ“Š **ì‹¤í–‰ ê²°ê³¼:** ({len(results)}í–‰)\n"

                if results:
                    # í…Œì´ë¸” í˜•íƒœë¡œ ê²°ê³¼ í‘œì‹œ (ìµœëŒ€ 10í–‰)
                    result += f"| {' | '.join(column_names)} |\n"
                    result += f"|{'|'.join(['---'] * len(column_names))}|\n"

                    for i, row in enumerate(results[:10]):
                        result += f"| {' | '.join(str(val) if val is not None else 'NULL' for val in row)} |\n"

                    if len(results) > 10:
                        result += f"\n... ë° {len(results) - 10}í–‰ ë”"
                else:
                    result += "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                connection.commit()
                result = f"âœ… SQL ìƒì„± ë° ì‹¤í–‰ ì™„ë£Œ!\n\n"
                result += f"ğŸ” **ìƒì„±ëœ SQL:**\n```sql\n{sql_query}\n```\n\n"
                result += f"ğŸ“Š **ì‹¤í–‰ ê²°ê³¼:** ì¿¼ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except ClientError as e:
            error_code = e.response["Error"]["Code"]
            if error_code == "ExpiredTokenException":
                return f"âŒ AWS í† í°ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 'aws configure'ë¡œ ìê²© ì¦ëª…ì„ ë‹¤ì‹œ ì„¤ì •í•´ì£¼ì„¸ìš”."
            elif error_code == "UnauthorizedOperation":
                return f"âŒ AWS ê¶Œí•œì´ ë¶€ì¡±í•©ë‹ˆë‹¤. Secrets Manager ë° Bedrock ì ‘ê·¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            else:
                return f"âŒ AWS ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(e)}"
        except MySQLError as e:
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            if "tunnel_used" in locals() and tunnel_used:
                self.cleanup_ssh_tunnel()
            return f"âŒ Text-to-SQL ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"

    async def generate_sql_with_claude(
        self, natural_query: str, schema_info: dict
    ) -> str:
        """Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜"""
        try:
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            schema_text = "ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´:\n\n"
            for table_name, columns in schema_info.items():
                schema_text += f"í…Œì´ë¸”: {table_name}\n"
                for col in columns:
                    col_name, data_type, is_nullable, key, extra, comment = col
                    key_info = f" [{key}]" if key else ""
                    extra_info = f" {extra}" if extra else ""
                    comment_info = f" -- {comment}" if comment else ""
                    schema_text += f"  - {col_name}: {data_type}{key_info}{extra_info}{comment_info}\n"
                schema_text += "\n"

            prompt = f"""ë‹¹ì‹ ì€ Aurora MySQL 8.0 ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ ì§ˆë¬¸ì„ ì •í™•í•œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

{schema_text}

ìì—°ì–´ ì§ˆë¬¸: {natural_query}

ìš”êµ¬ì‚¬í•­:
1. Aurora MySQL 8.0 ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”
2. ì£¼ì–´ì§„ ìŠ¤í‚¤ë§ˆì˜ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
3. ì ì ˆí•œ JOIN, WHERE, ORDER BY ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”
4. SQL ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
5. ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ëë‚´ì„¸ìš”

SQL ì¿¼ë¦¬:"""

            # Claude í˜¸ì¶œ
            bedrock_client = boto3.client("bedrock-runtime", region_name="us-west-2")

            body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "messages": [{"role": "user", "content": prompt}],
            }

            response = bedrock_client.invoke_model(
                modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
                body=json.dumps(body),
            )

            response_body = json.loads(response["body"].read())
            sql_query = response_body["content"][0]["text"].strip()

            # SQL ì¿¼ë¦¬ ì •ë¦¬ (ì½”ë“œ ë¸”ë¡ ì œê±° ë“±)
            if sql_query.startswith("```sql"):
                sql_query = sql_query[6:]
            if sql_query.endswith("```"):
                sql_query = sql_query[:-3]
            sql_query = sql_query.strip()

            return sql_query

        except Exception as e:
            return f"âŒ SQL ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def get_table_index(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            cursor.execute(
                """
                SELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, NON_UNIQUE, 
                       INDEX_TYPE, CARDINALITY, NULLABLE, INDEX_COMMENT
                FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY INDEX_NAME, SEQ_IN_INDEX
            """,
                (table_name,),
            )

            indexes = cursor.fetchall()

            if not indexes:
                result = (
                    f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\nâŒ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
                )
            else:
                result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\n"

                # ì¸ë±ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                index_groups = {}
                for idx in indexes:
                    idx_name = idx[0]
                    if idx_name not in index_groups:
                        index_groups[idx_name] = []
                    index_groups[idx_name].append(idx)

                result += f"ğŸ“Š **ì¸ë±ìŠ¤ ëª©ë¡** ({len(index_groups)}ê°œ):\n"

                for idx_name, idx_cols in index_groups.items():
                    first_col = idx_cols[0]
                    is_unique = "ê³ ìœ " if first_col[3] == 0 else "ì¼ë°˜"
                    idx_type = first_col[4]
                    comment = first_col[7] or ""

                    result += f"\nğŸ”¹ **{idx_name}** ({is_unique} ì¸ë±ìŠ¤)\n"
                    result += f"   - íƒ€ì…: {idx_type}\n"

                    # ì»¬ëŸ¼ ëª©ë¡
                    columns = [f"{col[1]}" for col in idx_cols]
                    result += f"   - ì»¬ëŸ¼: {', '.join(columns)}\n"

                    if comment:
                        result += f"   - ì„¤ëª…: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” ì¸ë±ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    # === DDL ê²€ì¦ ê´€ë ¨ ë©”ì„œë“œ ===

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            result = await self.validate_ddl(ddl_content, database_secret, filename)
            return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def validate_sql_files(
        self, filenames: List[str], database_secret: Optional[str] = None
    ) -> str:
        """ë³µìˆ˜ SQL íŒŒì¼ ê²€ì¦ ë° í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            if not filenames:
                return "ê²€ì¦í•  SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # ê°œë³„ íŒŒì¼ ê²€ì¦
            results = []
            for filename in filenames:
                logger.info(f"ê²€ì¦ ì¤‘: {filename}")
                result = await self.validate_sql_file(filename, database_secret)
                results.append(f"âœ… {filename}: {result.split(chr(10))[0]}")  # ì²« ì¤„ë§Œ

            # 2ê°œ ì´ìƒì´ë©´ í†µí•© ë³´ê³ ì„œ ìƒì„±
            if len(filenames) >= 2:
                consolidated_report = await self.auto_generate_consolidated_report()
                results.append("\n" + consolidated_report)

            return "\n".join(results)
        except Exception as e:
            return f"SQL íŒŒì¼ë“¤ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def auto_generate_consolidated_report(self) -> str:
        """ìµœê·¼ ìƒì„±ëœ ê°œë³„ ë³´ê³ ì„œë“¤ì„ ìˆ˜ì§‘í•´ì„œ í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            import re
            from datetime import timedelta

            # ìµœê·¼ 5ë¶„ ë‚´ì— ìƒì„±ëœ validation_report ì°¾ê¸°
            now = datetime.now()
            recent_reports = []

            for html_file in OUTPUT_DIR.glob('validation_report_*.html'):
                mtime = datetime.fromtimestamp(html_file.stat().st_mtime)
                if now - mtime < timedelta(minutes=5):
                    recent_reports.append(html_file)

            if len(recent_reports) < 2:
                return "í†µí•© ë³´ê³ ì„œ ìƒì„± ì¡°ê±´ ë¯¸ë‹¬ (ìµœê·¼ ë³´ê³ ì„œ 2ê°œ ë¯¸ë§Œ)"

            recent_reports.sort(key=lambda x: x.stat().st_mtime)

            # í†µê³„ ê³„ì‚°
            passed_count = 0
            failed_count = 0
            report_rows = ''

            for i, report_file in enumerate(recent_reports, 1):
                with open(report_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # íŒŒì¼ëª… ì¶”ì¶œ
                filename = report_file.name.replace('validation_report_', '').replace('.html', '')
                filename_match = re.match(r'(.+?)_\d{8}_\d{6}$', filename)
                if filename_match:
                    sql_filename = filename_match.group(1)
                else:
                    sql_filename = filename

                # ìƒíƒœ í™•ì¸
                if 'FAIL' in content and '<div class="status-badge">' in content:
                    status = 'FAIL'
                    status_icon = 'âŒ'
                    status_color = '#dc3545'
                    failed_count += 1
                else:
                    status = 'PASS'
                    status_icon = 'âœ…'
                    status_color = '#28a745'
                    passed_count += 1

                # SQL íƒ€ì… ì¶”ì¶œ
                sql_type_match = re.search(r'<h4>ğŸ”§ SQL íƒ€ì…</h4>\s*<p>([^<]+)</p>', content)
                sql_type = sql_type_match.group(1) if sql_type_match else 'UNKNOWN'

                report_rows += f'''
    <tr>
        <td>{i}</td>
        <td><a href="{report_file.name}" target="_blank">{sql_filename}</a></td>
        <td>{sql_type}</td>
        <td style="color: {status_color}; font-weight: bold;">{status_icon} {status}</td>
    </tr>
    '''

            total_files = len(recent_reports)
            pass_rate = (passed_count / total_files * 100 if total_files > 0 else 0)

            # í†µí•© ë³´ê³ ì„œ HTML ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            consolidated_file = OUTPUT_DIR / f'consolidated_validation_report_{timestamp}.html'

            html_content = f'''<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SQL í†µí•© ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
            text-align: center;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 2em;
            font-weight: bold;
        }}
        .pass {{ color: #28a745; }}
        .fail {{ color: #dc3545; }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }}
        th {{
            background-color: #667eea;
            color: white;
        }}
        tr:hover {{
            background-color: #f8f9fa;
        }}
        a {{
            color: #667eea;
            text-decoration: none;
        }}
        a:hover {{
            text-decoration: underline;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š SQL í†µí•© ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>

        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“ ì´ íŒŒì¼ ìˆ˜</h4>
                    <p>{total_files}</p>
                </div>
                <div class="summary-item">
                    <h4 class="pass">âœ… í†µê³¼</h4>
                    <p class="pass">{passed_count}</p>
                </div>
                <div class="summary-item">
                    <h4 class="fail">âŒ ì‹¤íŒ¨</h4>
                    <p class="fail">{failed_count}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ“ˆ í†µê³¼ìœ¨</h4>
                    <p>{pass_rate:.1f}%</p>
                </div>
            </div>

            <h2>ğŸ“‹ ê²€ì¦ ê²°ê³¼ ìƒì„¸</h2>
            <table>
                <thead>
                    <tr>
                        <th>#</th>
                        <th>íŒŒì¼ëª…</th>
                        <th>SQL íƒ€ì…</th>
                        <th>ê²€ì¦ ê²°ê³¼</th>
                    </tr>
                </thead>
                <tbody>
                    {report_rows}
                </tbody>
            </table>

            <p style="color: #6c757d; font-size: 0.9em;">
                ğŸ’¡ íŒŒì¼ëª…ì„ í´ë¦­í•˜ë©´ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            </p>
        </div>

        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
'''

            # íŒŒì¼ ì €ì¥
            with open(consolidated_file, 'w', encoding='utf-8') as f:
                f.write(html_content)

            logger.info(f"í†µí•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {consolidated_file}")

            return f"ğŸ“Š í†µí•© ê²€ì¦ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {consolidated_file}\n   ì´ {total_files}ê°œ íŒŒì¼, í†µê³¼: {passed_count}, ì‹¤íŒ¨: {failed_count}, í†µê³¼ìœ¨: {pass_rate:.1f}%"

        except Exception as e:
            logger.error(f"í†µí•© ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"í†µí•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_ddl(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """DDL/DML ê²€ì¦ ì‹¤í–‰ (ì—°ê²° ì¬ì‚¬ìš© íŒ¨í„´ ì ìš©)"""
        try:
            # filenameì—ì„œ basenameë§Œ ì¶”ì¶œ (ì „ì²´ ê²½ë¡œê°€ ì˜¬ ìˆ˜ ìˆìŒ)
            import os
            base_filename = os.path.basename(filename)

            # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
            debug_log_path = (
                LOGS_DIR
                / f"debug_log_{base_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            )

            debug_log(f"validate_ddl ì‹œì‘ - íŒŒì¼: {base_filename}")
            debug_log(f"SQL ë‚´ìš©: {ddl_content.strip()}")

            issues = []
            db_connection_info = None
            schema_validation = None
            claude_analysis_result = None  # Claude ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
            constraint_validation = None
            explain_result = None

            # ë³€ìˆ˜ ì´ˆê¸°í™”
            dml_column_issues = []

            # 1. ê¸°ë³¸ ë¬¸ë²• ê²€ì¦ - ê°œì„ ëœ ì„¸ë¯¸ì½œë¡  ê²€ì¦
            semicolon_valid = self.validate_semicolon_usage(ddl_content)
            if not semicolon_valid:
                issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ ì‹¤íŒ¨")
            else:
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ í†µê³¼")

            # 2. SQL íƒ€ì… í™•ì¸
            sql_type = self.sql_parser.extract_ddl_type(ddl_content, debug_log)
            debug_log(f"SQL íƒ€ì…: {sql_type}")

            # 3. SQL íƒ€ì…ì— ë”°ë¥¸ ê²€ì¦ ë¶„ê¸°
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT", "MIXED_SELECT"]
            skip_types = ["SHOW", "SET", "USE"]  # ìŠ¤í‚µí•  SQL íƒ€ì…

            if database_secret:
                try:
                    debug_log("Lambda ê¸°ë°˜ ê²€ì¦ ì‹œì‘ (ë¡œì»¬ DB ì—°ê²° ì—†ìŒ)")

                    # SQL íƒ€ì…ë³„ ê²€ì¦ ë¶„ê¸°
                    if sql_type in skip_types:
                        debug_log(
                            f"SQL íƒ€ì… ìŠ¤í‚µ: {sql_type} (SHOW/SET/USE êµ¬ë¬¸ì€ ê²€ì¦í•˜ì§€ ì•ŠìŒ)"
                        )

                    # DDL ê²€ì¦
                    elif sql_type in ddl_types:
                        debug_log(f"DDL ê²€ì¦ ìˆ˜í–‰: {sql_type}")
                        debug_log("=== Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¡œì§ ì‹œì‘ ===")

                        # Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ìœ¼ë¡œ ë³€ê²½
                        ddl_validation = await self.validate_schema_lambda(
                            database_secret,
                            self.selected_database,
                            ddl_content
                        )
                        debug_log(
                            f"Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: Success={ddl_validation.get('success')}, Valid={ddl_validation.get('valid')}, Issues={len(ddl_validation.get('issues', []))}"
                        )

                        # Lambda ê²°ê³¼ ì²˜ë¦¬
                        if ddl_validation.get('success'):
                            if ddl_validation.get('issues'):
                                issues.extend(ddl_validation['issues'])
                            if ddl_validation.get('warnings'):
                                # ê²½ê³ ëŠ” issuesì— ì¶”ê°€í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ
                                debug_log(f"ê²½ê³ : {ddl_validation['warnings']}")
                        else:
                            # Lambda í˜¸ì¶œ ì‹¤íŒ¨
                            error_msg = ddl_validation.get('error', 'Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨')
                            issues.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {error_msg}")
                            debug_log(f"Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {error_msg}")

                    # DQL(DML) ê²€ì¦ - MIXED_SELECT í¬í•¨
                    elif sql_type in dql_types:
                        debug_log(f"DQL ê²€ì¦ ìˆ˜í–‰: {sql_type}")

                        # MIXED_SELECTì¸ ê²½ìš° DDLê³¼ DML ëª¨ë‘ ê²€ì¦
                        if sql_type == "MIXED_SELECT":
                            debug_log("=== í˜¼í•© SQL íŒŒì¼ ê²€ì¦ ì‹œì‘ ===")

                            # 1. DDL êµ¬ë¬¸ ê²€ì¦ (Lambda ì‚¬ìš©)
                            debug_log("í˜¼í•© íŒŒì¼ ë‚´ Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")
                            ddl_validation = await self.validate_schema_lambda(
                                database_secret,
                                self.selected_database,
                                ddl_content
                            )

                            # íƒ€ì… ì²´í¬: ddl_validationì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
                            if not isinstance(ddl_validation, dict):
                                logger.error(f"ddl_validationì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(ddl_validation)}, ë‚´ìš©: {str(ddl_validation)[:200]}")
                                issues.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: Lambda ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ (íƒ€ì…: {type(ddl_validation).__name__})")
                                ddl_validation = {'success': False, 'error': f'ì‘ë‹µ íƒ€ì… ì˜¤ë¥˜: {type(ddl_validation).__name__}'}

                            debug_log(
                                f"í˜¼í•© íŒŒì¼ Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: Success={ddl_validation.get('success')}, Valid={ddl_validation.get('valid')}, Issues={len(ddl_validation.get('issues', []))}"
                            )

                            # Lambda ê²°ê³¼ ì²˜ë¦¬
                            if ddl_validation.get('success'):
                                if ddl_validation.get('issues'):
                                    issues.extend(ddl_validation['issues'])
                                if ddl_validation.get('warnings'):
                                    debug_log(f"ê²½ê³ : {ddl_validation['warnings']}")
                            else:
                                error_msg = ddl_validation.get('error', 'Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨')
                                issues.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {error_msg}")
                                debug_log(f"Lambda ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {error_msg}")

                        # DML ê²€ì¦ (Lambda EXPLAIN ì‚¬ìš©)
                        debug_log("=== Lambda EXPLAIN ê²€ì¦ ì‹œì‘ ===")

                        # íŒŒì¼ ë‚´ ìƒì„±ëœ í…Œì´ë¸” ëª©ë¡ ì¶”ì¶œ (ì„ì‹œ ìŠ¤í‚¤ë§ˆ ì‹œë®¬ë ˆì´ì…˜)
                        created_tables = set()
                        skipped_queries = []  # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ (Claude ë¶„ì„ìš©)

                        try:
                            ddl_statements = self.sql_parser.parse_ddl_detailed(ddl_content)
                            for stmt in ddl_statements:
                                if stmt.get('type') == 'CREATE_TABLE':
                                    table_name = stmt.get('table', '').lower()
                                    if table_name:
                                        created_tables.add(table_name)
                            debug_log(f"íŒŒì¼ ë‚´ ìƒì„±ëœ í…Œì´ë¸” ëª©ë¡: {created_tables}")
                        except Exception as parse_error:
                            debug_log(f"DDL íŒŒì‹± ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {parse_error}")

                        # SQLì„ ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬
                        if sqlparse:
                            statements = sqlparse.split(ddl_content)
                        else:
                            statements = [
                                stmt.strip() for stmt in ddl_content.split(";") if stmt.strip()
                            ]
                        debug_log(f"ì´ {len(statements)}ê°œì˜ ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬")

                        # ê° SELECT ì¿¼ë¦¬ì— ëŒ€í•´ Lambda EXPLAIN ì‹¤í–‰
                        for i, stmt in enumerate(statements):
                            if not stmt.strip():
                                continue

                            # ì£¼ì„ ì œê±°
                            cleaned_stmt = re.sub(r"--.*$", "", stmt, flags=re.MULTILINE)
                            cleaned_stmt = re.sub(r"/\*.*?\*/", "", cleaned_stmt, flags=re.DOTALL)
                            cleaned_stmt = cleaned_stmt.strip()

                            if not cleaned_stmt:
                                continue

                            # DML ì¿¼ë¦¬ë§Œ EXPLAIN ì‹¤í–‰ (SELECT, UPDATE, DELETE, INSERT, REPLACE)
                            dml_pattern = re.match(r"^\s*(SELECT|UPDATE|DELETE|INSERT|REPLACE)", cleaned_stmt, re.IGNORECASE)
                            if not dml_pattern:
                                debug_log(f"ì¿¼ë¦¬ {i+1}: DML ì¿¼ë¦¬ê°€ ì•„ë‹ˆë¯€ë¡œ EXPLAIN ìŠ¤í‚µ")
                                continue

                            # íŒŒì¼ ë‚´ ìƒì„±ëœ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸
                            references_new_table = False
                            referenced_tables = []
                            for table in created_tables:
                                # FROM, JOIN, INTO ì ˆì—ì„œ í…Œì´ë¸” ì°¸ì¡° í™•ì¸
                                if re.search(rf'\b(FROM|JOIN|INTO)\s+`?{table}`?\b', cleaned_stmt, re.IGNORECASE):
                                    references_new_table = True
                                    referenced_tables.append(table)

                            # ìƒˆ í…Œì´ë¸” ì°¸ì¡° ì‹œ EXPLAIN ìŠ¤í‚µ
                            if references_new_table:
                                debug_log(f"ì¿¼ë¦¬ {i+1}: íŒŒì¼ ë‚´ ìƒì„±ëœ í…Œì´ë¸” ì°¸ì¡° ({', '.join(referenced_tables)}) - EXPLAIN ìŠ¤í‚µ")
                                # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ ê¸°ë¡ (Claude ë¶„ì„ìš©)
                                skipped_queries.append({
                                    'query_num': i+1,
                                    'query': cleaned_stmt[:100] + ('...' if len(cleaned_stmt) > 100 else ''),
                                    'tables': referenced_tables
                                })
                                continue

                            debug_log(f"ì¿¼ë¦¬ {i+1}: Lambda EXPLAIN ì‹¤í–‰ ì¤‘...")
                            explain_result = await self.explain_query_lambda(
                                database_secret,
                                self.selected_database,
                                cleaned_stmt
                            )

                            # íƒ€ì… ì²´í¬: explain_resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
                            if not isinstance(explain_result, dict):
                                logger.error(f"ì¿¼ë¦¬ {i+1} explain_resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(explain_result)}, ë‚´ìš©: {str(explain_result)[:200]}")
                                issues.append(f"ì¿¼ë¦¬ {i+1} EXPLAIN ì˜¤ë¥˜: Lambda ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ (íƒ€ì…: {type(explain_result).__name__})")
                                continue

                            # Lambda ê²°ê³¼ ì²˜ë¦¬
                            if explain_result.get('success'):
                                debug_log(f"ì¿¼ë¦¬ {i+1}: Lambda EXPLAIN ì„±ê³µ - {len(explain_result.get('performance_issues', []))}ê°œ ì„±ëŠ¥ ì´ìŠˆ")

                                # ì„±ëŠ¥ ì´ìŠˆê°€ ìˆìœ¼ë©´ issuesì— ì¶”ê°€
                                if explain_result.get('performance_issues'):
                                    for perf_issue in explain_result['performance_issues']:
                                        if "âŒ" in perf_issue or "ì‹¤íŒ¨" in perf_issue:
                                            issues.append(f"ì¿¼ë¦¬ {i+1}: {perf_issue}")
                                            debug_log(f"ì„±ëŠ¥ ì´ìŠˆ ì¶”ê°€: {perf_issue}")
                            else:
                                error_msg = explain_result.get('error', 'Lambda EXPLAIN ì‹¤íŒ¨')
                                debug_log(f"ì¿¼ë¦¬ {i+1}: Lambda EXPLAIN ì˜¤ë¥˜ - {error_msg}")
                                issues.append(f"ì¿¼ë¦¬ {i+1} EXPLAIN ì˜¤ë¥˜: {error_msg}")

                        debug_log("=== Lambda EXPLAIN ê²€ì¦ ì™„ë£Œ ===")

                    else:
                        debug_log(f"ì•Œ ìˆ˜ ì—†ëŠ” SQL íƒ€ì…: {sql_type}")

                except Exception as e:
                    debug_log(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì˜¤ë¥˜: {e}")
                    issues.append(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")

            # Lambda ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Claude ë¶„ì„
            dml_column_issues = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)

            # 4. Claudeë¥¼ í†µí•œ ê²€ì¦
            try:
                debug_log("Claude ê²€ì¦ ì‹œì‘ (Lambda ê²°ê³¼ ê¸°ë°˜)")

                # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±
                schema_validation_summary = self.create_schema_validation_summary(
                    issues, dml_column_issues
                )
                debug_log(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ìš”ì•½ ìƒì„±: {schema_validation_summary}")

                # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ í™•ì¸ (DML ê²€ì¦ì—ì„œ ì •ì˜ë˜ì—ˆì„ ê²½ìš°)
                skipped_info = locals().get('skipped_queries', [])
                debug_log(f"ìŠ¤í‚µëœ ì¿¼ë¦¬ ê°œìˆ˜: {len(skipped_info)}")

                # Claude ê²€ì¦ (ìŠ¤í‚¤ë§ˆ ì •ë³´ëŠ” Lambdaì—ì„œ ì´ë¯¸ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”)
                claude_result = await self.validate_with_claude(
                    ddl_content,
                    database_secret,
                    None,  # relevant_schema_info ì œê±°
                    None,  # explain_info_str ì œê±°
                    sql_type,
                    schema_validation_summary,
                    skipped_queries=skipped_info,  # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ ì „ë‹¬
                )
                debug_log(f"Claude ê²€ì¦ ê²°ê³¼: {claude_result}")

                # Claude ê²°ê³¼ë¥¼ í•­ìƒ ì €ì¥ (ì„±ê³µ/ì‹¤íŒ¨ ìƒê´€ì—†ì´)
                claude_analysis_result = claude_result

                # Claude ì‘ë‹µ ë¶„ì„ - ë” ì—„ê²©í•œ ê²€ì¦
                if "ì˜¤ë¥˜:" in claude_result or "ì¡´ì¬í•˜ì§€ ì•Š" in claude_result:
                    issues.append(f"Claude ê²€ì¦: {claude_result}")
                    debug_log("Claude ê²€ì¦ì—ì„œ ì˜¤ë¥˜ ë°œê²¬")
                elif "ê²€ì¦ í†µê³¼" in claude_result:
                    debug_log("Claude ê²€ì¦ í†µê³¼")
                else:
                    debug_log("Claude ê²€ì¦ ì™„ë£Œ")

            except Exception as e:
                logger.error(f"Claude ê²€ì¦ ì˜¤ë¥˜: {e}")
                issues.append(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                debug_log(f"Claude ê²€ì¦ ì˜ˆì™¸: {e}")
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ Claude ê²°ê³¼ ì„¤ì •
                claude_analysis_result = f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

            # DML ì»¬ëŸ¼ ì´ìŠˆë¥¼ ê¸°ì¡´ ì´ìŠˆ ëª©ë¡ì— ì¶”ê°€
            if dml_column_issues:
                for dml_issue in dml_column_issues:
                    issues.append(dml_issue["message"])
                debug_log(
                    f"DML ì»¬ëŸ¼ ì´ìŠˆ {len(dml_column_issues)}ê°œë¥¼ ìµœì¢… ì´ìŠˆ ëª©ë¡ì— ì¶”ê°€"
                )

            # ê²€ì¦ ì™„ë£Œ
            debug_log(f"ìµœì¢… ì´ìŠˆ ê°œìˆ˜: {len(issues)}")
            debug_log(f"ì´ìŠˆ ëª©ë¡: {issues}")

            # Claude ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ìƒíƒœ ê²°ì •
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("ê²€ì¦ í†µê³¼")
            )

            # ê²°ê³¼ ìƒì„± - Claude ê²€ì¦ì´ ì„±ê³µì´ë©´ ìš°ì„ ì ìœ¼ë¡œ PASS ì²˜ë¦¬
            if claude_success and not any(
                "ì˜¤ë¥˜:" in issue or "ì‹¤íŒ¨" in issue or "ì¡´ì¬í•˜ì§€ ì•Š" in issue
                for issue in issues
            ):
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
                debug_log("Claude ê²€ì¦ ì„±ê³µìœ¼ë¡œ ìµœì¢… ìƒíƒœë¥¼ PASSë¡œ ì„¤ì •")
            elif not issues:
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
            else:
                # ì„±ëŠ¥ ë¬¸ì œì™€ ê¸°íƒ€ ë¬¸ì œ ë¶„ë¥˜
                performance_issues = [
                    issue for issue in issues if "ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ" in str(issue)
                ]
                claude_issues = [
                    issue for issue in issues if "Claude ê²€ì¦:" in str(issue)
                ]
                other_issues = [
                    issue
                    for issue in issues
                    if issue not in performance_issues and issue not in claude_issues
                ]

                # ë¬¸ì œ ìš”ì•½ ìƒì„±
                problem_parts = []
                if performance_issues:
                    unique_performance = len(
                        set(str(issue) for issue in performance_issues)
                    )
                    if unique_performance == 1:
                        problem_parts.append("ì„±ëŠ¥ ë¬¸ì œ")
                    else:
                        problem_parts.append(f"ì„±ëŠ¥ ë¬¸ì œ {unique_performance}ê±´")

                if claude_issues:
                    problem_parts.append("AI ë¶„ì„ ë¬¸ì œ")

                if other_issues:
                    problem_parts.append(f"ê¸°íƒ€ ë¬¸ì œ {len(other_issues)}ê±´")

                if (
                    len(problem_parts) == 1
                    and "ì„±ëŠ¥ ë¬¸ì œ" in problem_parts[0]
                    and not other_issues
                    and not claude_issues
                ):
                    summary = "âŒ ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ ë°œê²¬"
                else:
                    summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {', '.join(problem_parts)}"

                status = "FAIL"

            debug_log(f"ìµœì¢… ìƒíƒœ: {status}, ìš”ì•½: {summary}")

            # ë³´ê³ ì„œ ìƒì„± (HTML í˜•ì‹)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # filenameì—ì„œ basenameë§Œ ì¶”ì¶œ (ì „ì²´ ê²½ë¡œê°€ ì˜¬ ìˆ˜ ìˆìŒ)
            import os
            base_filename = os.path.basename(filename)
            report_path = OUTPUT_DIR / f"validation_report_{base_filename}_{timestamp}.html"

            # HTML ë³´ê³ ì„œ ìƒì„±
            debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            debug_log(f"dml_column_issues ê°’: {dml_column_issues}")
            debug_log(f"report_path ê°’: {report_path}")

            # output ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ì„ ê²½ìš°)
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

            try:
                await self.generate_html_report(
                    report_path,
                    base_filename,  # ì „ì²´ ê²½ë¡œê°€ ì•„ë‹Œ basenameë§Œ ì „ë‹¬
                    ddl_content,
                    sql_type,
                    status,
                    summary,
                    issues,
                    db_connection_info,
                    schema_validation,
                    constraint_validation,
                    database_secret,
                    explain_result,
                    claude_analysis_result,  # Claude ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    dml_column_issues,  # DML ì»¬ëŸ¼ ì´ìŠˆ ì¶”ê°€
                )
                debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            except Exception as html_error:
                debug_log(f"HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {html_error}")
                import traceback

                debug_log(f"HTML ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

            return f"{summary}\n\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}\nğŸ” ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"

        except Exception as e:
            return f"SQL ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def validate_semicolon_usage(self, ddl_content: str) -> bool:
        """ê°œì„ ëœ ì„¸ë¯¸ì½œë¡  ê²€ì¦ - ë…ë¦½ì ì¸ ë¬¸ì¥ì€ ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©"""
        content = ddl_content.strip()

        # ë¹ˆ ë‚´ìš©ì€ í†µê³¼
        if not content:
            return True

        # ì£¼ì„ ì œê±°í•˜ê³  ì‹¤ì œ SQL êµ¬ë¬¸ë§Œ ì¶”ì¶œ
        lines = content.split("\n")
        sql_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith("--") and not line.startswith("/*"):
                sql_lines.append(line)

        if not sql_lines:
            return True

        # ì‹¤ì œ SQL êµ¬ë¬¸ ê²°í•©
        actual_sql = " ".join(sql_lines).strip()

        # ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° (ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
        statements = [stmt.strip() for stmt in actual_sql.split(";") if stmt.strip()]

        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë…ë¦½ì ì¸ ë‹¨ì¼ êµ¬ë¬¸ì¸ì§€ í™•ì¸
        if len(statements) == 1:
            # ë‹¨ì¼ êµ¬ë¬¸ì¸ ê²½ìš° ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©
            single_stmt = statements[0].upper().strip()

            # SET, USE, SHOW ë“± ë…ë¦½ì ì¸ êµ¬ë¬¸ë“¤ì€ ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©
            independent_keywords = [
                "SET SESSION",
                "SET GLOBAL",
                "SET @",
                "SET @@",
                "USE ",
                "SHOW ",
                "DESCRIBE ",
                "DESC ",
                "EXPLAIN ",
                "SELECT ",
                "INSERT ",
                "UPDATE ",
                "DELETE ",
                "CREATE TABLE",
                "CREATE INDEX",
                "ALTER TABLE",
                "DROP TABLE",
            ]

            for keyword in independent_keywords:
                if single_stmt.startswith(keyword):
                    return True

        # ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ì„ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ ì„¸ë¯¸ì½œë¡ ì´ ìˆì–´ì•¼ í•¨
        return content.endswith(";")


    def detect_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ê°ì§€"""
        ddl_upper = ddl_content.upper().strip()

        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("DROP TABLE"):
            return "DROP_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP INDEX"):
            return "DROP_INDEX"
        elif ddl_upper.startswith("INSERT"):
            return "INSERT"
        elif ddl_upper.startswith("UPDATE"):
            return "UPDATE"
        elif ddl_upper.startswith("DELETE"):
            return "DELETE"
        elif ddl_upper.startswith("SELECT"):
            return "SELECT"
        else:
            return "UNKNOWN"

    def create_schema_validation_summary(
        self, issues: list, dml_column_issues: list
    ) -> str:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ Claudeì—ê²Œ ì „ë‹¬í•  í˜•íƒœë¡œ ìƒì„±"""
        if not issues and not dml_column_issues:
            return "ìŠ¤í‚¤ë§ˆ ê²€ì¦: ëª¨ë“  ê²€ì¦ í†µê³¼"

        summary_parts = []
        if issues:
            summary_parts.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¬¸ì œì  ({len(issues)}ê°œ):")
            for i, issue in enumerate(issues, 1):  # ëª¨ë“  ë¬¸ì œ í‘œì‹œ
                summary_parts.append(f"  {i}. {issue}")

        if dml_column_issues:
            summary_parts.append(f"ì»¬ëŸ¼ ê²€ì¦ ë¬¸ì œì  ({len(dml_column_issues)}ê°œ):")
            for i, issue in enumerate(dml_column_issues, 1):  # ëª¨ë“  ë¬¸ì œ í‘œì‹œ
                summary_parts.append(f"  {i}. {issue}")

        return "\n".join(summary_parts)

    async def validate_with_claude(
        self,
        ddl_content: str,
        database_secret: str = None,
        schema_info: dict = None,
        explain_info: str = None,
        sql_type: str = None,
        schema_validation_summary: str = None,
        skipped_queries: list = None,
    ) -> str:
        """
        Claude cross-region í”„ë¡œíŒŒì¼ì„ í™œìš©í•œ DDL ê²€ì¦ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨)

        Args:
            skipped_queries: íŒŒì¼ ë‚´ ìƒì„±ëœ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ì—¬ EXPLAINì´ ìŠ¤í‚µëœ ì¿¼ë¦¬ ëª©ë¡
        """
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ê³  database_secretì´ ìˆìœ¼ë©´ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ
        if schema_info is None and database_secret:
            try:
                schema_info = await self.extract_current_schema_info(database_secret)
            except Exception as e:
                logger.warning(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                schema_info = {}

        # ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆœì„œ ê³ ë ¤)
        schema_context = ""
        if schema_info:
            schema_details = []

            # ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ì²˜ë¦¬
            # íƒ€ì… ì²´í¬: schema_infoì˜ ê°’ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            try:
                sorted_items = sorted(
                    schema_info.items(),
                    key=lambda x: x[1].get("order", 0) if isinstance(x[1], dict) else 0
                )
            except Exception as sort_error:
                logger.error(f"schema_info ì •ë ¬ ì˜¤ë¥˜: {sort_error}, schema_info íƒ€ì…: {type(schema_info)}")
                # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                sorted_items = list(schema_info.items()) if isinstance(schema_info, dict) else []

            for key, info in sorted_items:
                # íƒ€ì… ì²´í¬: infoê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
                if not isinstance(info, dict):
                    logger.warning(f"schema_info[{key}]ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(info)}")
                    continue

                order = info.get("order", 0)

                if info.get("type") == "table":
                    table_name = info.get("table_name", key)

                    if "columns" in info:
                        # ALTER TABLE ì¼€ì´ìŠ¤
                        columns_info = [
                            f"{col['name']}({col['data_type']})"
                            for col in info["columns"]
                        ]
                        schema_details.append(f"[{order}] ALTER TABLE '{table_name}':")
                        schema_details.append(f"  - DBì— ì¡´ì¬: {info['exists']}")
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - íŒŒì¼ ë‚´ ìƒì„±ë¨: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - ìœ íš¨ ì¡´ì¬: {info.get('effective_exists', info['exists'])}"
                        )

                        if info["exists"] and columns_info:
                            schema_details.append(
                                f"  - ê¸°ì¡´ ì»¬ëŸ¼: {', '.join(columns_info)}"
                            )

                        if info.get("alter_type") and info.get("target_column"):
                            schema_details.append(
                                f"  - ALTER ì‘ì—…: {info['alter_type']} {info['target_column']}"
                            )
                    else:
                        # CREATE/DROP TABLE ì¼€ì´ìŠ¤
                        action = (
                            "CREATE"
                            if "CREATE" in key or info.get("exists") == False
                            else "DROP"
                        )
                        schema_details.append(
                            f"[{order}] {action} TABLE '{table_name}': DBì¡´ì¬={info.get('exists', 'Unknown')}"
                        )

                elif info.get("type") == "index":
                    table_name = info.get("table_name", key.split(".")[0])
                    index_name = info.get("index_name", "Unknown")

                    if "duplicate_column_indexes" in info:
                        # CREATE INDEX ì¼€ì´ìŠ¤
                        schema_details.append(
                            f"[{order}] CREATE INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - í…Œì´ë¸” DBì¡´ì¬: {info['table_exists']}"
                        )
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - í…Œì´ë¸” íŒŒì¼ë‚´ìƒì„±: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - í…Œì´ë¸” ìœ íš¨ì¡´ì¬: {info.get('effective_exists', info['table_exists'])}"
                        )
                        schema_details.append(
                            f"  - ìƒì„±í•  ì»¬ëŸ¼: {', '.join(info['target_columns'])}"
                        )

                        # DBì˜ ì¤‘ë³µ ì¸ë±ìŠ¤
                        if info["duplicate_column_indexes"]:
                            schema_details.append("  - DBì˜ ë™ì¼ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤:")
                            for dup_idx in info["duplicate_column_indexes"]:
                                schema_details.append(
                                    f"    * {dup_idx['name']} ({dup_idx['columns']}) - {'UNIQUE' if dup_idx['unique'] else 'NON-UNIQUE'}"
                                )

                        # íŒŒì¼ ë‚´ ì¤‘ë³µ ì¸ë±ìŠ¤
                        if info.get("file_duplicate_indexes"):
                            schema_details.append("  - íŒŒì¼ ë‚´ ë™ì¼ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤:")
                            for dup_idx in info["file_duplicate_indexes"]:
                                schema_details.append(
                                    f"    * [{dup_idx['order']}] {dup_idx['name']} ({','.join(dup_idx['columns'])})"
                                )

                        if not info["duplicate_column_indexes"] and not info.get(
                            "file_duplicate_indexes"
                        ):
                            schema_details.append("  - ì¤‘ë³µ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤ ì—†ìŒ")
                    else:
                        # DROP INDEX ì¼€ì´ìŠ¤
                        schema_details.append(
                            f"[{order}] DROP INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - í…Œì´ë¸” ì¡´ì¬: {info['table_exists']}"
                        )
                        schema_details.append(
                            f"  - ì¸ë±ìŠ¤ ì¡´ì¬: {info['index_exists']}"
                        )

            if schema_details:
                schema_context = f"""
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì‹¤í–‰ ìˆœì„œë³„):
{chr(10).join(schema_details)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ DDLì˜ ì ì ˆì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. íŒŒì¼ ë‚´ì—ì„œ ë¨¼ì € ìƒì„±ëœ í…Œì´ë¸”ì€ ì´í›„ ALTER/INDEX ì‘ì—…ì—ì„œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
2. ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€
3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”/ì¸ë±ìŠ¤ì— ëŒ€í•œ DROP ì‹œë„
4. ì‹¤í–‰ ìˆœì„œìƒ ë…¼ë¦¬ì  ì˜¤ë¥˜
"""
            else:
                schema_context = """
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
        else:
            schema_context = """
ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        # EXPLAIN ì •ë³´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        explain_context = ""
        if explain_info:
            explain_context = f"""
EXPLAIN ë¶„ì„ ê²°ê³¼:
{explain_info}

ìœ„ EXPLAIN ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì„±ëŠ¥ìƒ ë¬¸ì œê°€ ìˆëŠ”ì§€ë„ í•¨ê»˜ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì°¸ê³ : DDL êµ¬ë¬¸(CREATE, ALTER, DROP ë“±)ì— ëŒ€í•´ì„œëŠ” EXPLAINì„ ì‹¤í–‰í•˜ì§€ ì•Šìœ¼ë©°, 
SELECT, UPDATE, DELETE, INSERT ë“±ì˜ DML êµ¬ë¬¸ì— ëŒ€í•´ì„œë§Œ EXPLAIN ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        schema_validation_context = ""
        schema_has_errors = False
        if schema_validation_summary:
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            schema_has_errors = (
                "ì˜¤ë¥˜" in schema_validation_summary
                or "ì‹¤íŒ¨" in schema_validation_summary
                or "ì¡´ì¬í•˜ì§€ ì•Š" in schema_validation_summary
                or "ì´ë¯¸ ì¡´ì¬" in schema_validation_summary
            )

            schema_validation_context = f"""
ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼:
{schema_validation_summary}

ìœ„ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì¢…í•©ì ì¸ íŒë‹¨ì„ í•´ì£¼ì„¸ìš”.
ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ëœ ê²½ìš°, ì‹¤íŒ¨ë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ê³  ì™œ ë¬¸ì œê°€ ë‚˜ì™”ëŠ”ì§€ë„ ì„¤ëª…í•˜ë©´ì„œ ê²€ì¦í•´ì£¼ì„¸ìš”.
"""

        # ìŠ¤í‚µëœ ì¿¼ë¦¬ ì •ë³´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì„ì‹œ ìŠ¤í‚¤ë§ˆ ì‹œë®¬ë ˆì´ì…˜)
        skipped_queries_context = ""
        if skipped_queries and len(skipped_queries) > 0:
            skipped_details = []
            for sq in skipped_queries:
                query_summary = sq.get('query', '')
                tables = ', '.join(sq.get('tables', []))
                skipped_details.append(f"  - ì¿¼ë¦¬ {sq.get('query_num')}: {query_summary}")
                skipped_details.append(f"    ì°¸ì¡° í…Œì´ë¸”: {tables}")

            skipped_queries_context = f"""
**ì¤‘ìš”: ì„±ëŠ¥ ê²€ì¦ì´ ìŠ¤í‚µëœ ì¿¼ë¦¬**

ë‹¤ìŒ ì¿¼ë¦¬ë“¤ì€ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ë¯€ë¡œ, ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ EXPLAIN ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤:

{chr(10).join(skipped_details)}

**ê²€ì¦ ì§€ì¹¨:**
1. ìœ„ ì¿¼ë¦¬ë“¤ì€ **ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰**í•˜ê³ , ì„±ëŠ¥ ê²€ì¦ì€ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
2. í…Œì´ë¸”ì´ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ë¯€ë¡œ "í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€"ëŠ” ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤.
3. ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìœ¼ë¯€ë¡œ, **ì„±ëŠ¥ ë¬¸ì œë¡œ ì¸í•œ ì‹¤íŒ¨ íŒì •ì€ í•˜ì§€ ë§ˆì„¸ìš”**.
4. ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ë‹¤ë©´ "ê²€ì¦ í†µê³¼ (ì„±ëŠ¥ ë¶„ì„ ë¯¸ì‹¤í–‰: íŒŒì¼ ë‚´ ìƒì„± í…Œì´ë¸” ì°¸ì¡°)"ë¡œ í‘œì‹œí•˜ì„¸ìš”.
5. ì‚¬ìš©ìì—ê²Œ "ì‹¤ì œ í…Œì´ë¸” ìƒì„± í›„ ë³„ë„ì˜ ì„±ëŠ¥ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
"""

        # Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ
        knowledge_context = ""
        try:
            knowledge_info = await self.query_knowledge_base(ddl_content, sql_type)
            if knowledge_info and knowledge_info != "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                knowledge_context = f"""
Knowledge Base ì°¸ê³  ì •ë³´:
{knowledge_info}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê²€ì¦ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
"""
        except Exception as e:
            logger.warning(f"Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

        # DDLê³¼ DQLì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ë¶„
        ddl_types = [
            "CREATE_TABLE",
            "ALTER_TABLE",
            "CREATE_INDEX",
            "DROP_TABLE",
            "DROP_INDEX",
        ]
        dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

        if sql_type in ddl_types:
            # DDL ê²€ì¦ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ DDL ë¬¸ì„ Aurora MySQL ë¬¸ë²•ìœ¼ë¡œ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {schema_context}

        {schema_validation_context}

        {skipped_queries_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ë§Œ í™•ì¸í•˜ì„¸ìš”.

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "DDLì´ Aurora MySQLì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ ë°˜ë“œì‹œ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì„±ëŠ¥ ê°œì„ ì´ë‚˜ ëª¨ë²” ì‚¬ë¡€ëŠ” "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”  
        3. ì‹¤í–‰ì„ ë§‰ëŠ” ì‹¬ê°í•œ ë¬¸ë²• ì˜¤ë¥˜ë§Œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”

        **ì˜ˆì‹œ:**
        - ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ìš°: "ê²€ì¦ í†µê³¼"
        - ê°œì„ ì  ìˆëŠ” ê²½ìš°: "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: NULL ì†ì„±ì„ ëª…ì‹œí•˜ë©´ ë” ëª…í™•í•©ë‹ˆë‹¤)"
        - ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°: "ì˜¤ë¥˜: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•©ë‹ˆë‹¤"

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        elif sql_type in dql_types:
            # DQL ê²€ì¦ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ DQL(DML) ì¿¼ë¦¬ë¥¼ Aurora MySQLì—ì„œ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {explain_context}

        {schema_validation_context}

        {skipped_queries_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        1. Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        2. ì„±ëŠ¥ìƒ ë¬¸ì œê°€ ìˆëŠ”ì§€ ë¶„ì„
        3. ì¸ë±ìŠ¤ ì‚¬ìš© íš¨ìœ¨ì„± ê²€í† 
        4. **ì¤‘ìš”: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìœ¼ë©´ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬**

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì„±ëŠ¥ ë¬¸ì œ ì‹¤íŒ¨ ê¸°ì¤€:**
        ë‹¤ìŒê³¼ ê°™ì€ ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ë°œê²¬ë˜ë©´ ë°˜ë“œì‹œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”:
        - 1ì²œë§Œ í–‰ ì´ìƒì˜ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì— ëŒ€í•œ ì „ì²´ í…Œì´ë¸” ìŠ¤ìº” (Full Table Scan)
        - WHERE ì ˆ ì—†ëŠ” ëŒ€ìš©ëŸ‰ í…Œì´ë¸” UPDATE/DELETE
        - ì¸ë±ìŠ¤ ì—†ëŠ” ëŒ€ìš©ëŸ‰ í…Œì´ë¸” JOIN
        - ì¹´ë””ë„ë¦¬í‹°ê°€ ë§¤ìš° ë†’ì€ GROUP BYë‚˜ ORDER BY ì‘ì—…
        - ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ëŠ” ë³µì¡í•œ ì„œë¸Œì¿¼ë¦¬

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "ì¿¼ë¦¬ê°€ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìœ¼ë©´ "ì˜¤ë¥˜: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ..."ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”
        3. ê²½ë¯¸í•œ ì„±ëŠ¥ ê°œì„ ì ë§Œ ìˆìœ¼ë©´ "ê²€ì¦ í†µê³¼ (ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”
        4. ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ëŠ” "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”

        **ì˜ˆì‹œ:**
        - ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì„±ëŠ¥ ë¬¸ì œ ì—†ìŒ: "ê²€ì¦ í†µê³¼"
        - ê²½ë¯¸í•œ ì„±ëŠ¥ ê°œì„ ì : "ê²€ì¦ í†µê³¼ (ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­: ì¸ë±ìŠ¤ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”)"
        - ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ: "ì˜¤ë¥˜: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - 1ì²œë§Œ í–‰ ì´ìƒ í…Œì´ë¸”ì˜ ì „ì²´ ìŠ¤ìº”ìœ¼ë¡œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ë¶ˆê°€"
        - ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°: "ì˜¤ë¥˜: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•©ë‹ˆë‹¤"

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ SQL ë¬¸ì„ Aurora MySQL ë¬¸ë²•ìœ¼ë¡œ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {schema_context}

        {explain_context}

        {schema_validation_context}

        {skipped_queries_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ë§Œ í™•ì¸í•˜ì„¸ìš”. 

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "SQLì´ Aurora MySQLì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ ë°˜ë“œì‹œ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì„±ëŠ¥ ê°œì„ ì´ë‚˜ ëª¨ë²” ì‚¬ë¡€ëŠ” "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”  
        3. ì‹¤í–‰ì„ ë§‰ëŠ” ì‹¬ê°í•œ ë¬¸ë²• ì˜¤ë¥˜ë§Œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”
        4. ê¶Œì¥ì‚¬í•­ì„ ì œì•ˆí• ë•Œ, Aurora MySQL 8.0 ì´ ì•„ë‹Œ ê¸°ëŠ¥ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ê¶Œì¥í•˜ì§€ ë§ˆì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì¿¼ë¦¬ìºì‹œ ê¸°ëŠ¥ì€ 8.0ë¶€í„° ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ mysqlê¸°ëŠ¥ì„ ê¶Œì¥í•˜ê±°ë‚˜ ê±°ë¡ í•˜ì§€ ë§ˆì„¸ìš”.

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """

        claude_input = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,  # í† í° ìˆ˜ë¥¼ 4ë°°ë¡œ ì¦ê°€
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            }
        )

        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile í˜¸ì¶œ
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            response_body = json.loads(response.get("body").read())

            # response_bodyê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(response_body, dict):
                logger.error(f"Claude ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(response_body)}")
                return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(response_body).__name__})"

            # ì•ˆì „í•œ íƒ€ì… ì²´í¬ ë° ë°ì´í„° ì¶”ì¶œ
            content = response_body.get("content", [])
            if isinstance(content, list) and len(content) > 0:
                first_content = content[0]
                if isinstance(first_content, dict):
                    text_result = first_content.get("text", "")
                    if text_result:
                        return text_result
                    else:
                        logger.error("Claude ì‘ë‹µì˜ text í•„ë“œê°€ ë¹„ì–´ìˆìŒ")
                        return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: text í•„ë“œê°€ ë¹„ì–´ìˆìŒ"
                else:
                    logger.error(f"Claude ì‘ë‹µì˜ content[0]ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(first_content)}")
                    return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: content[0]ì˜ íƒ€ì…ì´ {type(first_content).__name__}ì…ë‹ˆë‹¤"
            else:
                logger.error(f"Claude ì‘ë‹µì˜ contentê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(content)}")
                return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: contentê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤"

        except Exception as e:
            logger.warning(
                f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}"
            )
            # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())

                # response_bodyê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if not isinstance(response_body, dict):
                    logger.error(f"Claude 3.7 ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(response_body)}")
                    return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(response_body).__name__})"

                # ì•ˆì „í•œ íƒ€ì… ì²´í¬ ë° ë°ì´í„° ì¶”ì¶œ
                content = response_body.get("content", [])
                if isinstance(content, list) and len(content) > 0:
                    first_content = content[0]
                    if isinstance(first_content, dict):
                        text_result = first_content.get("text", "")
                        if text_result:
                            return text_result
                        else:
                            logger.error("Claude 3.7 ì‘ë‹µì˜ text í•„ë“œê°€ ë¹„ì–´ìˆìŒ")
                            return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: text í•„ë“œê°€ ë¹„ì–´ìˆìŒ"
                    else:
                        logger.error(f"Claude 3.7 ì‘ë‹µì˜ content[0]ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(first_content)}")
                        return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: content[0]ì˜ íƒ€ì…ì´ {type(first_content).__name__}ì…ë‹ˆë‹¤"
                else:
                    logger.error(f"Claude 3.7 ì‘ë‹µì˜ contentê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(content)}")
                    return f"Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: contentê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤"

            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def generate_performance_recommendations_with_claude(
        self,
        metrics_summary: str,
        correlation_analysis: str,
        outliers_analysis: str,
        slow_queries: str,
        cpu_queries: str,
        temp_queries: str,
        database_secret: str = None,
    ) -> Dict[str, Any]:
        """
        Claudeë¥¼ í™œìš©í•˜ì—¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ê³¼ ì¿¼ë¦¬ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê¶Œì¥ì‚¬í•­ ìƒì„±
        """
        try:
            # Knowledge Baseì—ì„œ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ ì¡°íšŒ
            knowledge_context = ""
            try:
                knowledge_info = await self.query_knowledge_base(
                    "database performance optimization recommendations", "PERFORMANCE"
                )
                if knowledge_info and knowledge_info != "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                    knowledge_context = f"""
Knowledge Base ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ:
{knowledge_info}

ìœ„ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
            except Exception as e:
                logger.warning(f"Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

            prompt = f"""
ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì í™” ê¶Œì¥ì‚¬í•­ê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ìƒì„±í•´ì£¼ì„¸ìš”:

**ë©”íŠ¸ë¦­ ìš”ì•½:**
{metrics_summary}

**ìƒê´€ê´€ê³„ ë¶„ì„:**
{correlation_analysis}

**ì´ìƒ ì§•í›„ ë¶„ì„:**
{outliers_analysis}

**ëŠë¦° ì¿¼ë¦¬ ë¶„ì„:**
{slow_queries}

**CPU ì§‘ì•½ì  ì¿¼ë¦¬:**
{cpu_queries}

**ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬:**
{temp_queries}

{knowledge_context}

**ìš”êµ¬ì‚¬í•­:**
1. ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”
2. ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ì„ ì œì•ˆí•˜ì„¸ìš”
3. ê° ê¶Œì¥ì‚¬í•­ì— ëŒ€í•´ ì˜ˆìƒ íš¨ê³¼ì™€ êµ¬í˜„ ë‚œì´ë„ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ì•¡ì…˜ ì•„ì´í…œì€ ë‹´ë‹¹ì, ì˜ˆìƒ ì†Œìš”ì‹œê°„, ìš°ì„ ìˆœìœ„ë¥¼ í¬í•¨í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
    "immediate_improvements": [
        {{
            "category": "ëª¨ë‹ˆí„°ë§/ì„±ëŠ¥/ìš©ëŸ‰ê³„íš",
            "title": "êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­ ì œëª©",
            "description": "ìƒì„¸ ì„¤ëª…",
            "items": ["êµ¬ì²´ì ì¸ ì‹¤í–‰ í•­ëª©1", "êµ¬ì²´ì ì¸ ì‹¤í–‰ í•­ëª©2"],
            "expected_impact": "ì˜ˆìƒ íš¨ê³¼",
            "difficulty": "ë‚®ìŒ/ì¤‘ê°„/ë†’ìŒ"
        }}
    ],
    "action_items": [
        {{
            "priority": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ",
            "item": "êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ",
            "estimated_time": "ì˜ˆìƒ ì†Œìš”ì‹œê°„",
            "assignee": "ë‹´ë‹¹ì ì—­í• ",
            "rationale": "ì´ ì•¡ì…˜ì´ í•„ìš”í•œ ì´ìœ "
        }}
    ]
}}

ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë°œê²¬ëœ ë¬¸ì œì ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”.
"""

            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4096,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 í˜¸ì¶œ
            try:
                logger.info(f"Claude Sonnet 4 í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸ID: {sonnet_4_model_id}")
                logger.debug(f"ì…ë ¥ ë°ì´í„° í¬ê¸°: {len(claude_input)} bytes")

                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                logger.info("Claude Sonnet 4 ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

                response_body = json.loads(response.get("body").read())
                logger.debug(f"ì‘ë‹µ ë³¸ë¬¸ íŒŒì‹± ì™„ë£Œ: {list(response_body.keys())}")

                claude_response = response_body.get("content", [{}])[0].get("text", "")
                logger.info(
                    f"Claude ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(claude_response)} characters"
                )
                logger.debug(f"Claude ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {claude_response[:200]}...")

                # JSON íŒŒì‹± ì‹œë„ - ë¨¼ì € ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í™•ì¸
                try:
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                    import re

                    markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                    markdown_match = re.search(
                        markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                    )

                    if markdown_match:
                        json_content = markdown_match.group(1).strip()
                        logger.info(
                            f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ, ê¸¸ì´: {len(json_content)}"
                        )
                        parsed_result = json.loads(json_content)
                    else:
                        # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì§ì ‘ íŒŒì‹± ì‹œë„
                        logger.info("ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ìŒ, ì§ì ‘ JSON íŒŒì‹± ì‹œë„")
                        parsed_result = json.loads(claude_response)

                    logger.info("Claude ì‘ë‹µ JSON íŒŒì‹± ì„±ê³µ")
                    logger.debug(f"íŒŒì‹±ëœ ê²°ê³¼ í‚¤: {list(parsed_result.keys())}")

                    # í•„ìš”í•œ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if isinstance(parsed_result, dict) and (
                        "immediate_improvements" in parsed_result
                        or "action_items" in parsed_result
                    ):
                        improvements_count = len(
                            parsed_result.get("immediate_improvements", [])
                        )
                        actions_count = len(parsed_result.get("action_items", []))
                        logger.info(
                            f"ìœ íš¨í•œ ê¶Œì¥ì‚¬í•­ íŒŒì‹± ì™„ë£Œ: {improvements_count}ê°œ ê°œì„ ì‚¬í•­, {actions_count}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                        )
                        return parsed_result
                    else:
                        logger.warning("íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                        return self._get_default_recommendations()

                except json.JSONDecodeError as json_err:
                    logger.error(f"Claude Sonnet 4 ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
                    logger.info("í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
                    # í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                    parsed_result = self._parse_claude_text_response(claude_response)
                    if parsed_result:
                        logger.info("í…ìŠ¤íŠ¸ íŒŒì‹± ì„±ê³µ")
                        return parsed_result
                    logger.error(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ ë‚´ìš©: {claude_response}")
                    return self._get_default_recommendations()

            except Exception as e:
                logger.error(
                    f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ - ì—ëŸ¬ íƒ€ì…: {type(e).__name__}"
                )
                logger.error(f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
                if hasattr(e, "response"):
                    logger.error(
                        f"AWS ì‘ë‹µ ì½”ë“œ: {e.response.get('Error', {}).get('Code', 'Unknown')}"
                    )
                    logger.error(
                        f"AWS ì‘ë‹µ ë©”ì‹œì§€: {e.response.get('Error', {}).get('Message', 'Unknown')}"
                    )

                # Claude 3.7 Sonnet í˜¸ì¶œ (fallback)
                try:
                    logger.info(
                        f"Claude 3.7 Sonnet fallback ì‹œì‘ - ëª¨ë¸ID: {sonnet_3_7_model_id}"
                    )

                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    logger.info("Claude 3.7 Sonnet ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

                    response_body = json.loads(response.get("body").read())
                    claude_response = response_body.get("content", [{}])[0].get(
                        "text", ""
                    )
                    logger.info(
                        f"Claude 3.7 ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(claude_response)} characters"
                    )

                    try:
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                        import re

                        markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                        markdown_match = re.search(
                            markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                        )

                        if markdown_match:
                            json_content = markdown_match.group(1).strip()
                            logger.info(
                                f"Claude 3.7 ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ, ê¸¸ì´: {len(json_content)}"
                            )
                            parsed_result = json.loads(json_content)
                        else:
                            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì§ì ‘ íŒŒì‹± ì‹œë„
                            logger.info(
                                "Claude 3.7 ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ìŒ, ì§ì ‘ JSON íŒŒì‹± ì‹œë„"
                            )
                            parsed_result = json.loads(claude_response)

                        logger.info("Claude 3.7 ì‘ë‹µ JSON íŒŒì‹± ì„±ê³µ")

                        # í•„ìš”í•œ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if isinstance(parsed_result, dict) and (
                            "immediate_improvements" in parsed_result
                            or "action_items" in parsed_result
                        ):
                            improvements_count = len(
                                parsed_result.get("immediate_improvements", [])
                            )
                            actions_count = len(parsed_result.get("action_items", []))
                            logger.info(
                                f"Claude 3.7 ìœ íš¨í•œ ê¶Œì¥ì‚¬í•­ íŒŒì‹± ì™„ë£Œ: {improvements_count}ê°œ ê°œì„ ì‚¬í•­, {actions_count}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                            )
                            return parsed_result
                        else:
                            logger.warning("Claude 3.7 íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                            return self._get_default_recommendations()

                    except json.JSONDecodeError as json_err:
                        logger.error(f"Claude 3.7 ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
                        logger.info("Claude 3.7 í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
                        # í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                        parsed_result = self._parse_claude_text_response(
                            claude_response
                        )
                        if parsed_result:
                            logger.info("Claude 3.7 í…ìŠ¤íŠ¸ íŒŒì‹± ì„±ê³µ")
                            return parsed_result
                        logger.error(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ ë‚´ìš©: {claude_response}")
                        return self._get_default_recommendations()

                except Exception as fallback_e:
                    logger.error(
                        f"Claude 3.7 Sonnet fallback ì‹¤íŒ¨ - ì—ëŸ¬ íƒ€ì…: {type(fallback_e).__name__}"
                    )
                    logger.error(
                        f"Claude 3.7 Sonnet fallback ì‹¤íŒ¨ - ì—ëŸ¬ ë©”ì‹œì§€: {str(fallback_e)}"
                    )
                    if hasattr(fallback_e, "response"):
                        logger.error(
                            f"AWS fallback ì‘ë‹µ ì½”ë“œ: {fallback_e.response.get('Error', {}).get('Code', 'Unknown')}"
                        )
                        logger.error(
                            f"AWS fallback ì‘ë‹µ ë©”ì‹œì§€: {fallback_e.response.get('Error', {}).get('Message', 'Unknown')}"
                        )
                    return self._get_default_recommendations()

        except Exception as e:
            logger.error(
                f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ì—ëŸ¬ íƒ€ì…: {type(e).__name__}"
            )
            logger.error(f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            logger.error(
                f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True
            )
            return self._get_default_recommendations()

    def _parse_claude_text_response(
        self, text_response: str
    ) -> Optional[Dict[str, Any]]:
        """Claude í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            import re

            logger.info(f"Claude ì‘ë‹µ íŒŒì‹± ì‹œì‘, ì‘ë‹µ ê¸¸ì´: {len(text_response)}")
            logger.debug(f"ì‘ë‹µ ì‹œì‘ ë¶€ë¶„: {text_response[:200]}")

            # ë¨¼ì € ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ
            markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
            markdown_match = re.search(
                markdown_pattern, text_response, re.DOTALL | re.IGNORECASE
            )

            if markdown_match:
                json_content = markdown_match.group(1).strip()
                logger.info(
                    f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ, ê¸¸ì´: {len(json_content)}"
                )
                logger.debug(f"ì¶”ì¶œëœ JSON ì‹œì‘ ë¶€ë¶„: {json_content[:200]}")
                try:
                    parsed = json.loads(json_content)
                    if isinstance(parsed, dict) and (
                        "immediate_improvements" in parsed or "action_items" in parsed
                    ):
                        logger.info(
                            f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ JSON íŒŒì‹± ì„±ê³µ - ê°œì„ ì‚¬í•­: {len(parsed.get('immediate_improvements', []))}ê°œ, ì•¡ì…˜ì•„ì´í…œ: {len(parsed.get('action_items', []))}ê°œ"
                        )
                        return parsed
                    else:
                        logger.warning("íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                except json.JSONDecodeError as e:
                    logger.error(f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.debug(
                        f"íŒŒì‹± ì‹¤íŒ¨í•œ JSON ë‚´ìš© (ì²˜ìŒ 500ì): {json_content[:500]}"
                    )
            else:
                logger.warning("ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ íŒ¨í„´ë“¤ ì‹œë„
            logger.info("ëŒ€ì²´ JSON íŒ¨í„´ ë§¤ì¹­ ì‹œë„")
            json_patterns = [
                r'(\{[^{}]*"immediate_improvements"[^{}]*\})',
                r'(\{.*?"immediate_improvements".*?\})',
                r'(\{.*?"action_items".*?\})',
                r"(\{.*?\})",
            ]

            for i, pattern in enumerate(json_patterns):
                logger.debug(f"íŒ¨í„´ {i+1} ì‹œë„: {pattern}")
                matches = re.findall(pattern, text_response, re.DOTALL | re.IGNORECASE)
                logger.debug(f"íŒ¨í„´ {i+1}ì—ì„œ {len(matches)}ê°œ ë§¤ì¹˜ ë°œê²¬")
                for j, match in enumerate(matches):
                    try:
                        parsed = json.loads(match)
                        if isinstance(parsed, dict) and (
                            "immediate_improvements" in parsed
                            or "action_items" in parsed
                        ):
                            logger.info(f"JSON íŒ¨í„´ ë§¤ì¹­ ì„±ê³µ: íŒ¨í„´ {i+1}, ë§¤ì¹˜ {j+1}")
                            return parsed
                    except json.JSONDecodeError as e:
                        logger.debug(f"íŒ¨í„´ {i+1}, ë§¤ì¹˜ {j+1} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue

            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
            logger.info("êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
            return self._extract_from_structured_text(text_response)

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _extract_from_structured_text(self, text: str) -> Optional[Dict[str, Any]]:
        """êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
        try:
            import re

            result = {"immediate_improvements": [], "action_items": []}

            # ê°œì„ ì‚¬í•­ ì„¹ì…˜ ì°¾ê¸°
            improvements_pattern = r'"immediate_improvements":\s*\[(.*?)\]'
            improvements_match = re.search(improvements_pattern, text, re.DOTALL)

            if improvements_match:
                improvements_text = improvements_match.group(1)
                # ê° ê°œì„ ì‚¬í•­ íŒŒì‹±
                item_pattern = r"\{([^{}]+)\}"
                for item_match in re.finditer(item_pattern, improvements_text):
                    item_text = item_match.group(1)
                    improvement = self._parse_improvement_item(item_text)
                    if improvement:
                        result["immediate_improvements"].append(improvement)

            # ì•¡ì…˜ ì•„ì´í…œ ì„¹ì…˜ ì°¾ê¸°
            actions_pattern = r'"action_items":\s*\[(.*?)\]'
            actions_match = re.search(actions_pattern, text, re.DOTALL)

            if actions_match:
                actions_text = actions_match.group(1)
                # ê° ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹±
                for item_match in re.finditer(item_pattern, actions_text):
                    item_text = item_match.group(1)
                    action = self._parse_action_item(item_text)
                    if action:
                        result["action_items"].append(action)

            if result["immediate_improvements"] or result["action_items"]:
                return result

            return None

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _parse_improvement_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """ê°œì„ ì‚¬í•­ ì•„ì´í…œ íŒŒì‹±"""
        try:
            import re

            improvement = {}

            # ê° í•„ë“œ ì¶”ì¶œ
            fields = {
                "category": r'"category":\s*"([^"]+)"',
                "title": r'"title":\s*"([^"]+)"',
                "description": r'"description":\s*"([^"]+)"',
                "expected_impact": r'"expected_impact":\s*"([^"]+)"',
                "difficulty": r'"difficulty":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    improvement[field] = match.group(1)

            # items ë°°ì—´ ì¶”ì¶œ
            items_pattern = r'"items":\s*\[(.*?)\]'
            items_match = re.search(items_pattern, item_text, re.DOTALL)
            if items_match:
                items_text = items_match.group(1)
                items = re.findall(r'"([^"]+)"', items_text)
                improvement["items"] = items

            return improvement if improvement else None

        except Exception as e:
            logger.error(f"ê°œì„ ì‚¬í•­ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _parse_action_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹±"""
        try:
            import re

            action = {}

            # ê° í•„ë“œ ì¶”ì¶œ
            fields = {
                "priority": r'"priority":\s*"([^"]+)"',
                "item": r'"item":\s*"([^"]+)"',
                "estimated_time": r'"estimated_time":\s*"([^"]+)"',
                "assignee": r'"assignee":\s*"([^"]+)"',
                "rationale": r'"rationale":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    action[field] = match.group(1)

            return action if action else None

        except Exception as e:
            logger.error(f"ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _generate_recommendations_html(self, recommendations: Dict[str, Any]) -> str:
        """Claude ê¶Œì¥ì‚¬í•­ì„ HTMLë¡œ ë³€í™˜"""
        try:
            html_parts = []

            # ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­
            html_parts.append("<h4>ğŸš€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­</h4>")

            improvements = recommendations.get("immediate_improvements", [])
            if not improvements:
                html_parts.append(
                    '<div class="info-box">í˜„ì¬ ë¶„ì„ëœ ë°ì´í„°ì—ì„œëŠ” ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</div>'
                )
            else:
                for improvement in improvements:
                    category = improvement.get("category", "ê¸°íƒ€")
                    title = improvement.get("title", "ê°œì„ ì‚¬í•­")
                    description = improvement.get("description", "")
                    items = improvement.get("items", [])
                    expected_impact = improvement.get("expected_impact", "")
                    difficulty = improvement.get("difficulty", "ì¤‘ê°„")

                    html_parts.append(
                        f"""
                    <div class="recommendation">
                        <strong>{category}: {title}</strong>
                        <p style="margin: 10px 0; color: #666;">{description}</p>
                        <ul style="margin-top: 10px; margin-left: 20px;">
                    """
                    )

                    for item in items:
                        html_parts.append(f"<li>{item}</li>")

                    html_parts.append(
                        f"""
                        </ul>
                        <div style="margin-top: 10px; font-size: 0.9em;">
                            <span style="color: #27ae60;"><strong>ì˜ˆìƒ íš¨ê³¼:</strong> {expected_impact}</span> | 
                            <span style="color: #3498db;"><strong>êµ¬í˜„ ë‚œì´ë„:</strong> {difficulty}</span>
                        </div>
                    </div>
                    """
                    )

            # ì•¡ì…˜ ì•„ì´í…œ í…Œì´ë¸”
            html_parts.append("<h4>ğŸ“‹ ì•¡ì…˜ ì•„ì´í…œ</h4>")

            actions = recommendations.get("action_items", [])
            if not actions:
                html_parts.append(
                    '<div class="info-box">í˜„ì¬ ë¶„ì„ëœ ë°ì´í„°ì—ì„œëŠ” íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</div>'
                )
            else:
                html_parts.append(
                    """
                    <table class="table">
                        <thead>
                            <tr>
                                <th>ìš°ì„ ìˆœìœ„</th>
                                <th>í•­ëª©</th>
                                <th>ì˜ˆìƒ ì†Œìš”ì‹œê°„</th>
                                <th>ë‹´ë‹¹ì</th>
                                <th>ê·¼ê±°</th>
                            </tr>
                        </thead>
                        <tbody>
                """
                )

                for action in actions:
                    priority = action.get("priority", "ì¤‘ê°„")
                    item = action.get("item", "ì•¡ì…˜ ì•„ì´í…œ")
                    estimated_time = action.get("estimated_time", "ë¯¸ì •")
                    assignee = action.get("assignee", "ë‹´ë‹¹ì")
                    rationale = action.get("rationale", "")

                    # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìŠ¤íƒ€ì¼ í´ë˜ìŠ¤
                    priority_class = {
                        "ë†’ìŒ": "status-critical",
                        "ì¤‘ê°„": "status-warning",
                        "ë‚®ìŒ": "status-good",
                    }.get(priority, "status-warning")

                    html_parts.append(
                        f"""
                            <tr>
                                <td><span class="{priority_class}">{priority}</span></td>
                                <td>{item}</td>
                                <td>{estimated_time}</td>
                                <td>{assignee}</td>
                                <td style="font-size: 0.9em; color: #666;">{rationale}</td>
                            </tr>
                    """
                    )

                html_parts.append(
                    """
                        </tbody>
                    </table>
                """
                )

            return "".join(html_parts)

        except Exception as e:
            logger.error(f"HTML ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f'<div class="issue">ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}</div>'

    def _get_default_recommendations(self) -> Dict[str, Any]:
        """Claude í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ ë°˜í™˜"""
        return {
            "immediate_improvements": [
                {
                    "category": "ëª¨ë‹ˆí„°ë§",
                    "title": "ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                    "description": "ê¸°ë³¸ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•",
                    "items": [
                        "CloudWatch ì•ŒëŒ ì„¤ì •",
                        "Performance Insights í™œì„±í™”",
                        "ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ ì •ê¸° ë¶„ì„",
                    ],
                    "expected_impact": "ì„±ëŠ¥ ë¬¸ì œ ì¡°ê¸° ë°œê²¬",
                    "difficulty": "ë‚®ìŒ",
                }
            ],
            "action_items": [
                {
                    "priority": "ë†’ìŒ",
                    "item": "CloudWatch ì•ŒëŒ ì„¤ì •",
                    "estimated_time": "1ì¼",
                    "assignee": "DBA",
                    "rationale": "ì„±ëŠ¥ ë¬¸ì œ ì¡°ê¸° ê°ì§€ë¥¼ ìœ„í•´ í•„ìš”",
                }
            ],
        }

    async def extract_current_schema_info(
        self, database_secret: str, use_ssh_tunnel: bool = False  # EC2ì—ì„œëŠ” VPC ì§ì ‘ ì—°ê²°
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘: database_secret={database_secret}")
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            logger.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}")

            schema_info = {"tables": [], "columns": {}, "indexes": {}}

            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            cursor.execute(
                """
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """
            )
            tables = [row[0] for row in cursor.fetchall()]
            schema_info["tables"] = tables
            logger.info(f"ë°œê²¬ëœ í…Œì´ë¸”: {tables}")

            # ê° í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT column_name, data_type, character_maximum_length, 
                           numeric_precision, numeric_scale, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table,),
                )

                columns = []
                for col_row in cursor.fetchall():
                    col_info = {
                        "name": col_row[0],
                        "data_type": col_row[1],
                        "max_length": col_row[2],
                        "precision": col_row[3],
                        "scale": col_row[4],
                        "is_nullable": col_row[5],
                        "default_value": col_row[6],
                    }
                    columns.append(col_info)

                schema_info["columns"][table] = columns

                # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT index_name, column_name, non_unique, seq_in_index
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table,),
                )

                indexes = {}
                for idx_row in cursor.fetchall():
                    idx_name = idx_row[0]
                    col_name = idx_row[1]
                    is_unique = idx_row[2] == 0

                    if idx_name not in indexes:
                        indexes[idx_name] = {"columns": [], "unique": is_unique}
                    indexes[idx_name]["columns"].append(col_name)

                schema_info["indexes"][table] = indexes

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            logger.info(
                f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(schema_info['tables'])}ê°œ í…Œì´ë¸”, {len(schema_info['columns'])}ê°œ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´"
            )
            return schema_info

        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            if "tunnel_used" in locals() and tunnel_used:
                self.cleanup_ssh_tunnel()
            return {}












    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ (utils/parsers.py ìœ„ì„)"""
        return parse_data_type(data_type_str)

    async def generate_html_report(
        self,
        report_path: Path,
        filename: str,
        ddl_content: str,
        sql_type: str,
        status: str,
        summary: str,
        issues: List[str],
        db_connection_info: Optional[Dict],
        schema_validation: Optional[Dict],
        constraint_validation: Optional[Dict],
        database_secret: Optional[str],
        explain_result: Optional[Dict] = None,
        claude_analysis_result: Optional[str] = None,  # Claude ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        dml_column_issues: List[str] = None,  # DML ì»¬ëŸ¼ ì´ìŠˆ ì¶”ê°€
    ):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        # dml_column_issues ì´ˆê¸°í™”
        if dml_column_issues is None:
            dml_column_issues = []

        # ìƒì„¸ ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
        try:
            with open(
                OUTPUT_DIR / "html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(f"=== HTML ìƒì„± í•¨ìˆ˜ ì‹œì‘ ===\n")
                f.write(f"report_path: {report_path}\n")
                f.write(f"filename: {filename}\n")
                f.write(f"sql_type: {sql_type}\n")
                f.write(f"status: {status}\n")
                f.write(f"issues ê°œìˆ˜: {len(issues)}\n")
                f.flush()
        except Exception as debug_e:
            logger.error(f"ë””ë²„ê·¸ ë¡œê·¸ ì‘ì„± ì˜¤ë¥˜: {debug_e}")

        # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
        try:
            with open(
                OUTPUT_DIR / "html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(
                    f"HTML ìƒì„± í•¨ìˆ˜ í˜¸ì¶œë¨ - claude_analysis_result: {claude_analysis_result}\n"
                )
                f.flush()
        except:
            pass
        try:
            # Claude ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœ ì¬í‰ê°€
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("ê²€ì¦ í†µê³¼")
            )

            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜ - Claude ê²€ì¦ ê²°ê³¼ ìš°ì„  ë°˜ì˜
            if (
                claude_success
                and status == "FAIL"
                and not any(
                    "ì˜¤ë¥˜:" in issue or "ì‹¤íŒ¨" in issue or "ì¡´ì¬í•˜ì§€ ì•Š" in issue
                    for issue in issues
                )
            ):
                # Claudeê°€ ì„±ê³µì´ê³  ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ì—†ìœ¼ë©´ PASSë¡œ ë³€ê²½
                status = "PASS"
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."

            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"

            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­ì— ë”°ë¼)
            db_info_section = ""

            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜ - Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
            claude_issues = []
            other_issues = []

            for issue in issues:
                if issue.startswith("Claude ê²€ì¦:"):
                    claude_issues.append(issue[12:].strip())  # "Claude ê²€ì¦:" ì œê±°
                else:
                    other_issues.append(issue)

            # ê¸°íƒ€ ê²€ì¦ ë¬¸ì œ ì„¹ì…˜ ì œê±° (ì¤‘ë³µ ë°©ì§€)

            # Claude ê²€ì¦ ê²°ê³¼ ë‚´ìš© ì¤€ë¹„ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ í¬í•¨)
            schema_validation_summary = self.create_schema_validation_summary(
                issues, dml_column_issues
            )

            # Claude ê²€ì¦ê³¼ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ í†µí•©í•œ ë‚´ìš© ìƒì„±
            combined_validation_content = ""

            # Claude AI ê²€ì¦ ê²°ê³¼ ì¶”ê°€ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ëŠ” ìˆ¨ê¹€)
            claude_content = (
                claude_analysis_result
                if claude_analysis_result
                else "Claude ê²€ì¦ ê²°ê³¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            combined_validation_content += f"""
<div class="validation-subsection">
    <h4>ğŸ“‹ SQLê²€ì¦ê²°ê³¼</h4>
    <pre class="validation-text">{claude_content}</pre>
</div>
"""

            # ì „ì²´ ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš°
            success_section = ""
            if not issues:
                success_section = """
                <div class="issues-section success" style="display: none;">
                    <h3>âœ… ê²€ì¦ ê²°ê³¼</h3>
                    <p class="no-issues">ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</p>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SQL ê²€ì¦ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
        }}
        .claude-section {{
            margin: 30px 0;
            padding: 25px;
            border-radius: 8px;
            border: 2px solid #667eea;
            background: #f8f9ff;
            box-shadow: 0 2px 10px rgba(102, 126, 234, 0.1);
        }}
        .claude-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            font-size: 1.3em;
        }}
        .claude-result {{
            margin: 20px 0;
            padding: 0;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            box-shadow: 0 1px 5px rgba(0,0,0,0.05);
        }}
        .claude-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 800px;  /* 400pxì—ì„œ 800pxë¡œ ì¦ê°€ */
            overflow-y: auto;
            min-height: 100px;
            resize: vertical;  /* ì‚¬ìš©ìê°€ ìˆ˜ì§ìœ¼ë¡œ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥ */
        }}
        .validation-subsection {{
            margin: 15px 0;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #28a745;
            background: #f8fff9;
        }}
        .validation-subsection h4 {{
            margin: 0 0 10px 0;
            color: #495057;
            font-size: 1.1em;
        }}
        .validation-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 13px;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 300px;
            overflow-y: auto;
        }}
        .success-text {{
            color: #28a745;
            font-weight: 500;
            margin: 0;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} SQL ê²€ì¦ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ SQL íƒ€ì…</h4>
                    <p>{sql_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ SQL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="claude-section">
                <h3>ğŸ” í†µí•© ê²€ì¦ ê²°ê³¼ (ìŠ¤í‚¤ë§ˆ + ì¿¼ë¦¬ì„±ëŠ¥)</h3>
                <div class="claude-result">
                    {combined_validation_content}
                </div>
            </div>
            
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì œê±°
            try:
                with open(report_path, "r", encoding="utf-8") as f:
                    html_content = f.read()

                # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì œê±° - ë” ì •í™•í•œ ë°©ë²•
                lines = html_content.split("\n")
                new_lines = []
                i = 0

                while i < len(lines):
                    line = lines[i]

                    # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì‹œì‘ ê°ì§€
                    if '<div class="info-section" style="display: none;">' in line:
                        # ë‹¤ìŒ ë¼ì¸ë“¤ì„ í™•ì¸í•˜ì—¬ ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ì¸ì§€ íŒë‹¨
                        if i + 1 < len(lines) and "ğŸ“Š ê²€ì¦ ê²°ê³¼" in lines[i + 1]:
                            # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ì´ë¯€ë¡œ </div>ê¹Œì§€ ìŠ¤í‚µ
                            i += 1  # í˜„ì¬ div ë¼ì¸ ìŠ¤í‚µ
                            while i < len(lines):
                                if "</div>" in lines[i]:
                                    i += 1  # </div> ë¼ì¸ë„ ìŠ¤í‚µ
                                    break
                                i += 1
                            continue

                    new_lines.append(line)
                    i += 1

                html_content = "\n".join(new_lines)

                with open(report_path, "w", encoding="utf-8") as f:
                    f.write(html_content)
            except Exception as e:
                pass

            # íŒŒì¼ ìƒì„± í™•ì¸ ë””ë²„ê·¸
            try:
                with open(
                    OUTPUT_DIR / "html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    f.write(f"HTML íŒŒì¼ ìƒì„± ì™„ë£Œ: {report_path}\n")
                    f.write(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {report_path.exists()}\n")
                    f.flush()
            except:
                pass

        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ë¥¼ ë””ë²„ê·¸ íŒŒì¼ì— ê¸°ë¡
            try:
                with open(
                    OUTPUT_DIR / "html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    import traceback

                    f.write(f"HTML ìƒì„± ì˜¤ë¥˜: {e}\n")
                    f.write(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}\n")
                    f.flush()
            except:
                pass

    async def generate_consolidated_html_report(
        self, validation_results: List[Dict], database_secret: str
    ) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì˜ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            # íŒŒì¼ë³„ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"
                status_class = "success" if result["status"] == "PASS" else "error"

                issues_html = ""
                if result["issues"]:
                    issues_html = "<ul class='issues-list'>"
                    for issue in result["issues"]:
                        issues_html += f"<li>{issue}</li>"
                    issues_html += "</ul>"
                else:
                    issues_html = "<p class='no-issues'>ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>"

                # ê°œë³„ íŒŒì¼ ê²€ì¦ì—ì„œë§Œ ìƒì„¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ë¯€ë¡œ ë§í¬ ì—†ì´ íŒŒì¼ëª…ë§Œ í‘œì‹œ
                filename_display = result["filename"]

                file_sections += f"""
                <div class="file-section {status_class}">
                    <h3>{status_icon} {i}. {filename_display}</h3>
                    <div class="file-details">
                        <div class="file-info">
                            <span><strong>DDL íƒ€ì…:</strong> {result['ddl_type']}</span>
                            <span><strong>ìƒíƒœ:</strong> {result['status']}</span>
                            <span><strong>ë¬¸ì œ ìˆ˜:</strong> {len(result['issues'])}ê°œ</span>
                        </div>
                        <div class="sql-code">
{result['ddl_content']}
                        </div>
                        {f'<div class="issues-section">{issues_html}</div>' if result['issues'] else ''}
                    </div>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© SQL ê²€ì¦ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 20px 30px;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-section.success {{
            border-left: 4px solid #28a745;
        }}
        .file-section.error {{
            border-left: 4px solid #dc3545;
        }}
        .file-section h3 {{
            margin: 0;
            padding: 15px 20px;
            background: #f8f9fa;
            border-bottom: 1px solid #e9ecef;
        }}
        .file-section h3 a {{
            color: #495057;
            text-decoration: none;
        }}
        .file-section h3 a:hover {{
            color: #007bff;
            text-decoration: underline;
        }}
        .file-details {{
            padding: 20px;
        }}
        .file-info {{
            display: flex;
            gap: 20px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }}
        .file-info span {{
            background: #e9ecef;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
            font-size: 0.9em;
        }}
        .issues-section {{
            margin-top: 15px;
            padding: 15px;
            background: #fff5f5;
            border: 1px solid #fed7d7;
            border-radius: 6px;
        }}
        .issues-section h4 {{
            margin: 0 0 10px 0;
            color: #c53030;
        }}
        .issues-list {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
            color: #c53030;
        }}
        .no-issues {{
            color: #38a169;
            margin: 0;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
                margin: 20px;
            }}
            .file-section {{
                margin: 20px 15px;
            }}
            .file-info {{
                flex-direction: column;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© SQL ê²€ì¦ë³´ê³ ì„œ</h1>
            <p>ë°ì´í„°ë² ì´ìŠ¤: {database_secret}</p>
            <p>ê²€ì¦ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
        
        <div class="summary-stats">
            <div class="stat-item">
                <div class="stat-number">{total_files}</div>
                <div class="stat-label">ì´ íŒŒì¼ ìˆ˜</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                <div class="stat-label">í†µê³¼</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                <div class="stat-label">ì‹¤íŒ¨</div>
            </div>
            <div class="stat-item">
                <div class="stat-number">{round(passed_files/total_files*100) if total_files > 0 else 0}%</div>
                <div class="stat-label">ì„±ê³µë¥ </div>
            </div>
        </div>
        
        {file_sections}
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>"""

            # íŒŒì¼ ì €ì¥
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_report(
        self,
        keyword: Optional[str] = None,
        report_files: Optional[List[str]] = None,
        date_filter: Optional[str] = None,
        latest_count: Optional[int] = None,
    ) -> str:
        """ê¸°ì¡´ HTML ë³´ê³ ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ë³´ê³ ì„œ íŒŒì¼ ìˆ˜ì§‘
            if report_files:
                # íŠ¹ì • íŒŒì¼ë“¤ ì§€ì •ëœ ê²½ìš°
                html_files = [
                    OUTPUT_DIR / f for f in report_files if (OUTPUT_DIR / f).exists()
                ]
            else:
                # validation_reportë¡œ ì‹œì‘í•˜ëŠ” HTML íŒŒì¼ë§Œ (debug_log ì œì™¸)
                html_files = list(OUTPUT_DIR.glob("validation_report_*.html"))

                # í‚¤ì›Œë“œ í•„í„°ë§
                if keyword:
                    html_files = [f for f in html_files if keyword in f.name]

                # ë‚ ì§œ í•„í„°ë§ (YYYYMMDD í˜•ì‹)
                if date_filter:
                    html_files = [f for f in html_files if date_filter in f.name]

                # ìµœì‹  íŒŒì¼ ê°œìˆ˜ ì œí•œ
                if latest_count:
                    # íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
                    html_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                    html_files = html_files[:latest_count]

            if not html_files:
                return f"ì¡°ê±´ì— ë§ëŠ” HTML ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œ: {keyword}, ë‚ ì§œ: {date_filter}, ê°œìˆ˜: {latest_count})"

            # ê° ë³´ê³ ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ
            report_data = []
            for html_file in html_files:
                try:
                    content = html_file.read_text(encoding="utf-8")

                    # íŒŒì¼ëª…ì—ì„œ ì›ë³¸ SQL íŒŒì¼ëª… ì¶”ì¶œ
                    sql_filename = html_file.name.replace(
                        "validation_report_", ""
                    ).replace(".html", "")
                    if "_2025" in sql_filename:
                        sql_filename = sql_filename.split("_2025")[0] + ".sql"

                    # ê²€ì¦ ê²°ê³¼ ì¶”ì¶œ - HTML êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ì¶”ì¶œ
                    if (
                        'status-badge">PASS' in content
                        or "âœ… SQL ê²€ì¦ë³´ê³ ì„œ" in content
                    ):
                        status = "PASS"
                        status_icon = "âœ…"
                    elif (
                        'status-badge">FAIL' in content
                        or "âŒ SQL ê²€ì¦ë³´ê³ ì„œ" in content
                    ):
                        status = "FAIL"
                        status_icon = "âŒ"
                    else:
                        # ê¸°ë³¸ê°’ìœ¼ë¡œ FAIL ì²˜ë¦¬
                        status = "FAIL"
                        status_icon = "âŒ"

                    # SQL ë‚´ìš© ì¼ë¶€ ì¶”ì¶œ (HTML íŒŒì¼ì—ì„œë§Œ)
                    sql_preview = "SQL ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    if "sql-code" in content:
                        import re

                        sql_match = re.search(
                            r'<div class="sql-code"[^>]*>(.*?)</div>',
                            content,
                            re.DOTALL,
                        )
                        if sql_match:
                            sql_preview = sql_match.group(1).strip()[:100] + "..."

                    # ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                    summary = "ìƒì„¸ ë‚´ìš©ì€ ê°œë³„ ë³´ê³ ì„œ ì°¸ì¡°"
                    if "Claude AI ë¶„ì„" in content:
                        summary = "AI ë¶„ì„ ì™„ë£Œ"

                    report_data.append(
                        {
                            "filename": sql_filename,
                            "html_file": html_file.name,
                            "status": status,
                            "status_icon": status_icon,
                            "sql_preview": sql_preview,
                            "summary": summary,
                        }
                    )

                except Exception as e:
                    logger.error(f"ë³´ê³ ì„œ íŒŒì‹± ì˜¤ë¥˜ {html_file}: {e}")
                    continue

            if not report_data:
                return "ìœ íš¨í•œ ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # í†µí•© ë³´ê³ ì„œ HTML ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # í†µê³„ ê³„ì‚°
            total_reports = len(report_data)
            passed_reports = sum(1 for r in report_data if r["status"] == "PASS")
            failed_reports = total_reports - passed_reports

            # í…Œì´ë¸” í–‰ ìƒì„±
            table_rows = ""
            for i, data in enumerate(report_data, 1):
                table_rows += f"""
                <tr onclick="openReport('{data['html_file']}')" style="cursor: pointer;">
                    <td>{i}</td>
                    <td>{data['status_icon']} {data['filename']}</td>
                    <td><code>{data['sql_preview']}</code></td>
                    <td><span class="status-badge {data['status'].lower()}">{data['status']}</span></td>
                    <td>{data['summary']}</td>
                </tr>
                """

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Segoe UI', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 2.5em; font-weight: 300; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #667eea; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #333; }}
        .stat-label {{ color: #666; margin-top: 5px; }}
        .table-container {{ margin: 30px; overflow-x: auto; }}
        table {{ width: 100%; border-collapse: collapse; background: white; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #f8f9fa; font-weight: 600; }}
        tr:hover {{ background: #f8f9fa; }}
        .status-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .status-badge.pass {{ background: #d4edda; color: #155724; }}
        .status-badge.fail {{ background: #f8d7da; color: #721c24; }}
        code {{ background: #f1f3f4; padding: 2px 4px; border-radius: 3px; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{total_reports}</div>
                <div class="stat-label">ì´ ë³´ê³ ì„œ</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{passed_reports}</div>
                <div class="stat-label">ê²€ì¦ í†µê³¼</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{failed_reports}</div>
                <div class="stat-label">ê²€ì¦ ì‹¤íŒ¨</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{round(passed_reports/total_reports*100) if total_reports > 0 else 0}%</div>
                <div class="stat-label">ì„±ê³µë¥ </div>
            </div>
        </div>
        
        <div class="table-container">
            <h2>ğŸ“‹ ë³´ê³ ì„œ ëª©ë¡ (í´ë¦­í•˜ì—¬ ìƒì„¸ ë³´ê¸°)</h2>
            <table>
                <thead>
                    <tr>
                        <th>#</th>
                        <th>íŒŒì¼ëª…</th>
                        <th>SQL ë¯¸ë¦¬ë³´ê¸°</th>
                        <th>ê²€ì¦ ê²°ê³¼</th>
                        <th>ìš”ì•½</th>
                    </tr>
                </thead>
                <tbody>
                    {table_rows}
                </tbody>
            </table>
        </div>
    </div>
    
    <script>
        function openReport(filename) {{
            window.open(filename, '_blank');
        }}
    </script>
</body>
</html>"""

            report_path.write_text(html_content, encoding="utf-8")

            return f"""ğŸ“Š í†µí•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ“ˆ ìš”ì•½:
â€¢ ì´ ë³´ê³ ì„œ: {total_reports}ê°œ
â€¢ ê²€ì¦ í†µê³¼: {passed_reports}ê°œ ({round(passed_reports/total_reports*100)}%)
â€¢ ê²€ì¦ ì‹¤íŒ¨: {failed_reports}ê°œ ({round(failed_reports/total_reports*100)}%)

ğŸ“„ í†µí•© ë³´ê³ ì„œ: {report_path}

ğŸ’¡ ì‚¬ìš©ë²•: í…Œì´ë¸”ì˜ ê° í–‰ì„ í´ë¦­í•˜ë©´ í•´ë‹¹ ìƒì„¸ ë³´ê³ ì„œê°€ ìƒˆ ì°½ì—ì„œ ì—´ë¦½ë‹ˆë‹¤."""

        except Exception as e:
            return f"í†µí•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def extract_key_metrics_from_csv(self, csv_filename: str) -> dict:
        """CSV íŒŒì¼ì—ì„œ ì§ì ‘ í•µì‹¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜"""
        import csv
        import statistics

        metrics = {}

        try:
            # CSV íŒŒì¼ ì½ê¸°
            csv_path = DATA_DIR / csv_filename

            with open(csv_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                data = list(reader)

            if not data:
                raise ValueError("CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í†µê³„ ê³„ì‚°
            def calculate_stats(column_name):
                values = []
                for row in data:
                    if column_name in row and row[column_name]:
                        try:
                            values.append(float(row[column_name]))
                        except ValueError:
                            continue

                if values:
                    return {
                        "mean": statistics.mean(values),
                        "min": min(values),
                        "max": max(values),
                    }
                return {"mean": 0.0, "min": 0.0, "max": 0.0}

            # CPU ì‚¬ìš©ë¥ 
            cpu_stats = calculate_stats("CPUUtilization")
            metrics["cpu_mean"] = cpu_stats["mean"]
            metrics["cpu_min"] = cpu_stats["min"]
            metrics["cpu_max"] = cpu_stats["max"]

            # DB Load
            dbload_stats = calculate_stats("DBLoad")
            metrics["dbload_mean"] = dbload_stats["mean"]
            metrics["dbload_min"] = dbload_stats["min"]
            metrics["dbload_max"] = dbload_stats["max"]

            # ì—°ê²° ìˆ˜
            conn_stats = calculate_stats("DatabaseConnections")
            metrics["connections_mean"] = conn_stats["mean"]
            metrics["connections_min"] = conn_stats["min"]
            metrics["connections_max"] = conn_stats["max"]

            # Read IOPS
            read_stats = calculate_stats("ReadIOPS")
            metrics["read_iops_mean"] = read_stats["mean"]
            metrics["read_iops_min"] = read_stats["min"]
            metrics["read_iops_max"] = read_stats["max"]

            # Write IOPS
            write_stats = calculate_stats("WriteIOPS")
            metrics["write_iops_mean"] = write_stats["mean"]
            metrics["write_iops_min"] = write_stats["min"]
            metrics["write_iops_max"] = write_stats["max"]

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚°
            memory_stats = calculate_stats("FreeableMemory")
            if memory_stats["mean"] > 0:
                total_memory = 8 * 1024 * 1024 * 1024  # 8GB
                metrics["memory_usage_mean"] = (
                    (total_memory - memory_stats["mean"]) / total_memory * 100
                )
                metrics["memory_usage_min"] = (
                    (total_memory - memory_stats["max"]) / total_memory * 100
                )
                metrics["memory_usage_max"] = (
                    (total_memory - memory_stats["min"]) / total_memory * 100
                )
            else:
                metrics["memory_usage_mean"] = 0.0
                metrics["memory_usage_min"] = 0.0
                metrics["memory_usage_max"] = 0.0

            print(f"DEBUG: Extracted metrics from CSV: {metrics}", file=sys.stderr)

        except Exception as e:
            print(f"Error reading CSV file {csv_filename}: {e}", file=sys.stderr)
            # ê¸°ë³¸ê°’ ì„¤ì •
            for key in [
                "cpu_mean",
                "cpu_min",
                "cpu_max",
                "dbload_mean",
                "dbload_min",
                "dbload_max",
                "connections_mean",
                "connections_min",
                "connections_max",
                "read_iops_mean",
                "read_iops_min",
                "read_iops_max",
                "write_iops_mean",
                "write_iops_min",
                "write_iops_max",
                "memory_usage_mean",
                "memory_usage_min",
                "memory_usage_max",
            ]:
                metrics[key] = 0.0

        return metrics

    def extract_key_metrics(self, summary_text: str) -> dict:
        """í•µì‹¬ ë©”íŠ¸ë¦­ë§Œ ì¶”ì¶œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
        import re

        # í†µê³„ í…Œì´ë¸”ì—ì„œ í•µì‹¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = {}

        # ì‹¤ì œ pandas describe() ì¶œë ¥ì—ì„œ ì§ì ‘ ê°’ ì¶”ì¶œ
        lines = summary_text.split("\n")

        # mean, min, max ë¼ì¸ì„ ì°¾ì•„ì„œ ê°’ë“¤ì„ ì¶”ì¶œ
        for line in lines:
            # mean ë¼ì¸ ì²˜ë¦¬
            if line.strip().startswith("mean") and "CPUUtilization" in summary_text:
                # ê³µë°±ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ìˆ«ì ê°’ë“¤ë§Œ ì¶”ì¶œ
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics["cpu_mean"] = float(parts[2])  # CPUUtilization
                        metrics["dbload_mean"] = float(parts[3])  # DBLoad
                        metrics["connections_mean"] = float(
                            parts[6]
                        )  # DatabaseConnections
                        freeable_memory_mean = float(
                            parts[7]
                        )  # FreeableMemory (scientific notation)
                        metrics["read_iops_mean"] = float(parts[10])  # ReadIOPS
                        metrics["write_iops_mean"] = float(parts[12])  # WriteIOPS

                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° (8GB ê°€ì •)
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics["memory_usage_mean"] = (
                            (total_memory - freeable_memory_mean) / total_memory * 100
                        )
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing mean line: {e}", file=sys.stderr)

            # min ë¼ì¸ ì²˜ë¦¬
            elif line.strip().startswith("min") and "CPUUtilization" in summary_text:
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics["cpu_min"] = float(parts[2])
                        metrics["dbload_min"] = float(parts[3])
                        metrics["connections_min"] = float(parts[6])
                        freeable_memory_max = float(
                            parts[7]
                        )  # min freeable = max usage
                        metrics["read_iops_min"] = float(parts[10])
                        metrics["write_iops_min"] = float(parts[12])

                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìµœëŒ€ê°’ ê³„ì‚°
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics["memory_usage_max"] = (
                            (total_memory - freeable_memory_max) / total_memory * 100
                        )
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing min line: {e}", file=sys.stderr)

            # max ë¼ì¸ ì²˜ë¦¬
            elif line.strip().startswith("max") and "CPUUtilization" in summary_text:
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics["cpu_max"] = float(parts[2])
                        metrics["dbload_max"] = float(parts[3])
                        metrics["connections_max"] = float(parts[6])
                        freeable_memory_min = float(
                            parts[7]
                        )  # max freeable = min usage
                        metrics["read_iops_max"] = float(parts[10])
                        metrics["write_iops_max"] = float(parts[12])

                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìµœì†Œê°’ ê³„ì‚°
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics["memory_usage_min"] = (
                            (total_memory - freeable_memory_min) / total_memory * 100
                        )
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing max line: {e}", file=sys.stderr)

        return metrics

    def format_metrics_as_html(self, metrics: dict) -> str:
        """ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ë¥¼ HTMLë¡œ í¬ë§· (Week 3: ReportGeneratorë¡œ ìœ„ì„)"""
        return self.report_generator.format_metrics_as_html(metrics)

    def convert_urls_to_html_links(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ì˜ URLì„ HTML ë§í¬ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ëª…ì„ ë§í¬ë¡œ ë§Œë“¦ (Week 3: ReportGeneratorë¡œ ìœ„ì„)"""
        return self.report_generator.convert_urls_to_html_links(text)

    async def generate_comprehensive_performance_report(
        self,
        database_secret: str,
        db_instance_identifier: str,
        region: Optional[str] = None,
        hours: int = 24,
    ) -> str:
        """ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„±"""
        # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_performance_{db_instance_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        debug_log(
            f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì‹œì‘ - ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}"
        )

        try:
            # regionì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ í”„ë¡œíŒŒì¼ì˜ ê¸°ë³¸ ë¦¬ì „ ì‚¬ìš©
            if region is None:
                region = self.default_region

            # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸ ë° ì¸ìŠ¤í„´ìŠ¤ identifier ë³€í™˜
            original_identifier = db_instance_identifier
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_log(f"ë¦¬ì „ ì„¤ì •: {region}, íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")

            # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            debug_log("ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘")
            metrics_result = await self.collect_db_metrics(
                db_instance_identifier, hours, None, region
            )
            if "ì˜¤ë¥˜" in metrics_result:
                debug_log(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {metrics_result}")
                return f"âŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {metrics_result}"

            debug_log("ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")

            # CSV íŒŒì¼ëª… ë° Pre-signed URL ì¶”ì¶œ
            csv_file = None
            presigned_url = None
            for line in metrics_result.split("\n"):
                if "S3 ì €ì¥ ìœ„ì¹˜:" in line:
                    csv_file = line.split(": ")[-1]
                elif "ë‹¤ìš´ë¡œë“œ URL" in line and "https://" in line:
                    presigned_url = line.split(": ", 1)[-1]

            if not csv_file:
                debug_log("ë©”íŠ¸ë¦­ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return "âŒ ë©”íŠ¸ë¦­ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

            csv_filename = Path(csv_file).name
            debug_log(f"CSV íŒŒì¼ëª…: {csv_filename}")
            if presigned_url:
                debug_log(f"Pre-signed URL ì¶”ì¶œ ì„±ê³µ")

            # 2. ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘ ë° íŒŒì¼ ì¶”ì 
            debug_log("ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œì‘")
            generated_files = []  # ìƒì„±ëœ íŒŒì¼ë“¤ ì¶”ì 

            slow_queries = await self.collect_slow_queries(database_secret)
            cpu_queries = await self.collect_cpu_intensive_queries(
                database_secret, db_instance_identifier, None, None
            )
            temp_queries = await self.collect_temp_space_intensive_queries(
                database_secret, db_instance_identifier, None, None
            )

            # URLì„ HTML ë§í¬ë¡œ ë³€í™˜
            slow_queries = self.convert_urls_to_html_links(slow_queries)
            cpu_queries = self.convert_urls_to_html_links(cpu_queries)
            temp_queries = self.convert_urls_to_html_links(temp_queries)

            # SQL íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ì 
            sql_files = list(Path("sql").glob("*.sql")) if Path("sql").exists() else []
            for sql_file in sql_files:
                if any(
                    keyword in sql_file.name.lower()
                    for keyword in ["slow", "cpu", "memory", "temp"]
                ):
                    generated_files.append(
                        {
                            "name": sql_file.name,
                            "type": "SQL ì¿¼ë¦¬",
                            "path": f"../sql/{sql_file.name}",
                        }
                    )

            debug_log("ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ")

            # 3. ë©”íŠ¸ë¦­ ë¶„ì„ ë° íŒŒì¼ ì¶”ì 
            debug_log("ë©”íŠ¸ë¦­ ë¶„ì„ ì‹œì‘")
            summary = await self.get_metric_summary(csv_filename)

            # í•µì‹¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ - CSV íŒŒì¼ì—ì„œ ì§ì ‘ ê³„ì‚°
            key_metrics_dict = self.extract_key_metrics_from_csv(csv_filename)
            key_metrics_html = self.format_metrics_as_html(key_metrics_dict)

            correlation = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )
            outliers = await self.detect_metric_outliers(
                csv_filename, 2.0, skip_html_report=True
            )

            # ì•„ì›ƒë¼ì´ì–´ ê²°ê³¼ì—ì„œ ë§í¬ ë¶€ë¶„ ì œê±° (ì¢…í•©ë³´ê³ ì„œìš©)
            if "ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ:" in outliers:
                outliers_lines = outliers.split("\n")
                filtered_lines = []
                skip_next = False
                for line in outliers_lines:
                    if "ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ:" in line:
                        break  # ì´ ë¼ì¸ë¶€í„°ëŠ” ëª¨ë‘ ì œì™¸
                    filtered_lines.append(line)
                outliers = "\n".join(filtered_lines).strip()

            # ì„ê³„ê°’ ëª¨ë‹¬ HTML ìƒì„±
            metric_thresholds = self.load_metric_thresholds()
            threshold_modal_html = self.generate_threshold_html(metric_thresholds)

            # ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì¶”ì 
            data_files = (
                list(Path("data").glob("*.csv")) if Path("data").exists() else []
            )
            for data_file in data_files:
                if db_instance_identifier in data_file.name:
                    generated_files.append(
                        {
                            "name": data_file.name,
                            "type": "ë¶„ì„ ë°ì´í„°",
                            "path": f"../data/{data_file.name}",
                        }
                    )

            debug_log("ë©”íŠ¸ë¦­ ë¶„ì„ ì™„ë£Œ")

            # 4. ìƒê´€ê´€ê³„ ë¶„ì„ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
            correlation_analysis = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )

            # 5. Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
            debug_log("Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹œì‘")
            logger.info("Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹œì‘")
            try:
                claude_recommendations = (
                    await self.generate_performance_recommendations_with_claude(
                        summary,
                        correlation_analysis,
                        outliers,
                        slow_queries,
                        cpu_queries,
                        temp_queries,
                        database_secret,
                    )
                )
                debug_log("Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì™„ë£Œ")
                logger.info(
                    f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì™„ë£Œ: {len(claude_recommendations.get('immediate_improvements', []))}ê°œ ê°œì„ ì‚¬í•­, {len(claude_recommendations.get('action_items', []))}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                )
            except Exception as e:
                debug_log(f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
                logger.error(f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ ì‚¬ìš©: {e}")
                claude_recommendations = self._get_default_recommendations()

            # 6. HTML ë³´ê³ ì„œ ìƒì„±
            debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            report_path = (
                OUTPUT_DIR
                / f"comprehensive_performance_report_{db_instance_identifier}_{timestamp}.html"
            )

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ - {db_instance_identifier}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; text-align: center; }}
        .header h1 {{ font-size: 2.5em; margin-bottom: 10px; }}
        .header .subtitle {{ font-size: 1.2em; opacity: 0.9; }}
        .section {{ background: white; margin-bottom: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); overflow: hidden; }}
        .section-header {{ background: #2c3e50; color: white; padding: 20px; font-size: 1.3em; font-weight: bold; }}
        .section-content {{ padding: 25px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 25px; }}
        .metric-card {{ background: #f8f9fa; border-left: 4px solid #3498db; padding: 20px; border-radius: 5px; }}
        .metric-card.warning {{ border-left-color: #f39c12; }}
        .metric-card.danger {{ border-left-color: #e74c3c; }}
        .metric-card.success {{ border-left-color: #27ae60; }}
        .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 10px; }}
        .metric-value {{ font-size: 1.8em; font-weight: bold; color: #3498db; }}
        .metric-unit {{ font-size: 0.9em; color: #7f8c8d; }}
        .query-box {{ background: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; font-family: 'Courier New', monospace; font-size: 0.9em; overflow-x: auto; }}
        .status-good {{ color: #27ae60; font-weight: bold; }}
        .status-warning {{ color: #f39c12; font-weight: bold; }}
        .status-critical {{ color: #e74c3c; font-weight: bold; }}
        .table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .table th, .table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .table th {{ background: #34495e; color: white; }}
        .table tr:hover {{ background: #f5f5f5; }}
        .recommendation {{ background: #e8f5e9; border-left: 4px solid #4caf50; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .issue {{ background: #ffebee; border-left: 4px solid #f44336; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .info-box {{ background: #e3f2fd; border-left: 4px solid #2196f3; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .file-link {{ color: #007bff; text-decoration: none; font-weight: bold; }}
        .file-link:hover {{ text-decoration: underline; color: #0056b3; }}
        .toc {{ background: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 30px; }}
        .toc ul {{ list-style: none; }}
        .toc li {{ margin: 5px 0; }}
        .toc a {{ color: #3498db; text-decoration: none; }}
        .toc a:hover {{ text-decoration: underline; }}
        .btn {{ background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 14px; }}
        .btn:hover {{ background: #0056b3; }}
        .modal {{ display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0,0,0,0.4); }}
        .modal-content {{ background-color: #fefefe; margin: 5% auto; padding: 20px; border: none; border-radius: 10px; width: 80%; max-width: 800px; box-shadow: 0 4px 20px rgba(0,0,0,0.3); }}
        .close {{ color: #aaa; float: right; font-size: 28px; font-weight: bold; cursor: pointer; }}
        .close:hover, .close:focus {{ color: black; text-decoration: none; }}
        .threshold-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .threshold-table th, .threshold-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .threshold-table th {{ background: #f8f9fa; font-weight: bold; }}
        .threshold-table tr:hover {{ background: #f5f5f5; }}
        @media (max-width: 768px) {{
            .container {{ padding: 10px; }}
            .header {{ padding: 20px; }}
            .header h1 {{ font-size: 2em; }}
            .metric-grid {{ grid-template-columns: 1fr; }}
            .modal-content {{ width: 95%; margin: 10% auto; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ—„ï¸ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ</h1>
            <div class="subtitle">ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„</div>
            <div style="margin-top: 15px; font-size: 1em;">
                <strong>ì¸ìŠ¤í„´ìŠ¤:</strong> {db_instance_identifier} | 
                <strong>ë¦¬ì „:</strong> {region} | 
                <strong>ë¶„ì„ ê¸°ê°„:</strong> {hours}ì‹œê°„ | 
                <strong>ìƒì„±ì¼ì‹œ:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            </div>
        </div>

        <div class="toc">
            <h3>ğŸ“‹ ëª©ì°¨</h3>
            <ul>
                <li><a href="#executive-summary">1. ìš”ì•½ ì •ë³´</a></li>
                <li><a href="#performance-metrics">2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„</a></li>
                <li><a href="#correlation-analysis">3. ìƒê´€ê´€ê³„ ë¶„ì„</a></li>
                <li><a href="#outlier-analysis">4. ì´ìƒ ì§•í›„ ë¶„ì„</a></li>
                <li><a href="#slow-queries">5. ëŠë¦° ì¿¼ë¦¬ ë¶„ì„</a></li>
                <li><a href="#resource-intensive">6. ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬</a></li>
                <li><a href="#recommendations">7. ìµœì í™” ê¶Œì¥ì‚¬í•­</a></li>
            </ul>
        </div>

        <div class="section" id="executive-summary">
            <div class="section-header">ğŸ“Š 1. ìš”ì•½ ì •ë³´ (Executive Summary)</div>
            <div class="section-content">
                <div class="info-box">
                    <strong>ë¶„ì„ ê°œìš”:</strong> {hours}ì‹œê°„ ë™ì•ˆì˜ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¢…í•© ì§„ë‹¨ ê²°ê³¼ì…ë‹ˆë‹¤.
                </div>
                
                <h4>ğŸ“Š ì¸ìŠ¤í„´ìŠ¤ ì •ë³´</h4>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">ğŸ—„ï¸ ì¸ìŠ¤í„´ìŠ¤ ID</div>
                        <div class="metric-value">{db_instance_identifier}</div>
                        <div class="metric-unit">{region}</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ“… ë¶„ì„ ê¸°ê°„</div>
                        <div class="metric-value">{hours}</div>
                        <div class="metric-unit">ì‹œê°„</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸</div>
                        <div class="metric-value">288</div>
                        <div class="metric-unit">ê°œ</div>
                    </div>
                </div>
                
                <h4>ğŸ“ˆ í•µì‹¬ ì„±ëŠ¥ í†µê³„</h4>
                {key_metrics_html}
            </div>
        </div>

        <div class="section" id="performance-metrics">
            <div class="section-header">ğŸ“ˆ 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„</div>
            <div class="section-content">
                <h4>ğŸ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ</h4>
                <div class="metric-grid">
                    <div class="metric-card success">
                        <div class="metric-title">ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ</div>
                        <div class="metric-value">ì™„ë£Œ</div>
                        <div class="metric-unit">ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„±ê³µ</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ“… ë¶„ì„ ê¸°ê°„</div>
                        <div class="metric-value">{hours}</div>
                        <div class="metric-unit">ì‹œê°„</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ—„ï¸ ì¸ìŠ¤í„´ìŠ¤</div>
                        <div class="metric-value">{db_instance_identifier}</div>
                        <div class="metric-unit">{region}</div>
                    </div>
                </div>
                
                <h4>ğŸ“Š ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´</h4>
                <div class="info-box">
                    {"ë©”íŠ¸ë¦­ ë°ì´í„°ëŠ” <strong><a href='" + presigned_url + "' target='_blank'>" + csv_filename + "</a></strong> íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (ë‹¤ìš´ë¡œë“œ ë§í¬ëŠ” 7ì¼ê°„ ìœ íš¨)." if presigned_url else "ë©”íŠ¸ë¦­ ë°ì´í„°ëŠ” <strong>" + csv_filename + "</strong> íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
                </div>
            </div>
        </div>

        <div class="section" id="correlation-analysis">
            <div class="section-header">ğŸ”— 3. ìƒê´€ê´€ê³„ ë¶„ì„</div>
            <div class="section-content">
                <h4>ğŸ“ˆ ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{correlation}</pre>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ ìƒê´€ê´€ê³„ ì¸ì‚¬ì´íŠ¸:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>ë†’ì€ ìƒê´€ê´€ê³„(r > 0.7)ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì€ í•¨ê»˜ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤</li>
                        <li>CPU ì‚¬ìš©ë¥ ê³¼ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ìµœì í™”í•˜ì„¸ìš”</li>
                        <li>ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ê³¼ I/O ë©”íŠ¸ë¦­ì˜ ê´€ê³„ë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ì„¸ìš”</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="outlier-analysis">
            <div class="section-header">ğŸš¨ 4. ì´ìƒ ì§•í›„ ë¶„ì„ (Outlier Detection)</div>
            <div class="section-content">
                <h4>âš ï¸ ë°œê²¬ëœ ì•„ì›ƒë¼ì´ì–´</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{outliers}</pre>
                
                <div style="margin: 15px 0;">
                    <button class="btn" onclick="document.getElementById('thresholdModal').style.display='block'">
                        ğŸ“Š ì„ê³„ê°’ ì„¤ì • ë³´ê¸°
                    </button>
                </div>
                
                <div class="issue">
                    <strong>ğŸ” ì£¼ì˜ì‚¬í•­:</strong> ì•„ì›ƒë¼ì´ì–´ê°€ ë°œê²¬ëœ ì‹œì ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ì™€ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.
                </div>
            </div>
        </div>

        <div class="section" id="slow-queries">
            <div class="section-header">ğŸŒ 5. Slow ì¿¼ë¦¬ ë¶„ì„ (Slow Query Analysis)</div>
            <div class="section-content">
                <h4>ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼</h4>
                <div class="query-box">{slow_queries}</div>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ Slow ì¿¼ë¦¬ ìµœì í™” ê°€ì´ë“œ:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>ì¸ë±ìŠ¤ ì¶”ê°€ ë˜ëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤ ìµœì í™”</li>
                        <li>WHERE ì ˆ ì¡°ê±´ ìˆœì„œ ìµœì í™”</li>
                        <li>JOIN ì¡°ê±´ ë° ìˆœì„œ ê²€í† </li>
                        <li>ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš(EXPLAIN) ë¶„ì„</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="resource-intensive">
            <div class="section-header">ğŸ’¾ 6. ë¦¬ì†ŒìŠ¤ ì†Œëª¨ê°€ í° ì¿¼ë¦¬ ë¶„ì„</div>
            <div class="section-content">
                <h4>âš¡ CPU ì†Œë¹„ê°€ ë§ì€ ì¿¼ë¦¬</h4>
                <div class="query-box">{cpu_queries}</div>
                
                <h4>ğŸ’¿ ì„ì‹œ ê³µê°„ì„ ë§ì´ ì†Œë¹„í•˜ëŠ” ì¿¼ë¦¬</h4>
                <div class="query-box">{temp_queries}</div>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì „ëµ:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li><strong>CPU:</strong> ë³µì¡í•œ ì—°ì‚° ìµœì í™”, í•¨ìˆ˜ ì‚¬ìš© ìµœì†Œí™”</li>
                        <li><strong>ì„ì‹œ ê³µê°„:</strong> GROUP BY, ORDER BY ìµœì í™”, ì¸ë±ìŠ¤ í™œìš©</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="recommendations">
            <div class="section-header">ğŸ¯ 7. ìµœì í™” ê¶Œì¥ì‚¬í•­</div>
            <div class="section-content">
                <div class="info-box" style="margin-bottom: 20px;">
                    <strong>ğŸ¤– AI ê¸°ë°˜ ë¶„ì„:</strong> ì´ ê¶Œì¥ì‚¬í•­ë“¤ì€ Claude AIê°€ ì‹¤ì œ ì„±ëŠ¥ ë©”íŠ¸ë¦­, ìƒê´€ê´€ê³„ ë¶„ì„, ëŠë¦° ì¿¼ë¦¬ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìƒì„±í•œ ë§ì¶¤í˜• ì œì•ˆì…ë‹ˆë‹¤.
                </div>
                {self._generate_recommendations_html(claude_recommendations)}
            </div>
        </div>

        <div class="section">
            <div class="section-content">
                <div style="text-align: center; margin-top: 30px; padding: 30px; color: #7f8c8d; font-size: 0.9em;">
                    <em>ì´ ë³´ê³ ì„œëŠ” {db_instance_identifier} ì¸ìŠ¤í„´ìŠ¤ì˜ {hours}ì‹œê°„ ë™ì•ˆì˜ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
                    ì •í™•í•œ ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 1ì£¼ì¼ ì´ìƒì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</em>
                </div>
            </div>
        </div>
    </div>
    
    <!-- ì„ê³„ê°’ ì„¤ì • ëª¨ë‹¬ -->
    {threshold_modal_html}
    
    <script>
        // ëª¨ë‹¬ ì œì–´ JavaScript
        var modal = document.getElementById('thresholdModal');
        var span = document.getElementsByClassName('close')[0];
        
        span.onclick = function() {{
            modal.style.display = 'none';
        }}
        
        window.onclick = function(event) {{
            if (event.target == modal) {{
                modal.style.display = 'none';
            }}
        }}
    </script>
</body>
</html>"""

            # HTML íŒŒì¼ ì €ì¥
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"HTML ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_path}")

            return f"""ğŸ—„ï¸ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë³´ê³ ì„œ**
â€¢ ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}
â€¢ ë¦¬ì „: {region}
â€¢ ë¶„ì„ ê¸°ê°„: {hours}ì‹œê°„
â€¢ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ **ìƒì„±ëœ íŒŒì¼ë“¤:**
â€¢ ì¢…í•© ë³´ê³ ì„œ: {report_path.name}
â€¢ ë©”íŠ¸ë¦­ ë°ì´í„°: <a href="file://{DATA_DIR / csv_filename}" target="_blank">{csv_filename}</a>
â€¢ ìƒê´€ê´€ê³„ ë¶„ì„: í¬í•¨ë¨

ğŸ“ˆ **í¬í•¨ëœ ë¶„ì„:**
âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½ ë° í†µê³„
âœ… ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
âœ… ì´ìƒ ì§•í›„(ì•„ì›ƒë¼ì´ì–´) íƒì§€
âœ… ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘ ë° ë¶„ì„
âœ… ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬ ë¶„ì„ (CPU, ë©”ëª¨ë¦¬, ì„ì‹œê³µê°„)
âœ… ìµœì í™” ê¶Œì¥ì‚¬í•­ ë° ì•¡ì…˜ ì•„ì´í…œ
âœ… ë°˜ì‘í˜• HTML ë””ìì¸

ğŸ’¡ **ì£¼ìš” íŠ¹ì§•:**
â€¢ ëª¨ë°”ì¼ ìµœì í™”ëœ ë°˜ì‘í˜• ë””ìì¸
â€¢ ìƒì„¸í•œ ë©”íŠ¸ë¦­ ë¶„ì„ ë° ì‹œê°í™”
â€¢ ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì í™” ê¶Œì¥ì‚¬í•­
â€¢ ìš°ì„ ìˆœìœ„ë³„ ì•¡ì…˜ ì•„ì´í…œ ì œê³µ

ğŸ” ë³´ê³ ì„œë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.
ğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"""

        except Exception as e:
            debug_log(f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            logger.error(f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return (
                f"âŒ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}\nğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"
            )

    async def generate_cluster_performance_report(
        self,
        database_secret: str,
        db_cluster_identifier: str,
        hours: int = 24,
        region: str = "ap-northeast-2",
    ) -> str:
        """
        Aurora í´ëŸ¬ìŠ¤í„° ì „ìš© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        - í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ (ë¶€í•˜ ë¶„ì‚°, ë ˆí”Œë¦¬ì¼€ì´ì…˜ ì§€ì—°, HLL ë“±)
        - ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ ë§í¬ ì œê³µ
        - Writer/Reader ì—­í• ë³„ ë¹„êµ ë¶„ì„
        """
        # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_cluster_{db_cluster_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        try:
            debug_log(f"í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {db_cluster_identifier}")

            # 1. database_secretì—ì„œ ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì°¾ê¸°
            debug_log("RDS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
            rds_client = boto3.client("rds", region_name=region)

            # Secretì—ì„œ í˜¸ìŠ¤íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            debug_log("Secret ì •ë³´ ì¡°íšŒ")
            secrets_client = boto3.client(
                "secretsmanager", region_name=region, verify=False
            )
            get_secret_value_response = secrets_client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            secret_info = json.loads(secret)
            host = secret_info.get("host", "")
            debug_log(f"í˜¸ìŠ¤íŠ¸ ì •ë³´: {host}")

            # í˜¸ìŠ¤íŠ¸ ì •ë³´ë¡œ ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
            actual_cluster_id = None
            if host:
                # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì¡°íšŒí•´ì„œ ì—”ë“œí¬ì¸íŠ¸ ë§¤ì¹­
                debug_log("í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ ë° ë§¤ì¹­")
                clusters = rds_client.describe_db_clusters()["DBClusters"]
                for cluster in clusters:
                    if cluster.get("Endpoint", "") in host or host in cluster.get(
                        "Endpoint", ""
                    ):
                        actual_cluster_id = cluster["DBClusterIdentifier"]
                        break

            # ì‹¤ì œ í´ëŸ¬ìŠ¤í„° IDê°€ ì—†ìœ¼ë©´ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ ê°’ ì‚¬ìš©
            if not actual_cluster_id:
                actual_cluster_id = db_cluster_identifier

            debug_log(f"ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ID: {actual_cluster_id}")

            cluster_info = rds_client.describe_db_clusters(
                DBClusterIdentifier=actual_cluster_id
            )["DBClusters"][0]

            cluster_members = cluster_info["DBClusterMembers"]
            writer_instance = next(
                (m for m in cluster_members if m["IsClusterWriter"]), None
            )
            reader_instances = [m for m in cluster_members if not m["IsClusterWriter"]]

            debug_log(f"í´ëŸ¬ìŠ¤í„° êµ¬ì„±: Writer 1ê°œ, Reader {len(reader_instances)}ê°œ")

            # 2. ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
            instance_reports = {}
            cluster_metrics = {}

            # Writer ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬
            if writer_instance:
                writer_id = writer_instance["DBInstanceIdentifier"]
                debug_log(f"Writer ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬: {writer_id}")

                # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
                writer_report = await self.generate_comprehensive_performance_report(
                    database_secret, writer_id, region, hours
                )
                instance_reports[writer_id] = {
                    "role": "Writer",
                    "report": writer_report,
                    "is_writer": True,
                }
                debug_log(
                    f"Writer ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ë³´ê³ ì„œ ìƒì„±: {writer_id},{writer_report}"
                )

                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics_result = await self.collect_db_metrics(writer_id, region, hours)
                # ì‹¤ì œ ìƒì„±ëœ íŒŒì¼ëª… ì¶”ì¶œ
                if "ì €ì¥ ìœ„ì¹˜:" in metrics_result:
                    csv_path = metrics_result.split("ì €ì¥ ìœ„ì¹˜: ")[-1].strip()
                    cluster_metrics[writer_id] = csv_path.split("/")[
                        -1
                    ]  # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                else:
                    cluster_metrics[writer_id] = (
                        f"database_metrics_{writer_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            else:
                debug_log("Writer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # Reader ì¸ìŠ¤í„´ìŠ¤ë“¤ ì²˜ë¦¬
            for reader in reader_instances:
                reader_id = reader["DBInstanceIdentifier"]
                debug_log(f"Reader ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬: {reader_id}")

                # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
                reader_report = await self.generate_comprehensive_performance_report(
                    database_secret, reader_id, region, hours
                )
                instance_reports[reader_id] = {
                    "role": "Reader",
                    "report": reader_report,
                    "is_writer": False,
                }

                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics_result = await self.collect_db_metrics(reader_id, region, hours)
                # ì‹¤ì œ ìƒì„±ëœ íŒŒì¼ëª… ì¶”ì¶œ
                if "ì €ì¥ ìœ„ì¹˜:" in metrics_result:
                    csv_path = metrics_result.split("ì €ì¥ ìœ„ì¹˜: ")[-1].strip()
                    cluster_metrics[reader_id] = csv_path.split("/")[
                        -1
                    ]  # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                else:
                    cluster_metrics[reader_id] = (
                        f"database_metrics_{reader_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            debug_log("í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ ì‹œì‘")
            # 3. í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            cluster_level_metrics = await self._collect_cluster_level_metrics(
                actual_cluster_id, region, hours
            )

            # 4. í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìˆ˜ì§‘ (ìµœê·¼ 7ì¼)
            cluster_events = await self._collect_cluster_events(
                actual_cluster_id, region, 7 * 24  # 7ì¼
            )

            # 5. í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„
            cluster_analysis = await self._analyze_cluster_metrics(
                cluster_metrics, cluster_info, cluster_level_metrics, cluster_events
            )

            # 4. í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ ìƒì„±
            debug_log("í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = (
                f"cluster_performance_report_{actual_cluster_id}_{timestamp}.html"
            )
            report_path = Path.cwd() / "output" / report_filename

            html_content = await self._generate_cluster_html_report(
                cluster_info,
                instance_reports,
                cluster_analysis,
                timestamp,
                cluster_level_metrics,
                cluster_events,
            )

            # ë³´ê³ ì„œ ì €ì¥
            report_path.parent.mkdir(exist_ok=True)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")

            return f"""âœ… Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ—ï¸ í´ëŸ¬ìŠ¤í„° ì •ë³´:
â€¢ í´ëŸ¬ìŠ¤í„° ID: {actual_cluster_id}
â€¢ ì—”ì§„: {cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}
â€¢ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜: {len(cluster_members)}ê°œ (Writer: 1ê°œ, Reader: {len(reader_instances)}ê°œ)
â€¢ ë¶„ì„ ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„

ğŸ“Š ìƒì„±ëœ ë³´ê³ ì„œ:
â€¢ ğŸ¯ í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ: file://{report_path}
â€¢ ğŸ“‹ ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ë³´ê³ ì„œ: {len(instance_reports)}ê°œ

ğŸ” ì£¼ìš” ë¶„ì„ ë‚´ìš©:
â€¢ í´ëŸ¬ìŠ¤í„° ë¶€í•˜ ë¶„ì‚° ìƒíƒœ
â€¢ Writer/Reader ì„±ëŠ¥ ë¹„êµ
â€¢ ë ˆí”Œë¦¬ì¼€ì´ì…˜ ì§€ì—° ë¶„ì„
â€¢ ì¸ìŠ¤í„´ìŠ¤ ê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ
â€¢ í´ëŸ¬ìŠ¤í„° ìµœì í™” ê¶Œì¥ì‚¬í•­

ğŸ“„ í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ ,
   ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë¶„ì„ì€ ë§í¬ë¥¼ í†µí•´ ì ‘ê·¼í•˜ì„¸ìš”.
ğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"""

        except Exception as e:
            debug_log(f"í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"âŒ í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}\nğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"

    async def _collect_cluster_level_metrics(
        self, cluster_id: str, region: str, hours: int
    ) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Lambda ì‚¬ìš©)"""
        try:
            # Lambda í•¨ìˆ˜ í˜¸ì¶œ
            result = await self._call_lambda('collect-cluster-metrics', {
                'cluster_id': cluster_id,
                'region': region,
                'hours': hours
            })

            if result.get('success'):
                # Lambda ì‘ë‹µì—ì„œ metrics ë°ì´í„° ì¶”ì¶œ
                # LambdaëŠ” ISO í˜•ì‹ ë¬¸ìì—´ë¡œ Timestampë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ datetime ê°ì²´ë¡œ ë³€í™˜
                metrics_data = result.get('metrics', {})

                # Timestampë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
                for metric_name, datapoints in metrics_data.items():
                    for dp in datapoints:
                        if 'Timestamp' in dp and isinstance(dp['Timestamp'], str):
                            from dateutil import parser
                            dp['Timestamp'] = parser.parse(dp['Timestamp'])

                return metrics_data
            else:
                error_msg = result.get('error', 'Unknown error')
                logger.error(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨ (Lambda): {error_msg}")
                return {}

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {}

    async def _collect_cluster_events(
        self, cluster_id: str, region: str, hours: int
    ) -> List[Dict]:
        """í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìˆ˜ì§‘ (Lambda ì‚¬ìš©)"""
        try:
            # Lambda í•¨ìˆ˜ í˜¸ì¶œ
            result = await self._call_lambda('collect-cluster-events', {
                'cluster_id': cluster_id,
                'region': region,
                'hours': hours
            })

            if result.get('success'):
                return result.get('events', [])
            else:
                error_msg = result.get('error', 'Unknown error')
                logger.error(f"í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (Lambda): {error_msg}")
                return []

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []

    def _categorize_event_severity(self, message: str) -> str:
        """ì´ë²¤íŠ¸ ë©”ì‹œì§€ ê¸°ë°˜ ì‹¬ê°ë„ ë¶„ë¥˜"""
        message_lower = message.lower()

        if any(
            keyword in message_lower
            for keyword in [
                "failed",
                "error",
                "critical",
                "fatal",
                "crash",
                "corruption",
            ]
        ):
            return "HIGH"
        elif any(
            keyword in message_lower
            for keyword in ["warning", "slow", "timeout", "retry", "restart", "reboot"]
        ):
            return "MEDIUM"
        else:
            return "LOW"

    async def _analyze_cluster_metrics(
        self,
        cluster_metrics: Dict,
        cluster_info: Dict,
        cluster_level_metrics: Dict = None,
        cluster_events: List = None,
    ) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë©”íŠ¸ë¦­ ë¶„ì„"""
        try:
            if not ANALYSIS_AVAILABLE:
                logger.warning("ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                return {"error": "ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            analysis = {
                "load_distribution": {},
                "replication_lag": {},
                "resource_comparison": {},
                "recommendations": [],
            }

            logger.info(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë¶„ì„ ì‹œì‘: {cluster_metrics}")

            # ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ë©”íŠ¸ë¦­ ë¡œë“œ ë° ë¹„êµ - data í´ë”ì—ì„œ ì§ì ‘ ì°¾ê¸°
            metrics_data = {}

            # í´ëŸ¬ìŠ¤í„° ë©¤ë²„ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ID ê°€ì ¸ì˜¤ê¸°
            for member in cluster_info["DBClusterMembers"]:
                instance_id = member["DBInstanceIdentifier"]

                # data í´ë”ì—ì„œ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì˜ ìµœì‹  CSV íŒŒì¼ ì°¾ê¸°
                data_dir = Path("data")
                csv_files = list(data_dir.glob(f"database_metrics_{instance_id}_*.csv"))

                if csv_files:
                    # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ (íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
                    latest_csv = max(csv_files, key=lambda x: x.name.split("_")[-1])
                    logger.info(f"ë©”íŠ¸ë¦­ íŒŒì¼ ë°œê²¬: {instance_id} -> {latest_csv}")

                    try:
                        df = pd.read_csv(latest_csv)
                        metrics_data[instance_id] = df
                        logger.info(
                            f"ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {instance_id} ({len(df)} í–‰)"
                        )
                    except Exception as e:
                        logger.warning(f"ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {latest_csv}: {e}")
                else:
                    logger.warning(f"ë©”íŠ¸ë¦­ íŒŒì¼ ì—†ìŒ: {instance_id}")

            logger.info(f"ë¡œë“œëœ ë©”íŠ¸ë¦­ ë°ì´í„°: {len(metrics_data)}ê°œ ì¸ìŠ¤í„´ìŠ¤")

            if len(metrics_data) >= 2:
                # Writer vs Reader ë¹„êµ
                writer_data = None
                reader_data = []

                for instance_id, df in metrics_data.items():
                    # í´ëŸ¬ìŠ¤í„° ë©¤ë²„ ì •ë³´ì—ì„œ ì—­í•  í™•ì¸
                    is_writer = any(
                        m["DBInstanceIdentifier"] == instance_id
                        and m["IsClusterWriter"]
                        for m in cluster_info["DBClusterMembers"]
                    )

                    logger.info(
                        f"ì¸ìŠ¤í„´ìŠ¤ ì—­í•  í™•ì¸: {instance_id} -> {'Writer' if is_writer else 'Reader'}"
                    )

                    if is_writer:
                        writer_data = df
                    else:
                        reader_data.append((instance_id, df))

                # ë¶€í•˜ ë¶„ì‚° ë¶„ì„
                if writer_data is not None and reader_data:
                    logger.info("ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì‹œì‘")
                    try:
                        analysis["load_distribution"] = self._analyze_load_distribution(
                            writer_data, reader_data
                        )
                        logger.info("ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì˜¤ë¥˜: {e}")
                        analysis["load_distribution"] = {}
                else:
                    logger.warning(
                        f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ë¶ˆê°€: writer_data={writer_data is not None}, reader_data={len(reader_data)}"
                    )

                # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ
                try:
                    logger.info("ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì‹œì‘")
                    analysis["resource_comparison"] = self._compare_resource_usage(
                        metrics_data
                    )
                    logger.info("ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì˜¤ë¥˜: {e}")
                    analysis["resource_comparison"] = {}
            else:
                logger.warning(
                    f"ë¶„ì„ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ë°ì´í„° ë¶€ì¡±: {len(metrics_data)}ê°œ (ìµœì†Œ 2ê°œ í•„ìš”)"
                )

            return analysis

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def _analyze_load_distribution(self, writer_df, reader_data):
        """ë¶€í•˜ ë¶„ì‚° ìƒíƒœ ë¶„ì„"""
        try:
            writer_cpu = (
                writer_df["CPUUtilization"].mean()
                if "CPUUtilization" in writer_df.columns
                else 0
            )
            writer_connections = (
                writer_df["DatabaseConnections"].mean()
                if "DatabaseConnections" in writer_df.columns
                else 0
            )

            reader_stats = []
            for reader_id, reader_df in reader_data:
                reader_cpu = (
                    reader_df["CPUUtilization"].mean()
                    if "CPUUtilization" in reader_df.columns
                    else 0
                )
                reader_connections = (
                    reader_df["DatabaseConnections"].mean()
                    if "DatabaseConnections" in reader_df.columns
                    else 0
                )
                reader_stats.append(
                    {
                        "instance_id": reader_id,
                        "cpu": reader_cpu,
                        "connections": reader_connections,
                    }
                )

            return {
                "writer": {"cpu": writer_cpu, "connections": writer_connections},
                "readers": reader_stats,
                "balance_score": self._calculate_balance_score(
                    writer_cpu, [r["cpu"] for r in reader_stats]
                ),
            }
        except Exception as e:
            logger.error(f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}

    def _calculate_balance_score(self, writer_cpu, reader_cpus):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ ê³„ì‚° (0-100)"""
        if not reader_cpus:
            return 0

        total_cpu = writer_cpu + sum(reader_cpus)
        if total_cpu == 0:
            return 100

        # ì´ìƒì ì¸ ë¶„ì‚°: Writerê°€ ì „ì²´ ë¶€í•˜ì˜ 60-70% ë‹´ë‹¹
        writer_ratio = writer_cpu / total_cpu
        ideal_ratio = 0.65

        deviation = abs(writer_ratio - ideal_ratio)
        score = max(0, 100 - (deviation * 200))  # í¸ì°¨ê°€ í´ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ

        return round(score, 1)

    def _compare_resource_usage(self, metrics_data):
        """ì¸ìŠ¤í„´ìŠ¤ ê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ"""
        try:
            comparison = {}

            for instance_id, df in metrics_data.items():
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° (FreeableMemory ê¸°ë°˜)
                memory_usage_percent = 0
                if "FreeableMemory" in df.columns and not df["FreeableMemory"].empty:
                    freeable_memory = df["FreeableMemory"].mean()
                    # ê°€ì •: ì´ ë©”ëª¨ë¦¬ 16GB (16 * 1024 * 1024 * 1024 bytes)
                    total_memory = 16 * 1024 * 1024 * 1024
                    memory_usage_percent = (
                        (total_memory - freeable_memory) / total_memory
                    ) * 100

                comparison[instance_id] = {
                    "cpu_avg": (
                        df["CPUUtilization"].mean()
                        if "CPUUtilization" in df.columns
                        else 0
                    ),
                    "memory_usage_percent": memory_usage_percent,
                    "connections_avg": (
                        df["DatabaseConnections"].mean()
                        if "DatabaseConnections" in df.columns
                        else 0
                    ),
                    "read_iops": (
                        df["ReadIOPS"].mean() if "ReadIOPS" in df.columns else 0
                    ),
                    "write_iops": (
                        df["WriteIOPS"].mean() if "WriteIOPS" in df.columns else 0
                    ),
                }

            return comparison
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì˜¤ë¥˜: {e}")
            return {}

    async def _generate_cluster_html_report(
        self,
        cluster_info,
        instance_reports,
        cluster_analysis,
        timestamp,
        cluster_level_metrics=None,
        cluster_events=None,
    ):
        """í´ëŸ¬ìŠ¤í„° í†µí•© HTML ë³´ê³ ì„œ ìƒì„± - ê¸°ì¡´ í˜•íƒœë¡œ ë‹¨ìˆœí™”"""

        cluster_id = cluster_info["DBClusterIdentifier"]
        engine_info = f"{cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}"

        # ì¸ìŠ¤í„´ìŠ¤ ë§í¬ ìƒì„±
        instance_links = []
        for instance_id, report_info in instance_reports.items():
            role = report_info["role"]
            report_text = report_info["report"]
            if "comprehensive_performance_report_" in report_text:
                import re

                match = re.search(
                    r"comprehensive_performance_report_[^.]+\.html", report_text
                )
                if match:
                    report_filename = match.group(0)
                    instance_links.append(
                        f"""
                    <tr>
                        <td><span class="role-badge {'writer' if report_info['is_writer'] else 'reader'}">{role}</span></td>
                        <td>{instance_id}</td>
                        <td><a href="{report_filename}" target="_blank" class="detail-link">ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ë³´ê¸°</a></td>
                    </tr>
                    """
                    )

        # ë¶€í•˜ ë¶„ì‚° ë¶„ì„ HTML ìƒì„± (ì œê±°)
        load_analysis_html = ""

        # ë¦¬ì†ŒìŠ¤ ë¹„êµ HTML ìƒì„±
        resource_comparison_html = self._generate_resource_comparison_html(
            cluster_analysis.get("resource_comparison", {})
        )

        return f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ - {cluster_id}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f7fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.2em; }}
        .header .subtitle {{ margin-top: 10px; opacity: 0.9; font-size: 1.1em; }}
        
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; padding: 30px; }}
        .summary-card {{ background: #f8f9fa; border-radius: 8px; padding: 20px; border-left: 4px solid #007bff; }}
        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .summary-card .value {{ font-size: 1.8em; font-weight: bold; color: #007bff; }}
        
        .section {{ margin: 20px 30px; }}
        .section-header {{ background: #e9ecef; padding: 15px; border-radius: 8px; font-weight: bold; font-size: 1.2em; color: #495057; }}
        .section-content {{ padding: 20px 0; }}
        
        .instance-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .instance-table th, .instance-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }}
        .instance-table th {{ background: #f8f9fa; font-weight: bold; }}
        
        .role-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .role-badge.writer {{ background: #d4edda; color: #155724; }}
        .role-badge.reader {{ background: #d1ecf1; color: #0c5460; }}
        
        .detail-link {{ color: #007bff; text-decoration: none; font-weight: bold; }}
        .detail-link:hover {{ text-decoration: underline; }}
        
        .recommendation {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 15px; margin: 15px 0; }}
        .recommendation h4 {{ margin: 0 0 10px 0; color: #856404; }}
        
        .resource-grid {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0; }}
        .resource-card {{ background: #f8f9fa; border-radius: 8px; padding: 15px; }}
        .resource-card h4 {{ margin: 0 0 15px 0; color: #495057; }}
        .metric-bar {{ background: #e9ecef; height: 20px; border-radius: 10px; margin: 5px 0; position: relative; }}
        .metric-fill {{ height: 100%; border-radius: 10px; }}
        .metric-label {{ font-size: 0.9em; color: #6c757d; }}
        
        @media (max-width: 768px) {{
            .summary-grid {{ grid-template-columns: 1fr; }}
            .resource-grid {{ grid-template-columns: 1fr; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ—ï¸ Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ</h1>
            <div class="subtitle">í´ëŸ¬ìŠ¤í„°: {cluster_id} | ì—”ì§„: {engine_info} | ìƒì„±ì¼ì‹œ: {timestamp}</div>
        </div>
        
        <div class="summary-grid">
            <div class="summary-card">
                <h3>ğŸ“Š í´ëŸ¬ìŠ¤í„° êµ¬ì„±</h3>
                <div class="value">{len(cluster_info['DBClusterMembers'])}ê°œ ì¸ìŠ¤í„´ìŠ¤</div>
                <div>Writer: 1ê°œ, Reader: {len([m for m in cluster_info['DBClusterMembers'] if not m['IsClusterWriter']])}ê°œ</div>
            </div>
            <div class="summary-card">
                <h3>ğŸ”„ í´ëŸ¬ìŠ¤í„° ìƒíƒœ</h3>
                <div class="value" style="color: #28a745">{cluster_info.get('Status', 'AVAILABLE')}</div>
                <div>Multi-AZ: {'Yes' if cluster_info.get('MultiAZ', False) else 'No'}</div>
            </div>
            <div class="summary-card">
                <h3>ğŸ” ë³´ì•ˆ ì„¤ì •</h3>
                <div class="value">ğŸ”’</div>
                <div>ì•”í˜¸í™”: {'í™œì„±í™”' if cluster_info.get('StorageEncrypted', False) else 'ë¹„í™œì„±í™”'}</div>
            </div>
            <div class="summary-card">
                <h3>ğŸ’¾ ë°±ì—… ì„¤ì •</h3>
                <div class="value">{cluster_info.get('BackupRetentionPeriod', 0)}ì¼</div>
                <div>ìë™ ë°±ì—…: {'í™œì„±í™”' if cluster_info.get('BackupRetentionPeriod', 0) > 0 else 'ë¹„í™œì„±í™”'}</div>
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">ğŸ“‹ ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ</div>
            <div class="section-content">
                <table class="instance-table">
                    <thead>
                        <tr>
                            <th>ì—­í• </th>
                            <th>ì¸ìŠ¤í„´ìŠ¤ ID</th>
                            <th>ìƒì„¸ ë¶„ì„</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(instance_links)}
                    </tbody>
                </table>
                <div class="recommendation">
                    <h4>ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ</h4>
                    <p>ê° ì¸ìŠ¤í„´ìŠ¤ì˜ "ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ë³´ê¸°" ë§í¬ë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì˜ ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                </div>
            </div>
        </div>
        
        {load_analysis_html}
        
        {resource_comparison_html}
    </div>
</body>
</html>"""

    def _generate_load_analysis_html(self, load_distribution: Dict) -> str:
        """ë¶€í•˜ ë¶„ì‚° ë¶„ì„ HTML ìƒì„± (ë¹„í™œì„±í™”)"""
        return ""

    def _generate_resource_comparison_html(self, resource_comparison: Dict) -> str:
        """ë¦¬ì†ŒìŠ¤ ë¹„êµ HTML ìƒì„±"""
        if not resource_comparison:
            return ""

        cards_html = ""
        for instance_id, metrics in resource_comparison.items():
            cpu_usage = metrics.get("cpu_avg", 0)
            memory_usage = metrics.get("memory_usage_percent", 0)
            connections_avg = metrics.get("connections_avg", 0)
            read_iops = metrics.get("read_iops", 0)
            write_iops = metrics.get("write_iops", 0)

            cards_html += f"""
            <div class="resource-card">
                <h4>{instance_id}</h4>
                <div class="metric-label">CPU ì‚¬ìš©ë¥ : {cpu_usage:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(cpu_usage, 100)}%; background: #007bff;"></div>
                </div>
                
                <div class="metric-label">ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_usage:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(memory_usage, 100)}%; background: #28a745;"></div>
                </div>
                
                <div class="metric-label">í‰ê·  ì—°ê²° ìˆ˜: {connections_avg:.1f}</div>
                <div class="metric-label">Read IOPS: {read_iops:.1f}</div>
                <div class="metric-label">Write IOPS: {write_iops:.1f}</div>
            </div>"""

        return f"""
        <div class="section">
            <div class="section-header">ğŸ“Š ì¸ìŠ¤í„´ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ</div>
            <div class="section-content">
                <div class="resource-grid">
                    {cards_html}
                </div>
            </div>
        </div>"""

    def _generate_cluster_metrics_table(self, cluster_metrics: Dict) -> str:
        """í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ í‘œ HTML ìƒì„±"""
        if not cluster_metrics:
            return '<div class="no-data">í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</div>'

        # ì£¼ìš” ë©”íŠ¸ë¦­ë“¤ì— ëŒ€í•œ í†µê³„ ê³„ì‚°
        important_metrics = [
            ("CPUUtilization", "CPU ì‚¬ìš©ë¥ ", "%"),
            ("FreeableMemory", "ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬", "MB"),
            ("ReadIOPS", "ì½ê¸° IOPS", "IOPS"),
            ("WriteIOPS", "ì“°ê¸° IOPS", "IOPS"),
            ("DatabaseConnections", "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜", "ê°œ"),
            ("AuroraReplicaLag", "Aurora ë³µì œ ì§€ì—°", "ms"),
        ]

        table_html = """
        <table class="metrics-table">
            <thead>
                <tr>
                    <th>ë©”íŠ¸ë¦­</th>
                    <th>í‰ê· </th>
                    <th>ìµœëŒ€ê°’</th>
                    <th>ìµœì†Œê°’</th>
                    <th>ë‹¨ìœ„</th>
                </tr>
            </thead>
            <tbody>
        """

        for metric_name, display_name, unit in important_metrics:
            if metric_name in cluster_metrics:
                datapoints = cluster_metrics[metric_name]

                if datapoints:
                    avg_values = [float(point["Average"]) for point in datapoints]
                    max_values = [float(point["Maximum"]) for point in datapoints]
                    min_values = [float(point["Minimum"]) for point in datapoints]

                    overall_avg = sum(avg_values) / len(avg_values)
                    overall_max = max(max_values)
                    overall_min = min(min_values)

                    # ë©”ëª¨ë¦¬ëŠ” MB ë‹¨ìœ„ë¡œ ë³€í™˜
                    if metric_name == "FreeableMemory":
                        overall_avg = overall_avg / (1024 * 1024)
                        overall_max = overall_max / (1024 * 1024)
                        overall_min = overall_min / (1024 * 1024)

                    table_html += f"""
                    <tr>
                        <td class="metric-name">{display_name}</td>
                        <td class="metric-value">{overall_avg:.2f}</td>
                        <td class="metric-value">{overall_max:.2f}</td>
                        <td class="metric-value">{overall_min:.2f}</td>
                        <td>{unit}</td>
                    </tr>
                    """

        table_html += "</tbody></table>"

        if len([m for m, _, _ in important_metrics if m in cluster_metrics]) == 0:
            return '<div class="no-data">í‘œì‹œí•  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</div>'

        return table_html

    def _generate_events_table(self, events: List[Dict]) -> str:
        """ì´ë²¤íŠ¸ í…Œì´ë¸” HTML ìƒì„±"""
        if not events:
            return '<div class="no-data">ìµœê·¼ 7ì¼ê°„ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.</div>'

        # ìµœê·¼ 20ê°œ ì´ë²¤íŠ¸ë§Œ í‘œì‹œ
        recent_events = events[:20]

        table_html = """
        <table class="events-table">
            <thead>
                <tr>
                    <th>ì¼ì‹œ</th>
                    <th>ì‹¬ê°ë„</th>
                    <th>ì†ŒìŠ¤</th>
                    <th>ë©”ì‹œì§€</th>
                    <th>ì¹´í…Œê³ ë¦¬</th>
                </tr>
            </thead>
            <tbody>
        """

        for event in recent_events:
            severity_class = event["severity"].lower()
            categories = ", ".join(event.get("event_categories", []))

            table_html += f"""
            <tr>
                <td>{event['date']}</td>
                <td><span class="severity-badge {event['severity']}">{event['severity']}</span></td>
                <td>{event.get('source_id', 'N/A')}</td>
                <td>{event['message']}</td>
                <td>{categories}</td>
            </tr>
            """

        table_html += "</tbody></table>"

        if len(events) > 20:
            table_html += f'<div style="text-align: center; margin-top: 10px; color: #6c757d;">ì´ {len(events)}ê°œ ì´ë²¤íŠ¸ ì¤‘ ìµœê·¼ 20ê°œë§Œ í‘œì‹œ</div>'

        return table_html

    def _generate_chart_scripts(self, cluster_metrics: Dict) -> str:
        """ì°¨íŠ¸ ìƒì„± ë¹„í™œì„±í™” - ê°„ë‹¨í•œ ë©”íŠ¸ë¦­ í‘œì‹œë§Œ"""
        return ""

    def _get_balance_color(self, score):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ ë°˜í™˜"""
        if score >= 80:
            return "#28a745"  # ë…¹ìƒ‰
        elif score >= 60:
            return "#ffc107"  # ë…¸ë€ìƒ‰
        else:
            return "#dc3545"  # ë¹¨ê°„ìƒ‰

    def _get_balance_status(self, score):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ì— ë”°ë¥¸ ìƒíƒœ ë©”ì‹œì§€"""
        if score >= 80:
            return "ìš°ìˆ˜í•œ ë¶„ì‚°"
        elif score >= 60:
            return "ë³´í†µ ë¶„ì‚°"
        else:
            return "ê°œì„  í•„ìš”"

    def _generate_load_distribution_html(self, load_analysis):
        """ë¶€í•˜ ë¶„ì‚° ë¶„ì„ HTML ìƒì„±"""
        if not load_analysis:
            return """
            <div class="recommendation">
                <h4 style="color: #dc3545;">âš ï¸ ë¶€í•˜ ë¶„ì‚° ë°ì´í„° ë¶„ì„ ë¶ˆê°€</h4>
                <p><strong>ì›ì¸:</strong> í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>
                <p><strong>í™•ì¸ì‚¬í•­:</strong></p>
                <ul>
                    <li>ê° ì¸ìŠ¤í„´ìŠ¤ì˜ CloudWatch ë©”íŠ¸ë¦­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê³  ìˆëŠ”ì§€ í™•ì¸</li>
                    <li>Writerì™€ Reader ì¸ìŠ¤í„´ìŠ¤ê°€ ëª¨ë‘ í™œì„± ìƒíƒœì¸ì§€ í™•ì¸</li>
                    <li>ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ê°„ ë™ì•ˆ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸</li>
                </ul>
                <p><strong>ê¶Œì¥ì¡°ì¹˜:</strong> ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ì—¬ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ì„±ëŠ¥ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”.</p>
            </div>
            """

        writer = load_analysis.get("writer", {})
        readers = load_analysis.get("readers", [])

        html = f"""
        <div style="margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 5px;">
            <h4>Writer ì¸ìŠ¤í„´ìŠ¤</h4>
            <p>CPU: {writer.get('cpu', 0):.1f}% | ì—°ê²° ìˆ˜: {writer.get('connections', 0):.1f}</p>
        </div>
        """

        if readers:
            html += "<h4>Reader ì¸ìŠ¤í„´ìŠ¤ë“¤</h4>"
            for reader in readers:
                html += f"""
                <div style="margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 5px;">
                    <strong>{reader['instance_id']}</strong><br>
                    CPU: {reader['cpu']:.1f}% | ì—°ê²° ìˆ˜: {reader['connections']:.1f}
                </div>
                """

        return html

    def _generate_cluster_recommendations(self, cluster_analysis, cluster_info):
        """í´ëŸ¬ìŠ¤í„° ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ë¶€í•˜ ë¶„ì‚° ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        load_dist = cluster_analysis.get("load_distribution", {})
        balance_score = load_dist.get("balance_score", 0)

        if balance_score < 60:
            recommendations.append(
                {
                    "priority": "ë†’ìŒ",
                    "title": "ë¶€í•˜ ë¶„ì‚° ê°œì„  í•„ìš”",
                    "description": "Writerì™€ Reader ê°„ ë¶€í•˜ ë¶„ì‚°ì´ ë¶ˆê· í˜•í•©ë‹ˆë‹¤. ì½ê¸° ì¿¼ë¦¬ë¥¼ Reader ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì‚°í•˜ì„¸ìš”.",
                    "action": "ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì½ê¸° ì „ìš© ì¿¼ë¦¬ë¥¼ Reader ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¼ìš°íŒ…",
                }
            )

        # ì•”í˜¸í™” ê¶Œì¥ì‚¬í•­
        if not cluster_info.get("StorageEncrypted"):
            recommendations.append(
                {
                    "priority": "ì¤‘ê°„",
                    "title": "ìŠ¤í† ë¦¬ì§€ ì•”í˜¸í™” í™œì„±í™”",
                    "description": "ë°ì´í„° ë³´ì•ˆì„ ìœ„í•´ ìŠ¤í† ë¦¬ì§€ ì•”í˜¸í™”ë¥¼ í™œì„±í™”í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    "action": "ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ì•”í˜¸í™” ì˜µì…˜ í™œì„±í™”",
                }
            )

        # ë°±ì—… ì„¤ì • ê¶Œì¥ì‚¬í•­
        backup_retention = cluster_info.get("BackupRetentionPeriod", 0)
        if backup_retention < 7:
            recommendations.append(
                {
                    "priority": "ì¤‘ê°„",
                    "title": "ë°±ì—… ë³´ì¡´ ê¸°ê°„ ì—°ì¥",
                    "description": f"í˜„ì¬ ë°±ì—… ë³´ì¡´ ê¸°ê°„ì´ {backup_retention}ì¼ì…ë‹ˆë‹¤. ìµœì†Œ 7ì¼ ì´ìƒ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    "action": "ë°±ì—… ë³´ì¡´ ê¸°ê°„ì„ 7-35ì¼ë¡œ ì„¤ì •",
                }
            )

        # HTML ìƒì„±
        if not recommendations:
            return "<div class='recommendation'><h4>âœ… ìš°ìˆ˜í•œ í´ëŸ¬ìŠ¤í„° ì„¤ì •</h4><p>í˜„ì¬ í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì˜ ë”°ë¥´ê³  ìˆìŠµë‹ˆë‹¤.</p></div>"

        html = ""
        for rec in recommendations:
            priority_color = {
                "ë†’ìŒ": "#dc3545",
                "ì¤‘ê°„": "#ffc107",
                "ë‚®ìŒ": "#28a745",
            }.get(rec["priority"], "#6c757d")
            html += f"""
            <div class="recommendation">
                <h4 style="color: {priority_color};">ğŸ¯ {rec['title']} (ìš°ì„ ìˆœìœ„: {rec['priority']})</h4>
                <p><strong>ì„¤ëª…:</strong> {rec['description']}</p>
                <p><strong>ê¶Œì¥ ì¡°ì¹˜:</strong> {rec['action']}</p>
            </div>
            """

        return html

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"

            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # íŒŒì¼ ë³µì‚¬
            import shutil

            shutil.copy2(source, target_path)

            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    # === ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œ ===

    def setup_cloudwatch_client(self, region_name: str = "us-east-1"):
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

        ë¦¬íŒ©í† ë§: Week 2 - CloudWatchManager ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        result = self.cloudwatch_manager.setup_cloudwatch_client(region_name)
        # ë©”ì¸ ì„œë²„ì˜ cloudwatch ì†ì„±ë„ ì—…ë°ì´íŠ¸
        self.cloudwatch = self.cloudwatch_manager.cloudwatch
        return result

    async def collect_db_metrics(
        self,
        db_instance_identifier: str,
        hours: int = 24,
        metrics: Optional[List[str]] = None,
        region: str = "us-east-1",
    ) -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

        ë¦¬íŒ©í† ë§: Week 2 - CloudWatchManager ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        result = await self.cloudwatch_manager.collect_db_metrics(
            db_instance_identifier, hours, metrics, region
        )
        # current_instance_class ë™ê¸°í™”
        self.current_instance_class = self.cloudwatch_manager.current_instance_class
        return result

    async def analyze_metric_correlation(
        self, csv_file: str, target_metric: str = "CPUUtilization", top_n: int = 10
    ) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„

        ë¦¬íŒ©í† ë§: Week 2 - CloudWatchManager ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        return await self.cloudwatch_manager.analyze_metric_correlation(
            csv_file, target_metric, top_n
        )

    def load_metric_thresholds(self) -> dict:
        """input í´ë”ì—ì„œ ìµœì‹  ì„ê³„ê°’ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            input_dir = Path(__file__).parent / "input"
            if not input_dir.exists():
                input_dir.mkdir(exist_ok=True)

            # metric_thresholds_*.txt íŒŒì¼ ì¤‘ ìµœì‹  íŒŒì¼ ì°¾ê¸°
            threshold_files = list(input_dir.glob("metric_thresholds_*.txt"))
            if not threshold_files:
                return self.get_default_thresholds()

            latest_file = max(threshold_files, key=lambda f: f.stat().st_mtime)

            thresholds = {}
            current_metric = None

            with open(latest_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue

                    if line.startswith("[") and line.endswith("]"):
                        current_metric = line[1:-1]
                        thresholds[current_metric] = {}
                    elif "=" in line and current_metric:
                        key, value = line.split("=", 1)
                        key = key.strip()
                        value = value.strip()

                        # ìˆ«ì ë³€í™˜
                        if key in [
                            "min",
                            "max",
                            "high_threshold",
                            "low_threshold",
                            "spike_factor",
                        ]:
                            try:
                                thresholds[current_metric][key] = (
                                    float(value) if value != "None" else None
                                )
                            except ValueError:
                                thresholds[current_metric][key] = None
                        else:
                            thresholds[current_metric][key] = value

            return thresholds

        except Exception as e:
            debug_log(f"ì„ê³„ê°’ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self.get_default_thresholds()

    def get_default_thresholds(self) -> dict:
        """ê¸°ë³¸ ì„ê³„ê°’ ë°˜í™˜"""
        return {
            "CPUUtilization": {
                "min": 0,
                "max": 100,
                "high_threshold": 80,
                "method": "absolute",
            },
            "DatabaseConnections": {
                "min": 0,
                "max": None,
                "spike_factor": 3.0,
                "method": "spike",
            },
            "FreeableMemory": {
                "min": 0,
                "max": None,
                "low_threshold": 0.1,
                "method": "percentage",
            },
            "ReadLatency": {
                "min": 0,
                "max": None,
                "high_threshold": 0.01,
                "method": "absolute",
            },
            "WriteLatency": {
                "min": 0,
                "max": None,
                "high_threshold": 0.01,
                "method": "absolute",
            },
            "ReadIOPS": {"min": 0, "max": None, "spike_factor": 5.0, "method": "spike"},
            "WriteIOPS": {
                "min": 0,
                "max": None,
                "spike_factor": 5.0,
                "method": "spike",
            },
            "NetworkReceiveThroughput": {
                "min": 0,
                "max": None,
                "spike_factor": 3.0,
                "method": "spike",
            },
            "NetworkTransmitThroughput": {
                "min": 0,
                "max": None,
                "spike_factor": 3.0,
                "method": "spike",
            },
            "DBLoad": {
                "min": 0,
                "max": None,
                "high_threshold": 2.0,
                "method": "dynamic",
            },
            "DBLoadCPU": {
                "min": 0,
                "max": None,
                "high_threshold": 2.0,
                "method": "dynamic",
            },
            "DBLoadNonCPU": {
                "min": 0,
                "max": None,
                "high_threshold": 1.0,
                "method": "dynamic",
            },
            "BufferCacheHitRatio": {
                "min": 0,
                "max": 100,
                "low_threshold": 80,
                "method": "percentage",
            },
        }

    def get_dynamic_dbload_threshold(self, instance_class: str) -> float:
        """ì¸ìŠ¤í„´ìŠ¤ í´ë˜ìŠ¤ë³„ DBLoad ì„ê³„ê°’ ë°˜í™˜"""
        # vCPU ìˆ˜ ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì •
        vcpu_mapping = {
            # t3/t4g ì‹œë¦¬ì¦ˆ
            "t3.micro": 2,
            "t3.small": 2,
            "t3.medium": 2,
            "t3.large": 2,
            "t3.xlarge": 4,
            "t3.2xlarge": 8,
            "t4g.micro": 2,
            "t4g.small": 2,
            "t4g.medium": 2,
            "t4g.large": 2,
            "t4g.xlarge": 4,
            "t4g.2xlarge": 8,
            # r5/r6i ì‹œë¦¬ì¦ˆ
            "r5.large": 2,
            "r5.xlarge": 4,
            "r5.2xlarge": 8,
            "r5.4xlarge": 16,
            "r5.8xlarge": 32,
            "r5.12xlarge": 48,
            "r5.16xlarge": 64,
            "r5.24xlarge": 96,
            "r6i.large": 2,
            "r6i.xlarge": 4,
            "r6i.2xlarge": 8,
            "r6i.4xlarge": 16,
            "r6i.8xlarge": 32,
            "r6i.12xlarge": 48,
            "r6i.16xlarge": 64,
            "r6i.24xlarge": 96,
            "r6i.32xlarge": 128,
            # m5/m6i ì‹œë¦¬ì¦ˆ
            "m5.large": 2,
            "m5.xlarge": 4,
            "m5.2xlarge": 8,
            "m5.4xlarge": 16,
            "m5.8xlarge": 32,
            "m5.12xlarge": 48,
            "m5.16xlarge": 64,
            "m5.24xlarge": 96,
            "m6i.large": 2,
            "m6i.xlarge": 4,
            "m6i.2xlarge": 8,
            "m6i.4xlarge": 16,
            "m6i.8xlarge": 32,
            "m6i.12xlarge": 48,
            "m6i.16xlarge": 64,
            "m6i.24xlarge": 96,
            "m6i.32xlarge": 128,
        }

        vcpu_count = vcpu_mapping.get(instance_class, 2)  # ê¸°ë³¸ê°’ 2 vCPU
        # DBLoad ì„ê³„ê°’ = vCPU ìˆ˜ * 0.8 (80% í™œìš©ë¥  ê¸°ì¤€)
        return vcpu_count * 0.8

    async def detect_metric_outliers(
        self, csv_file: str, std_threshold: float = 3.0, skip_html_report: bool = False
    ) -> str:
        """ê°œì„ ëœ ì•„ì›ƒë¼ì´ì–´ íƒì§€ - ë©”íŠ¸ë¦­ë³„ ë§ì¶¤ ê¸°ì¤€ ì ìš©"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            # ì„ê³„ê°’ íŒŒì¼ì—ì„œ ë¡œë“œ
            metric_thresholds = self.load_metric_thresholds()

            result = f"ğŸ” ê°œì„ ëœ ì•„ì›ƒë¼ì´ì–´ íƒì§€ ê²°ê³¼:\n\n"
            outlier_summary = []
            critical_issues = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë§ì¶¤ ì•„ì›ƒë¼ì´ì–´ íƒì§€
            for column in df.columns:
                series = df[column]
                config = metric_thresholds.get(column, {"method": "iqr"})
                outliers = pd.Series(dtype=float)

                if config["method"] == "dynamic":
                    # ë™ì  ì„ê³„ê°’ ê¸°ì¤€ (DBLoad ë“±)
                    if column in ["DBLoad", "DBLoadCPU", "DBLoadNonCPU"]:
                        instance_class = getattr(
                            self, "current_instance_class", "r5.large"
                        )
                        dynamic_threshold = self.get_dynamic_dbload_threshold(
                            instance_class
                        )
                        outliers = series[series > dynamic_threshold]
                    else:
                        # ë‹¤ë¥¸ ë©”íŠ¸ë¦­ì€ ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©
                        if "high_threshold" in config:
                            outliers = series[series > config["high_threshold"]]

                elif config["method"] == "absolute":
                    # ì ˆëŒ€ê°’ ê¸°ì¤€ (CPU, Latency ë“±)
                    if "high_threshold" in config:
                        outliers = series[series > config["high_threshold"]]
                    if "low_threshold" in config:
                        low_outliers = series[series < config["low_threshold"]]
                        outliers = pd.concat([outliers, low_outliers])

                elif config["method"] == "spike":
                    # ê¸‰ê²©í•œ ë³€í™” íƒì§€ (Connections, IOPS, Network ë“±)
                    median = series.median()
                    mad = (series - median).abs().median()
                    threshold = median + config.get("spike_factor", 3.0) * mad
                    outliers = series[series > threshold]

                elif config["method"] == "percentage":
                    # ë°±ë¶„ìœ¨ ê¸°ì¤€ (Memory, Cache Hit Ratio ë“±)
                    if "low_threshold" in config:
                        outliers = series[series < config["low_threshold"]]

                else:
                    # IQR ë°©ì‹ (ê¸°ë³¸ê°’)
                    Q1 = series.quantile(0.25)
                    Q3 = series.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    outliers = series[(series < lower_bound) | (series > upper_bound)]

                # ë¬¼ë¦¬ì  ì œì•½ ì ìš©
                if config.get("min") is not None:
                    outliers = outliers[outliers >= config["min"]]
                if config.get("max") is not None:
                    outliers = outliers[outliers <= config["max"]]

                if not outliers.empty:
                    severity = "ğŸ”¥" if len(outliers) > len(series) * 0.1 else "âš ï¸"
                    result += f"{severity} {column} ì´ìƒ íƒì§€ ({len(outliers)}ê°œ):\n"

                    # ì‹¬ê°ë„ íŒì •
                    if column == "CPUUtilization" and outliers.max() > 90:
                        critical_issues.append(
                            f"CPU ì‚¬ìš©ë¥  ìœ„í—˜ ìˆ˜ì¤€: {outliers.max():.1f}%"
                        )
                    elif (
                        column in ["ReadLatency", "WriteLatency"]
                        and outliers.max() > 0.1
                    ):
                        critical_issues.append(
                            f"{column} ì§€ì—°ì‹œê°„ ê¸‰ì¦: {outliers.max():.3f}ì´ˆ"
                        )
                    elif column in ["DBLoad", "DBLoadCPU", "DBLoadNonCPU"]:
                        # ë™ì  ì„ê³„ê°’ ê¸°ë°˜ íŒì •
                        instance_class = getattr(
                            self, "current_instance_class", "r5.large"
                        )
                        dynamic_threshold = self.get_dynamic_dbload_threshold(
                            instance_class
                        )
                        if (
                            outliers.max() > dynamic_threshold * 1.5
                        ):  # ì„ê³„ê°’ì˜ 150% ì´ˆê³¼ ì‹œ ì‹¬ê°
                            critical_issues.append(
                                f"{column} ë¶€í•˜ ê³¼ë‹¤ (ì¸ìŠ¤í„´ìŠ¤: {instance_class}): {outliers.max():.1f} (ì„ê³„ê°’: {dynamic_threshold:.1f})"
                            )

                    # ìƒìœ„ 3ê°œ ì´ìƒê°’ë§Œ í‘œì‹œ
                    top_outliers = outliers.nlargest(3)
                    for timestamp, value in top_outliers.items():
                        result += f"   â€¢ {timestamp}: {value:.2f}\n"

                    if len(outliers) > 3:
                        result += f"   ... ë° {len(outliers) - 3}ê°œ ë”\n"
                    result += "\n"

                    outlier_summary.append(
                        {
                            "metric": column,
                            "count": len(outliers),
                            "max_value": outliers.max(),
                            "severity": (
                                "Critical"
                                if len(outliers) > len(series) * 0.1
                                else "Warning"
                            ),
                        }
                    )
                else:
                    result += f"âœ… {column}: ì •ìƒ ë²”ìœ„\n"

            # ì‹¬ê°í•œ ë¬¸ì œ ìš”ì•½
            if critical_issues:
                result += "\nğŸš¨ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”:\n"
                for issue in critical_issues:
                    result += f"â€¢ {issue}\n"

            # ì „ì²´ ìš”ì•½
            if outlier_summary:
                result += "\nğŸ“Š íƒì§€ ìš”ì•½:\n"
                critical_count = sum(
                    1 for s in outlier_summary if s["severity"] == "Critical"
                )
                warning_count = len(outlier_summary) - critical_count
                result += f"â€¢ ì‹¬ê°: {critical_count}ê°œ ë©”íŠ¸ë¦­\n"
                result += f"â€¢ ê²½ê³ : {warning_count}ê°œ ë©”íŠ¸ë¦­\n"
            else:
                result += "\nâœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.\n"

            # ì„ê³„ê°’ ì •ë³´ HTML ìƒì„±
            threshold_html = self.generate_threshold_html(metric_thresholds)

            # HTML ë³´ê³ ì„œ ìƒì„± (ì„ íƒì )
            debug_log(f"skip_html_report: {skip_html_report}")
            if not skip_html_report:
                debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì¤‘...")
                html_report_path = (
                    OUTPUT_DIR
                    / f"outlier_analysis_{csv_file.replace('.csv', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                )
                self.save_outlier_html_report(result, threshold_html, html_report_path)
                result += f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {self.format_file_link(str(html_report_path), 'ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ ë³´ê³ ì„œ ì—´ê¸°')}\n"
                result += "ğŸ’¡ ë³´ê³ ì„œì—ì„œ 'ì„ê³„ê°’ ì„¤ì •' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìƒì„¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            else:
                debug_log("HTML ë³´ê³ ì„œ ìƒì„± ê±´ë„ˆëœ€")

            return result

        except Exception as e:
            return f"ì•„ì›ƒë¼ì´ì–´ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def generate_threshold_html(self, thresholds: dict) -> str:
        """ì„ê³„ê°’ ì„¤ì •ì„ HTML í…Œì´ë¸”ë¡œ ìƒì„± (Week 3: ReportGeneratorë¡œ ìœ„ì„)"""
        return self.report_generator.generate_threshold_html(thresholds)

    def save_outlier_html_report(
        self, result: str, threshold_html: str, report_path: Path
    ):
        """ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ HTML ë³´ê³ ì„œ ì €ì¥ (Week 3: ReportGeneratorë¡œ ìœ„ì„)"""
        self.report_generator.save_outlier_html_report(result, threshold_html, report_path)

    async def perform_regression_analysis(
        self,
        csv_file: str,
        predictor_metric: str,
        target_metric: str = "CPUUtilization",
    ) -> str:
        """íšŒê·€ ë¶„ì„ ìˆ˜í–‰"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            # í•„ìš”í•œ ë©”íŠ¸ë¦­ í™•ì¸
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ë°ì´í„° ì¤€ë¹„
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN ê°’ ì²˜ë¦¬
            imputer = SimpleImputer(strategy="mean")
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± (2ì°¨)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_poly_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê³„ìˆ˜ ì¶œë ¥
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ({predictor_metric} â†’ {target_metric}):\n\n"
            result += f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥:\n"
            result += f"â€¢ Mean Squared Error: {mse:.4f}\n"
            result += f"â€¢ R-squared Score: {r2:.4f}\n\n"

            result += f"ğŸ”¢ ë‹¤í•­ íšŒê·€ ëª¨ë¸ (2ì°¨):\n"
            result += f"y = {coefficients[1]:.4f}xÂ² + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # í•´ì„ ì¶”ê°€
            result += "í•´ì„:\n"
            if r2 > 0.7:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"
            elif r2 > 0.5:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ì¤‘ê°„ ì˜ˆì¸¡ ì •í™•ë„)\n"
            else:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë‚®ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"

            # ê·¸ë˜í”„ ìƒì„± ì œê±°ë¨ - í…ìŠ¤íŠ¸ ê²°ê³¼ë§Œ ë°˜í™˜
            return result

        except Exception as e:
            return f"íšŒê·€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def list_data_files(self) -> str:
        """ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ“ ë°ì´í„° íŒŒì¼ ëª©ë¡:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"â€¢ {file.name}\n"
                result += f"  í¬ê¸°: {file_size:,} bytes\n"
                result += f"  ìˆ˜ì •ì¼: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            result = f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ({csv_file}):\n\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë©”íŠ¸ë¦­ ìˆ˜: {len(df.columns)}ê°œ\n\n"

            result += "ğŸ“Š ë©”íŠ¸ë¦­ ëª©ë¡:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}ê°œ ë°ì´í„°)\n"

            # ê¸°ë³¸ í†µê³„
            result += f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def validate_column_type_compatibility(
        self, existing_column: Dict[str, Any], new_definition: str, debug_log
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± ê²€ì¦"""
        debug_log(
            f"íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ ì‹œì‘: ê¸°ì¡´={existing_column['data_type']}, ìƒˆë¡œìš´={new_definition}"
        )

        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        debug_log(f"íŒŒì‹±ëœ ìƒˆ íƒ€ì…: {new_type_info}")

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT", "LONGTEXT", "MEDIUMTEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(f"í˜¸í™˜ì„± ë¬¸ì œ: {existing_type} -> {new_type_info['type']}")

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(f"ê¸¸ì´ ì¶•ì†Œ ë¬¸ì œ: {existing_length} -> {new_length}")

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(
                    f"ì •ë°€ë„ ì¶•ì†Œ ë¬¸ì œ: ({existing_precision},{existing_scale}) -> ({new_precision},{new_scale})"
                )

        result = {"compatible": len(issues) == 0, "issues": issues}

        debug_log(
            f"íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ: compatible={result['compatible']}, issues={len(issues)}"
        )
        return result

    async def debug_cloudwatch_collection(
        self, database_secret: str, start_time: str, end_time: str
    ) -> str:
        """CloudWatch ìˆ˜ì§‘ ë””ë²„ê·¸ í•¨ìˆ˜"""
        try:
            # ì‹œê°„ ë³€í™˜ (KST -> UTC)
            start_dt = self.convert_kst_to_utc(start_time)
            end_dt = self.convert_kst_to_utc(end_time)

            # ì‹œí¬ë¦¿ì—ì„œ DB ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            secrets_client = boto3.client(
                "secretsmanager", region_name="ap-northeast-2"
            )
            secret_response = secrets_client.get_secret_value(SecretId=database_secret)
            secret_data = json.loads(secret_response["SecretString"])

            # DB í´ëŸ¬ìŠ¤í„° ì‹ë³„ì ì¶”ì¶œ
            db_host = secret_data.get("host", "")
            if ".cluster-" in db_host:
                cluster_identifier = db_host.split(".cluster-")[0]
            else:
                return "âŒ Aurora í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

            # CloudWatch ìˆ˜ì§‘ ì‹œë„
            logs_client = boto3.client("logs", region_name="ap-northeast-2")
            log_group_name = f"/aws/rds/cluster/{cluster_identifier}/slowquery"

            start_time_ms = int(start_dt.timestamp() * 1000)
            end_time_ms = int(end_dt.timestamp() * 1000)

            print(f"DEBUG: í´ëŸ¬ìŠ¤í„° ID: {cluster_identifier}", file=sys.stderr)
            print(f"DEBUG: ë¡œê·¸ ê·¸ë£¹: {log_group_name}", file=sys.stderr)
            print(f"DEBUG: ì‹œê°„ ë²”ìœ„: {start_dt} ~ {end_dt} (UTC)", file=sys.stderr)
            print(f"DEBUG: íƒ€ì„ìŠ¤íƒ¬í”„: {start_time_ms} ~ {end_time_ms}", file=sys.stderr)

            response = logs_client.filter_log_events(
                logGroupName=log_group_name,
                startTime=start_time_ms,
                endTime=end_time_ms,
            )

            events_count = len(response.get("events", []))
            print(f"DEBUG: ê²€ìƒ‰ëœ ì´ë²¤íŠ¸ ìˆ˜: {events_count}", file=sys.stderr)

            if events_count > 0:
                # ì²« ë²ˆì§¸ ì´ë²¤íŠ¸ í™•ì¸
                first_event = response["events"][0]
                print(f"DEBUG: ì²« ë²ˆì§¸ ì´ë²¤íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„: {first_event['timestamp']}", file=sys.stderr)
                print(
                    f"DEBUG: ì²« ë²ˆì§¸ ì´ë²¤íŠ¸ ë©”ì‹œì§€ ë¯¸ë¦¬ë³´ê¸°: {first_event['message'][:100]}...", file=sys.stderr
                )

                # íŒŒì‹± í…ŒìŠ¤íŠ¸
                message = first_event["message"].replace("\\n", "\n")
                print(f"DEBUG: Query_time íŒ¨í„´ ì¡´ì¬? {'# Query_time: ' in message}", file=sys.stderr)

                if "# Query_time: " in message:
                    lines = message.split("\n")
                    print(f"DEBUG: ë¶„í• ëœ ë¼ì¸ ìˆ˜: {len(lines)}", file=sys.stderr)
                    for i, line in enumerate(lines[:5]):  # ì²˜ìŒ 5ì¤„ë§Œ
                        print(f"DEBUG: Line {i}: {repr(line)}", file=sys.stderr)

            return f"âœ… ë””ë²„ê·¸ ì™„ë£Œ: {events_count}ê°œ ì´ë²¤íŠ¸ ë°œê²¬"

        except Exception as e:
            import traceback

            return f"âŒ ë””ë²„ê·¸ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"

    async def collect_slow_queries(
        self, database_secret: str, start_time: str = None, end_time: str = None
    ) -> str:
        """ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ (CloudWatch â†’ ë¡œì»¬íŒŒì¼ â†’ Performance Schema ìˆœì„œ)"""
        try:
            # ì‹œê°„ëŒ€ ì²˜ë¦¬ (KST -> UTC)
            if start_time and end_time:
                start_dt = self.convert_kst_to_utc(start_time)
                end_dt = self.convert_kst_to_utc(end_time)
            else:
                # ê¸°ë³¸ê°’: 24ì‹œê°„ ì „ë¶€í„° í˜„ì¬ê¹Œì§€ (UTC ê¸°ì¤€)
                end_dt = datetime.utcnow()
                start_dt = end_dt - timedelta(hours=24)

            # ì‹œí¬ë¦¿ì—ì„œ DB ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            secrets_client = boto3.client(
                "secretsmanager", region_name=self.default_region
            )
            secret_response = secrets_client.get_secret_value(SecretId=database_secret)
            secret_data = json.loads(secret_response["SecretString"])

            # DB í´ëŸ¬ìŠ¤í„° ì‹ë³„ì ì¶”ì¶œ
            db_host = secret_data.get("host", "")
            if ".cluster-" in db_host:
                cluster_identifier = db_host.split(".cluster-")[0]
            else:
                return "âŒ Aurora í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

            # 1ë‹¨ê³„: CloudWatch Logsì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œë„
            cloudwatch_result = await self._collect_from_cloudwatch(
                cluster_identifier, start_dt, end_dt
            )

            if cloudwatch_result["success"]:
                return cloudwatch_result["message"]

            # 2ë‹¨ê³„: ë¡œì»¬ íŒŒì¼ì—ì„œ ìˆ˜ì§‘ ì‹œë„
            local_file_result = await self._collect_from_local_file(
                database_secret, start_dt, end_dt
            )

            if local_file_result["success"]:
                return local_file_result["message"]

            # 3ë‹¨ê³„: Performance Schemaì—ì„œ ìˆ˜ì§‘ ì‹œë„
            performance_result = await self._collect_from_performance_schema(
                database_secret, start_dt, end_dt
            )

            if performance_result["success"]:
                return performance_result["message"]

            # 4ë‹¨ê³„: Log exports ì„¤ì • ì œì•ˆ
            return await self._suggest_log_exports_setup(
                cluster_identifier, cloudwatch_result["message"]
            )

        except ValueError as ve:
            return f"âŒ {str(ve)}"
        except Exception as e:
            import traceback

            return f"âŒ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"

    async def _collect_from_local_file(
        self, database_secret: str, start_dt: datetime, end_dt: datetime
    ) -> dict:
        """ë¡œì»¬ ìŠ¬ë¡œìš° ì¿¼ë¦¬ íŒŒì¼ì—ì„œ ìˆ˜ì§‘"""
        try:
            # ê¸°ì¡´ ì—°ê²° ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            if not self.setup_shared_connection(database_secret, None, True):
                return {"success": False, "message": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"}

            cursor = self.shared_cursor

            # ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
            cursor.execute("SHOW VARIABLES LIKE 'slow_query_log_file'")
            result = cursor.fetchone()
            if not result:
                return {
                    "success": False,
                    "message": "ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                }

            log_file_path = result[1]

            # íŒŒì¼ ë‚´ìš© ì½ê¸° ì‹œë„ (LOAD_FILE í•¨ìˆ˜ ì‚¬ìš©)
            try:
                cursor.execute(f"SELECT LOAD_FILE('{log_file_path}')")
                file_content = cursor.fetchone()

                if not file_content or not file_content[0]:
                    return {
                        "success": False,
                        "message": f"ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŒ: {log_file_path}",
                    }

                content = (
                    file_content[0].decode("utf-8")
                    if isinstance(file_content[0], bytes)
                    else str(file_content[0])
                )

                # ì‹œê°„ ë²”ìœ„ í•„í„°ë§ì„ ìœ„í•œ ë¡œê·¸ íŒŒì‹±
                slow_queries = self._parse_slow_query_log(content, start_dt, end_dt)

                if slow_queries:
                    # íŒŒì¼ ìƒì„±
                    current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"slow_queries_local_file_{current_date}.sql"
                    file_path = SQL_DIR / filename

                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"-- ë¡œì»¬ ìŠ¬ë¡œìš° ì¿¼ë¦¬ íŒŒì¼ ìˆ˜ì§‘ ê²°ê³¼\n")
                        f.write(f"-- íŒŒì¼ ê²½ë¡œ: {log_file_path}\n")
                        f.write(
                            f"-- ìˆ˜ì§‘ ê¸°ê°„: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (KST)\n"
                        )
                        f.write(f"-- ì´ {len(slow_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                        for i, query in enumerate(slow_queries, 1):
                            f.write(f"-- ìŠ¬ë¡œìš° ì¿¼ë¦¬ #{i}\n")
                            if "time" in query:
                                f.write(f"-- {query['time']}\n")
                            if "query_time" in query:
                                f.write(f"-- {query['query_time']}\n")
                            if "user_host" in query:
                                f.write(f"-- {query['user_host']}\n")
                            f.write(f"{query['sql']};\n\n")

                    # S3ì— ì—…ë¡œë“œ ë° Pre-signed URL ìƒì„±
                    try:
                        import boto3
                        s3_client = boto3.client('s3', region_name=self.default_region)
                        s3_bucket = QUERY_RESULTS_DEV_BUCKET
                        s3_key = f"sql-files/slow-queries/{filename}"

                        s3_client.upload_file(str(file_path), s3_bucket, s3_key)
                        logger.info(f"SQL íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                        presigned_url = s3_client.generate_presigned_url(
                            'get_object',
                            Params={'Bucket': s3_bucket, 'Key': s3_key},
                            ExpiresIn=604800  # 7ì¼
                        )

                        return {
                            "success": True,
                            "message": f"âœ… ë¡œì»¬ íŒŒì¼ì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ {len(slow_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\níŒŒì¼ ê²½ë¡œ: {log_file_path}\nğŸ”— ë‹¤ìš´ë¡œë“œ (7ì¼ ìœ íš¨): {presigned_url}",
                        }
                    except Exception as s3_error:
                        logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_error}")
                        return {
                            "success": True,
                            "message": f"âœ… ë¡œì»¬ íŒŒì¼ì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ {len(slow_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\níŒŒì¼ ê²½ë¡œ: {log_file_path}",
                        }
                else:
                    return {
                        "success": False,
                        "message": f"ë¡œì»¬ íŒŒì¼ì—ì„œ í•´ë‹¹ ì‹œê°„ ë²”ìœ„ì˜ ìŠ¬ë¡œìš° ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    }

            except Exception as e:
                return {"success": False, "message": f"ë¡œì»¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"}

        except Exception as e:
            return {"success": False, "message": f"ë¡œì»¬ íŒŒì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}

    def _parse_slow_query_log(
        self, content: str, start_dt: datetime, end_dt: datetime
    ) -> list:
        """ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ ë‚´ìš© íŒŒì‹±"""
        slow_queries = []
        lines = content.split("\n")
        current_query = {}
        sql_lines = []

        for line in lines:
            line = line.strip()

            if line.startswith("# Time:"):
                # ì´ì „ ì¿¼ë¦¬ ì €ì¥
                if current_query and sql_lines:
                    current_query["sql"] = " ".join(sql_lines)

                    # ì‹œê°„ ë²”ìœ„ ì²´í¬
                    if self._is_within_time_range(
                        current_query.get("time", ""), start_dt, end_dt
                    ):
                        slow_queries.append(current_query.copy())

                # ìƒˆ ì¿¼ë¦¬ ì‹œì‘
                current_query = {"time": line}
                sql_lines = []

            elif line.startswith("# Query_time:"):
                current_query["query_time"] = line
            elif line.startswith("# User@Host:"):
                current_query["user_host"] = line
            elif (
                not line.startswith("#")
                and line
                and not line.startswith("SET timestamp")
            ):
                if not line.startswith("use "):
                    sql_lines.append(line)

        # ë§ˆì§€ë§‰ ì¿¼ë¦¬ ì²˜ë¦¬
        if current_query and sql_lines:
            current_query["sql"] = " ".join(sql_lines)
            if self._is_within_time_range(
                current_query.get("time", ""), start_dt, end_dt
            ):
                slow_queries.append(current_query)

        return slow_queries

    def _is_within_time_range(
        self, time_str: str, start_dt: datetime, end_dt: datetime
    ) -> bool:
        """ì‹œê°„ ë¬¸ìì—´ì´ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        try:
            # # Time: 2025-09-05T14:30:45.123456Z í˜•ì‹ íŒŒì‹±
            if "Time:" in time_str:
                time_part = time_str.split("Time:")[1].strip()
                # Zë¥¼ ì œê±°í•˜ê³  ë§ˆì´í¬ë¡œì´ˆ ë¶€ë¶„ ì²˜ë¦¬
                if "Z" in time_part:
                    time_part = time_part.replace("Z", "")
                if "." in time_part:
                    time_part = time_part.split(".")[0]

                query_time = datetime.strptime(time_part, "%Y-%m-%dT%H:%M:%S")
                return start_dt <= query_time <= end_dt
        except:
            pass
        return True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨

    async def _collect_from_cloudwatch(
        self, cluster_identifier: str, start_dt: datetime, end_dt: datetime
    ) -> dict:
        """CloudWatch Logsì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ (Lambda ì‚¬ìš©)"""
        try:
            logger.info(f"Lambdaë¡œ Slow Query ìˆ˜ì§‘: {cluster_identifier}")

            # Lambda í˜¸ì¶œ
            lambda_result = await self._call_lambda('collect-slow-queries-cloudwatch', {
                'cluster_identifier': cluster_identifier,
                'start_time': start_dt.isoformat(),
                'end_time': end_dt.isoformat(),
                'region': self.default_region
            })

            if not lambda_result.get('success'):
                return {
                    "success": False,
                    "message": lambda_result.get('message', lambda_result.get('error', 'Lambda í˜¸ì¶œ ì‹¤íŒ¨'))
                }

            # Lambdaì—ì„œ ë°›ì€ ë°ì´í„°ë¡œ íŒŒì¼ ìƒì„± (ë¡œì»¬ ì²˜ë¦¬)
            instances_data = lambda_result.get('instances', {})
            instance_files = []
            total_queries = 0

            for instance_id, slow_queries in instances_data.items():
                if slow_queries:
                    # ì¸ìŠ¤í„´ìŠ¤ë³„ íŒŒì¼ ìƒì„±
                    current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"slow_queries_{instance_id}_{current_date}.sql"
                    file_path = SQL_DIR / filename

                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(
                            f"-- CloudWatch Logs ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ê²°ê³¼ (ì¸ìŠ¤í„´ìŠ¤: {instance_id})\n"
                        )
                        f.write(
                            f"-- ìˆ˜ì§‘ ê¸°ê°„: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (Local)\n"
                        )
                        f.write(f"-- ì´ {len(slow_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                        for i, query in enumerate(slow_queries, 1):
                            f.write(
                                f"-- ìŠ¬ë¡œìš° ì¿¼ë¦¬ #{i} (ì¸ìŠ¤í„´ìŠ¤: {instance_id})\n"
                            )
                            if "time" in query:
                                f.write(f"-- {query['time']}\n")
                            if "query_time" in query:
                                f.write(f"-- {query['query_time']}\n")
                            if "user_host" in query:
                                f.write(f"-- {query['user_host']}\n")
                            f.write(f"{query['sql']};\n\n")

                    # S3ì— ì—…ë¡œë“œ ë° Pre-signed URL ìƒì„±
                    try:
                        import boto3
                        s3_client = boto3.client('s3', region_name=self.default_region)
                        s3_bucket = QUERY_RESULTS_DEV_BUCKET
                        s3_key = f"sql-files/slow-queries/{filename}"

                        s3_client.upload_file(str(file_path), s3_bucket, s3_key)
                        logger.info(f"SQL íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                        presigned_url = s3_client.generate_presigned_url(
                            'get_object',
                            Params={'Bucket': s3_bucket, 'Key': s3_key},
                            ExpiresIn=604800  # 7ì¼
                        )

                        instance_files.append(
                            f"{instance_id}: {filename} ({len(slow_queries)}ê°œ)\n  ğŸ”— {presigned_url}"
                        )
                    except Exception as s3_error:
                        logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_error}")
                        instance_files.append(
                            f"{instance_id}: {self.format_file_link(str(file_path), filename)} ({len(slow_queries)}ê°œ)"
                        )
                    total_queries += len(slow_queries)

            if instance_files:
                return {
                    "success": True,
                    "message": f"âœ… CloudWatchì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë³„ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_queries}ê°œ)\n"
                    + "\n".join([f"â€¢ {file_info}" for file_info in instance_files]),
                }
            else:
                return {
                    "success": False,
                    "message": f"CloudWatch Logsì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                }

        except Exception as e:
            logger.error(f"Slow Query ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "message": f"CloudWatch Logs ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

    async def _collect_from_performance_schema(
        self, database_secret: str, start_dt: datetime, end_dt: datetime
    ) -> dict:
        """Performance Schemaì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘"""
        try:
            # ê¸°ì¡´ ì—°ê²° ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            if not self.setup_shared_connection(database_secret, None, True):
                return {"success": False, "message": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"}

            cursor = self.shared_cursor

            # Performance Schemaì—ì„œ ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
            query = """
            SELECT 
                DIGEST_TEXT as sql_text,
                COUNT_STAR as exec_count,
                AVG_TIMER_WAIT/1000000000000 as avg_time_sec,
                MAX_TIMER_WAIT/1000000000000 as max_time_sec,
                SUM_TIMER_WAIT/1000000000000 as total_time_sec,
                FIRST_SEEN,
                LAST_SEEN
            FROM performance_schema.events_statements_summary_by_digest 
            WHERE AVG_TIMER_WAIT/1000000000000 > 1.0
            AND LAST_SEEN >= %s
            ORDER BY AVG_TIMER_WAIT DESC 
            LIMIT 50
            """

            cursor.execute(query, (start_dt,))
            results = cursor.fetchall()

            if results:
                # íŒŒì¼ ìƒì„±
                current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"slow_queries_performance_schema_{current_date}.sql"
                file_path = SQL_DIR / filename

                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- Performance Schema ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ê²°ê³¼\n")
                    f.write(
                        f"-- ìˆ˜ì§‘ ê¸°ê°„: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (Local)\n"
                    )
                    f.write(f"-- ì´ {len(results)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, row in enumerate(results, 1):
                        (
                            sql_text,
                            exec_count,
                            avg_time,
                            max_time,
                            total_time,
                            first_seen,
                            last_seen,
                        ) = row
                        f.write(f"-- ìŠ¬ë¡œìš° ì¿¼ë¦¬ #{i}\n")
                        f.write(
                            f"-- ì‹¤í–‰íšŸìˆ˜: {exec_count}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n"
                        )
                        f.write(
                            f"-- ì´ ì‹œê°„: {total_time:.3f}ì´ˆ, ë§ˆì§€ë§‰ ì‹¤í–‰: {last_seen}\n"
                        )
                        f.write(f"{sql_text};\n\n")

                # S3ì— ì—…ë¡œë“œ ë° Pre-signed URL ìƒì„±
                try:
                    import boto3
                    s3_client = boto3.client('s3', region_name=self.default_region)
                    s3_bucket = QUERY_RESULTS_DEV_BUCKET
                    s3_key = f"sql-files/slow-queries/{filename}"

                    s3_client.upload_file(str(file_path), s3_bucket, s3_key)
                    logger.info(f"SQL íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                    presigned_url = s3_client.generate_presigned_url(
                        'get_object',
                        Params={'Bucket': s3_bucket, 'Key': s3_key},
                        ExpiresIn=604800  # 7ì¼
                    )

                    return {
                        "success": True,
                        "message": f"âœ… Performance Schemaì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\nê²€ìƒ‰ ê¸°ê°„: {start_dt} ~ {end_dt} (UTC)\nğŸ”— ë‹¤ìš´ë¡œë“œ (7ì¼ ìœ íš¨): {presigned_url}",
                    }
                except Exception as s3_error:
                    logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_error}")
                    return {
                        "success": True,
                        "message": f"âœ… Performance Schemaì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\nê²€ìƒ‰ ê¸°ê°„: {start_dt} ~ {end_dt} (UTC)",
                    }
            else:
                return {
                    "success": False,
                    "message": f"Performance Schemaì—ì„œë„ ìŠ¬ë¡œìš° ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                }

        except Exception as e:
            return {
                "success": False,
                "message": f"Performance Schema ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
            }

    async def _suggest_log_exports_setup(
        self, cluster_identifier: str, cloudwatch_error: str
    ) -> str:
        """Log exports ì„¤ì • ì œì•ˆ ë° ìë™ ì„¤ì •"""
        try:
            # RDS í´ë¼ì´ì–¸íŠ¸ë¡œ í˜„ì¬ ì„¤ì • í™•ì¸
            rds_client = boto3.client("rds", region_name="ap-northeast-2")

            try:
                response = rds_client.describe_db_clusters(
                    DBClusterIdentifier=cluster_identifier
                )
                cluster = response["DBClusters"][0]
                enabled_logs = cluster.get("EnabledCloudwatchLogsExports", [])

                result_msg = f"ğŸ” **ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ê²°ê³¼**\n\n"
                result_msg += f"âŒ CloudWatch Logs: {cloudwatch_error}\n"
                result_msg += f"âŒ Performance Schema: ìŠ¬ë¡œìš° ì¿¼ë¦¬ ì—†ìŒ\n\n"
                result_msg += f"ğŸ“Š **í˜„ì¬ Log Exports ì„¤ì •**\n"
                result_msg += f"í´ëŸ¬ìŠ¤í„°: {cluster_identifier}\n"
                result_msg += (
                    f"í™œì„±í™”ëœ ë¡œê·¸: {enabled_logs if enabled_logs else 'ì—†ìŒ'}\n\n"
                )

                if "slowquery" not in enabled_logs:
                    result_msg += f"ğŸ’¡ **í•´ê²° ë°©ì•ˆ**\n"
                    result_msg += f"Aurora í´ëŸ¬ìŠ¤í„°ì—ì„œ SlowQuery ë¡œê·¸ë¥¼ CloudWatchë¡œ ì „ì†¡í•˜ë„ë¡ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n"
                    result_msg += f"**ìë™ ì„¤ì •ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**\n"
                    result_msg += f"ë‹¤ìŒ ëª…ë ¹ì´ ì‹¤í–‰ë©ë‹ˆë‹¤:\n"
                    result_msg += f"```\n"
                    result_msg += f"aws rds modify-db-cluster \\\n"
                    result_msg += f"  --db-cluster-identifier {cluster_identifier} \\\n"
                    result_msg += f"  --cloudwatch-logs-configuration 'EnableLogTypes=slowquery'\n"
                    result_msg += f"```\n\n"
                    result_msg += f"ì„¤ì • í›„ ì•½ 5-10ë¶„ í›„ë¶€í„° CloudWatch Logsì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    result_msg += f"**ì„¤ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)**"

                    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°ëŠ” MCPì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
                    return result_msg
                else:
                    result_msg += (
                        f"âœ… SlowQuery ë¡œê·¸ ì „ì†¡ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
                    )
                    result_msg += f"ë¡œê·¸ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ì´ìœ :\n"
                    result_msg += f"1. ì‹¤ì œë¡œ 1ì´ˆ ì´ìƒ ì‹¤í–‰ë˜ëŠ” ì¿¼ë¦¬ê°€ ì—†ìŒ\n"
                    result_msg += f"2. ë¡œê·¸ ì „ì†¡ì— ì§€ì—°ì´ ìˆìŒ (ìµœëŒ€ 10ë¶„)\n"
                    result_msg += f"3. ë¡œê·¸ ë³´ì¡´ ì •ì±…ìœ¼ë¡œ ì¸í•œ ì‚­ì œ\n"
                    return result_msg

            except Exception as e:
                return f"âŒ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

        except Exception as e:
            return f"âŒ Log exports ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {str(e)}"

    async def enable_slow_query_log_exports(self, cluster_identifier: str) -> str:
        """Aurora í´ëŸ¬ìŠ¤í„°ì˜ SlowQuery ë¡œê·¸ CloudWatch ì „ì†¡ í™œì„±í™”"""
        try:
            rds_client = boto3.client("rds", region_name="ap-northeast-2")

            response = rds_client.modify_db_cluster(
                DBClusterIdentifier=cluster_identifier,
                CloudwatchLogsConfiguration={"EnableLogTypes": ["slowquery"]},
                ApplyImmediately=True,
            )

            return (
                f"âœ… SlowQuery ë¡œê·¸ CloudWatch ì „ì†¡ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                f"í´ëŸ¬ìŠ¤í„°: {cluster_identifier}\n"
                f"ìƒíƒœ: {response['DBCluster']['Status']}\n"
                f"ì•½ 5-10ë¶„ í›„ë¶€í„° CloudWatch Logsì—ì„œ ìŠ¬ë¡œìš° ì¿¼ë¦¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        except Exception as e:
            return f"âŒ SlowQuery ë¡œê·¸ í™œì„±í™” ì‹¤íŒ¨: {str(e)}"

    async def collect_cpu_intensive_queries(
        self,
        database_secret: str,
        db_instance_identifier: str = None,
        start_time: str = None,
        end_time: str = None,
    ) -> str:
        """CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„± (Lambda ì‚¬ìš©)"""
        try:
            # ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆë¡œ ìœ„ì„
            lambda_result = await self.lambda_client.collect_cpu_intensive_queries(
                database_secret, db_instance_identifier, start_time, end_time
            )

            if not lambda_result.get('success'):
                error_msg = lambda_result.get('error', 'Lambda í˜¸ì¶œ ì‹¤íŒ¨')
                return f"âŒ CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {error_msg}"

            # Lambdaì—ì„œ ë°›ì€ ì¿¼ë¦¬ ë°ì´í„°ë¡œ íŒŒì¼ ìƒì„± (ë¡œì»¬ ì²˜ë¦¬)
            queries = lambda_result.get('queries', [])

            if queries:
                # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
                current_date = datetime.now().strftime("%Y%m%d")
                instance_suffix = (
                    f"_{db_instance_identifier}" if db_instance_identifier else ""
                )
                filename = f"cpu_intensive_queries{instance_suffix}_{current_date}.sql"
                file_path = SQL_DIR / filename

                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- CPU ì§‘ì•½ì  ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n")
                    f.write(f"-- ì´ {len(queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query_info in enumerate(queries, 1):
                        sql = query_info.get('sql', '')
                        source = query_info.get('source', 'unknown')
                        exec_count = query_info.get('exec_count', 0)
                        avg_time = query_info.get('avg_time', 0.0)

                        f.write(f"-- CPU ì§‘ì•½ì  ì¿¼ë¦¬ #{i} (ì¶œì²˜: {source})\n")
                        if exec_count:
                            f.write(f"-- ì‹¤í–‰ íšŸìˆ˜: {exec_count}, í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ\n")
                        f.write(f"{sql};\n\n")

                # S3ì— ì—…ë¡œë“œ ë° Pre-signed URL ìƒì„±
                try:
                    import boto3
                    s3_client = boto3.client('s3', region_name=self.default_region)
                    s3_bucket = QUERY_RESULTS_DEV_BUCKET
                    s3_key = f"sql-files/cpu-intensive/{filename}"

                    s3_client.upload_file(str(file_path), s3_bucket, s3_key)
                    logger.info(f"SQL íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                    presigned_url = s3_client.generate_presigned_url(
                        'get_object',
                        Params={'Bucket': s3_bucket, 'Key': s3_key},
                        ExpiresIn=604800
                    )

                    return f"âœ… CPU ì§‘ì•½ì  ì¿¼ë¦¬ {len(queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\nğŸ”— ë‹¤ìš´ë¡œë“œ (7ì¼ ìœ íš¨): {presigned_url}"
                except Exception as s3_error:
                    logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_error}")
                    return f"âœ… CPU ì§‘ì•½ì  ì¿¼ë¦¬ {len(queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {self.format_file_link(str(file_path), filename)}"
            else:
                return f"âœ… CPU ì§‘ì•½ì  ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        except Exception as e:
            logger.error(f"CPU ì§‘ì•½ ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return f"âŒ CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

    async def collect_temp_space_intensive_queries(
        self,
        database_secret: str,
        db_instance_identifier: str = None,
        start_time: str = None,
        end_time: str = None,
    ) -> str:
        """ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„± (Lambda ì‚¬ìš©)"""
        try:
            # ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆë¡œ ìœ„ì„
            lambda_result = await self.lambda_client.collect_temp_space_intensive_queries(
                database_secret, db_instance_identifier, start_time, end_time
            )

            if not lambda_result.get('success'):
                error_msg = lambda_result.get('error', 'Lambda í˜¸ì¶œ ì‹¤íŒ¨')
                return f"âŒ ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {error_msg}"

            # Lambdaì—ì„œ ë°›ì€ ì¿¼ë¦¬ ë°ì´í„°ë¡œ íŒŒì¼ ìƒì„± (ë¡œì»¬ ì²˜ë¦¬)
            queries = lambda_result.get('queries', [])

            if queries:
                # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
                current_date = datetime.now().strftime("%Y%m%d")
                instance_suffix = (
                    f"_{db_instance_identifier}" if db_instance_identifier else ""
                )
                filename = (
                    f"temp_space_intensive_queries{instance_suffix}_{current_date}.sql"
                )
                file_path = SQL_DIR / filename

                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(
                        f"-- ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n"
                    )
                    f.write(f"-- ì´ {len(queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query_info in enumerate(queries, 1):
                        sql = query_info.get('sql', '')
                        temp_tables = query_info.get('temp_tables', 0)
                        temp_disk_tables = query_info.get('temp_disk_tables', 0)
                        sort_rows = query_info.get('sort_rows', 0)

                        f.write(f"-- ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ #{i}\n")
                        if temp_tables or temp_disk_tables:
                            f.write(f"-- ì„ì‹œ í…Œì´ë¸”: {temp_tables}ê°œ, ë””ìŠ¤í¬ ì„ì‹œ í…Œì´ë¸”: {temp_disk_tables}ê°œ, ì •ë ¬ í–‰: {sort_rows}ê°œ\n")
                        f.write(f"{sql};\n\n")

                # S3ì— ì—…ë¡œë“œ ë° Pre-signed URL ìƒì„±
                try:
                    import boto3
                    s3_client = boto3.client('s3', region_name=self.default_region)
                    s3_bucket = QUERY_RESULTS_DEV_BUCKET
                    s3_key = f"sql-files/temp-intensive/{filename}"

                    s3_client.upload_file(str(file_path), s3_bucket, s3_key)
                    logger.info(f"SQL íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                    presigned_url = s3_client.generate_presigned_url(
                        'get_object',
                        Params={'Bucket': s3_bucket, 'Key': s3_key},
                        ExpiresIn=604800  # 7ì¼
                    )

                    return f"âœ… ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ {len(queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}\nğŸ”— ë‹¤ìš´ë¡œë“œ (7ì¼ ìœ íš¨): {presigned_url}"
                except Exception as s3_error:
                    logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_error}")
                    return f"âœ… ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ {len(queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {self.format_file_link(str(file_path), filename)}"
            else:
                return f"âœ… ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        except Exception as e:
            logger.error(f"Temp ê³µê°„ ì§‘ì•½ ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return f"âŒ ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

    async def validate_schema_lambda(
        self,
        database_secret: str,
        database: str,
        ddl_content: str,
        region: str = "ap-northeast-2"
    ) -> dict:
        """DDL ìŠ¤í‚¤ë§ˆ ê²€ì¦ (Lambda ì‚¬ìš©)

        ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        return await self.lambda_client.validate_schema(
            database_secret, database, ddl_content, region
        )

    async def explain_query_lambda(
        self,
        database_secret: str,
        database: str,
        query: str,
        region: str = "ap-northeast-2"
    ) -> dict:
        """ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„ (Lambda ì‚¬ìš©)

        ë¦¬íŒ©í† ë§: Week 1 - LambdaClient ëª¨ë“ˆë¡œ ìœ„ì„
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë©”ì„œë“œ ìœ ì§€
        """
        return await self.lambda_client.explain_query(
            database_secret, database, query, region
        )

    async def analyze_aurora_mysql_error_logs(
        self, keyword: str, start_datetime_str: str, end_datetime_str: str
    ) -> str:
        """Aurora MySQL ì—ëŸ¬ ë¡œê·¸ ë¶„ì„"""
        try:
            # ì‹œê°„ ë³€í™˜ (KST -> UTC)
            start_time_utc = self.convert_kst_to_utc(start_datetime_str)
            end_time_utc = self.convert_kst_to_utc(end_datetime_str)

            logger.info(f"ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì‹œì‘: {start_time_utc} ~ {end_time_utc} (UTC)")

            # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            rds_client = boto3.client("rds", region_name=self.default_region)

            # í‚¤ì›Œë“œë¡œ ì‹œí¬ë¦¿ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            secret_lists = await self.get_secrets_by_keyword(keyword)
            if not secret_lists:
                return f"âŒ '{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."

            results = []
            processed_instances = []

            for secret_name in secret_lists:
                try:
                    # ì‹œí¬ë¦¿ì—ì„œ DB ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    secret_data = await self.get_secret(secret_name)
                    db_host = secret_data.get("host", "")
                    
                    # í˜¸ìŠ¤íŠ¸ëª…ì—ì„œ ì¸ìŠ¤í„´ìŠ¤/í´ëŸ¬ìŠ¤í„° ì‹ë³„ì ì¶”ì¶œ
                    # ì˜ˆ: mysql-instance.cluster-xxx.region.rds.amazonaws.com
                    if not db_host:
                        logger.warning(f"ì‹œí¬ë¦¿ {secret_name}ì— host ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ì¸ ê²½ìš° í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
                    if ".cluster-" in db_host:
                        cluster_id = db_host.split(".")[0]
                        try:
                            response = rds_client.describe_db_clusters(
                                DBClusterIdentifier=cluster_id
                            )
                            instances = [
                                member["DBInstanceIdentifier"]
                                for member in response["DBClusters"][0]["DBClusterMembers"]
                            ]
                        except Exception as e:
                            logger.warning(f"í´ëŸ¬ìŠ¤í„° {cluster_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")
                            continue
                    else:
                        # ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš°
                        instance_id = db_host.split(".")[0]
                        instances = [instance_id]

                    for instance in instances:
                        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
                        if instance in processed_instances:
                            continue
                        processed_instances.append(instance)
                        
                        log_content = []

                        # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                        try:
                            log_file_list = rds_client.describe_db_log_files(
                                DBInstanceIdentifier=instance, FilenameContains="error"
                            )
                        except Exception as e:
                            logger.error(f"ì¸ìŠ¤í„´ìŠ¤ {instance} ë¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                            results.append(
                                f"<{instance}>\në¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n</{instance}>"
                            )
                            continue

                        for log_file_info in log_file_list["DescribeDBLogFiles"]:
                            log_filename = log_file_info["LogFileName"]
                            last_written = datetime.fromtimestamp(
                                log_file_info["LastWritten"] / 1000
                            )

                            if start_time_utc <= last_written <= end_time_utc:
                                # ë¡œê·¸ íŒŒì¼ ë‚´ìš© ë‹¤ìš´ë¡œë“œ
                                try:
                                    response = rds_client.download_db_log_file_portion(
                                        DBInstanceIdentifier=instance,
                                        LogFileName=log_filename,
                                        Marker="0",
                                    )

                                    log_data = response.get("LogFileData", "")
                                    lines = log_data.splitlines()

                                    # ì¤‘ìš”í•œ ì—ëŸ¬ ë¡œê·¸ í•­ëª© í•„í„°ë§
                                    error_keywords = [
                                        "error",
                                        "warning",
                                        "critical",
                                        "failed",
                                        "crash",
                                        "exception",
                                        "fatal",
                                        "corruption",
                                    ]

                                    for line in lines:
                                        if any(kw in line.lower() for kw in error_keywords):
                                            log_content.append(line)
                                except Exception as e:
                                    logger.error(f"ë¡œê·¸ íŒŒì¼ {log_filename} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                                    continue

                        # ë¡œê·¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ ê²°ê³¼ì— ì¶”ê°€
                        if log_content:
                            # ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í•  (ìµœëŒ€ 5000ì)
                            content_chunks = self._split_log_content(log_content, 5000)
                            for i, chunk in enumerate(content_chunks):
                                chunk_header = (
                                    f"<{instance}_chunk_{i+1}>"
                                    if len(content_chunks) > 1
                                    else f"<{instance}>"
                                )
                                chunk_footer = (
                                    f"</{instance}_chunk_{i+1}>"
                                    if len(content_chunks) > 1
                                    else f"</{instance}>"
                                )
                                results.append(
                                    f"{chunk_header}\n{chunk}\n{chunk_footer}"
                                )
                        else:
                            results.append(
                                f"<{instance}>\ní•´ë‹¹ ê¸°ê°„ì— ì—ëŸ¬ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n</{instance}>"
                            )

                except Exception as e:
                    logger.error(f"ì‹œí¬ë¦¿ {secret_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    results.append(
                        f"<{secret_name}>\në¡œê·¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n</{secret_name}>"
                    )

            if not results:
                return "âŒ ë¶„ì„í•  ì—ëŸ¬ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # Claudeë¥¼ í†µí•œ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„
            analysis_result = await self._analyze_error_logs_with_claude(results)

            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path("output") / f"error_log_analysis_{timestamp}.html"

            # HTML ë³´ê³ ì„œ ìƒì„±
            html_report = await self._generate_error_log_html_report(
                results,
                analysis_result,
                keyword,
                start_datetime_str,
                end_datetime_str,
                output_path,
            )

            return f"""âœ… Aurora MySQL ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì™„ë£Œ

ğŸ“Š ë¶„ì„ ìš”ì•½:
â€¢ ë¶„ì„ ê¸°ê°„: {start_datetime_str} ~ {end_datetime_str}
â€¢ ëŒ€ìƒ í‚¤ì›Œë“œ: {keyword}
â€¢ ì°¾ì€ ì‹œí¬ë¦¿: {len(secret_lists)}ê°œ
â€¢ ë¶„ì„ëœ ì¸ìŠ¤í„´ìŠ¤: {len(processed_instances)}ê°œ
â€¢ ìˆ˜ì§‘ëœ ë¡œê·¸ ì²­í¬: {len(results)}ê°œ

ğŸ¤– Claude AI ë¶„ì„ ê²°ê³¼:
{analysis_result}

ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {html_report}
"""

        except Exception as e:
            logger.error(f"ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"âŒ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _split_log_content(self, log_lines: List[str], max_chars: int) -> List[str]:
        """ë¡œê·¸ ë‚´ìš©ì„ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• """
        chunks = []
        current_chunk = []
        current_size = 0

        for line in log_lines:
            line_size = len(line) + 1  # +1 for newline

            if current_size + line_size > max_chars and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size

        if current_chunk:
            chunks.append("\n".join(current_chunk))

        return chunks

    async def _analyze_error_logs_with_claude(self, log_results: List[str]) -> str:
        """Claudeë¥¼ í†µí•œ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„"""
        try:
            # ë¡œê·¸ ë‚´ìš© ê²°í•©
            combined_logs = "\n".join(log_results)

            prompt = f"""ì•„ë˜ëŠ” Aurora MySQL 3.5 ì¸ìŠ¤í„´ìŠ¤ì˜ ì—ëŸ¬ ë¡œê·¸ì…ë‹ˆë‹¤. ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì—ëŸ¬ë¡œê·¸ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì‚¬í•­ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”:

<instanceëª…>ê³¼ </instanceëª…> ì‚¬ì´ì— ìˆëŠ” ë¡œê·¸ëŠ” í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì˜ error logì…ë‹ˆë‹¤.

ì–´ë–¤ í‚¤ì›Œë“œì˜ ì—ëŸ¬ê°€ ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚¬ëŠ”ì§€, ì—ëŸ¬ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì§‘ê³„ë„ ë¶€íƒí•©ë‹ˆë‹¤.
ì•„ë˜ì™€ ê°™ì€ í¬ë§·ìœ¼ë¡œ ê° ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ ì—ëŸ¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì§‘ê³„í•˜ê³ , ë¶„ì„í•œ ë‚´ìš©ì„ ë„£ì–´ì£¼ì„¸ìš”.

ì˜ˆë¥¼ ë“¤ì–´ aborted connectionì´ ëª‡ê±´ìˆì—ˆê³ , ê·¸ê²ƒì€ ì–´ë–¤ ì˜í–¥ì„ ê°€ì§€ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë¶„ì„í• ë•Œ ì–´ëŠ ì¸ìŠ¤í„´ìŠ¤ì— ìˆëŠ” ì–´ë–¤ ë‚´ìš©ì„ ê·¼ê±°ë¡œ í–ˆëŠ”ì§€ ëª…í™•í•˜ê²Œ í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ëª¨ë¥´ê² ë‹¤ê³  í•©ë‹ˆë‹¤.

**ë¶„ì„ ê²°ê³¼:**

1. **ì „ì²´ ìš”ì•½**
   - ì´ ì—ëŸ¬ ê±´ìˆ˜: Xê±´
   - ì‹¬ê°ë„ë³„ ë¶„ë¥˜: ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ
   - ì£¼ìš” ì—ëŸ¬ íŒ¨í„´: 

2. **ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë¶„ì„**

3. **ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­**

<context>
ì‹¬ê°í•œ ì—ëŸ¬ í‚¤ì›Œë“œ:
1. "Fatal error" - ì˜í–¥ë„: ë§¤ìš° ë†’ìŒ (ë°ì´í„°ë² ì´ìŠ¤ ì„œë²„ ì¤‘ì§€/ì¬ì‹œì‘ ê°€ëŠ¥)
2. "Out of memory" - ì˜í–¥ë„: ë†’ìŒ (ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜/ì¿¼ë¦¬ ì‹¤íŒ¨)
3. "Disk full" - ì˜í–¥ë„: ë†’ìŒ (ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì“°ê¸° ì‘ì—… ì‹¤íŒ¨)
4. "Connection refused" - ì˜í–¥ë„: ì¤‘ê°„~ë†’ìŒ (í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ë¬¸ì œ)
5. "InnoDB: Corruption" - ì˜í–¥ë„: ë†’ìŒ (ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ, ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)

ì£¼ì˜ê°€ í•„ìš”í•œ ì—ëŸ¬ í‚¤ì›Œë“œ:
6. "Slow query" - ì˜í–¥ë„: ì¤‘ê°„ (ì„±ëŠ¥ ì €í•˜)
7. "Lock wait timeout exceeded" - ì˜í–¥ë„: ì¤‘ê°„ (ë™ì‹œì„± ë¬¸ì œ)
8. "Warning" - ì˜í–¥ë„: ë‚®ìŒ~ì¤‘ê°„ (ì ì¬ì  ë¬¸ì œ)
9. "Table is full" - ì˜í–¥ë„: ì¤‘ê°„ (í…Œì´ë¸” ìš©ëŸ‰ ì´ˆê³¼)
10. "Deadlock found" - ì˜í–¥ë„: ì¤‘ê°„ (íŠ¸ëœì­ì…˜ ì¶©ëŒ)
</context>

{combined_logs}
"""

            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4096,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 í˜¸ì¶œ ì‹œë„
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                claude_response = response_body.get("content", [{}])[0].get("text", "")
                logger.info("Claude Sonnet 4ë¡œ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì™„ë£Œ")
                return claude_response

            except Exception as e:
                logger.warning(f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨, fallback ì‹œë„: {e}")

                # Claude 3.7 Sonnet í˜¸ì¶œ (fallback)
                try:
                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    response_body = json.loads(response.get("body").read())
                    claude_response = response_body.get("content", [{}])[0].get(
                        "text", ""
                    )
                    logger.info("Claude 3.7 Sonnetìœ¼ë¡œ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ì™„ë£Œ")
                    return claude_response

                except Exception as e2:
                    logger.error(f"Claude í˜¸ì¶œ ì™„ì „ ì‹¤íŒ¨: {e2}")
                    return f"Claude ë¶„ì„ ì‹¤íŒ¨: {str(e2)}"

        except Exception as e:
            logger.error(f"ì—ëŸ¬ ë¡œê·¸ Claude ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _generate_error_log_html_report(
        self,
        log_results: List[str],
        analysis_result: str,
        keyword: str,
        start_time: str,
        end_time: str,
        output_path: Path,
    ) -> str:
        """ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora MySQL ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.5em; }}
        .header .subtitle {{ margin-top: 10px; opacity: 0.9; }}
        .content {{ padding: 30px; }}
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .summary-card {{ background: #f8f9fa; border-left: 4px solid #007bff; padding: 20px; border-radius: 5px; }}
        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .summary-card .value {{ font-size: 1.5em; font-weight: bold; color: #007bff; }}
        .analysis-section {{ margin-bottom: 30px; }}
        .analysis-section h2 {{ color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
        .log-content {{ background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: monospace; font-size: 0.9em; max-height: 400px; overflow-y: auto; }}
        .error-high {{ color: #dc3545; font-weight: bold; }}
        .error-medium {{ color: #fd7e14; }}
        .error-low {{ color: #6c757d; }}
        .footer {{ text-align: center; padding: 20px; color: #6c757d; border-top: 1px solid #dee2e6; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ” Aurora MySQL ì—ëŸ¬ ë¡œê·¸ ë¶„ì„</h1>
            <div class="subtitle">ìƒì„±ì¼ì‹œ: {timestamp}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-card">
                    <h3>ë¶„ì„ ê¸°ê°„</h3>
                    <div class="value">{start_time}<br>~<br>{end_time}</div>
                </div>
                <div class="summary-card">
                    <h3>ëŒ€ìƒ í‚¤ì›Œë“œ</h3>
                    <div class="value">{keyword}</div>
                </div>
                <div class="summary-card">
                    <h3>ë¡œê·¸ ì²­í¬ ìˆ˜</h3>
                    <div class="value">{len(log_results)}ê°œ</div>
                </div>
            </div>
            
            <div class="analysis-section">
                <h2>ğŸ¤– Claude AI ë¶„ì„ ê²°ê³¼</h2>
                <div class="log-content">
                    {analysis_result.replace(chr(10), '<br>')}
                </div>
            </div>
            
            <div class="analysis-section">
                <h2>ğŸ“‹ ì›ë³¸ ë¡œê·¸ ë°ì´í„°</h2>
                {''.join([f'<div class="log-content">{log.replace(chr(10), "<br>")}</div>' for log in log_results])}
            </div>
        </div>
        
        <div class="footer">
            <p>DB Assistant MCP Server - Aurora MySQL ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ</p>
        </div>
    </div>
</body>
</html>"""

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # HTML íŒŒì¼ ì €ì¥
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            return str(output_path)

        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def save_to_vector_store(
        self,
        content: str,
        topic: str,
        category: str = "examples",
        tags: list = None,
        force_save: bool = False,
        auto_summarize: bool = True,
    ) -> str:
        """ëŒ€í™” ë‚´ìš©ì„ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ (ìë™ ìš”ì•½, ì¤‘ë³µ ë° ìƒì¶© ê²€ì‚¬ í¬í•¨)"""
        try:
            import os
            from datetime import datetime
            import re

            # 0. ì¤‘ìš”ë„ ë° ê¸¸ì´ ê¸°ë°˜ ì €ì¥ ë°©ì‹ ê²°ì •
            original_content = content
            s3_full_content_path = None
            importance_score = self._calculate_importance_score(content)

            # ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë©´ ê¸¸ì´ì™€ ìƒê´€ì—†ì´ ì „ì²´ ì €ì¥, ë‚®ìœ¼ë©´ ê¸¸ì´ì— ë”°ë¼ ì²˜ë¦¬
            if importance_score >= 0.7:  # ë†’ì€ ì¤‘ìš”ë„
                # ì „ì²´ ë‚´ìš©ì„ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
                pass
            elif len(content) > 1000:
                if importance_score >= 0.4:  # ì¤‘ê°„ ì¤‘ìš”ë„
                    # ìš”ì•½í•´ì„œ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
                    if auto_summarize:
                        content = await self._summarize_content(
                            content, topic, category
                        )
                else:  # ë‚®ì€ ì¤‘ìš”ë„
                    # ì›ë³¸ì„ S3ì— ì €ì¥í•˜ê³  ë©”íƒ€ì •ë³´ë§Œ ë²¡í„° ì €ì¥ì†Œì—
                    s3_full_content_path = await self._save_full_content_to_s3(
                        content, topic, category
                    )
                    content = await self._create_metadata_summary(
                        content, topic, s3_full_content_path
                    )

            # 1. ê°•ì œ ì €ì¥ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¤‘ë³µ/ìƒì¶© ê²€ì‚¬
            if not force_save:
                duplicate_check = await self._check_content_similarity(
                    content, category
                )

                if duplicate_check["is_duplicate"]:
                    return f"""âš ï¸ ì¤‘ë³µëœ ë‚´ìš©ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!

ğŸ” ìœ ì‚¬í•œ ê¸°ì¡´ ë¬¸ì„œ:
ğŸ“„ íŒŒì¼: {duplicate_check["similar_file"]}
ğŸ“Š ìœ ì‚¬ë„: {duplicate_check["similarity_score"]:.1%}

ğŸ’¡ ë‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:
1. 'update_vector_content' ë„êµ¬ë¡œ ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
2. 'save_to_vector_store'ì— force_save=trueë¡œ ê°•ì œ ì €ì¥
3. ì €ì¥ ì·¨ì†Œ"""

                if duplicate_check["has_conflict"]:
                    return f"""ğŸš¨ ê¸°ì¡´ ë‚´ìš©ê³¼ ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!

âš ï¸ ìƒì¶© ë‚´ìš©:
{duplicate_check["conflict_details"]}

ğŸ¤” ë‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:
1. ìƒˆë¡œìš´ ì •ë³´ê°€ ë§ë‹¤ë©´ 'update_vector_content'ë¡œ ê¸°ì¡´ ë¬¸ì„œ êµì²´
2. ê¸°ì¡´ ì •ë³´ê°€ ë§ë‹¤ë©´ ì €ì¥ ì·¨ì†Œ
3. ë‘˜ ë‹¤ ë§ë‹¤ë©´ 'save_to_vector_store'ì— force_save=trueë¡œ ë³„ë„ ì €ì¥"""

            # 2. ì¤‘ë³µ/ìƒì¶©ì´ ì—†ìœ¼ë©´ ì •ìƒ ì €ì¥ ì§„í–‰
            date_str = datetime.now().strftime("%Y%m%d")
            clean_topic = re.sub(r"[^a-zA-Z0-9]", "", topic.lower())[:10]
            if not clean_topic:
                clean_topic = "content"

            filename = f"{date_str}_{clean_topic}.md"

            # vector í´ë” ìƒì„±
            vector_dir = "vector"
            os.makedirs(vector_dir, exist_ok=True)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            if tags is None:
                tags = ["conversation", "analysis"]

            metadata_tags = tags + ["database", "optimization", "best-practices"]

            # YAML í—¤ë” ìƒì„±
            yaml_header = f"""---
title: "{topic}"
category: "{category}"
tags: {metadata_tags}
version: "1.0"
last_updated: "{datetime.now().strftime('%Y-%m-%d')}"
author: "DB Assistant"
source: "conversation"
similarity_checked: true
---

"""

            # íŒŒì¼ ë‚´ìš© ìƒì„±
            file_content = yaml_header + content

            # ë¡œì»¬ íŒŒì¼ ì €ì¥
            local_path = os.path.join(vector_dir, filename)
            with open(local_path, "w", encoding="utf-8") as f:
                f.write(file_content)

            # S3 í‚¤ ìƒì„±
            s3_key = f"{category}/{filename}"

            # ê¸°ì¡´ íŒŒì¼ ë²„ì „ í™•ì¸
            version_info = await self._check_file_version_in_s3(s3_key)

            # ë²„ì „ ì—…ë°ì´íŠ¸
            if version_info["exists"]:
                current_version = version_info["version"]
                try:
                    major, minor = map(int, current_version.split("."))
                    new_version = f"{major}.{minor + 1}"
                except:
                    new_version = "1.1"
            else:
                new_version = "1.0"

            # ë©”íƒ€ë°ì´í„° ìƒì„± (S3 ê²½ë¡œ ì •ë³´ í¬í•¨)
            if tags is None:
                tags = ["conversation", "analysis"]

            metadata_tags = tags + ["database", "optimization", "best-practices"]

            # YAML í—¤ë” ìƒì„± (ë²„ì „ ì •ë³´ í¬í•¨)
            yaml_header = f"""---
title: "{topic}"
category: "{category}"
tags: {metadata_tags}
version: "{new_version}"
last_updated: "{datetime.now().strftime('%Y-%m-%d')}"
author: "DB Assistant"
source: "conversation"
s3_path: "s3://{BEDROCK_AGENT_BUCKET}/{s3_key}"
similarity_checked: true
---

"""

            # íŒŒì¼ ë‚´ìš© ìƒì„±
            file_content = yaml_header + content

            # ë¡œì»¬ íŒŒì¼ ì €ì¥
            local_path = os.path.join(vector_dir, filename)
            with open(local_path, "w", encoding="utf-8") as f:
                f.write(file_content)

            # S3ì— ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì—…ë¡œë“œ
            s3_client = boto3.client("s3", region_name="us-east-1")

            s3_client.upload_file(
                local_path,
                BEDROCK_AGENT_BUCKET,
                s3_key,
                ExtraArgs={
                    "ContentType": "text/markdown",
                    "Metadata": {
                        "version": new_version,
                        "category": category,
                        "title": topic,
                        "author": "DB Assistant",
                        "tags": ",".join(metadata_tags),
                    },
                },
            )

            logger.info(f"ë²¡í„° ì €ì¥ì†Œì— íŒŒì¼ ì €ì¥ ì™„ë£Œ: {s3_key}")

            # ìë™ìœ¼ë¡œ Knowledge Base ë™ê¸°í™” ì‹¤í–‰
            sync_result = await self.sync_knowledge_base()

            # ì¤‘ìš”ë„ ë° ì €ì¥ ë°©ì‹ ì •ë³´ ì¶”ê°€
            storage_info = f"\nğŸ¯ ì¤‘ìš”ë„: {importance_score:.2f}"

            if importance_score >= 0.7:
                storage_info += " (ë†’ìŒ - ì „ì²´ ì €ì¥)"
            elif len(original_content) > 1000:
                if importance_score >= 0.4:
                    storage_info += " (ì¤‘ê°„ - ìš”ì•½ ì €ì¥)"
                    if s3_full_content_path:
                        storage_info += f"\nğŸ“„ ì „ì²´ ë‚´ìš©: {s3_full_content_path}"
                    storage_info += (
                        f"\nğŸ“ ìš”ì•½: {len(original_content)} â†’ {len(content)} ë¬¸ì"
                    )
                else:
                    storage_info += " (ë‚®ìŒ - ë©”íƒ€ì •ë³´ë§Œ)"
                    if s3_full_content_path:
                        storage_info += f"\nğŸ“„ ì „ì²´ ë‚´ìš©: {s3_full_content_path} ({len(original_content)} ë¬¸ì)"
                        storage_info += (
                            f"\nğŸ“ ë²¡í„° ì €ì¥: ë©”íƒ€ì •ë³´ë§Œ ({len(content)} ë¬¸ì)"
                        )
            else:
                storage_info += " (ì „ì²´ ì €ì¥)"

            return f"""âœ… ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ ì™„ë£Œ!

ğŸ“ ë¡œì»¬ ì €ì¥: {local_path}
â˜ï¸ S3 ì €ì¥: s3://{BEDROCK_AGENT_BUCKET}/{s3_key}
ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {category}
ğŸ“ ë²„ì „: {new_version} {"(ì—…ë°ì´íŠ¸)" if version_info["exists"] else "(ì‹ ê·œ)"}
ğŸ”– íƒœê·¸: {', '.join(metadata_tags)}{storage_info}
âœ… ì¤‘ë³µ/ìƒì¶© ê²€ì‚¬: í†µê³¼

ğŸ”„ Knowledge Base ë™ê¸°í™” ìë™ ì‹¤í–‰:
{sync_result}"""

        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return f"âŒ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _check_content_similarity(self, new_content: str, category: str) -> dict:
        """ê¸°ì¡´ ë‚´ìš©ê³¼ ì¤‘ë³µ/ìƒì¶© ê²€ì‚¬"""
        try:
            # 1. Knowledge Baseì—ì„œ ìœ ì‚¬í•œ ë‚´ìš© ê²€ìƒ‰
            similar_docs = await self._search_similar_content(new_content, category)

            # 2. Claude AIë¡œ ì¤‘ë³µ/ìƒì¶© ë¶„ì„
            if similar_docs:
                analysis = await self._analyze_content_conflicts(
                    new_content, similar_docs
                )
                return analysis

            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

        except Exception as e:
            logger.error(f"ë‚´ìš© ìœ ì‚¬ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

    async def _search_similar_content(self, content: str, category: str) -> list:
        """Knowledge Baseì—ì„œ ìœ ì‚¬í•œ ë‚´ìš© ê²€ìƒ‰"""
        try:
            # ë‚´ìš©ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(content)
            search_query = " ".join(keywords[:5])  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ ì‚¬ìš©

            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": search_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {
                        "numberOfResults": 3,
                        "overrideSearchType": "SEMANTIC",
                    }
                },
            )

            similar_docs = []
            for result in response.get("retrievalResults", []):
                if result["score"] > 0.7:  # 70% ì´ìƒ ìœ ì‚¬ë„
                    similar_docs.append(
                        {
                            "content": result["content"]["text"],
                            "score": result["score"],
                            "source": result["location"]["s3Location"]["uri"],
                        }
                    )

            return similar_docs

        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë‚´ìš© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _extract_keywords(self, content: str) -> list:
        """ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re

        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê¸°ë²• ì‚¬ìš© ê°€ëŠ¥)
        words = re.findall(r"\b[a-zA-Zê°€-í£]{3,}\b", content.lower())

        # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„
        db_keywords = [
            "mysql",
            "aurora",
            "index",
            "query",
            "performance",
            "optimization",
            "table",
            "database",
            "sql",
            "schema",
            "connection",
            "error",
            "log",
        ]

        # ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„  ì •ë ¬
        keywords = []
        for keyword in db_keywords:
            if keyword in words:
                keywords.append(keyword)

        # ë‚˜ë¨¸ì§€ í‚¤ì›Œë“œ ì¶”ê°€ (ë¹ˆë„ìˆœ)
        word_freq = {}
        for word in words:
            if word not in keywords and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1

        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords.extend([word for word, freq in sorted_words[:10]])

        return keywords

    async def _analyze_content_conflicts(
        self, new_content: str, similar_docs: list
    ) -> dict:
        """Claude AIë¡œ ë‚´ìš© ì¤‘ë³µ/ìƒì¶© ë¶„ì„"""
        try:
            similar_content = "\n\n".join(
                [
                    f"ë¬¸ì„œ {i+1}: {doc['content'][:500]}..."
                    for i, doc in enumerate(similar_docs)
                ]
            )

            prompt = f"""ë‹¤ìŒ ìƒˆë¡œìš´ ë‚´ìš©ê³¼ ê¸°ì¡´ ë¬¸ì„œë“¤ì„ ë¹„êµí•˜ì—¬ ì¤‘ë³µì„±ê³¼ ìƒì¶©ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ìƒˆë¡œìš´ ë‚´ìš©:
{new_content}

ê¸°ì¡´ ë¬¸ì„œë“¤:
{similar_content}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì¤‘ë³µì„±: ìƒˆë¡œìš´ ë‚´ìš©ì´ ê¸°ì¡´ ë¬¸ì„œì™€ 80% ì´ìƒ ìœ ì‚¬í•œê°€?
2. ìƒì¶©ì„±: ìƒˆë¡œìš´ ë‚´ìš©ì´ ê¸°ì¡´ ë¬¸ì„œì™€ ëª¨ìˆœë˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
DUPLICATE: true/false
CONFLICT: true/false
SIMILARITY_SCORE: 0.0-1.0
SIMILAR_FILE: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ë²ˆí˜¸
CONFLICT_DETAILS: ìƒì¶©ë˜ëŠ” ë‚´ìš© ì„¤ëª… (ìƒì¶©ì´ ìˆì„ ê²½ìš°ë§Œ)"""

            response = self.bedrock_runtime.invoke_model(
                modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
                body=json.dumps(
                    {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 1000,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                ),
            )

            response_body = json.loads(response.get("body").read())
            analysis_text = response_body.get("content", [{}])[0].get("text", "")

            # ì‘ë‹µ íŒŒì‹±
            is_duplicate = "DUPLICATE: true" in analysis_text.lower()
            has_conflict = "CONFLICT: true" in analysis_text.lower()

            # ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ
            similarity_score = 0.0
            if similar_docs:
                similarity_score = similar_docs[0]["score"]

            # ê°€ì¥ ìœ ì‚¬í•œ íŒŒì¼ ì¶”ì¶œ
            similar_file = None
            if similar_docs:
                similar_file = similar_docs[0]["source"].split("/")[-1]

            # ìƒì¶© ë‚´ìš© ì¶”ì¶œ
            conflict_details = None
            if has_conflict:
                lines = analysis_text.split("\n")
                for line in lines:
                    if "CONFLICT_DETAILS:" in line:
                        conflict_details = line.split("CONFLICT_DETAILS:")[1].strip()
                        break

            return {
                "is_duplicate": is_duplicate,
                "has_conflict": has_conflict,
                "similarity_score": similarity_score,
                "similar_file": similar_file,
                "conflict_details": conflict_details,
            }

        except Exception as e:
            logger.error(f"ë‚´ìš© ìƒì¶© ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

    def _calculate_importance_score(self, content: str) -> float:
        """ë‚´ìš©ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (0.0-1.0)"""
        import re

        content_lower = content.lower()
        score = 0.0

        # 1. ê³ ì¤‘ìš”ë„ í‚¤ì›Œë“œ (0.3ì )
        high_importance_keywords = [
            "error",
            "critical",
            "urgent",
            "performance",
            "optimization",
            "security",
            "vulnerability",
            "bug",
            "issue",
            "problem",
            "solution",
            "fix",
            "troubleshooting",
            "best practice",
        ]
        for keyword in high_importance_keywords:
            if keyword in content_lower:
                score += 0.03

        # 2. ê¸°ìˆ ì  ë‚´ìš© (0.2ì )
        technical_keywords = [
            "sql",
            "query",
            "index",
            "database",
            "mysql",
            "aurora",
            "configuration",
            "parameter",
            "schema",
            "table",
            "connection",
        ]
        for keyword in technical_keywords:
            if keyword in content_lower:
                score += 0.02

        # 3. êµ¬ì²´ì  ìˆ˜ì¹˜/ëª…ë ¹ì–´ í¬í•¨ (0.2ì )
        if re.search(r"\d+\.\d+|\d+%|[0-9]+\s*(mb|gb|ms|sec)", content_lower):
            score += 0.1
        if re.search(r"(select|insert|update|delete|create|alter|drop)", content_lower):
            score += 0.1

        # 4. êµ¬ì¡°í™”ëœ ë‚´ìš© (0.2ì )
        if content.count("\n") > 5:  # ì—¬ëŸ¬ ì¤„
            score += 0.05
        if content.count("```") >= 2:  # ì½”ë“œ ë¸”ë¡
            score += 0.1
        if re.search(r"^\s*[-*+]\s", content, re.MULTILINE):  # ë¦¬ìŠ¤íŠ¸
            score += 0.05

        # 5. ê¸¸ì´ ë³´ì • (0.1ì )
        if len(content) > 500:
            score += 0.05
        if len(content) > 2000:
            score += 0.05

        return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

    async def _save_full_content_to_s3(
        self, content: str, topic: str, category: str
    ) -> str:
        """ê¸´ ë‚´ìš©ì„ S3ì— ì €ì¥í•˜ê³  ê²½ë¡œ ë°˜í™˜"""
        try:
            from datetime import datetime
            import re

            date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            clean_topic = re.sub(r"[^a-zA-Z0-9]", "", topic.lower())[:10]
            filename = f"full_content_{date_str}_{clean_topic}.md"
            s3_key = f"{category}/full_content/{filename}"

            s3_client = boto3.client("s3", region_name="us-east-1")
            s3_client.put_object(
                Bucket=BEDROCK_AGENT_BUCKET,
                Key=s3_key,
                Body=content.encode("utf-8"),
                ContentType="text/markdown",
            )

            s3_path = f"s3://{BEDROCK_AGENT_BUCKET}/{s3_key}"
            logger.info(f"ì „ì²´ ë‚´ìš© S3 ì €ì¥: {s3_path}")
            return s3_path

        except Exception as e:
            logger.error(f"S3 ì „ì²´ ë‚´ìš© ì €ì¥ ì˜¤ë¥˜: {e}")
            return None

    async def _create_metadata_summary(
        self, content: str, topic: str, s3_path: str
    ) -> str:
        """ê¸´ ë‚´ìš©ì˜ ë©”íƒ€ì •ë³´ ìš”ì•½ ìƒì„±"""
        try:
            prompt = f"""ë‹¤ìŒ ê¸´ ë‚´ìš©ì„ ë©”íƒ€ì •ë³´ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì „ì²´ ë‚´ìš©ì€ S3ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì£¼ì œ: {topic}
ì „ì²´ ë‚´ìš© ìœ„ì¹˜: {s3_path}

ì›ë³¸ ë‚´ìš©:
{content[:2000]}...

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë©”íƒ€ì •ë³´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
## {topic} - ë©”íƒ€ì •ë³´

**ğŸ“„ ì „ì²´ ë‚´ìš©**: {s3_path}
**ğŸ“Š ë‚´ìš© í¬ê¸°**: {len(content)} ë¬¸ì
**ğŸ” ì£¼ìš” í‚¤ì›Œë“œ**: [í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ]
**ğŸ“ ìš”ì•½**: [3-4ì¤„ í•µì‹¬ ìš”ì•½]
**ğŸ¯ ì£¼ìš” ë‚´ìš©**:
- [í•µì‹¬ í¬ì¸íŠ¸ 1]
- [í•µì‹¬ í¬ì¸íŠ¸ 2]
- [í•µì‹¬ í¬ì¸íŠ¸ 3]

**ğŸ’¡ í™œìš© ë°©ë²•**: ì „ì²´ ë‚´ìš©ì´ í•„ìš”í•œ ê²½ìš° ìœ„ S3 ê²½ë¡œì—ì„œ í™•ì¸ ê°€ëŠ¥"""

            response = self.bedrock_runtime.invoke_model(
                modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
                body=json.dumps(
                    {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 1000,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                ),
            )

            response_body = json.loads(response.get("body").read())
            metadata_summary = response_body.get("content", [{}])[0].get("text", "")

            logger.info(
                f"ë©”íƒ€ì •ë³´ ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(content)} -> {len(metadata_summary)} ë¬¸ì"
            )
            return metadata_summary

        except Exception as e:
            logger.error(f"ë©”íƒ€ì •ë³´ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"## {topic} - ë©”íƒ€ì •ë³´\n\n**ğŸ“„ ì „ì²´ ë‚´ìš©**: {s3_path}\n**ğŸ“Š ë‚´ìš© í¬ê¸°**: {len(content)} ë¬¸ì\n**ğŸ’¡ í™œìš© ë°©ë²•**: ì „ì²´ ë‚´ìš©ì´ í•„ìš”í•œ ê²½ìš° ìœ„ S3 ê²½ë¡œì—ì„œ í™•ì¸ ê°€ëŠ¥"

    async def _summarize_content(self, content: str, topic: str, category: str) -> str:
        """ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ìš”ì•½"""
        try:
            prompt = f"""ë‹¤ìŒ ë‚´ìš©ì„ {category} ì¹´í…Œê³ ë¦¬ì— ë§ê²Œ í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {topic}
ì¹´í…Œê³ ë¦¬: {category}

ì›ë³¸ ë‚´ìš©:
{content}

ìš”ì•½ ê¸°ì¤€:
1. í•µì‹¬ ê¸°ìˆ  ì •ë³´ì™€ í•´ê²°ì±… ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ëª…ë ¹ì–´, ì„¤ì •ê°’ì€ ìœ ì§€
3. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë°˜ë³µ ë‚´ìš© ì œê±°
4. ì›ë³¸ ê¸¸ì´ì˜ 30-50% ìˆ˜ì¤€ìœ¼ë¡œ ì••ì¶•
5. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìœ ì§€

ìš”ì•½ëœ ë‚´ìš©:"""

            response = self.bedrock_runtime.invoke_model(
                modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
                body=json.dumps(
                    {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 2000,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                ),
            )

            response_body = json.loads(response.get("body").read())
            summarized = response_body.get("content", [{}])[0].get("text", content)

            logger.info(f"ë‚´ìš© ìš”ì•½ ì™„ë£Œ: {len(content)} -> {len(summarized)} ë¬¸ì")
            return summarized

        except Exception as e:
            logger.error(f"ë‚´ìš© ìš”ì•½ ì˜¤ë¥˜: {e}")
            return content  # ìš”ì•½ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜

    async def sync_knowledge_base(self) -> str:
        """Knowledge Base ë°ì´í„° ì†ŒìŠ¤ ë™ê¸°í™”"""
        try:
            bedrock_agent_client = boto3.client(
                "bedrock-agent", region_name="us-east-1"
            )

            response = bedrock_agent_client.start_ingestion_job(
                knowledgeBaseId=KNOWLEDGE_BASE_ID, dataSourceId=DATA_SOURCE_ID
            )

            job_id = response["ingestionJob"]["ingestionJobId"]
            status = response["ingestionJob"]["status"]

            logger.info(f"Knowledge Base ë™ê¸°í™” ì‹œì‘: {job_id}")

            return f"""âœ… Knowledge Base ë™ê¸°í™” ì‹œì‘!

ğŸ”„ ì‘ì—… ID: {job_id}
ğŸ“Š ìƒíƒœ: {status}
â° ì‹œì‘ ì‹œê°„: {response['ingestionJob']['startedAt']}

ğŸ’¡ ë™ê¸°í™”ê°€ ì™„ë£Œë˜ë©´ ìƒˆë¡œìš´ ë‚´ìš©ì„ Knowledge Baseì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìƒíƒœ í™•ì¸: AWS ì½˜ì†” > Bedrock > Knowledge Base > ë°ì´í„° ì†ŒìŠ¤"""

        except Exception as e:
            logger.error(f"Knowledge Base ë™ê¸°í™” ì˜¤ë¥˜: {e}")
            return f"âŒ Knowledge Base ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def query_vector_store(self, query: str, max_results: int = 5) -> str:
        """ë²¡í„° ì €ì¥ì†Œì—ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
        try:
            bedrock_agent_runtime = boto3.client(
                "bedrock-agent-runtime", region_name="us-east-1"
            )

            # Knowledge Baseì—ì„œ ê²€ìƒ‰
            response = bedrock_agent_runtime.retrieve(
                knowledgeBaseId=KNOWLEDGE_BASE_ID,
                retrievalQuery={"text": query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": max_results}
                },
            )

            if not response.get("retrievalResults"):
                return f"""ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.

ğŸ” ê²€ìƒ‰ì–´: '{query}'
ğŸ’¡ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.
ğŸ“ ì˜ˆì‹œ: 'HLL', 'lock', 'performance', 'SQL' ë“±"""

            results = []
            for i, result in enumerate(response["retrievalResults"], 1):
                content = result["content"]["text"]
                score = result.get("score", 0)

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° S3 ê²½ë¡œ ì •ë³´ í¬í•¨
                metadata = result.get("metadata", {})
                s3_uri = metadata.get("x-amz-bedrock-kb-source-uri", "")
                source_file = metadata.get("source", "")

                # S3 URIì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                if s3_uri:
                    source = s3_uri
                    # S3ì—ì„œ ì „ì²´ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                    try:
                        full_content = await self._get_full_content_from_s3(s3_uri)
                        if full_content:
                            content = full_content
                    except Exception as e:
                        logger.warning(f"S3ì—ì„œ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                else:
                    source = source_file if source_file else "ì•Œ ìˆ˜ ì—†ìŒ"

                # ë‚´ìš© ê¸¸ì´ ì œí•œ
                preview = content[:500] + "..." if len(content) > 500 else content

                results.append(
                    f"""ğŸ“„ **ê²°ê³¼ {i}** (ê´€ë ¨ë„: {score:.2f})
ğŸ“ ì¶œì²˜: {source}
ğŸ“ ë‚´ìš©:
{preview}
"""
                )

            return f"""ğŸ” **ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰ ê²°ê³¼**

ğŸ” ê²€ìƒ‰ì–´: "{query}"
ğŸ“Š ì´ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬

{chr(10).join(results)}

ğŸ’¡ ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ í‚¤ì›Œë“œë¥¼ ì„¸ë¶„í™”í•´ë³´ì„¸ìš”."""

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    async def _get_full_content_from_s3(self, s3_uri: str) -> str:
        """S3 URIì—ì„œ ì „ì²´ íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        try:
            # S3 URI íŒŒì‹± (s3://bucket/key í˜•ì‹)
            if not s3_uri.startswith("s3://"):
                return ""

            uri_parts = s3_uri[5:].split("/", 1)
            if len(uri_parts) != 2:
                return ""

            bucket_name = uri_parts[0]
            object_key = uri_parts[1]

            # S3 í´ë¼ì´ì–¸íŠ¸ë¡œ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            s3_client = boto3.client("s3", region_name="us-east-1")
            response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
            content = response["Body"].read().decode("utf-8")

            logger.info(f"S3ì—ì„œ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {s3_uri}")
            return content

        except Exception as e:
            logger.error(f"S3ì—ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {s3_uri}: {e}")
            return ""

    async def _check_file_version_in_s3(self, s3_key: str) -> dict:
        """S3ì—ì„œ íŒŒì¼ ë²„ì „ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤"""
        try:
            s3_client = boto3.client("s3", region_name="us-east-1")

            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ë©”íƒ€ë°ì´í„° í™•ì¸
            try:
                response = s3_client.head_object(Bucket=BEDROCK_AGENT_BUCKET, Key=s3_key)

                # ë©”íƒ€ë°ì´í„°ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
                metadata = response.get("Metadata", {})
                current_version = metadata.get("version", "1.0")
                last_modified = response.get("LastModified")

                return {
                    "exists": True,
                    "version": current_version,
                    "last_modified": last_modified,
                    "etag": response.get("ETag", "").strip('"'),
                }

            except s3_client.exceptions.NoSuchKey:
                return {
                    "exists": False,
                    "version": "1.0",
                    "last_modified": None,
                    "etag": None,
                }

        except Exception as e:
            logger.error(f"S3 íŒŒì¼ ë²„ì „ í™•ì¸ ì‹¤íŒ¨ {s3_key}: {e}")
            return {
                "exists": False,
                "version": "1.0",
                "last_modified": None,
                "etag": None,
            }

    async def update_vector_content(
        self, filename: str, new_content: str, update_mode: str = "append"
    ) -> str:
        """ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        try:
            import os
            from datetime import datetime

            # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
            local_path = os.path.join("vector", filename)

            if not os.path.exists(local_path):
                return f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
            with open(local_path, "r", encoding="utf-8") as f:
                existing_content = f.read()

            # YAML í—¤ë”ì™€ ë³¸ë¬¸ ë¶„ë¦¬
            if existing_content.startswith("---"):
                parts = existing_content.split("---", 2)
                if len(parts) >= 3:
                    yaml_header = f"---{parts[1]}---"
                    existing_body = parts[2].strip()
                else:
                    yaml_header = ""
                    existing_body = existing_content
            else:
                yaml_header = ""
                existing_body = existing_content

            # ì—…ë°ì´íŠ¸ ëª¨ë“œì— ë”°ë¥¸ ë‚´ìš© ì²˜ë¦¬
            if update_mode == "replace":
                updated_body = new_content
            else:  # append
                updated_body = f"{existing_body}\n\n## ì—…ë°ì´íŠ¸ ({datetime.now().strftime('%Y-%m-%d %H:%M')})\n\n{new_content}"

            # YAML í—¤ë” ì—…ë°ì´íŠ¸
            if yaml_header:
                # last_updated í•„ë“œ ì—…ë°ì´íŠ¸
                import re

                yaml_header = re.sub(
                    r'last_updated: "[^"]*"',
                    f'last_updated: "{datetime.now().strftime("%Y-%m-%d")}"',
                    yaml_header,
                )
                # version ì—…ë°ì´íŠ¸
                version_match = re.search(r'version: "([^"]*)"', yaml_header)
                if version_match:
                    current_version = version_match.group(1)
                    try:
                        version_num = float(current_version) + 0.1
                        yaml_header = re.sub(
                            r'version: "[^"]*"',
                            f'version: "{version_num:.1f}"',
                            yaml_header,
                        )
                    except:
                        pass

            # ìƒˆë¡œìš´ íŒŒì¼ ë‚´ìš© ìƒì„±
            updated_content = (
                f"{yaml_header}\n\n{updated_body}" if yaml_header else updated_body
            )

            # ë¡œì»¬ íŒŒì¼ ì—…ë°ì´íŠ¸
            with open(local_path, "w", encoding="utf-8") as f:
                f.write(updated_content)

            # S3 ì—…ë°ì´íŠ¸
            s3_client = boto3.client("s3", region_name="us-east-1")

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ ë˜ëŠ” YAMLì—ì„œ)
            category = "examples"  # ê¸°ë³¸ê°’
            if yaml_header:
                category_match = re.search(r'category: "([^"]*)"', yaml_header)
                if category_match:
                    category = category_match.group(1)

            s3_key = f"{category}/{filename}"

            s3_client.upload_file(
                local_path,
                BEDROCK_AGENT_BUCKET,
                s3_key,
                ExtraArgs={"ContentType": "text/markdown"},
            )

            logger.info(f"ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {s3_key}")

            # ìë™ìœ¼ë¡œ Knowledge Base ë™ê¸°í™” ì‹¤í–‰
            sync_result = await self.sync_knowledge_base()

            return f"""âœ… ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!

ğŸ“ ë¡œì»¬ íŒŒì¼: {local_path}
â˜ï¸ S3 íŒŒì¼: s3://{BEDROCK_AGENT_BUCKET}/{s3_key}
ğŸ”„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: {update_mode}
ğŸ“ ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M')}

ğŸ”„ Knowledge Base ë™ê¸°í™” ìë™ ì‹¤í–‰:
{sync_result}"""

        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return f"âŒ ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# MCP ì„œë²„ ì„¤ì •
server = Server("db-assistant-mcp-server")
db_assistant = DBAssistantMCPServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="list_databases",
            description="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="select_database",
            description="íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ (USE ëª…ë ¹ì–´ ì‹¤í–‰). ë¨¼ì € list_databasesë¡œ ëª©ë¡ì„ í™•ì¸í•œ í›„ ë²ˆí˜¸ë‚˜ ì´ë¦„ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”.",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ì„ íƒí•  ë°ì´í„°ë² ì´ìŠ¤ (ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„)",
                    },
                },
                "required": ["database_secret", "database_selection"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_schema",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="text_to_sql",
            description="ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "natural_language_query": {
                        "type": "string",
                        "description": "ìì—°ì–´ë¡œ ì‘ì„±ëœ ì¿¼ë¦¬ ìš”ì²­",
                    },
                },
                "required": ["database_secret", "natural_language_query"],
            },
        ),
        types.Tool(
            name="get_table_index",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­)",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)",
                        "default": "us-east-1",
                    },
                },
                "required": ["db_instance_identifier"],
            },
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ìƒìœ„ Nê°œ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: 10)",
                        "default": 10,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="ê°œì„ ëœ ì•„ì›ƒë¼ì´ì–´ íƒì§€ - ë©”íŠ¸ë¦­ë³„ ë§ì¶¤ ì„ê³„ê°’ê³¼ ë¬¼ë¦¬ì  ì œì•½ ì ìš©",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "std_threshold": {
                        "type": "number",
                        "description": "IQR ë°©ì‹ìš© ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3.0, ë©”íŠ¸ë¦­ë³„ ë§ì¶¤ ê¸°ì¤€ ìš°ì„  ì ìš©)",
                        "default": 3.0,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="ë©”íŠ¸ë¦­ ê°„ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "predictor_metric": {
                        "type": "string",
                        "description": "ì˜ˆì¸¡ ë³€ìˆ˜ ë©”íŠ¸ë¦­",
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                },
                "required": ["csv_file", "predictor_metric"],
            },
        ),
        types.Tool(
            name="list_data_files",
            description="ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV íŒŒì¼ì˜ ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ìš”ì•½í•  CSV íŒŒì¼ëª…"}
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="debug_cloudwatch_collection",
            description="CloudWatch ìŠ¬ë¡œìš° ì¿¼ë¦¬ ìˆ˜ì§‘ ë””ë²„ê·¸",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM:SS, KST ê¸°ì¤€)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM:SS, KST ê¸°ì¤€)",
                    },
                },
                "required": ["database_secret", "start_time", "end_time"],
            },
        ),
        types.Tool(
            name="collect_slow_queries",
            description="ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ì—ì„œ ëŠë¦° ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM:SS, KST ê¸°ì¤€, ì„ íƒì‚¬í•­)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM:SS, KST ê¸°ì¤€, ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="enable_slow_query_log_exports",
            description="Aurora í´ëŸ¬ìŠ¤í„°ì˜ SlowQuery ë¡œê·¸ CloudWatch ì „ì†¡ì„ í™œì„±í™”í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                },
                "required": ["cluster_identifier"],
            },
        ),
        types.Tool(
            name="collect_cpu_intensive_queries",
            description="CPU ì§‘ì•½ì  ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_temp_space_intensive_queries",
            description="ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="validate_schema_lambda",
            description="DDL ìŠ¤í‚¤ë§ˆë¥¼ ê²€ì¦í•©ë‹ˆë‹¤ (Lambda ì‚¬ìš©) - CREATE TABLE, ALTER TABLE, DROP TABLE, CREATE INDEX ì§€ì›",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„",
                    },
                    "ddl_content": {
                        "type": "string",
                        "description": "ê²€ì¦í•  DDL êµ¬ë¬¸",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                    },
                },
                "required": ["database_secret", "database", "ddl_content"],
            },
        ),
        types.Tool(
            name="explain_query_lambda",
            description="SQL ì¿¼ë¦¬ ì‹¤í–‰ ê³„íšì„ ë¶„ì„í•©ë‹ˆë‹¤ (Lambda ì‚¬ìš©) - ì„±ëŠ¥ ì´ìŠˆ ìë™ ê°ì§€ ë° ê°œì„  ê¶Œì¥ì‚¬í•­ ì œê³µ",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„",
                    },
                    "query": {
                        "type": "string",
                        "description": "ë¶„ì„í•  SQL ì¿¼ë¦¬",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                    },
                },
                "required": ["database_secret", "database", "query"],
            },
        ),
        types.Tool(
            name="analyze_aurora_mysql_error_logs",
            description="Aurora MySQL ì—ëŸ¬ ë¡œê·¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì‹œí¬ë¦¿ ì´ë¦„ í•„í„°ë§ìš©)",
                    },
                    "start_datetime_str": {
                        "type": "string",
                        "description": "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹)",
                    },
                    "end_datetime_str": {
                        "type": "string",
                        "description": "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM:SS í˜•ì‹)",
                    },
                },
                "required": ["keyword", "start_datetime_str", "end_datetime_str"],
            },
        ),
        types.Tool(
            name="test_individual_query_validation",
            description="ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ë””ë²„ê·¸ìš©)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                },
                "required": ["database_secret", "filename"],
            },
        ),
        types.Tool(
            name="generate_consolidated_report",
            description="ê¸°ì¡´ HTML ë³´ê³ ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í†µí•© ë³´ê³ ì„œ ìƒì„±",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "í•„í„°ë§í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    },
                    "report_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "íŠ¹ì • ë³´ê³ ì„œ íŒŒì¼ëª… ëª©ë¡ (ì„ íƒì‚¬í•­)",
                    },
                    "date_filter": {
                        "type": "string",
                        "description": "ë‚ ì§œ í•„í„° (YYYYMMDD í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                    "latest_count": {
                        "type": "integer",
                        "description": "ìµœì‹  íŒŒì¼ ê°œìˆ˜ ì œí•œ (ì„ íƒì‚¬í•­)",
                    },
                },
            },
        ),
        types.Tool(
            name="save_to_vector_store",
            description="ëŒ€í™” ë‚´ìš©ì´ë‚˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„° ì €ì¥ì†Œ(Knowledge Base)ì— ì €ì¥í•©ë‹ˆë‹¤ (ì¤‘ë³µ/ìƒì¶© ê²€ì‚¬ í¬í•¨)",
            inputSchema={
                "type": "object",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "ì €ì¥í•  ë‚´ìš©",
                    },
                    "topic": {
                        "type": "string",
                        "description": "ì£¼ì œëª… (10ì ì´ë‚´ ì˜ë¬¸)",
                    },
                    "category": {
                        "type": "string",
                        "description": "ì¹´í…Œê³ ë¦¬ (database-standards, performance-optimization, troubleshooting, examples ì¤‘ ì„ íƒ)",
                        "enum": [
                            "database-standards",
                            "performance-optimization",
                            "troubleshooting",
                            "examples",
                        ],
                        "default": "examples",
                    },
                    "tags": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "íƒœê·¸ ëª©ë¡ (ì„ íƒì‚¬í•­)",
                    },
                    "force_save": {
                        "type": "boolean",
                        "description": "ì¤‘ë³µ/ìƒì¶© ê²€ì‚¬ ë¬´ì‹œí•˜ê³  ê°•ì œ ì €ì¥ (ì„ íƒì‚¬í•­)",
                        "default": False,
                    },
                    "auto_summarize": {
                        "type": "boolean",
                        "description": "1000ì ì´ìƒ ë‚´ìš© ìë™ ìš”ì•½ (ì„ íƒì‚¬í•­)",
                        "default": True,
                    },
                },
                "required": ["content", "topic"],
            },
        ),
        types.Tool(
            name="update_vector_content",
            description="ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {
                        "type": "string",
                        "description": "ì—…ë°ì´íŠ¸í•  íŒŒì¼ëª…",
                    },
                    "new_content": {
                        "type": "string",
                        "description": "ìƒˆë¡œìš´ ë‚´ìš© (ê¸°ì¡´ ë‚´ìš©ì— ì¶”ê°€ë¨)",
                    },
                    "update_mode": {
                        "type": "string",
                        "description": "ì—…ë°ì´íŠ¸ ëª¨ë“œ (append: ì¶”ê°€, replace: êµì²´)",
                        "enum": ["append", "replace"],
                        "default": "append",
                    },
                },
                "required": ["filename", "new_content"],
            },
        ),
        types.Tool(
            name="sync_knowledge_base",
            description="Knowledge Base ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),
        types.Tool(
            name="query_vector_store",
            description="ë²¡í„° ì €ì¥ì†Œ(Knowledge Base)ì—ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)",
                        "default": 5,
                    },
                },
                "required": ["query"],
            },
        ),
        types.Tool(
            name="generate_comprehensive_performance_report",
            description="Oracle AWR ìŠ¤íƒ€ì¼ì˜ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± (ë©”íŠ¸ë¦­ ë¶„ì„, ìƒê´€ê´€ê³„, ëŠë¦° ì¿¼ë¦¬, ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬ í¬í•¨)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_instance_identifier"],
            },
        ),
        types.Tool(
            name="generate_cluster_performance_report",
            description="Aurora í´ëŸ¬ìŠ¤í„° ì „ìš© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± (í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ + ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ ë§í¬)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_cluster_identifier"],
            },
        ),
        types.Tool(
            name="set_default_region",
            description="ê¸°ë³¸ AWS ë¦¬ì „ì„ ë³€ê²½í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "region_name": {
                        "type": "string",
                        "description": "ì„¤ì •í•  AWS ë¦¬ì „ (ì˜ˆ: ap-northeast-2, us-east-1, eu-west-1)",
                    }
                },
                "required": ["region_name"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await db_assistant.list_sql_files()
        elif name == "list_database_secrets":
            result = await db_assistant.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "test_database_connection":
            result = await db_assistant.test_database_connection(
                arguments["database_secret"]
            )
        elif name == "list_databases":
            result = await db_assistant.list_databases(arguments["database_secret"])
        elif name == "select_database":
            result = await db_assistant.select_database(
                arguments["database_secret"], arguments["database_selection"]
            )
        elif name == "get_schema_summary":
            result = await db_assistant.get_schema_summary(arguments["database_secret"])
        elif name == "get_table_schema":
            result = await db_assistant.get_table_schema(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "text_to_sql":
            result = await db_assistant.text_to_sql(
                arguments["database_secret"], arguments["natural_language_query"]
            )
        elif name == "get_table_index":
            result = await db_assistant.get_table_index(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "get_performance_metrics":
            result = await db_assistant.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "collect_db_metrics":
            result = await db_assistant.collect_db_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", "us-east-1"),
            )
        elif name == "analyze_metric_correlation":
            result = await db_assistant.analyze_metric_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10),
            )
        elif name == "detect_metric_outliers":
            result = await db_assistant.detect_metric_outliers(
                arguments["csv_file"], arguments.get("std_threshold", 3.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_assistant.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization"),
            )
        elif name == "list_data_files":
            result = await db_assistant.list_data_files()
        elif name == "validate_sql_file":
            result = await db_assistant.validate_sql_file(
                arguments["filename"], arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await db_assistant.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "get_metric_summary":
            result = await db_assistant.get_metric_summary(arguments["csv_file"])
        elif name == "debug_cloudwatch_collection":
            result = await db_assistant.debug_cloudwatch_collection(
                arguments["database_secret"],
                arguments["start_time"],
                arguments["end_time"],
            )
        elif name == "collect_slow_queries":
            result = await db_assistant.collect_slow_queries(
                arguments["database_secret"],
                arguments.get("start_time"),
                arguments.get("end_time"),
            )
        elif name == "enable_slow_query_log_exports":
            result = await db_assistant.enable_slow_query_log_exports(
                arguments["cluster_identifier"]
            )
        elif name == "collect_cpu_intensive_queries":
            result = await db_assistant.collect_cpu_intensive_queries(
                arguments["database_secret"],
                arguments.get("db_instance_identifier"),
                arguments.get("start_time"),
                arguments.get("end_time"),
            )
        elif name == "collect_temp_space_intensive_queries":
            result = await db_assistant.collect_temp_space_intensive_queries(
                arguments["database_secret"],
                arguments.get("db_instance_identifier"),
                arguments.get("start_time"),
                arguments.get("end_time"),
            )
        elif name == "validate_schema_lambda":
            validation_result = await db_assistant.validate_schema_lambda(
                arguments["database_secret"],
                arguments["database"],
                arguments["ddl_content"],
                arguments.get("region", "ap-northeast-2"),
            )
            # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
            if validation_result.get('success'):
                ddl_type = validation_result.get('ddl_type', 'UNKNOWN')
                table_name = validation_result.get('table_name', 'N/A')
                is_valid = validation_result.get('valid', False)
                issues = validation_result.get('issues', [])
                warnings = validation_result.get('warnings', [])
                s3_location = validation_result.get('s3_location', '')

                status = "âœ… ê²€ì¦ í†µê³¼" if is_valid else "âŒ ê²€ì¦ ì‹¤íŒ¨"
                result = f"{status}\n\n"
                result += f"DDL íƒ€ì…: {ddl_type}\n"
                result += f"í…Œì´ë¸” ì´ë¦„: {table_name}\n\n"

                if issues:
                    result += f"ë¬¸ì œì  ({len(issues)}ê°œ):\n"
                    for issue in issues:
                        result += f"  - {issue}\n"
                    result += "\n"

                if warnings:
                    result += f"ê²½ê³  ({len(warnings)}ê°œ):\n"
                    for warning in warnings:
                        result += f"  - {warning}\n"
                    result += "\n"

                result += f"S3 ì €ì¥ ìœ„ì¹˜: {s3_location}"
            else:
                error_msg = validation_result.get('error', 'Unknown error')
                result = f"âŒ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {error_msg}"
        elif name == "explain_query_lambda":
            explain_result = await db_assistant.explain_query_lambda(
                arguments["database_secret"],
                arguments["database"],
                arguments["query"],
                arguments.get("region", "ap-northeast-2"),
            )
            # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
            if explain_result.get('success'):
                query = explain_result.get('query', '')
                explain_data = explain_result.get('explain_data', [])
                performance_issues = explain_result.get('performance_issues', [])
                recommendations = explain_result.get('recommendations', [])
                s3_location = explain_result.get('s3_location', '')

                issue_count = explain_result.get('performance_issue_count', 0)
                status = "âœ… ì„±ëŠ¥ ì´ìŠˆ ì—†ìŒ" if issue_count == 0 else f"âš ï¸ ì„±ëŠ¥ ì´ìŠˆ {issue_count}ê°œ ë°œê²¬"

                result = f"{status}\n\n"
                result += f"ì¿¼ë¦¬: {query[:100]}...\n\n" if len(query) > 100 else f"ì¿¼ë¦¬: {query}\n\n"

                if performance_issues:
                    result += f"ì„±ëŠ¥ ì´ìŠˆ ({len(performance_issues)}ê°œ):\n"
                    for issue in performance_issues:
                        result += f"  - {issue}\n"
                    result += "\n"

                if recommendations:
                    result += f"ê°œì„  ê¶Œì¥ì‚¬í•­ ({len(recommendations)}ê°œ):\n"
                    for rec in recommendations:
                        result += f"  - {rec}\n"
                    result += "\n"

                result += f"S3 ì €ì¥ ìœ„ì¹˜: {s3_location}"
            else:
                error_msg = explain_result.get('error', 'Unknown error')
                result = f"âŒ EXPLAIN ë¶„ì„ ì‹¤íŒ¨: {error_msg}"
        elif name == "analyze_aurora_mysql_error_logs":
            result = await db_assistant.analyze_aurora_mysql_error_logs(
                arguments["keyword"],
                arguments["start_datetime_str"],
                arguments["end_datetime_str"],
            )
        elif name == "save_to_vector_store":
            result = await db_assistant.save_to_vector_store(
                arguments["content"],
                arguments["topic"],
                arguments.get("category", "examples"),
                arguments.get("tags"),
                arguments.get("force_save", False),
                arguments.get("auto_summarize", True),
            )
        elif name == "update_vector_content":
            result = await db_assistant.update_vector_content(
                arguments["filename"],
                arguments["new_content"],
                arguments.get("update_mode", "append"),
            )
        elif name == "sync_knowledge_base":
            result = await db_assistant.sync_knowledge_base()
        elif name == "query_vector_store":
            result = await db_assistant.query_vector_store(
                arguments["query"], arguments.get("max_results", 5)
            )
        elif name == "test_individual_query_validation":
            result = await db_assistant.test_individual_query_validation(
                arguments["database_secret"], arguments["filename"]
            )
        elif name == "generate_consolidated_report":
            result = await db_assistant.generate_consolidated_report(
                arguments.get("keyword"),
                arguments.get("report_files"),
                arguments.get("date_filter"),
                arguments.get("latest_count"),
            )
        elif name == "generate_comprehensive_performance_report":
            result = await db_assistant.generate_comprehensive_performance_report(
                arguments["database_secret"],
                arguments["db_instance_identifier"],
                arguments.get("region", "ap-northeast-2"),
                arguments.get("hours", 24),
            )
        elif name == "generate_cluster_performance_report":
            result = await db_assistant.generate_cluster_performance_report(
                arguments["database_secret"],
                arguments["db_cluster_identifier"],
                arguments.get("hours", 24),
                arguments.get("region", "ap-northeast-2"),
            )
        elif name == "set_default_region":
            result = db_assistant.set_default_region(arguments["region_name"])
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="db-assistant-mcp-server",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())

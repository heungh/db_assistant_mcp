#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ MCP ì„œë²„
CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ìƒê´€ê´€ê³„ ë¶„ì„, ì•„ì›ƒë¼ì´ì–´ íƒì§€, íšŒê·€ ë¶„ì„ ê¸°ëŠ¥ ì œê³µ
"""

import asyncio
import json
import os
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

import boto3
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ matplotlib ì‚¬ìš©
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
DATA_DIR = CURRENT_DIR / "data"

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)

class AnalyzeDBServer:
    def __init__(self):
        self.cloudwatch = None
        self.default_metrics = [
            'CPUUtilization', 'DatabaseConnections', 'DBLoad', 'DBLoadCPU', 
            'DBLoadNonCPU', 'FreeableMemory', 'ReadIOPS', 'WriteIOPS',
            'ReadLatency', 'WriteLatency', 'NetworkReceiveThroughput',
            'NetworkTransmitThroughput', 'BufferCacheHitRatio'
        ]

    def setup_cloudwatch_client(self, region_name: str = 'us-east-1'):
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            self.cloudwatch = boto3.client('cloudwatch', region_name=region_name)
            return True
        except Exception as e:
            logger.error(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def collect_metrics(self, db_instance_identifier: str, hours: int = 24, 
                            metrics: Optional[List[str]] = None, region: str = 'us-east-1') -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            if not metrics:
                metrics = self.default_metrics

            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            data = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
            for metric in metrics:
                try:
                    response = self.cloudwatch.get_metric_statistics(
                        Namespace='AWS/RDS',
                        MetricName=metric,
                        Dimensions=[
                            {
                                'Name': 'DBInstanceIdentifier',
                                'Value': db_instance_identifier
                            },
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5ë¶„ ê°„ê²©
                        Statistics=['Average']
                    )
                    
                    # ì‘ë‹µì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    for point in response['Datapoints']:
                        data.append({
                            'Timestamp': point['Timestamp'].replace(tzinfo=None),
                            'Metric': metric,
                            'Value': point['Average']
                        })
                except Exception as e:
                    logger.error(f"ë©”íŠ¸ë¦­ {metric} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

            if not data:
                return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            df = df.sort_values('Timestamp')

            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            pivot_df = df.pivot(index='Timestamp', columns='Metric', values='Value')

            # CSV íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = DATA_DIR / f'database_metrics_{db_instance_identifier}_{timestamp}.csv'
            pivot_df.to_csv(csv_file)

            return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {csv_file}"

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def analyze_correlation(self, csv_file: str, target_metric: str = 'CPUUtilization', 
                                top_n: int = 10) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"íƒ€ê²Ÿ ë©”íŠ¸ë¦­ '{target_metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ìƒê´€ ë¶„ì„
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(target_metric, errors='ignore')
            top_correlations = target_correlations.nlargest(top_n)

            # ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
            result = f"ğŸ“Š {target_metric}ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ {top_n}ê°œ ë©”íŠ¸ë¦­:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"â€¢ {metric}: {correlation:.4f}\n"

            # ì‹œê°í™”
            plt.figure(figsize=(12, 6))
            top_correlations.plot(kind='bar')
            plt.title(f'Top {top_n} Metrics Correlated with {target_metric}')
            plt.xlabel('Metrics')
            plt.ylabel('Correlation Coefficient')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'correlation_analysis_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"\nğŸ“ˆ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def detect_outliers(self, csv_file: str, std_threshold: float = 2.0) -> str:
        """ì•„ì›ƒë¼ì´ì–´ íƒì§€"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•œ í†µê³„ ê³„ì‚°
            stats = df.agg(['mean', 'max', 'min', 'std'])

            result = "ğŸ“Š ê° ë©”íŠ¸ë¦­ì˜ í†µê³„:\n"
            result += stats.to_string() + "\n\n"

            result += f"ğŸš¨ ì•„ì›ƒë¼ì´ì–´ íƒì§€ ê²°ê³¼ (ì„ê³„ê°’: Â±{std_threshold}Ïƒ):\n\n"

            outlier_summary = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ì•„ì›ƒë¼ì´ì–´ íƒì§€
            for column in df.columns:
                series = df[column]
                mean = series.mean()
                std = series.std()
                lower_bound = mean - std_threshold * std
                upper_bound = mean + std_threshold * std
                
                outliers = series[(series < lower_bound) | (series > upper_bound)]
                
                if not outliers.empty:
                    result += f"âš ï¸ {column} ë©”íŠ¸ë¦­ì˜ ì•„ì›ƒë¼ì´ì–´ ({len(outliers)}ê°œ):\n"
                    result += f"   ì •ìƒ ë²”ìœ„: {lower_bound:.2f} ~ {upper_bound:.2f}\n"
                    
                    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ í‘œì‹œ
                    for i, (timestamp, value) in enumerate(outliers.items()):
                        if i >= 5:
                            result += f"   ... ë° {len(outliers) - 5}ê°œ ë”\n"
                            break
                        result += f"   â€¢ {timestamp}: {value:.2f}\n"
                    result += "\n"
                    
                    outlier_summary.append({
                        'metric': column,
                        'count': len(outliers),
                        'percentage': (len(outliers) / len(series)) * 100
                    })
                else:
                    result += f"âœ… {column}: ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ\n"

            # ìš”ì•½ ì •ë³´
            if outlier_summary:
                result += "\nğŸ“‹ ì•„ì›ƒë¼ì´ì–´ ìš”ì•½:\n"
                for summary in outlier_summary:
                    result += f"â€¢ {summary['metric']}: {summary['count']}ê°œ ({summary['percentage']:.1f}%)\n"

            return result

        except Exception as e:
            return f"ì•„ì›ƒë¼ì´ì–´ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def perform_regression_analysis(self, csv_file: str, predictor_metric: str, 
                                        target_metric: str = 'CPUUtilization') -> str:
        """íšŒê·€ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)

            # í•„ìš”í•œ ë©”íŠ¸ë¦­ í™•ì¸
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ë°ì´í„° ì¤€ë¹„
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN ê°’ ì²˜ë¦¬
            imputer = SimpleImputer(strategy='mean')
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± (2ì°¨)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_poly_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê³„ìˆ˜ ì¶œë ¥
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ({predictor_metric} â†’ {target_metric}):\n\n"
            result += f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥:\n"
            result += f"â€¢ Mean Squared Error: {mse:.4f}\n"
            result += f"â€¢ R-squared Score: {r2:.4f}\n\n"
            
            result += f"ğŸ”¢ ë‹¤í•­ íšŒê·€ ëª¨ë¸ (2ì°¨):\n"
            result += f"y = {coefficients[1]:.4f}xÂ² + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            plt.figure(figsize=(12, 8))
            
            # ì‚°ì ë„
            plt.subplot(2, 1, 1)
            plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='ì‹¤ì œ ë°ì´í„°')
            
            # ì˜ˆì¸¡ ê³¡ì„ ì„ ìœ„í•œ ì •ë ¬ëœ ë°ì´í„°
            X_plot = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)
            X_plot_poly = poly_features.transform(X_plot)
            y_plot_pred = model.predict(X_plot_poly)
            
            plt.plot(X_plot, y_plot_pred, color='red', linewidth=2, label='ì˜ˆì¸¡ ëª¨ë¸')
            plt.title(f'{predictor_metric} vs {target_metric} íšŒê·€ ë¶„ì„')
            plt.xlabel(predictor_metric)
            plt.ylabel(target_metric)
            plt.legend()
            plt.grid(True, alpha=0.3)

            # ì”ì°¨ í”Œë¡¯
            plt.subplot(2, 1, 2)
            residuals = y_test - y_pred
            plt.scatter(y_pred, residuals, color='green', alpha=0.6)
            plt.axhline(y=0, color='red', linestyle='--')
            plt.title('ì”ì°¨ í”Œë¡¯ (Residual Plot)')
            plt.xlabel('ì˜ˆì¸¡ê°’')
            plt.ylabel('ì”ì°¨')
            plt.grid(True, alpha=0.3)

            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'regression_analysis_{predictor_metric}_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"íšŒê·€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def list_data_files(self) -> str:
        """ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ“ ë°ì´í„° íŒŒì¼ ëª©ë¡:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"â€¢ {file.name}\n"
                result += f"  í¬ê¸°: {file_size:,} bytes\n"
                result += f"  ìˆ˜ì •ì¼: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            
            result = f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ({csv_file}):\n\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë©”íŠ¸ë¦­ ìˆ˜: {len(df.columns)}ê°œ\n\n"
            
            result += "ğŸ“Š ë©”íŠ¸ë¦­ ëª©ë¡:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}ê°œ ë°ì´í„°)\n"

            # ê¸°ë³¸ í†µê³„
            result += f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# MCP ì„œë²„ ì„¤ì •
server = Server("analyze-db")

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì"
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­)"
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)",
                        "default": "us-east-1"
                    }
                },
                "required": ["db_instance_identifier"]
            }
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ìƒìœ„ Nê°œ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: 10)",
                        "default": 10
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="ë©”íŠ¸ë¦­ ë°ì´í„°ì—ì„œ ì•„ì›ƒë¼ì´ì–´ë¥¼ íƒì§€í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "std_threshold": {
                        "type": "number",
                        "description": "í‘œì¤€í¸ì°¨ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2.0)",
                        "default": 2.0
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="ë©”íŠ¸ë¦­ ê°„ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "predictor_metric": {
                        "type": "string",
                        "description": "ì˜ˆì¸¡ ë³€ìˆ˜ ë©”íŠ¸ë¦­"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    }
                },
                "required": ["csv_file", "predictor_metric"]
            }
        ),
        types.Tool(
            name="list_data_files",
            description="ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV íŒŒì¼ì˜ ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ìš”ì•½í•  CSV íŒŒì¼ëª…"
                    }
                },
                "required": ["csv_file"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        db_server = AnalyzeDBServer()
        
        if name == "collect_db_metrics":
            result = await db_server.collect_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", "us-east-1")
            )
        elif name == "analyze_metric_correlation":
            result = await db_server.analyze_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10)
            )
        elif name == "detect_metric_outliers":
            result = await db_server.detect_outliers(
                arguments["csv_file"],
                arguments.get("std_threshold", 2.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_server.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization")
            )
        elif name == "list_data_files":
            result = await db_server.list_data_files()
        elif name == "get_metric_summary":
            result = await db_server.get_metric_summary(arguments["csv_file"])
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"
        
        return [types.TextContent(type="text", text=result)]
    except Exception as e:
        error_msg = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return [types.TextContent(type="text", text=error_msg)]

async def main():
    # Stdin/stdoutë¥¼ í†µí•œ MCP ì„œë²„ ì‹¤í–‰
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="analyze-db",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )

if __name__ == "__main__":
    asyncio.run(main())
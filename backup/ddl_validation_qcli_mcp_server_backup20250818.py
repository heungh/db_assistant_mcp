#!/usr/bin/env python3
"""
SQL ê²€ì¦ ë° ì„±ëŠ¥ë¶„ì„ Amazon Q CLI MCP ì„œë²„
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

import boto3

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)


class DDLValidationQCLIServer:
    def __init__(self):
        self.bedrock_client = boto3.client(
            "bedrock-runtime", region_name="us-east-1", verify=False
        )
        self.knowledge_base_id = "0WQUBRHVR8"
        self.current_plan = None

    def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            secrets = self.get_secrets_by_keyword(keyword)
            if not secrets:
                return (
                    f"'{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                    if keyword
                    else "ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                )

            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:\n{secret_list}"
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            result = await self.validate_ddl(ddl_content, database_secret, filename)
            return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def validate_ddl(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """DDL ê²€ì¦ ì‹¤í–‰"""
        try:
            issues = []
            db_connection_info = None
            schema_validation = None
            constraint_validation = None

            # 1. ê¸°ë³¸ ë¬¸ë²• ê²€ì¦
            if not ddl_content.strip().endswith(";"):
                issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # 2. DDL íƒ€ì… í™•ì¸
            ddl_type = self.extract_ddl_type(ddl_content)

            # 6. Claudeë¥¼ í†µí•œ ê²€ì¦
            try:
                claude_result = await self.validate_with_claude(ddl_content)
                if (
                    "ë¬¸ì œ" in claude_result
                    or "ì˜¤ë¥˜" in claude_result
                    or "ìœ„ë°˜" in claude_result
                ):
                    issues.append(f"Claude ê²€ì¦: {claude_result[:200]}...")
            except Exception as e:
                logger.error(f"Claude ê²€ì¦ ì˜¤ë¥˜: {e}")
                issues.append(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

            # ê²°ê³¼ ìƒì„±
            if not issues:
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
            else:
                summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ"
                status = "FAIL"

            # ë³´ê³ ì„œ ìƒì„± (HTML í˜•ì‹)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"

            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"

            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜
            db_info_section = ""
            if db_connection_info:
                if db_connection_info["success"]:
                    db_info_section = f"""
                    <div class="info-section">
                        <h3>ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´</h3>
                        <table class="info-table">
                            <tr><td><strong>í˜¸ìŠ¤íŠ¸</strong></td><td>{db_connection_info.get('host', 'N/A')}</td></tr>
                            <tr><td><strong>í¬íŠ¸</strong></td><td>{db_connection_info.get('port', 'N/A')}</td></tr>
                            <tr><td><strong>ë°ì´í„°ë² ì´ìŠ¤</strong></td><td>{db_connection_info.get('current_database', 'N/A')}</td></tr>
                            <tr><td><strong>ì„œë²„ ë²„ì „</strong></td><td>{db_connection_info.get('server_version', 'N/A')}</td></tr>
                            <tr><td><strong>ì—°ê²° ë°©ì‹</strong></td><td>{db_connection_info.get('connection_method', 'N/A')}</td></tr>
                            <tr><td><strong>ì—°ê²° ìƒíƒœ</strong></td><td><span class="status-success">âœ… ì„±ê³µ</span></td></tr>
                        </table>
                    </div>
                    """
                else:
                    db_info_section = f"""
                    <div class="info-section">
                        <h3>ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´</h3>
                        <table class="info-table">
                            <tr><td><strong>ì—°ê²° ìƒíƒœ</strong></td><td><span class="status-error">âŒ ì‹¤íŒ¨</span></td></tr>
                            <tr><td><strong>ì˜¤ë¥˜</strong></td><td>{db_connection_info.get('error', 'N/A')}</td></tr>
                        </table>
                    </div>
                    """

            # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„¹ì…˜
            schema_section = ""
            if schema_validation and schema_validation["success"]:
                schema_section = """
                <div class="validation-section">
                    <h3>ğŸ—ï¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼</h3>
                    <div class="validation-results">
                """
                for result in schema_validation["validation_results"]:
                    status_class = (
                        "success"
                        if result["exists"] and not result["column_issues"]
                        else "error"
                    )
                    status_icon = (
                        "âœ…"
                        if result["exists"] and not result["column_issues"]
                        else "âŒ"
                    )

                    schema_section += f"""
                        <div class="validation-item {status_class}">
                            <h4>{status_icon} í…Œì´ë¸”: {result['table']}</h4>
                    """

                    if result["column_issues"]:
                        schema_section += "<ul class='issue-list'>"
                        for issue in result["column_issues"]:
                            schema_section += f"<li>{issue}</li>"
                        schema_section += "</ul>"

                    if result["existing_columns"]:
                        schema_section += f"<p><strong>ê¸°ì¡´ ì»¬ëŸ¼:</strong> {', '.join(result['existing_columns'])}</p>"

                    schema_section += "</div>"

                schema_section += """
                    </div>
                </div>
                """

            # ì œì•½ì¡°ê±´ ê²€ì¦ ì„¹ì…˜
            constraint_section = ""
            if constraint_validation and constraint_validation["success"]:
                constraint_section = """
                <div class="validation-section">
                    <h3>ğŸ”’ ì œì•½ì¡°ê±´ ê²€ì¦ ê²°ê³¼</h3>
                    <div class="validation-results">
                """
                for result in constraint_validation["constraint_results"]:
                    status_class = "success" if result["valid"] else "error"
                    status_icon = "âœ…" if result["valid"] else "âŒ"

                    constraint_section += f"""
                        <div class="validation-item {status_class}">
                            <h4>{status_icon} {result['type']}: {result['constraint']}</h4>
                    """

                    if result["issue"]:
                        constraint_section += (
                            f"<p class='error-message'>{result['issue']}</p>"
                        )

                    constraint_section += "</div>"

                constraint_section += """
                    </div>
                </div>
                """

            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜
            issues_section = ""
            if issues:
                issues_section = """
                <div class="issues-section">
                    <h3>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <ul class="issues-list">
                """
                for issue in issues:
                    issues_section += f"<li>{issue}</li>"
                issues_section += """
                    </ul>
                </div>
                """
            else:
                issues_section = """
                <div class="issues-section success">
                    <h3>âœ… ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <p class="no-issues">ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DDL ê²€ì¦ ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .validation-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .validation-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .validation-item {{
            margin: 15px 0;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #ccc;
        }}
        .validation-item.success {{
            background: #d4edda;
            border-left-color: #28a745;
        }}
        .validation-item.error {{
            background: #f8d7da;
            border-left-color: #dc3545;
        }}
        .validation-item h4 {{
            margin: 0 0 10px 0;
        }}
        .issue-list, .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issue-list li, .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .error-message {{
            color: #dc3545;
            font-style: italic;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            white-space: pre-wrap;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} DDL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ DDL íƒ€ì…</h4>
                    <p>{ddl_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ DDL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="info-section">
                <h3>ğŸ“Š ê²€ì¦ ê²°ê³¼</h3>
                <p style="font-size: 1.2em; font-weight: 500; color: {status_color};">{summary}</p>
            </div>
            
            {schema_section}
            {constraint_section}
            {issues_section}
        </div>
        
        <div class="footer">
            <p>Generated by DDL Validation Q CLI MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            return f"{summary}\n\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}"

        except Exception as e:
            return f"DDL ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def extract_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ì¶”ì¶œ"""
        ddl_upper = ddl_content.upper().strip()
        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP"):
            return "DROP"
        else:
            return "UNKNOWN"

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH í„°ë„ ì‹œì‘ (ssh_tunnel.sh ë°©ì‹ ì‚¬ìš©, SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ)
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",  # SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            process = subprocess.run(ssh_command, capture_output=True, text=True)

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            import subprocess

            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def get_db_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        if mysql is None:
            raise Exception(
                "mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )

        # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        session = boto3.session.Session()
        client = session.client(
            service_name="secretsmanager",
            region_name="ap-northeast-2",
            verify=False,
        )
        get_secret_value_response = client.get_secret_value(SecretId=database_secret)
        secret = get_secret_value_response["SecretString"]
        db_config = json.loads(secret)

        connection_config = None
        tunnel_used = False

        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(db_config.get("host")):
                connection_config = {
                    "host": "localhost",
                    "port": 3307,
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "database": db_config.get("dbname", db_config.get("database")),
                    "connection_timeout": 10,
                }
                tunnel_used = True

        if not connection_config:
            connection_config = {
                "host": db_config.get("host"),
                "port": db_config.get("port", 3306),
                "user": db_config.get("username"),
                "password": db_config.get("password"),
                "database": db_config.get("dbname", db_config.get("database")),
                "connection_timeout": 10,
            }

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "databases": databases,
                    "tables": tables,
                }

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"MySQL ì˜¤ë¥˜: {str(e)}"}
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"}

    async def validate_schema(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        try:
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed(ddl_content)
            if not ddl_info:
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

            connection, tunnel_used = await self.get_db_connection(
                database_secret, use_ssh_tunnel
            )
            cursor = connection.cursor()

            validation_results = []

            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                if ddl_type == "CREATE_TABLE":
                    result = await self.validate_create_table(cursor, ddl_statement)
                elif ddl_type == "ALTER_TABLE":
                    result = await self.validate_alter_table(cursor, ddl_statement)
                elif ddl_type == "CREATE_INDEX":
                    result = await self.validate_create_index(cursor, ddl_statement)
                elif ddl_type == "DROP_TABLE":
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == "DROP_INDEX":
                    result = await self.validate_drop_index(cursor, ddl_statement)
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"],
                    }

                validation_results.append(result)

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    async def validate_constraints(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ì œì•½ì¡°ê±´ ê²€ì¦ - FK, ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)

            connection, tunnel_used = await self.get_db_connection(
                database_secret, use_ssh_tunnel
            )
            cursor = connection.cursor()

            constraint_results = []

            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ê²€ì¦
            if constraints_info.get("foreign_keys"):
                for fk in constraints_info["foreign_keys"]:
                    # ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (fk["referenced_table"],),
                    )

                    ref_table_exists = cursor.fetchone()[0] > 0

                    if ref_table_exists:
                        # ì°¸ì¡° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_schema = DATABASE() 
                            AND table_name = %s AND column_name = %s
                        """,
                            (fk["referenced_table"], fk["referenced_column"]),
                        )

                        ref_column_exists = cursor.fetchone()[0] > 0

                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": ref_column_exists,
                                "issue": (
                                    None
                                    if ref_column_exists
                                    else f"ì°¸ì¡° ì»¬ëŸ¼ '{fk['referenced_table']}.{fk['referenced_column']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                                ),
                            }
                        )
                    else:
                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": False,
                                "issue": f"ì°¸ì¡° í…Œì´ë¸” '{fk['referenced_table']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                            }
                        )

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ"""
        ddl_statements = []

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(
            create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE
        )

        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append(
                {
                    "type": "CREATE_TABLE",
                    "table": table_name.lower(),
                    "columns": columns_info["columns"],
                    "constraints": columns_info["constraints"],
                }
            )

        # ALTER TABLE íŒŒì‹±
        alter_patterns = [
            # ADD COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "ADD_COLUMN",
            ),
            # DROP COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?",
                "DROP_COLUMN",
            ),
            # MODIFY COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "MODIFY_COLUMN",
            ),
            # CHANGE COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)",
                "CHANGE_COLUMN",
            ),
        ]

        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == "CHANGE_COLUMN":
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "old_column": old_column.lower(),
                            "new_column": new_column.lower(),
                            "column_definition": column_def.strip(),
                        }
                    )
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "column": column_name.lower(),
                            "column_definition": (
                                column_def.strip() if column_def else None
                            ),
                        }
                    )

        # CREATE INDEX íŒŒì‹±
        create_index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)"
        )
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name, columns in index_matches:
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": [
                        col.strip().strip("`").lower() for col in columns.split(",")
                    ],
                }
            )

        # DROP TABLE íŒŒì‹±
        drop_table_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?"
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)

        for table_name in drop_table_matches:
            ddl_statements.append({"type": "DROP_TABLE", "table": table_name.lower()})

        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name in drop_index_matches:
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )

        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEì˜ ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±"""
        columns = []
        constraints = []

        # ì»¬ëŸ¼ ì •ì˜ì™€ ì œì•½ì¡°ê±´ì„ ë¶„ë¦¬
        lines = [line.strip() for line in columns_def.split(",")]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì œì•½ì¡°ê±´ í™•ì¸
            if re.match(
                r"(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)",
                line,
                re.IGNORECASE,
            ):
                constraints.append(line)
            else:
                # ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±
                column_match = re.match(
                    r"`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?", line, re.IGNORECASE
                )
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ""

                    columns.append(
                        {
                            "name": column_name,
                            "data_type": data_type,
                            "attributes": attributes.strip(),
                        }
                    )

        return {"columns": columns, "constraints": constraints}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def validate_create_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        columns = ddl_statement["columns"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists, "columns_count": len(columns)},
        }

    async def validate_alter_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        alter_type = ddl_statement["alter_type"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues,
            }

        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        existing_columns = {
            row[0].lower(): {
                "data_type": row[1].upper(),
                "max_length": row[2],
                "precision": row[3],
                "scale": row[4],
                "is_nullable": row[5],
            }
            for row in cursor.fetchall()
        }

        # ALTER ìœ í˜•ë³„ ê²€ì¦
        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement["column"]
            if column_name in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement["column"]
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement["column"]
            new_definition = ddl_statement["column_definition"]

            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        elif alter_type == "CHANGE_COLUMN":
            old_column = ddl_statement["old_column"]
            new_column = ddl_statement["new_column"]
            new_definition = ddl_statement["column_definition"]

            if old_column not in existing_columns:
                issues.append(f"ê¸°ì¡´ ì»¬ëŸ¼ '{old_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ìƒˆ ì»¬ëŸ¼ëª… '{new_column}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"existing_columns": list(existing_columns.keys())},
        }

    async def validate_create_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}

            for column in columns:
                if column not in existing_columns:
                    issues.append(
                        f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "columns": columns},
        }

    async def validate_drop_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists},
        }

    async def validate_drop_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if not index_exists:
                issues.append(
                    f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )

        return {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name},
        }

    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ"""
        constraints = {"foreign_keys": [], "indexes": [], "primary_keys": []}

        # ì™¸ë˜í‚¤ íŒ¨í„´ ë§¤ì¹­
        fk_pattern = (
            r"FOREIGN\s+KEY\s*\(`?(\w+)`?\)\s*REFERENCES\s+`?(\w+)`?\s*\(`?(\w+)`?\)"
        )
        fk_matches = re.findall(fk_pattern, ddl_content, re.IGNORECASE)

        for column, ref_table, ref_column in fk_matches:
            constraints["foreign_keys"].append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return constraints

    async def analyze_current_schema(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„¸ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            schema_analysis = {
                "current_database": current_db,
                "tables": {},
                "indexes": {},
                "foreign_keys": {},
                "constraints": {},
            }

            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            for table_info in tables_info:
                table_name = table_info[0]
                schema_analysis["tables"][table_name] = {
                    "type": table_info[1],
                    "engine": table_info[2],
                    "rows": table_info[3],
                    "data_length": table_info[4],
                    "index_length": table_info[5],
                    "comment": table_info[6],
                    "columns": {},
                    "indexes": [],
                    "foreign_keys": [],
                }

                # ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT column_name, data_type, is_nullable, column_default,
                           column_key, extra, column_comment, character_maximum_length,
                           numeric_precision, numeric_scale
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table_name,),
                )

                columns_info = cursor.fetchall()
                for col_info in columns_info:
                    col_name = col_info[0]
                    schema_analysis["tables"][table_name]["columns"][col_name] = {
                        "data_type": col_info[1],
                        "is_nullable": col_info[2],
                        "default": col_info[3],
                        "key": col_info[4],
                        "extra": col_info[5],
                        "comment": col_info[6],
                        "max_length": col_info[7],
                        "precision": col_info[8],
                        "scale": col_info[9],
                    }

                # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT index_name, column_name, seq_in_index, non_unique,
                           index_type, comment
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table_name,),
                )

                indexes_info = cursor.fetchall()
                current_index = None
                for idx_info in indexes_info:
                    idx_name = idx_info[0]
                    if current_index != idx_name:
                        schema_analysis["tables"][table_name]["indexes"].append(
                            {
                                "name": idx_name,
                                "columns": [idx_info[1]],
                                "unique": idx_info[3] == 0,
                                "type": idx_info[4],
                                "comment": idx_info[5],
                            }
                        )
                        current_index = idx_name
                    else:
                        # ë³µí•© ì¸ë±ìŠ¤ì˜ ì¶”ê°€ ì»¬ëŸ¼
                        schema_analysis["tables"][table_name]["indexes"][-1][
                            "columns"
                        ].append(idx_info[1])

                # ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT kcu.constraint_name, kcu.column_name, kcu.referenced_table_name,
                           kcu.referenced_column_name, rc.update_rule, rc.delete_rule
                    FROM information_schema.key_column_usage kcu
                    JOIN information_schema.referential_constraints rc
                    ON kcu.constraint_name = rc.constraint_name 
                    AND kcu.table_schema = rc.constraint_schema
                    WHERE kcu.table_schema = DATABASE() AND kcu.table_name = %s
                    AND kcu.referenced_table_name IS NOT NULL
                """,
                    (table_name,),
                )

                fk_info = cursor.fetchall()
                for fk in fk_info:
                    schema_analysis["tables"][table_name]["foreign_keys"].append(
                        {
                            "constraint_name": fk[0],
                            "column": fk[1],
                            "referenced_table": fk[2],
                            "referenced_column": fk[3],
                            "update_rule": fk[4],
                            "delete_rule": fk[5],
                        }
                    )

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "schema_analysis": schema_analysis}

        except Exception as e:
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"}

    async def check_ddl_conflicts(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì  ì‚¬ì „ ê²€ì‚¬"""
        try:
            # í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë¶„ì„
            schema_result = await self.analyze_current_schema(
                database_secret, use_ssh_tunnel
            )
            if not schema_result["success"]:
                return schema_result

            schema = schema_result["schema_analysis"]
            ddl_type = self.extract_ddl_type(ddl_content)

            conflicts = []
            warnings = []
            recommendations = []

            if ddl_type == "CREATE_TABLE":
                conflicts.extend(
                    await self._check_create_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "ALTER_TABLE":
                conflicts.extend(
                    await self._check_alter_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "CREATE_INDEX":
                conflicts.extend(
                    await self._check_create_index_conflicts(ddl_content, schema)
                )
            elif ddl_type == "DROP":
                conflicts.extend(await self._check_drop_conflicts(ddl_content, schema))

            return {
                "success": True,
                "ddl_type": ddl_type,
                "conflicts": conflicts,
                "warnings": warnings,
                "recommendations": recommendations,
                "current_schema": schema,
            }

        except Exception as e:
            return {"success": False, "error": f"DDL ì¶©ëŒ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}"}

    async def _check_create_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?",
            ddl_content,
            re.IGNORECASE,
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name in schema["tables"]:
                if "IF NOT EXISTS" not in ddl_content.upper():
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”."
                    )
                else:
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSê°€ ìˆì–´ì„œ ì‹¤í–‰ì€ ë˜ì§€ë§Œ ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_alter_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """ALTER TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"ALTER\s+TABLE\s+`?(\w+)`?", ddl_content, re.IGNORECASE
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ADD COLUMN ê²€ì‚¬
            add_column_matches = re.findall(
                r"ADD\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in add_column_matches:
                if col_name in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                    )

            # DROP COLUMN ê²€ì‚¬
            drop_column_matches = re.findall(
                r"DROP\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in drop_column_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸
                    for fk in table_info["foreign_keys"]:
                        if fk["column"] == col_name:
                            conflicts.append(
                                f"ì»¬ëŸ¼ '{col_name}'ì€ ì™¸ë˜í‚¤ë¡œ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            )

            # MODIFY/CHANGE COLUMN ê²€ì‚¬
            modify_matches = re.findall(
                r"(?:MODIFY|CHANGE)\s+(?:COLUMN\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            for col_name in modify_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ìˆ˜ì •í•˜ë ¤ëŠ” ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_create_index_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE INDEX ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # ì¸ë±ìŠ¤ëª…ê³¼ í…Œì´ë¸”ëª… ì¶”ì¶œ
        index_match = re.search(
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\((.*?)\)",
            ddl_content,
            re.IGNORECASE,
        )
        if index_match:
            index_name = index_match.group(1)
            table_name = index_match.group(2)
            columns_str = index_match.group(3)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ì¸ë±ìŠ¤ ì¤‘ë³µ í™•ì¸
            for existing_index in table_info["indexes"]:
                if existing_index["name"] == index_name:
                    conflicts.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            columns = [col.strip().strip("`") for col in columns_str.split(",")]
            for col_name in columns:
                # í•¨ìˆ˜ë‚˜ í‘œí˜„ì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ì»¬ëŸ¼ëª…ë§Œ ê²€ì‚¬
                if col_name and not re.search(r"[()]", col_name):
                    if col_name not in table_info["columns"]:
                        conflicts.append(
                            f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def _check_drop_conflicts(self, ddl_content: str, schema: Dict) -> List[str]:
        """DROP ë¬¸ ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        if "DROP TABLE" in ddl_content.upper():
            # í…Œì´ë¸” ì‚­ì œ ê²€ì‚¬
            table_match = re.search(
                r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            if table_match:
                table_name = table_match.group(1)
                if table_name not in schema["tables"]:
                    if "IF EXISTS" not in ddl_content.upper():
                        conflicts.append(
                            f"ì‚­ì œí•˜ë ¤ëŠ” í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸ (ë‹¤ë¥¸ í…Œì´ë¸”ì—ì„œ ì´ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€)
                    for other_table, table_info in schema["tables"].items():
                        for fk in table_info["foreign_keys"]:
                            if fk["referenced_table"] == table_name:
                                conflicts.append(
                                    f"í…Œì´ë¸” '{table_name}'ì€ í…Œì´ë¸” '{other_table}'ì—ì„œ ì™¸ë˜í‚¤ë¡œ ì°¸ì¡°ë˜ê³  ìˆì–´ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                )

        elif "DROP INDEX" in ddl_content.upper():
            # ì¸ë±ìŠ¤ ì‚­ì œ ê²€ì‚¬
            index_match = re.search(
                r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            if index_match:
                index_name = index_match.group(1)
                table_name = index_match.group(2)

                if table_name not in schema["tables"]:
                    conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    table_info = schema["tables"][table_name]
                    index_exists = any(
                        idx["name"] == index_name for idx in table_info["indexes"]
                    )
                    if not index_exists:
                        conflicts.append(
                            f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def get_aurora_mysql_parameters(
        self,
        cluster_identifier: str,
        region: str = "ap-northeast-2",
        filter_type: str = "important",
        category: str = "all",
    ) -> str:
        """Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„° ì¡°íšŒ

        Args:
            cluster_identifier: í´ëŸ¬ìŠ¤í„° ì‹ë³„ì
            region: AWS ë¦¬ì „
            filter_type: í•„í„° íƒ€ì… (important, custom, all)
            category: íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)
        """
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
            clusters_response = rds_client.describe_db_clusters(
                DBClusterIdentifier=cluster_identifier
            )

            if not clusters_response["DBClusters"]:
                return f"âŒ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cluster_identifier}"

            cluster = clusters_response["DBClusters"][0]
            cluster_param_group = cluster.get(
                "DBClusterParameterGroup", "default.aurora-mysql8.0"
            )

            # ì¹´í…Œê³ ë¦¬ë³„ ì œëª© ì„¤ì •
            category_titles = {
                "all": "ì „ì²´",
                "security": "ë³´ì•ˆ ë° ì¸ì¦",
                "performance": "ì„±ëŠ¥ ìµœì í™”",
                "memory": "ë©”ëª¨ë¦¬ ê´€ë¦¬",
                "io": "I/O ë° ìŠ¤í† ë¦¬ì§€",
                "connection": "ì—°ê²° ê´€ë¦¬",
                "logging": "ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                "replication": "ë³µì œ ë° ë°±ì—…",
                "aurora": "Aurora íŠ¹í™” ê¸°ëŠ¥",
            }

            category_title = category_titles.get(category, category)

            result = f"""ğŸ“Š Aurora MySQL íŒŒë¼ë¯¸í„° ì •ë³´ ({category_title})

ğŸ”§ **í´ëŸ¬ìŠ¤í„° ì •ë³´:**
- í´ëŸ¬ìŠ¤í„° ID: {cluster_identifier}
- í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ê·¸ë£¹: {cluster_param_group}
- ì—”ì§„ ë²„ì „: {cluster.get('EngineVersion', 'N/A')}"""

            # í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ì¡°íšŒ
            cluster_params = await self._get_parameters(
                rds_client, cluster_param_group, "cluster", filter_type, category
            )
            if cluster_params:
                result += f"\n\nğŸ—ï¸ **í´ëŸ¬ìŠ¤í„° ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{cluster_params}"

            # ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if cluster.get("DBClusterMembers"):
                instance_id = cluster["DBClusterMembers"][0]["DBInstanceIdentifier"]
                instance_response = rds_client.describe_db_instances(
                    DBInstanceIdentifier=instance_id
                )
                if instance_response["DBInstances"]:
                    instance_param_group = instance_response["DBInstances"][0][
                        "DBParameterGroups"
                    ][0]["DBParameterGroupName"]
                    result += f"\n- ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ê·¸ë£¹: {instance_param_group}"

                    instance_params = await self._get_parameters(
                        rds_client,
                        instance_param_group,
                        "instance",
                        filter_type,
                        category,
                    )
                    if instance_params:
                        result += (
                            f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{instance_params}"
                        )
                    else:
                        if category == "all":
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— í‘œì‹œí•  íŒŒë¼ë¯¸í„° ì—†ìŒ"
                        else:
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** {category_title} ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„° ì—†ìŒ"

            return result

        except Exception as e:
            return f"âŒ Aurora íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_parameters(
        self,
        rds_client,
        param_group_name: str,
        level: str,
        filter_type: str,
        category: str,
    ) -> str:
        """íŒŒë¼ë¯¸í„° ì¡°íšŒ ë° í•„í„°ë§"""
        try:
            # íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if level == "cluster":
                response = rds_client.describe_db_cluster_parameters(
                    DBClusterParameterGroupName=param_group_name
                )
            else:
                response = rds_client.describe_db_parameters(
                    DBParameterGroupName=param_group_name
                )

            parameters = response["Parameters"]

            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒë¼ë¯¸í„° ì •ì˜ (í™•ì¥ëœ ë¶„ë¥˜)
            category_params = {
                "security": [
                    "activate_all_roles_on_login",
                    "authentication_kerberos_caseins_cmp",
                    "default_authentication_plugin",
                    "default_password_lifetime",
                    "check_proxy_users",
                    "mysql_native_password_proxy_users",
                    "sha256_password_proxy_users",
                    "validate_password_policy",
                ],
                "performance": [
                    "innodb_thread_concurrency",
                    "innodb_read_io_threads",
                    "innodb_write_io_threads",
                    "thread_cache_size",
                    "thread_stack",
                    "innodb_purge_threads",
                    "innodb_adaptive_flushing",
                    "innodb_adaptive_max_sleep_delay",
                    "innodb_concurrency_tickets",
                    "innodb_flushing_avg_loops",
                    "innodb_lru_scan_depth",
                    "innodb_max_dirty_pages_pct",
                    "innodb_max_purge_lag",
                    "innodb_max_purge_lag_delay",
                    "innodb_old_blocks_pct",
                    "innodb_old_blocks_time",
                    "innodb_parallel_read_threads",
                    "query_cache_size",
                    "query_cache_type",
                ],
                "memory": [
                    "innodb_buffer_pool_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "sort_buffer_size",
                    "read_buffer_size",
                    "read_rnd_buffer_size",
                    "join_buffer_size",
                    "innodb_log_buffer_size",
                    "key_buffer_size",
                    "innodb_buffer_pool_dump_at_shutdown",
                    "innodb_buffer_pool_load_at_startup",
                    "innodb_buffer_pool_dump_now",
                    "innodb_buffer_pool_load_now",
                    "innodb_change_buffer_max_size",
                    "bulk_insert_buffer_size",
                ],
                "io": [
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "innodb_log_file_size",
                    "innodb_io_capacity",
                    "innodb_io_capacity_max",
                    "innodb_flush_method",
                    "innodb_file_per_table",
                    "innodb_doublewrite",
                    "innodb_flush_neighbors",
                    "innodb_flush_log_at_timeout",
                    "innodb_log_compressed_pages",
                    "innodb_open_files",
                    "innodb_read_only",
                    "innodb_sort_buffer_size",
                ],
                "connection": [
                    "max_connections",
                    "max_user_connections",
                    "connect_timeout",
                    "interactive_timeout",
                    "wait_timeout",
                    "max_connect_errors",
                    "back_log",
                    "host_cache_size",
                    "max_allowed_packet",
                ],
                "logging": [
                    "general_log",
                    "slow_query_log",
                    "log_queries_not_using_indexes",
                    "long_query_time",
                    "log_slow_admin_statements",
                    "log_slow_slave_statements",
                    "general_log_file",
                    "slow_query_log_file",
                    "log_error",
                    "log_warnings",
                    "innodb_print_all_deadlocks",
                ],
                "replication": [
                    "binlog_format",
                    "expire_logs_days",
                    "max_binlog_size",
                    "binlog_cache_size",
                    "slave_net_timeout",
                    "slave_parallel_workers",
                    "binlog_checksum",
                    "binlog_group_commit_sync_delay",
                    "binlog_group_commit_sync_no_delay_count",
                    "binlog_order_commits",
                    "binlog_row_image",
                    "binlog_rows_query_log_events",
                    "binlog_stmt_cache_size",
                    "binlog_transaction_compression",
                    "binlog_backup",
                    "binlog_replication_globaldb",
                ],
                "aurora": [
                    "aurora_binlog_replication_max_yield_seconds",
                    "aurora_binlog_replication_sec_index_parallel_workers",
                    "aurora_enable_staggered_replica_restart",
                    "aurora_enhanced_binlog",
                    "aurora_full_double_precision_in_json",
                    "aurora_fwd_writer_idle_timeout",
                    "aurora_fwd_writer_max_connections_pct",
                    "aurora_in_memory_relaylog",
                    "aurora_jemalloc_background_thread",
                    "aurora_jemalloc_dirty_decay_ms",
                    "aurora_jemalloc_tcache_enabled",
                    "aurora_ml_inference_timeout",
                    "aurora_oom_response",
                    "aurora_parallel_query",
                    "aurora_read_replica_read_committed",
                    "aurora_replica_read_consistency",
                    "aurora_tmptable_enable_per_table_limit",
                    "aurora_use_vector_instructions",
                    "aurora_aurora_max_partitions_for_range",
                ],
            }

            # í•„í„°ë§ ì ìš©
            filtered_params = []

            if filter_type == "all":
                filtered_params = parameters
            elif filter_type == "custom":
                filtered_params = [p for p in parameters if p.get("Source") == "user"]
            elif filter_type == "important":
                important_params = [
                    "innodb_buffer_pool_size",
                    "max_connections",
                    "innodb_log_file_size",
                    "query_cache_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "binlog_format",
                    "character_set_server",
                    "collation_server",
                    "time_zone",
                    "aurora_parallel_query",
                    "aurora_oom_response",
                    "aws_default_s3_role",
                ]
                filtered_params = [
                    p for p in parameters if p["ParameterName"] in important_params
                ]

            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            if category != "all" and category in category_params:
                category_param_names = category_params[category]
                filtered_params = [
                    p
                    for p in filtered_params
                    if p["ParameterName"] in category_param_names
                ]

            if not filtered_params:
                return ""

            # ê²°ê³¼ í¬ë§·íŒ… (ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”)
            result = ""

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”
            if category == "all":
                # ì „ì²´ ì¡°íšŒ ì‹œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ í‘œì‹œ
                categorized_params = {}
                uncategorized_params = []

                for param in filtered_params:
                    param_name = param["ParameterName"]
                    found_category = None

                    for cat, param_list in category_params.items():
                        if param_name in param_list:
                            if cat not in categorized_params:
                                categorized_params[cat] = []
                            categorized_params[cat].append(param)
                            found_category = cat
                            break

                    if not found_category:
                        uncategorized_params.append(param)

                # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
                category_titles = {
                    "security": "ğŸ” ë³´ì•ˆ ë° ì¸ì¦",
                    "performance": "âš¡ ì„±ëŠ¥ ìµœì í™”",
                    "memory": "ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬",
                    "io": "ğŸ’¿ I/O ë° ìŠ¤í† ë¦¬ì§€",
                    "connection": "ğŸ”— ì—°ê²° ê´€ë¦¬",
                    "logging": "ğŸ“ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                    "replication": "ğŸ”„ ë³µì œ ë° ë°±ì—…",
                    "aurora": "â˜ï¸ Aurora íŠ¹í™” ê¸°ëŠ¥",
                }

                for cat in [
                    "security",
                    "performance",
                    "memory",
                    "io",
                    "connection",
                    "logging",
                    "replication",
                    "aurora",
                ]:
                    if cat in categorized_params:
                        result += f"\n{category_titles[cat]}:\n"
                        for param in categorized_params[cat]:
                            name = param["ParameterName"]
                            value = param.get("ParameterValue", "N/A")
                            source = param.get("Source", "N/A")
                            description = (
                                param.get("Description", "")[:50] + "..."
                                if len(param.get("Description", "")) > 50
                                else param.get("Description", "")
                            )

                            result += f"â€¢ {name}: {value} (Source: {source})\n"
                            if description and filter_type == "all":
                                result += f"  â””â”€ {description}\n"

                # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°
                if uncategorized_params:
                    result += f"\nğŸ”§ ê¸°íƒ€ íŒŒë¼ë¯¸í„°:\n"
                    for param in uncategorized_params:
                        name = param["ParameterName"]
                        value = param.get("ParameterValue", "N/A")
                        source = param.get("Source", "N/A")
                        description = (
                            param.get("Description", "")[:50] + "..."
                            if len(param.get("Description", "")) > 50
                            else param.get("Description", "")
                        )

                        result += f"â€¢ {name}: {value} (Source: {source})\n"
                        if description and filter_type == "all":
                            result += f"  â””â”€ {description}\n"
            else:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œ
                for param in filtered_params:
                    name = param["ParameterName"]
                    value = param.get("ParameterValue", "N/A")
                    source = param.get("Source", "N/A")
                    description = (
                        param.get("Description", "")[:50] + "..."
                        if len(param.get("Description", "")) > 50
                        else param.get("Description", "")
                    )

                    result += f"â€¢ {name}: {value} (Source: {source})\n"
                    if description and filter_type == "all":
                        result += f"  â””â”€ {description}\n"

            return result.rstrip()

        except Exception as e:
            return f"âŒ {level} íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_default_aurora_parameters(
        self, parameter_group_name: str, region: str
    ) -> str:
        """ê¸°ë³¸ Aurora íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´ ì¡°íšŒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ë“¤ ì¡°íšŒ
            param_groups_response = rds_client.describe_db_cluster_parameter_groups()

            default_group = None
            for group in param_groups_response["DBClusterParameterGroups"]:
                if "default.aurora-mysql" in group["DBClusterParameterGroupName"]:
                    default_group = group
                    break

            if not default_group:
                return (
                    f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parameter_group_name}"
                )

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë“¤ ì¡°íšŒ
            parameters_response = rds_client.describe_db_cluster_parameters(
                DBClusterParameterGroupName=default_group["DBClusterParameterGroupName"]
            )

            parameters = parameters_response["Parameters"]

            result = f"""ğŸ“Š Aurora MySQL ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì •ë³´

ğŸ”§ **íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´:**
- ê·¸ë£¹ëª…: {default_group['DBClusterParameterGroupName']}
- íŒ¨ë°€ë¦¬: {default_group['DBParameterGroupFamily']}
- ì„¤ëª…: {default_group['Description']}

ğŸ“‹ **ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì¼ë¶€):**"""

            # ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ë§Œ í‘œì‹œ
            important_params = [
                "innodb_buffer_pool_size",
                "max_connections",
                "innodb_log_file_size",
                "query_cache_size",
                "tmp_table_size",
                "max_heap_table_size",
            ]

            for param in parameters[:20]:  # ì²˜ìŒ 20ê°œë§Œ
                param_name = param["ParameterName"]
                if param_name in important_params:
                    value = param.get("ParameterValue", "N/A")
                    result += f"\nâ€¢ {param_name}: {value}"

            return result

        except Exception as e:
            return f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def create_execution_plan(self, operation: str, **kwargs) -> Dict[str, Any]:
        """ì‹¤í–‰ ê³„íš ìƒì„±"""
        plan_steps = []
        tool_name = operation

        if operation == "validate_sql_file":
            filename = kwargs.get("filename")
            database_secret = kwargs.get("database_secret")
            tool_name = "validate_sql_file"

            plan_steps = [
                {
                    "step": 1,
                    "action": "íŒŒì¼ ì¡´ì¬ í™•ì¸",
                    "target": filename,
                    "tool": "fs_read",
                },
                {
                    "step": 2,
                    "action": "DDL ë‚´ìš© ì½ê¸°",
                    "target": filename,
                    "tool": "fs_read",
                },
                {
                    "step": 3,
                    "action": "ê¸°ë³¸ ë¬¸ë²• ê²€ì¦",
                    "target": "DDL êµ¬ë¬¸",
                    "tool": "internal_parser",
                },
            ]

            if database_secret:
                plan_steps.extend(
                    [
                        {
                            "step": 4,
                            "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                            "target": database_secret,
                            "tool": "test_database_connection",
                        },
                        {
                            "step": 5,
                            "action": "ìŠ¤í‚¤ë§ˆ ê²€ì¦",
                            "target": "í˜„ì¬ ìŠ¤í‚¤ë§ˆì™€ ë¹„êµ",
                            "tool": "analyze_current_schema",
                        },
                        {
                            "step": 6,
                            "action": "ì œì•½ì¡°ê±´ ê²€ì¦",
                            "target": "FK, ì¸ë±ìŠ¤ ë“±",
                            "tool": "check_ddl_conflicts",
                        },
                    ]
                )

            plan_steps.extend(
                [
                    {
                        "step": len(plan_steps) + 1,
                        "action": "Claude AI ê²€ì¦",
                        "target": "ê³ ê¸‰ ë¶„ì„",
                        "tool": "claude_analysis",
                    },
                    {
                        "step": len(plan_steps) + 2,
                        "action": "HTML ë³´ê³ ì„œ ìƒì„±",
                        "target": "output ë””ë ‰í† ë¦¬",
                        "tool": "fs_write",
                    },
                ]
            )

        elif operation == "validate_all_sql":
            database_secret = kwargs.get("database_secret")
            tool_name = "validate_all_sql"

            plan_steps = [
                {
                    "step": 1,
                    "action": "SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
                    "target": "sql ë””ë ‰í† ë¦¬",
                    "tool": "list_sql_files",
                },
                {
                    "step": 2,
                    "action": "íŒŒì¼ ê°œìˆ˜ í™•ì¸",
                    "target": "ìµœëŒ€ 5ê°œ ì œí•œ",
                    "tool": "internal_check",
                },
                {
                    "step": 3,
                    "action": "ê° íŒŒì¼ ìˆœì°¨ ê²€ì¦",
                    "target": "ê°œë³„ íŒŒì¼",
                    "tool": "validate_sql_file",
                },
                {
                    "step": 4,
                    "action": "ì¢…í•© ê²°ê³¼ ìƒì„±",
                    "target": "ì „ì²´ ìš”ì•½",
                    "tool": "fs_write",
                },
            ]

        elif operation == "analyze_current_schema":
            database_secret = kwargs.get("database_secret")
            tool_name = "analyze_current_schema"

            plan_steps = [
                {
                    "step": 1,
                    "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                    "target": database_secret,
                    "tool": "test_database_connection",
                },
                {
                    "step": 2,
                    "action": "í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ",
                    "target": "information_schema",
                    "tool": "mysql_query",
                },
                {
                    "step": 3,
                    "action": "ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 4,
                    "action": "ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 5,
                    "action": "ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 6,
                    "action": "ìŠ¤í‚¤ë§ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±",
                    "target": "ì¢…í•© ì •ë³´",
                    "tool": "internal_analysis",
                },
            ]

        elif operation == "get_aurora_mysql_parameters":
            cluster_identifier = kwargs.get("cluster_identifier")
            tool_name = "get_aurora_mysql_parameters"

            plan_steps = [
                {
                    "step": 1,
                    "action": "í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ",
                    "target": cluster_identifier,
                    "tool": "use_aws",
                },
                {
                    "step": 2,
                    "action": "íŒŒë¼ë¯¸í„° ê·¸ë£¹ í™•ì¸",
                    "target": "ì ìš©ëœ ê·¸ë£¹",
                    "tool": "use_aws",
                },
                {
                    "step": 3,
                    "action": "íŒŒë¼ë¯¸í„° ê°’ ì¡°íšŒ",
                    "target": "ì£¼ìš” ì„¤ì •",
                    "tool": "use_aws",
                },
                {
                    "step": 4,
                    "action": "ì»¤ìŠ¤í…€ ì„¤ì • í•„í„°ë§",
                    "target": "ì‚¬ìš©ì ì •ì˜ ê°’",
                    "tool": "internal_filter",
                },
            ]

        else:
            plan_steps = [
                {
                    "step": 1,
                    "action": f"{operation} ì‹¤í–‰",
                    "target": "ê¸°ë³¸ ë™ì‘",
                    "tool": operation,
                }
            ]

        plan = {
            "operation": operation,
            "tool_name": tool_name,
            "parameters": kwargs,
            "steps": plan_steps,
            "created_at": datetime.now().isoformat(),
            "status": "created",
        }

        self.current_plan = plan
        return plan

    async def execute_with_auto_plan(self, operation: str, **kwargs) -> str:
        """ìë™ ì‹¤í–‰ ê³„íš ìƒì„± ë° í‘œì‹œ í›„ í™•ì¸"""
        try:
            # 1. ì‹¤í–‰ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)
            plan_display = self._format_plan_display(plan)

            # 2. ì‹¤í–‰ ê³„íš í‘œì‹œ ë° í™•ì¸ ìš”ì²­
            confirmation_message = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** 
   â€¢ 'y' ë˜ëŠ” 'yes': ì‹¤í–‰ ì§„í–‰
   â€¢ 'n' ë˜ëŠ” 'no': ì‹¤í–‰ ì·¨ì†Œ

ğŸ’¡ **ì°¸ê³ :** confirm_and_execute ë„êµ¬ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

            return confirmation_message

        except Exception as e:
            return f"âŒ ì‹¤í–‰ ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def execute_with_plan(self, operation: str, **kwargs) -> str:
        """ê³„íšì— ë”°ë¥¸ ì‹¤í–‰"""
        # ê¸°ì¡´ ê³„íšì´ ìˆëŠ”ì§€ í™•ì¸
        if self.current_plan and self.current_plan["operation"] == operation:
            plan = self.current_plan
            plan_display = self._format_plan_display(plan)

            # ì‚¬ìš©ì í™•ì¸ ìš”ì²­
            confirmation = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)
"""
            return confirmation
        else:
            # ìƒˆ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)
            plan_display = self._format_plan_display(plan)

            return f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)

ğŸ’¡ **ì°¸ê³ :** 'y' ë˜ëŠ” 'yes'ë¡œ ì‘ë‹µí•˜ë©´ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.
"""

    def _format_plan_display(self, plan: Dict[str, Any]) -> str:
        """ê³„íš í‘œì‹œ í˜•ì‹ ìƒì„±"""
        result = f"""ğŸ¯ **ì‘ì—…:** {plan['operation']}
ğŸ“… **ìƒì„± ì‹œê°„:** {plan['created_at']}
ğŸ”§ **ë§¤ì¹­ íˆ´:** {plan.get('tool_name', plan['operation'])}

ğŸ“ **ì‹¤í–‰ ë‹¨ê³„:**"""

        for step in plan["steps"]:
            tool_info = f" [{step.get('tool', 'internal')}]" if step.get("tool") else ""
            result += (
                f"\n   {step['step']}. {step['action']} â†’ {step['target']}{tool_info}"
            )

        if plan["parameters"]:
            result += f"\n\nâš™ï¸ **ë§¤ê°œë³€ìˆ˜:**"
            for key, value in plan["parameters"].items():
                if value:
                    result += f"\n   â€¢ {key}: {value}"

        return result

    async def confirm_and_execute(self, confirmation: str) -> str:
        """í™•ì¸ í›„ ì‹¤í–‰"""
        if not self.current_plan:
            return "âŒ ì‹¤í–‰í•  ê³„íšì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‘ì—…ì„ ìš”ì²­í•´ì£¼ì„¸ìš”."

        if confirmation.lower() not in ["y", "yes", "ì˜ˆ", "ã…‡"]:
            self.current_plan = None
            return "âŒ ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."

        # ê³„íšì— ë”°ë¼ ì‹¤ì œ ì‹¤í–‰
        plan = self.current_plan
        operation = plan["operation"]
        params = plan["parameters"]

        try:
            result = f"ğŸš€ **ì‹¤í–‰ ì‹œì‘:** {operation}\n\n"

            # ì‹¤ì œ ì‘ì—… ì‹¤í–‰
            if operation == "validate_sql_file":
                validation_result = await self.validate_sql_file(
                    params["filename"], params.get("database_secret")
                )
                result += validation_result

            elif operation == "test_database_connection":
                connection_result = await self.test_connection_only(
                    params["database_secret"]
                )
                result += connection_result

            elif operation == "validate_all_sql":
                validation_result = await self.validate_all_sql_files(
                    params.get("database_secret")
                )
                result += validation_result

            elif operation == "analyze_current_schema":
                schema_result = await self.analyze_current_schema(
                    params["database_secret"]
                )
                if schema_result["success"]:
                    schema = schema_result["schema_analysis"]
                    result += f"""âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ (DB: {schema['current_database']})

ğŸ“Š **ë¶„ì„ ê²°ê³¼:**
- ì´ í…Œì´ë¸” ìˆ˜: {len(schema['tables'])}ê°œ

ğŸ“‹ **í…Œì´ë¸” ìƒì„¸:**"""
                    for table_name, table_info in schema["tables"].items():
                        result += f"""
ğŸ”¹ **{table_name}** ({table_info['engine']})
   - ì»¬ëŸ¼: {len(table_info['columns'])}ê°œ
   - ì¸ë±ìŠ¤: {len(table_info['indexes'])}ê°œ  
   - ì™¸ë˜í‚¤: {len(table_info['foreign_keys'])}ê°œ
   - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""
                        if table_info["comment"]:
                            result += f"\n   - ì„¤ëª…: {table_info['comment']}"

                        # ì»¬ëŸ¼ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ“‹ **ì»¬ëŸ¼ ì •ë³´:**"
                        if table_info["columns"]:
                            for col_name, col_info in table_info["columns"].items():
                                data_type = col_info["data_type"]
                                if col_info["max_length"]:
                                    data_type += f"({col_info['max_length']})"
                                elif col_info["precision"] and col_info["scale"]:
                                    data_type += (
                                        f"({col_info['precision']},{col_info['scale']})"
                                    )
                                elif col_info["precision"]:
                                    data_type += f"({col_info['precision']})"

                                nullable = (
                                    "NULL"
                                    if col_info["is_nullable"] == "YES"
                                    else "NOT NULL"
                                )
                                key_info = (
                                    f" [{col_info['key']}]" if col_info["key"] else ""
                                )
                                extra_info = (
                                    f" {col_info['extra']}" if col_info["extra"] else ""
                                )
                                default_info = (
                                    f" DEFAULT {col_info['default']}"
                                    if col_info["default"]
                                    else ""
                                )

                                result += f"\n      â€¢ {col_name}: {data_type} {nullable}{key_info}{extra_info}{default_info}"
                                if col_info["comment"]:
                                    result += f" -- {col_info['comment']}"
                        else:
                            result += f"\n      ì»¬ëŸ¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        # ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ” **ì¸ë±ìŠ¤ ì •ë³´:**"
                        if table_info["indexes"]:
                            for idx in table_info["indexes"]:
                                unique_info = "UNIQUE " if idx["unique"] else ""
                                columns_str = ", ".join(idx["columns"])
                                result += f"\n      â€¢ {unique_info}{idx['name']} ({columns_str}) [{idx['type']}]"
                                if idx["comment"]:
                                    result += f" -- {idx['comment']}"
                        else:
                            result += f"\n      ì¸ë±ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # ì™¸ë˜í‚¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ”— **ì™¸ë˜í‚¤ ì •ë³´:**"
                        if table_info["foreign_keys"]:
                            for fk in table_info["foreign_keys"]:
                                result += f"\n      â€¢ {fk['constraint_name']}: {fk['column']} â†’ {fk['referenced_table']}.{fk['referenced_column']}"
                                result += f" (UPDATE: {fk['update_rule']}, DELETE: {fk['delete_rule']})"
                        else:
                            result += f"\n      ì™¸ë˜í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # í…Œì´ë¸” í†µê³„ ì •ë³´ ì¶”ê°€
                        if table_info["data_length"] or table_info["index_length"]:
                            result += f"\n\n   ğŸ“Š **í…Œì´ë¸” í†µê³„:**"
                            if table_info["data_length"]:
                                data_size = (
                                    table_info["data_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ë°ì´í„° í¬ê¸°: {data_size:.2f} MB"
                            if table_info["index_length"]:
                                index_size = (
                                    table_info["index_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ì¸ë±ìŠ¤ í¬ê¸°: {index_size:.2f} MB"
                else:
                    result += f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            elif operation == "check_ddl_conflicts":
                conflict_result = await self.check_ddl_conflicts(
                    params["ddl_content"], params["database_secret"]
                )
                if conflict_result["success"]:
                    conflicts = conflict_result["conflicts"]
                    ddl_type = conflict_result["ddl_type"]

                    if not conflicts:
                        result += f"âœ… DDL ì¶©ëŒ ê²€ì‚¬ í†µê³¼! ({ddl_type})\n\nì‹¤í–‰í•´ë„ ì•ˆì „í•©ë‹ˆë‹¤."
                    else:
                        result += (
                            f"âš ï¸ DDL ì¶©ëŒ ë°œê²¬! ({ddl_type})\n\nğŸš¨ **ë°œê²¬ëœ ë¬¸ì œë“¤:**\n"
                        )
                        for i, conflict in enumerate(conflicts, 1):
                            result += f"{i}. {conflict}\n"
                        result += "\nâŒ ì´ DDLì„ ì‹¤í–‰í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                else:
                    result += f"âŒ DDL ì¶©ëŒ ê²€ì‚¬ ì‹¤íŒ¨: {conflict_result['error']}"

            elif operation == "get_schema_summary":
                summary_result = await self.get_schema_summary(
                    params["database_secret"]
                )
                result += summary_result

            elif operation == "get_aurora_mysql_parameters":
                param_result = await self.get_aurora_mysql_parameters(
                    params["cluster_identifier"],
                    params.get("region", "ap-northeast-2"),
                    params.get("filter_type", "important"),
                    params.get("category", "all"),
                )
                result += param_result

            else:
                result += f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—…: {operation}"

            # ê³„íš ì™„ë£Œ
            self.current_plan = None
            result += f"\n\nâœ… **ì‹¤í–‰ ì™„ë£Œ:** {operation}"

            return result

        except Exception as e:
            self.current_plan = None
            return f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            schema_result = await self.analyze_current_schema(database_secret)
            if not schema_result["success"]:
                return f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            schema = schema_result["schema_analysis"]

            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {schema['current_database']})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(schema['tables'])}ê°œ):"""

            for table_name, table_info in schema["tables"].items():
                column_count = len(table_info["columns"])
                index_count = len(table_info["indexes"])
                fk_count = len(table_info["foreign_keys"])

                summary += f"""
  ğŸ”¹ **{table_name}** ({table_info['engine']})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ, ì™¸ë˜í‚¤: {fk_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""

                if table_info["comment"]:
                    summary += f"\n     - ì„¤ëª…: {table_info['comment']}"

            return summary

        except Exception as e:
            return f"ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_all_sql_files(
        self, database_secret: Optional[str] = None
    ) -> str:
        """ëª¨ë“  SQL íŒŒì¼ ê²€ì¦ (ìµœëŒ€ 5ê°œ)"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # ìµœëŒ€ 5ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
            files_to_process = sql_files[:5]
            if len(sql_files) > 5:
                logger.warning(
                    f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ìˆì§€ë§Œ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
                )

            results = []
            for sql_file in files_to_process:
                try:
                    result = await self.validate_sql_file(
                        sql_file.name, database_secret
                    )
                    results.append(f"**{sql_file.name}**: {result.split(chr(10))[0]}")
                except Exception as e:
                    results.append(f"**{sql_file.name}**: âŒ ê²€ì¦ ì‹¤íŒ¨ - {str(e)}")

            summary = f"ğŸ“Š ì´ {len(files_to_process)}ê°œ íŒŒì¼ ê²€ì¦ ì™„ë£Œ"
            if len(sql_files) > 5:
                summary += f" (ì „ì²´ {len(sql_files)}ê°œ ì¤‘ 5ê°œ ì²˜ë¦¬)"

            return f"{summary}\n\n" + "\n".join(results)

        except Exception as e:
            return f"ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"

            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # íŒŒì¼ ë³µì‚¬
            import shutil

            shutil.copy2(source, target_path)

            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    async def test_connection_only(self, database_secret: str) -> str:
        """ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"""
        try:
            connection_result = await self.test_database_connection(
                database_secret, use_ssh_tunnel=True
            )

            if connection_result["success"]:
                databases_list = "\n".join(
                    [f"   - {db}" for db in connection_result.get("databases", [])]
                )
                tables_list = "\n".join(
                    [f"   - {table}" for table in connection_result.get("tables", [])]
                )

                return f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- í˜¸ìŠ¤íŠ¸: {connection_result.get('host', 'N/A')}
- í¬íŠ¸: {connection_result.get('port', 'N/A')}
- ì—°ê²° ë°©ì‹: {connection_result.get('connection_method', 'N/A')}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {connection_result.get('current_database', 'N/A')}
- ì„œë²„ ë²„ì „: {connection_result.get('server_version', 'N/A')}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**
{databases_list if databases_list else '   (ì—†ìŒ)'}

**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**
{tables_list if tables_list else '   (ì—†ìŒ)'}"""
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {connection_result['error']}"

        except Exception as e:
            return f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def analyze_slow_queries(self, database_secret: str, limit: int = 10) -> str:
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸŒ **ëŠë¦° ì¿¼ë¦¬ ë¶„ì„** (ìƒìœ„ {limit}ê°œ)\n\n"

            # Performance Schemaì—ì„œ ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
            cursor.execute(
                f"""
                SELECT 
                    DIGEST_TEXT as query_pattern,
                    COUNT_STAR as exec_count,
                    ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                    ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                    ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec,
                    ROUND(SUM_ROWS_EXAMINED/COUNT_STAR, 0) as avg_rows_examined,
                    ROUND(SUM_ROWS_SENT/COUNT_STAR, 0) as avg_rows_sent
                FROM performance_schema.events_statements_summary_by_digest 
                WHERE DIGEST_TEXT IS NOT NULL 
                AND AVG_TIMER_WAIT > 1000000000
                ORDER BY AVG_TIMER_WAIT DESC 
                LIMIT {limit}
            """
            )

            slow_queries = cursor.fetchall()

            if not slow_queries:
                result += "âœ… ëŠë¦° ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    pattern,
                    count,
                    avg_time,
                    max_time,
                    total_time,
                    avg_examined,
                    avg_sent,
                ) in enumerate(slow_queries, 1):
                    pattern_short = (
                        (pattern[:80] + "...") if len(pattern) > 80 else pattern
                    )
                    result += f"**{i}. ì¿¼ë¦¬ íŒ¨í„´:**\n```sql\n{pattern_short}\n```\n"
                    result += f"ğŸ“ˆ **í†µê³„:**\n"
                    result += f"- ì‹¤í–‰ íšŸìˆ˜: {count:,}íšŒ\n"
                    result += f"- í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}ì´ˆ\n"
                    result += f"- ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {max_time:.3f}ì´ˆ\n"
                    result += f"- ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.3f}ì´ˆ\n"
                    result += f"- í‰ê·  ê²€ì‚¬ í–‰ ìˆ˜: {avg_examined:,.0f}í–‰\n"
                    result += f"- í‰ê·  ë°˜í™˜ í–‰ ìˆ˜: {avg_sent:,.0f}í–‰\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ëŠë¦° ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_table_io_stats(
        self, database_secret: str, schema_name: Optional[str] = None
    ) -> str:
        """í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ìŠ¤í‚¤ë§ˆ ì´ë¦„ ì„¤ì •
            if not schema_name:
                cursor.execute("SELECT DATABASE()")
                schema_name = cursor.fetchone()[0]

            result = f"ğŸ’¿ **í…Œì´ë¸” I/O í†µê³„** (ìŠ¤í‚¤ë§ˆ: {schema_name})\n\n"

            # í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ
            cursor.execute(
                """
                SELECT 
                    object_name as table_name,
                    count_read,
                    count_write,
                    count_read + count_write as total_io,
                    sum_timer_read/1000000000000 as read_time_sec,
                    sum_timer_write/1000000000000 as write_time_sec,
                    (sum_timer_read + sum_timer_write)/1000000000000 as total_time_sec
                FROM performance_schema.table_io_waits_summary_by_table 
                WHERE object_schema = %s
                AND count_read + count_write > 0
                ORDER BY count_read + count_write DESC 
                LIMIT 10
            """,
                (schema_name,),
            )

            io_stats = cursor.fetchall()

            if not io_stats:
                result += "ğŸ“Š I/O í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                result += "ğŸ“Š **ìƒìœ„ 10ê°œ í…Œì´ë¸” (I/O ê¸°ì¤€):**\n\n"
                for i, (
                    table,
                    read_count,
                    write_count,
                    total_io,
                    read_time,
                    write_time,
                    total_time,
                ) in enumerate(io_stats, 1):
                    result += f"**{i}. {table}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ I/O: {total_io:,}íšŒ ({total_time:.3f}ì´ˆ)\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” I/O í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_index_usage_stats(
        self, database_secret: str, table_name: Optional[str] = None
    ) -> str:
        """ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“‡ **ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„**"
            if table_name:
                result += f" (í…Œì´ë¸”: {table_name})"
            result += "\n\n"

            # ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ
            if table_name:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND object_name = %s
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC
                """,
                    (table_name,),
                )
            else:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC 
                    LIMIT 15
                """
                )

            index_stats = cursor.fetchall()

            if not index_stats:
                result += "ğŸ“Š ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    table,
                    index,
                    read_count,
                    write_count,
                    total_usage,
                    read_time,
                    write_time,
                ) in enumerate(index_stats, 1):
                    index_display = index if index else "PRIMARY"
                    result += f"**{i}. {table}.{index_display}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ ì‚¬ìš©: {total_usage:,}íšŒ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_connection_stats(self, database_secret: str) -> str:
        """ì—°ê²° ë° ì„¸ì…˜ í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”— **ì—°ê²° ë° ì„¸ì…˜ í†µê³„**\n\n"

            # í˜„ì¬ ì—°ê²° ìƒíƒœ
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total_connections,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections,
                    SUM(CASE WHEN COMMAND = 'Sleep' THEN 1 ELSE 0 END) as idle_connections
                FROM information_schema.processlist
            """
            )

            conn_stats = cursor.fetchone()
            if conn_stats:
                result += f"ğŸ“Š **í˜„ì¬ ì—°ê²° ìƒíƒœ:**\n"
                result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n"
                result += f"- ìœ íœ´ ì—°ê²°: {conn_stats[2]}ê°œ\n\n"

            # ì‚¬ìš©ìë³„ ì—°ê²° í†µê³„
            cursor.execute(
                """
                SELECT 
                    USER as username,
                    COUNT(*) as connection_count,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_count
                FROM information_schema.processlist
                GROUP BY USER
                ORDER BY connection_count DESC
            """
            )

            user_stats = cursor.fetchall()
            if user_stats:
                result += f"ğŸ‘¥ **ì‚¬ìš©ìë³„ ì—°ê²°:**\n"
                for user, total, active in user_stats:
                    result += f"- {user}: {total}ê°œ ì—°ê²° (í™œì„±: {active}ê°œ)\n"
                result += "\n"

            # Performance Schema ìŠ¤ë ˆë“œ ì •ë³´
            cursor.execute(
                """
                SELECT 
                    type,
                    COUNT(*) as thread_count
                FROM performance_schema.threads
                GROUP BY type
            """
            )

            thread_stats = cursor.fetchall()
            if thread_stats:
                result += f"ğŸ§µ **ìŠ¤ë ˆë“œ í†µê³„:**\n"
                for thread_type, count in thread_stats:
                    result += f"- {thread_type}: {count}ê°œ\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì—°ê²° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_memory_usage(self, database_secret: str) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„**\n\n"

            # ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ)
            cursor.execute(
                """
                SELECT 
                    event_name,
                    ROUND(sum_number_of_bytes_alloc/1024/1024, 2) as allocated_mb,
                    ROUND(sum_number_of_bytes_free/1024/1024, 2) as freed_mb,
                    ROUND((sum_number_of_bytes_alloc - sum_number_of_bytes_free)/1024/1024, 2) as current_mb
                FROM performance_schema.memory_summary_global_by_event_name
                WHERE sum_number_of_bytes_alloc > 0
                ORDER BY (sum_number_of_bytes_alloc - sum_number_of_bytes_free) DESC
                LIMIT 10
            """
            )

            memory_stats = cursor.fetchall()
            if memory_stats:
                result += f"ğŸ“Š **ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ):**\n"
                for event, allocated, freed, current in memory_stats:
                    event_short = event.replace("memory/", "").replace("sql/", "")
                    result += f"- {event_short}: {current:.2f}MB (í• ë‹¹: {allocated:.2f}MB, í•´ì œ: {freed:.2f}MB)\n"
                result += "\n"

            # ì£¼ìš” ë©”ëª¨ë¦¬ ê´€ë ¨ ì‹œìŠ¤í…œ ë³€ìˆ˜
            cursor.execute(
                """
                SHOW VARIABLES WHERE Variable_name IN (
                    'innodb_buffer_pool_size',
                    'key_buffer_size',
                    'query_cache_size',
                    'tmp_table_size',
                    'max_heap_table_size',
                    'sort_buffer_size',
                    'read_buffer_size',
                    'join_buffer_size'
                )
            """
            )

            variables = cursor.fetchall()
            if variables:
                result += f"âš™ï¸ **ì£¼ìš” ë©”ëª¨ë¦¬ ì„¤ì •:**\n"
                for var_name, var_value in variables:
                    # ë°”ì´íŠ¸ ë‹¨ìœ„ë¥¼ MBë¡œ ë³€í™˜
                    if var_value.isdigit():
                        mb_value = int(var_value) / 1024 / 1024
                        result += f"- {var_name}: {mb_value:.2f}MB\n"
                    else:
                        result += f"- {var_name}: {var_value}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_lock_analysis(self, database_secret: str) -> str:
        """ë½ ìƒíƒœ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”’ **ë½ ìƒíƒœ ë¶„ì„**\n\n"

            # í˜„ì¬ ë½ ìƒíƒœ (MySQL 8.0+)
            try:
                cursor.execute(
                    """
                    SELECT 
                        object_schema,
                        object_name,
                        lock_type,
                        lock_mode,
                        lock_status,
                        thread_id
                    FROM performance_schema.data_locks
                    LIMIT 10
                """
                )

                current_locks = cursor.fetchall()
                if current_locks:
                    result += f"ğŸ” **í˜„ì¬ í™œì„± ë½ (ìƒìœ„ 10ê°œ):**\n"
                    for (
                        schema,
                        table,
                        lock_type,
                        lock_mode,
                        status,
                        thread_id,
                    ) in current_locks:
                        result += f"- {schema}.{table}: {lock_type} ({lock_mode}) - {status} [Thread: {thread_id}]\n"
                    result += "\n"
                else:
                    result += f"âœ… **í˜„ì¬ í™œì„± ë½:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **í˜„ì¬ ë½ ì •ë³´:** Performance Schema data_locks í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ë½ ëŒ€ê¸° ìƒí™©
            try:
                cursor.execute(
                    """
                    SELECT 
                        requesting_thread_id,
                        blocking_thread_id,
                        object_schema,
                        object_name,
                        lock_type
                    FROM performance_schema.data_lock_waits
                    LIMIT 10
                """
                )

                lock_waits = cursor.fetchall()
                if lock_waits:
                    result += f"â³ **ë½ ëŒ€ê¸° ìƒí™©:**\n"
                    for (
                        req_thread,
                        block_thread,
                        schema,
                        table,
                        lock_type,
                    ) in lock_waits:
                        result += f"- Thread {req_thread}ì´ Thread {block_thread}ì— ì˜í•´ ëŒ€ê¸° ì¤‘\n"
                        result += f"  ëŒ€ìƒ: {schema}.{table} ({lock_type})\n"
                    result += "\n"
                else:
                    result += f"âœ… **ë½ ëŒ€ê¸°:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **ë½ ëŒ€ê¸° ì •ë³´:** Performance Schema data_lock_waits í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ëŒ€ê¸° ì´ë²¤íŠ¸ í†µê³„
            cursor.execute(
                """
                SELECT 
                    event_name,
                    count_star as event_count,
                    ROUND(sum_timer_wait/1000000000000, 6) as total_wait_sec,
                    ROUND(avg_timer_wait/1000000000000, 6) as avg_wait_sec
                FROM performance_schema.events_waits_summary_global_by_event_name
                WHERE event_name LIKE '%lock%' 
                AND count_star > 0
                ORDER BY sum_timer_wait DESC
                LIMIT 5
            """
            )

            wait_events = cursor.fetchall()
            if wait_events:
                result += f"â±ï¸ **ë½ ê´€ë ¨ ëŒ€ê¸° ì´ë²¤íŠ¸ (ìƒìœ„ 5ê°œ):**\n"
                for event, count, total_wait, avg_wait in wait_events:
                    event_short = event.replace("wait/synch/", "").replace(
                        "wait/io/", ""
                    )
                    result += f"- {event_short}: {count:,}íšŒ (ì´ {total_wait:.3f}ì´ˆ, í‰ê·  {avg_wait:.6f}ì´ˆ)\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_replication_status(self, database_secret: str) -> str:
        """ë³µì œ ìƒíƒœ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”„ **ë³µì œ ìƒíƒœ ë¶„ì„**\n\n"

            # ë³µì œ ì—°ê²° ìƒíƒœ (Performance Schema)
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        host,
                        port,
                        user,
                        source_connection_auto_failover,
                        connection_retry_interval,
                        connection_retry_count
                    FROM performance_schema.replication_connection_configuration
                """
                )

                repl_config = cursor.fetchall()
                if repl_config:
                    result += f"âš™ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:**\n"
                    for (
                        channel,
                        host,
                        port,
                        user,
                        auto_failover,
                        retry_interval,
                        retry_count,
                    ) in repl_config:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ì†ŒìŠ¤: {host}:{port} (ì‚¬ìš©ì: {user})\n"
                        result += f"  ìë™ ì¥ì• ì¡°ì¹˜: {auto_failover}\n"
                        result += (
                            f"  ì¬ì‹œë„: {retry_count}íšŒ, ê°„ê²©: {retry_interval}ì´ˆ\n\n"
                        )
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ìƒíƒœ
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        service_state,
                        received_transaction_set,
                        last_error_message,
                        last_error_timestamp
                    FROM performance_schema.replication_connection_status
                """
                )

                repl_status = cursor.fetchall()
                if repl_status:
                    result += f"ğŸ“Š **ë³µì œ ì—°ê²° ìƒíƒœ:**\n"
                    for channel, state, trans_set, error_msg, error_time in repl_status:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if trans_set:
                            result += f"  ìˆ˜ì‹  íŠ¸ëœì­ì…˜: {trans_set[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ë§ˆì§€ë§‰ ì˜¤ë¥˜: {error_msg}\n"
                            result += f"  ì˜¤ë¥˜ ì‹œê°„: {error_time}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ì§€ì—° ì •ë³´
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        worker_id,
                        service_state,
                        last_error_message,
                        last_applied_transaction
                    FROM performance_schema.replication_applier_status_by_worker
                """
                )

                worker_status = cursor.fetchall()
                if worker_status:
                    result += f"ğŸ‘· **ë³µì œ ì›Œì»¤ ìƒíƒœ:**\n"
                    for (
                        channel,
                        worker_id,
                        state,
                        error_msg,
                        last_trans,
                    ) in worker_status:
                        result += f"- ì±„ë„: {channel}, ì›Œì»¤: {worker_id}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if last_trans:
                            result += f"  ë§ˆì§€ë§‰ ì ìš©: {last_trans[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ì˜¤ë¥˜: {error_msg}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ
            try:
                cursor.execute("SHOW MASTER STATUS")
                master_status = cursor.fetchone()
                if master_status:
                    result += f"ğŸ“ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:**\n"
                    result += f"- íŒŒì¼: {master_status[0]}\n"
                    result += f"- ìœ„ì¹˜: {master_status[1]}\n"
                    if len(master_status) > 2 and master_status[2]:
                        result += f"- ë°”ì¸ë”© DB: {master_status[2]}\n"
                    if len(master_status) > 3 and master_status[3]:
                        result += f"- ì œì™¸ DB: {master_status[3]}\n"
                else:
                    result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸:** ë¹„í™œì„±í™”\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë³µì œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def validate_with_claude(self, ddl_content: str) -> str:
        """Claudeë¥¼ ì‚¬ìš©í•œ ê²€ì¦"""
        prompt = f"""
        ë‹¤ìŒ DDL ë¬¸ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:
        
        {ddl_content}
        
        ë¬¸ë²• ì˜¤ë¥˜, í‘œì¤€ ê·œì¹™ ìœ„ë°˜, ì„±ëŠ¥ ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
        ë¬¸ì œê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•´ì£¼ì„¸ìš”. ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ê²€ì¦ í†µê³¼"ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        try:
            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1024,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            response = self.bedrock_client.invoke_model(
                modelId="anthropic.claude-3-sonnet-20240229-v1:0", body=claude_input
            )

            response_body = json.loads(response.get("body").read())
            return response_body.get("content", [{}])[0].get("text", "")

        except Exception as e:
            logger.error(f"Claude í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# MCP ì„œë²„ ì„¤ì •
server = Server("ddl-qcli-validator")
ddl_validator = DDLValidationQCLIServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="validate_all_sql",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤ (ìµœëŒ€ 5ê°œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="analyze_current_schema",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="check_ddl_conflicts",
            description="DDL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì ì„ ì‚¬ì „ ê²€ì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "ddl_content": {"type": "string", "description": "ê²€ì‚¬í•  DDL ë¬¸"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["ddl_content", "database_secret"],
            },
        ),
        types.Tool(
            name="get_aurora_mysql_parameters",
            description="Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                    },
                    "filter_type": {
                        "type": "string",
                        "description": "í•„í„° íƒ€ì… (important: ì£¼ìš” íŒŒë¼ë¯¸í„°ë§Œ, custom: ì‚¬ìš©ì ì •ì˜ë§Œ, all: ì „ì²´)",
                        "enum": ["important", "custom", "all"],
                        "default": "important",
                    },
                    "category": {
                        "type": "string",
                        "description": "íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)",
                        "enum": [
                            "all",
                            "security",
                            "performance",
                            "memory",
                            "io",
                            "connection",
                            "logging",
                            "replication",
                            "aurora",
                        ],
                        "default": "all",
                    },
                },
                "required": ["cluster_identifier"],
            },
        ),
        types.Tool(
            name="create_execution_plan",
            description="ì‘ì—… ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "operation": {"type": "string", "description": "ì‹¤í–‰í•  ì‘ì—…ëª…"},
                    "parameters": {"type": "object", "description": "ì‘ì—… ë§¤ê°œë³€ìˆ˜"},
                },
                "required": ["operation"],
            },
        ),
        types.Tool(
            name="confirm_and_execute",
            description="ê³„íš í™•ì¸ í›„ ì‹¤í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "confirmation": {
                        "type": "string",
                        "description": "ì‹¤í–‰ í™•ì¸ (y/yes/n/no)",
                    }
                },
                "required": ["confirmation"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="analyze_slow_queries",
            description="ëŠë¦° ì¿¼ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "limit": {
                        "type": "integer",
                        "description": "ì¡°íšŒí•  ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)",
                        "default": 10,
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_io_stats",
            description="í…Œì´ë¸”ë³„ I/O í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "schema_name": {
                        "type": "string",
                        "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: í˜„ì¬ DB)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_index_usage_stats",
            description="ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "í…Œì´ë¸” ì´ë¦„ (ì„ íƒì‚¬í•­, ì „ì²´ ì¡°íšŒì‹œ ìƒëµ)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_connection_stats",
            description="ì—°ê²° ë° ì„¸ì…˜ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_memory_usage",
            description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_lock_analysis",
            description="ë½ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_replication_status",
            description="ë³µì œ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await ddl_validator.list_sql_files()
        elif name == "list_database_secrets":
            result = await ddl_validator.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "validate_sql_file":
            # ê³„íš ê¸°ë°˜ ì‹¤í–‰ í™•ì¸
            # ìë™ ê³„íš ìƒì„± ë° ì‹¤í–‰
            result = await ddl_validator.execute_with_auto_plan(
                "validate_sql_file",
                filename=arguments["filename"],
                database_secret=arguments.get("database_secret"),
            )
        elif name == "test_database_connection":
            result = await ddl_validator.execute_with_auto_plan(
                "test_database_connection", database_secret=arguments["database_secret"]
            )
        elif name == "validate_all_sql":
            # ìë™ ê³„íš ìƒì„± ë° ì‹¤í–‰
            result = await ddl_validator.execute_with_auto_plan(
                "validate_all_sql", database_secret=arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await ddl_validator.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "analyze_current_schema":
            result = await ddl_validator.execute_with_auto_plan(
                "analyze_current_schema", database_secret=arguments["database_secret"]
            )
        elif name == "check_ddl_conflicts":
            result = await ddl_validator.execute_with_auto_plan(
                "check_ddl_conflicts",
                ddl_content=arguments["ddl_content"],
                database_secret=arguments["database_secret"],
            )
        elif name == "get_schema_summary":
            result = await ddl_validator.execute_with_auto_plan(
                "get_schema_summary", database_secret=arguments["database_secret"]
            )
        elif name == "get_aurora_mysql_parameters":
            result = await ddl_validator.execute_with_auto_plan(
                "get_aurora_mysql_parameters",
                cluster_identifier=arguments["cluster_identifier"],
                region=arguments.get("region", "ap-northeast-2"),
                filter_type=arguments.get("filter_type", "important"),
                category=arguments.get("category", "all"),
            )
        elif name == "create_execution_plan":
            plan = await ddl_validator.create_execution_plan(
                arguments["operation"], **arguments.get("parameters", {})
            )
            result = ddl_validator._format_plan_display(plan)
        elif name == "confirm_and_execute":
            result = await ddl_validator.confirm_and_execute(arguments["confirmation"])
        elif name == "get_performance_metrics":
            result = await ddl_validator.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "analyze_slow_queries":
            result = await ddl_validator.analyze_slow_queries(
                arguments["database_secret"], arguments.get("limit", 10)
            )
        elif name == "get_table_io_stats":
            result = await ddl_validator.get_table_io_stats(
                arguments["database_secret"], arguments.get("schema_name")
            )
        elif name == "get_index_usage_stats":
            result = await ddl_validator.get_index_usage_stats(
                arguments["database_secret"], arguments.get("table_name")
            )
        elif name == "get_connection_stats":
            result = await ddl_validator.get_connection_stats(
                arguments["database_secret"]
            )
        elif name == "get_memory_usage":
            result = await ddl_validator.get_memory_usage(arguments["database_secret"])
        elif name == "get_lock_analysis":
            result = await ddl_validator.get_lock_analysis(arguments["database_secret"])
        elif name == "get_replication_status":
            result = await ddl_validator.get_replication_status(
                arguments["database_secret"]
            )
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="ddl-qcli-validator",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())

#!/usr/bin/env python3
"""
SQL ê²€ì¦ ë° ì„±ëŠ¥ë¶„ì„ Amazon Q CLI MCP ì„œë²„
"""

import asyncio
import json
import os
import re
import subprocess
import time
import traceback
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

import boto3

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"
DATA_DIR = CURRENT_DIR / "data"
LOG_DIR = CURRENT_DIR / "logs"

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR.mkdir(exist_ok=True)

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ì— ì¶œë ¥
log_file = LOG_DIR / "ddl_validation.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler(log_file, encoding="utf-8"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)


class DDLValidationQCLIServer:
    def __init__(self):
        self.bedrock_client = boto3.client(
            "bedrock-runtime", region_name="us-east-1", verify=False
        )
        self.knowledge_base_id = "0WQUBRHVR8"
        self.current_plan = None
        self.selected_cluster = None
        self.selected_database = None

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            try:
                subprocess.run(
                    ["pkill", "-f", "ssh.*54.180.79.255"],
                    capture_output=True,
                    timeout=5,
                )
            except:
                pass

            # SSH í„°ë„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤)
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-o",
                "ConnectTimeout=10",
                "-o",
                "ServerAliveInterval=60",
                "-o",
                "ServerAliveCountMax=3",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "-i",
                "/Users/heungh/test.pem",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ SSH í„°ë„ ì‹œì‘
            process = subprocess.Popen(
                ssh_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            time.sleep(3)

            # í”„ë¡œì„¸ìŠ¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            if process.poll() is None:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                stdout, stderr = process.communicate()
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {stderr.decode()}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            import subprocess

            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def get_db_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
    ):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        try:
            if mysql is None:
                raise Exception(
                    "mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )

            # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(
                SecretId=database_secret
            )
            if (
                not get_secret_value_response
                or "SecretString" not in get_secret_value_response
            ):
                raise Exception(f"ì‹œí¬ë¦¿ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {database_secret}")
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)

            connection_config = None
            tunnel_used = False

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            database_name = selected_database or db_config.get(
                "dbname", db_config.get("database")
            )

            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get("host")):
                    connection_config = {
                        "host": "localhost",
                        "port": 3307,
                        "user": str(db_config.get("username", "")),
                        "password": str(db_config.get("password", "")),
                        "connection_timeout": 10,
                    }
                    # databaseëŠ” Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
                    if database_name:
                        connection_config["database"] = str(database_name)
                    tunnel_used = True
        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            raise Exception(f"DB ì—°ê²° ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}\në””ë²„ê·¸: {error_details}")

        # ë‚˜ë¨¸ì§€ ì—°ê²° ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

        if not connection_config:
            # MySQL ì—°ê²°ì— í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë§Œ í¬í•¨
            connection_config = {
                "host": str(db_config.get("host", "")),
                "port": int(db_config.get("port", 3306)),
                "user": str(db_config.get("username", "")),
                "password": str(db_config.get("password", "")),
                "connection_timeout": 10,
            }
            # databaseëŠ” Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
            if database_name:
                connection_config["database"] = str(database_name)

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            if (
                not get_secret_value_response
                or "SecretString" not in get_secret_value_response
            ):
                raise Exception(f"ì‹œí¬ë¦¿ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {secret_name}")
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """Aurora MySQL ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ (ì„¸ë¡œ ë‚˜ì—´)"""
        try:
            # ëª¨ë“  ì‹œí¬ë¦¿ ì¡°íšŒ
            all_secrets = self.get_secrets_by_keyword("")

            # Aurora MySQL ê´€ë ¨ ì‹œí¬ë¦¿ë§Œ í•„í„°ë§
            aurora_mysql_secrets = []
            for secret in all_secrets:
                secret_lower = secret.lower()
                aurora_mysql_secrets.append(secret)

            # ì¶”ê°€ í‚¤ì›Œë“œ í•„í„°ë§ (ì‚¬ìš©ìê°€ í‚¤ì›Œë“œë¥¼ ì œê³µí•œ ê²½ìš°)
            if keyword:
                aurora_mysql_secrets = [
                    secret
                    for secret in aurora_mysql_secrets
                    if keyword.lower() in secret.lower()
                ]

            if not aurora_mysql_secrets:
                if keyword:
                    return (
                        f"'{keyword}' í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ Aurora MySQL ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                    )
                else:
                    return "Aurora MySQL ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."

            # ì„¸ë¡œë¡œ ë‚˜ì—´ (ê° ì‹œí¬ë¦¿ì„ ë³„ë„ ì¤„ì— í‘œì‹œ)
            result = "ğŸ—„ï¸ **Aurora MySQL ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:**\n\n"
            for i, secret in enumerate(aurora_mysql_secrets, 1):
                result += f"{i}. {secret}\n"

            return result
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def execute_dml_validation_workflow(
        self, sql_content: str, database_secret: str, filename: str
    ) -> str:
        """DML ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # 1ë‹¨ê³„: DML ì¿¼ë¦¬ ê°ì§€
            queries = self.extract_dml_queries(sql_content)
            if not queries:
                return f"âŒ {filename}ì—ì„œ DML ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 2ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
            analysis_result = await self.analyze_dml_performance(sql_content, database_secret, filename)
            
            if "error" in analysis_result:
                return f"âŒ DML ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {analysis_result['error']}"

            # 3ë‹¨ê³„: Claude AI ë¶„ì„
            claude_result = await self.validate_dml_with_claude(analysis_result, sql_content)

            # 4ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            validation_state = {
                "filename": filename,
                "sql_content": sql_content,
                "database_secret": database_secret,
                "analysis_result": analysis_result,
                "claude_result": claude_result,
                "total_queries": analysis_result.get('total_queries', 0),
                "issues": [],
                "warnings": [],
                "performance_issues": []
            }

            # ì„±ëŠ¥ ì´ìŠˆ ìˆ˜ì§‘
            for query_info in analysis_result.get('queries', []):
                if query_info.get('performance_issues'):
                    validation_state["performance_issues"].extend(query_info['performance_issues'])
                if query_info.get('error'):
                    validation_state["issues"].append(f"ì¿¼ë¦¬ #{query_info['query_number']}: {query_info['error']}")

            # Claude ê²°ê³¼ ë¶„ì„
            if "ì„±ëŠ¥ ë¶„ì„ í†µê³¼" not in claude_result:
                validation_state["issues"].append(f"Claude ì„±ëŠ¥ ë¶„ì„: {claude_result}")
            else:
                validation_state["warnings"].append("âœ… Claude ì„±ëŠ¥ ë¶„ì„ í†µê³¼")

            # 5ë‹¨ê³„: HTML ë³´ê³ ì„œ ìƒì„±
            await self.generate_dml_html_report(validation_state)

            # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
            return self.generate_dml_result_message(validation_state)

        except Exception as e:
            return f"âŒ DML ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {str(e)}"

    def generate_dml_result_message(self, state: dict) -> str:
        """DML ê²€ì¦ ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±"""
        filename = state.get('filename', 'unknown')
        total_queries = state.get('total_queries', 0)
        issues = state.get('issues', [])
        performance_issues = state.get('performance_issues', [])
        
        if issues or performance_issues:
            status = "âŒ ì„±ëŠ¥ ì´ìŠˆ ë°œê²¬"
            issue_count = len(issues) + len(performance_issues)
        else:
            status = "âœ… ì„±ëŠ¥ ë¶„ì„ í†µê³¼"
            issue_count = 0

        result_message = f"""
{status}

ğŸ“Š **DML ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼:**
â€¢ íŒŒì¼ëª…: {filename}
â€¢ ì´ ì¿¼ë¦¬ ìˆ˜: {total_queries}ê°œ
â€¢ ì„±ëŠ¥ ì´ìŠˆ: {issue_count}ê°œ

ğŸ“„ **ìƒì„¸ ë³´ê³ ì„œ:** output/dml_performance_report_{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html
"""

        if performance_issues:
            result_message += f"\nâš ï¸ **ê°ì§€ëœ ì„±ëŠ¥ ì´ìŠˆ:**\n"
            for issue in performance_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                result_message += f"â€¢ {issue}\n"
            if len(performance_issues) > 5:
                result_message += f"â€¢ ... ì™¸ {len(performance_issues) - 5}ê°œ ì¶”ê°€ ì´ìŠˆ\n"

        if issues:
            result_message += f"\nâŒ **ê¸°íƒ€ ë¬¸ì œ:**\n"
            for issue in issues[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                result_message += f"â€¢ {issue}\n"
            if len(issues) > 3:
                result_message += f"â€¢ ... ì™¸ {len(issues) - 3}ê°œ ì¶”ê°€ ë¬¸ì œ\n"

        return result_message

    async def generate_dml_html_report(self, state: dict) -> str:
        """DML ì„±ëŠ¥ ë¶„ì„ HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            filename = state.get('filename', 'unknown')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_filename = f"dml_performance_report_{filename}_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            analysis_result = state.get('analysis_result', {})
            claude_result = state.get('claude_result', '')
            
            # ì¿¼ë¦¬ë³„ ìƒì„¸ ì •ë³´ HTML ìƒì„±
            queries_html = ""
            for query_info in analysis_result.get('queries', []):
                query_num = query_info.get('query_number', 0)
                query_type = query_info.get('query_type', 'UNKNOWN')
                query_sql = query_info.get('query', '')
                
                # ì„±ëŠ¥ ì´ìŠˆ HTML
                issues_html = ""
                if query_info.get('performance_issues'):
                    issues_html = "<div class='performance-issues'><h5>ğŸ” ì„±ëŠ¥ ì´ìŠˆ</h5><ul>"
                    for issue in query_info['performance_issues']:
                        issues_html += f"<li>{issue}</li>"
                    issues_html += "</ul></div>"

                # EXPLAIN ê²°ê³¼ HTML
                explain_html = ""
                if query_info.get('explain_result'):
                    try:
                        explain_data = json.loads(query_info['explain_result']['EXPLAIN']) if isinstance(query_info['explain_result']['EXPLAIN'], str) else query_info['explain_result']['EXPLAIN']
                        explain_json = json.dumps(explain_data, indent=2, ensure_ascii=False)
                        explain_html = f"""
                        <div class='explain-section'>
                            <h5>ğŸ“Š ì‹¤í–‰ ê³„íš (EXPLAIN)</h5>
                            <pre class='explain-json'>{explain_json}</pre>
                        </div>
                        """
                    except:
                        explain_html = f"<div class='explain-section'><h5>ğŸ“Š ì‹¤í–‰ ê³„íš</h5><pre>{query_info['explain_result']}</pre></div>"

                # í…Œì´ë¸” í†µê³„ HTML
                stats_html = ""
                if query_info.get('table_stats'):
                    stats_html = "<div class='table-stats'><h5>ğŸ“ˆ í…Œì´ë¸” í†µê³„</h5><table class='stats-table'>"
                    stats_html += "<tr><th>í…Œì´ë¸”</th><th>í–‰ ìˆ˜</th><th>ë°ì´í„° í¬ê¸°</th><th>ì¸ë±ìŠ¤ í¬ê¸°</th></tr>"
                    for table, stats in query_info['table_stats'].items():
                        row_count = f"{stats.get('row_count', 0):,}"
                        data_size = f"{stats.get('data_size', 0):,} bytes"
                        index_size = f"{stats.get('index_size', 0):,} bytes"
                        stats_html += f"<tr><td>{table}</td><td>{row_count}</td><td>{data_size}</td><td>{index_size}</td></tr>"
                    stats_html += "</table></div>"

                queries_html += f"""
                <div class='query-section'>
                    <h4>ì¿¼ë¦¬ #{query_num} ({query_type})</h4>
                    <div class='query-sql'>
                        <h5>ğŸ“ SQL ì¿¼ë¦¬</h5>
                        <pre class='sql-code'>{query_sql}</pre>
                    </div>
                    {explain_html}
                    {stats_html}
                    {issues_html}
                </div>
                """

            # Claude ë¶„ì„ ê²°ê³¼ HTML
            claude_html = f"""
            <div class='claude-analysis'>
                <h3>ğŸ¤– Claude AI ì„±ëŠ¥ ë¶„ì„</h3>
                <pre class='claude-result'>{claude_result}</pre>
            </div>
            """

            # ì „ì²´ HTML ìƒì„±
            html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DML ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #007bff; }}
        .header h1 {{ color: #007bff; margin: 0; }}
        .summary {{ background: #e3f2fd; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .query-section {{ margin: 30px 0; padding: 25px; border: 1px solid #dee2e6; border-radius: 8px; background: #f8f9fa; }}
        .query-section h4 {{ color: #495057; margin-top: 0; }}
        .sql-code {{ background: #2d3748; color: #e2e8f0; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        .explain-json {{ background: #1a202c; color: #cbd5e0; padding: 15px; border-radius: 5px; overflow-x: auto; max-height: 400px; }}
        .performance-issues {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 5px; margin: 15px 0; }}
        .performance-issues h5 {{ color: #856404; margin-top: 0; }}
        .stats-table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        .stats-table th, .stats-table td {{ border: 1px solid #dee2e6; padding: 8px; text-align: left; }}
        .stats-table th {{ background: #f8f9fa; }}
        .claude-analysis {{ background: #f0f8ff; padding: 25px; border-radius: 8px; margin: 30px 0; }}
        .claude-result {{ background: white; border: 1px solid #e9ecef; padding: 20px; border-radius: 5px; white-space: pre-wrap; }}
        .explain-section, .table-stats {{ margin: 15px 0; }}
        .explain-section h5, .table-stats h5 {{ color: #495057; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸš€ DML ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ</h1>
            <p>íŒŒì¼: {filename} | ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="summary">
            <h3>ğŸ“Š ë¶„ì„ ìš”ì•½</h3>
            <p><strong>ì´ ì¿¼ë¦¬ ìˆ˜:</strong> {analysis_result.get('total_queries', 0)}ê°œ</p>
            <p><strong>ë¶„ì„ ëŒ€ìƒ:</strong> SELECT, UPDATE, DELETE, INSERT ì¿¼ë¦¬</p>
            <p><strong>ë¶„ì„ í•­ëª©:</strong> ì‹¤í–‰ ê³„íš, í…Œì´ë¸” í†µê³„, ì¸ë±ìŠ¤ ì‚¬ìš©, ì„±ëŠ¥ ì´ìŠˆ</p>
        </div>

        {queries_html}
        
        {claude_html}
        
        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>Generated by DB Assistant MCP Server | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
            """

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"DML HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""

    async def validate_multiple_dml_files(
        self, database_secret: str, sql_files: List[str]
    ) -> str:
        """ì—¬ëŸ¬ DML íŒŒì¼ ì¼ê´„ ì„±ëŠ¥ ë¶„ì„"""
        try:
            if len(sql_files) > 10:
                return "âŒ ìµœëŒ€ 10ê°œ íŒŒì¼ê¹Œì§€ë§Œ ë¶„ì„ ê°€ëŠ¥í•©ë‹ˆë‹¤."

            analysis_results = []
            
            for filename in sql_files:
                sql_file_path = SQL_DIR / filename
                if not sql_file_path.exists():
                    analysis_results.append({
                        "filename": filename,
                        "status": "FAIL",
                        "error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
                    })
                    continue

                with open(sql_file_path, "r", encoding="utf-8") as f:
                    sql_content = f.read()

                # DML ì¿¼ë¦¬ ê°ì§€
                if self.detect_sql_type(sql_content) != "DML":
                    analysis_results.append({
                        "filename": filename,
                        "status": "SKIP",
                        "error": "DML ì¿¼ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤"
                    })
                    continue

                # DML ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
                analysis_result = await self.analyze_dml_performance(sql_content, database_secret, filename)
                
                if "error" in analysis_result:
                    analysis_results.append({
                        "filename": filename,
                        "status": "FAIL",
                        "error": analysis_result["error"]
                    })
                    continue

                # Claude ë¶„ì„
                claude_result = await self.validate_dml_with_claude(analysis_result, sql_content)

                # ê²°ê³¼ ì •ë¦¬
                performance_issues = []
                for query_info in analysis_result.get('queries', []):
                    if query_info.get('performance_issues'):
                        performance_issues.extend(query_info['performance_issues'])

                claude_issues = []
                if "ì„±ëŠ¥ ë¶„ì„ í†µê³¼" not in claude_result:
                    claude_issues.append(claude_result)

                status = "PASS" if not performance_issues and not claude_issues else "FAIL"
                
                analysis_results.append({
                    "filename": filename,
                    "status": status,
                    "total_queries": analysis_result.get('total_queries', 0),
                    "performance_issues": performance_issues,
                    "claude_issues": claude_issues,
                    "analysis_result": analysis_result,
                    "claude_result": claude_result,
                    "sql_content": sql_content
                })

            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„±
            consolidated_report_path = await self.generate_consolidated_dml_html_report(
                analysis_results, database_secret
            )

            # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
            return self.generate_multiple_dml_result_message(analysis_results, consolidated_report_path)

        except Exception as e:
            return f"âŒ ì—¬ëŸ¬ DML íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_dml_html_report(
        self, analysis_results: List[Dict], database_secret: str
    ) -> str:
        """DML í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_filename = f"consolidated_dml_performance_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # í†µê³„ ê³„ì‚°
            total_files = len(analysis_results)
            passed_files = len([r for r in analysis_results if r["status"] == "PASS"])
            failed_files = len([r for r in analysis_results if r["status"] == "FAIL"])
            skipped_files = len([r for r in analysis_results if r["status"] == "SKIP"])
            
            total_queries = sum(r.get("total_queries", 0) for r in analysis_results)
            total_performance_issues = sum(len(r.get("performance_issues", [])) for r in analysis_results)

            # íŒŒì¼ë³„ ìƒì„¸ HTML ìƒì„±
            files_html = ""
            for result in analysis_results:
                filename = result["filename"]
                status = result["status"]
                status_icon = "âœ…" if status == "PASS" else "âŒ" if status == "FAIL" else "â­ï¸"
                
                if status == "SKIP":
                    files_html += f"""
                    <div class='file-section skip'>
                        <h4>{status_icon} {filename} (ê±´ë„ˆëœ€)</h4>
                        <p class='skip-reason'>{result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìœ ')}</p>
                    </div>
                    """
                    continue

                if status == "FAIL" and "error" in result:
                    files_html += f"""
                    <div class='file-section fail'>
                        <h4>{status_icon} {filename} (ì‹¤íŒ¨)</h4>
                        <p class='error-message'>{result['error']}</p>
                    </div>
                    """
                    continue

                # ì„±ê³µí•œ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                query_count = result.get("total_queries", 0)
                performance_issues = result.get("performance_issues", [])
                claude_issues = result.get("claude_issues", [])

                issues_html = ""
                if performance_issues:
                    issues_html += "<div class='performance-issues'><h5>ğŸ” ì„±ëŠ¥ ì´ìŠˆ</h5><ul>"
                    for issue in performance_issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        issues_html += f"<li>{issue}</li>"
                    if len(performance_issues) > 5:
                        issues_html += f"<li>... ì™¸ {len(performance_issues) - 5}ê°œ ì¶”ê°€ ì´ìŠˆ</li>"
                    issues_html += "</ul></div>"

                if claude_issues:
                    issues_html += "<div class='claude-issues'><h5>ğŸ¤– Claude ë¶„ì„</h5>"
                    for claude_issue in claude_issues:
                        issues_html += f"<pre class='claude-text'>{claude_issue}</pre>"
                    issues_html += "</div>"

                files_html += f"""
                <div class='file-section {status.lower()}'>
                    <h4>{status_icon} {filename}</h4>
                    <div class='file-summary'>
                        <p><strong>ì¿¼ë¦¬ ìˆ˜:</strong> {query_count}ê°œ</p>
                        <p><strong>ì„±ëŠ¥ ì´ìŠˆ:</strong> {len(performance_issues)}ê°œ</p>
                    </div>
                    {issues_html}
                </div>
                """

            # ì „ì²´ HTML ìƒì„±
            html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DML ì„±ëŠ¥ ë¶„ì„ í†µí•© ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #007bff; }}
        .header h1 {{ color: #007bff; margin: 0; }}
        .summary {{ background: #e3f2fd; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .summary-item {{ text-align: center; }}
        .summary-item .number {{ font-size: 2em; font-weight: bold; color: #007bff; }}
        .file-section {{ margin: 20px 0; padding: 20px; border-radius: 8px; border-left: 4px solid #dee2e6; }}
        .file-section.pass {{ background: #d4edda; border-left-color: #28a745; }}
        .file-section.fail {{ background: #f8d7da; border-left-color: #dc3545; }}
        .file-section.skip {{ background: #fff3cd; border-left-color: #ffc107; }}
        .file-section h4 {{ margin-top: 0; color: #495057; }}
        .file-summary {{ margin: 10px 0; }}
        .performance-issues {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 5px; margin: 15px 0; }}
        .claude-issues {{ background: #f0f8ff; border: 1px solid #b3d9ff; padding: 15px; border-radius: 5px; margin: 15px 0; }}
        .claude-text {{ background: white; border: 1px solid #e9ecef; padding: 15px; border-radius: 5px; white-space: pre-wrap; }}
        .skip-reason, .error-message {{ color: #856404; font-style: italic; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸš€ DML ì„±ëŠ¥ ë¶„ì„ í†µí•© ë³´ê³ ì„œ</h1>
            <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="summary">
            <h3>ğŸ“Š ë¶„ì„ ìš”ì•½</h3>
            <div class="summary-grid">
                <div class="summary-item">
                    <div class="number">{total_files}</div>
                    <div>ì´ íŒŒì¼</div>
                </div>
                <div class="summary-item">
                    <div class="number" style="color: #28a745;">{passed_files}</div>
                    <div>í†µê³¼</div>
                </div>
                <div class="summary-item">
                    <div class="number" style="color: #dc3545;">{failed_files}</div>
                    <div>ì‹¤íŒ¨</div>
                </div>
                <div class="summary-item">
                    <div class="number">{total_queries}</div>
                    <div>ì´ ì¿¼ë¦¬</div>
                </div>
                <div class="summary-item">
                    <div class="number" style="color: #ffc107;">{total_performance_issues}</div>
                    <div>ì„±ëŠ¥ ì´ìŠˆ</div>
                </div>
            </div>
        </div>

        <div class="files-section">
            <h3>ğŸ“„ íŒŒì¼ë³„ ë¶„ì„ ê²°ê³¼</h3>
            {files_html}
        </div>
        
        <div style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #dee2e6; color: #6c757d;">
            <p>Generated by DB Assistant MCP Server | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
            """

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"DML í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""

    def generate_multiple_dml_result_message(
        self, analysis_results: List[Dict], consolidated_report_path: str
    ) -> str:
        """ì—¬ëŸ¬ DML íŒŒì¼ ë¶„ì„ ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±"""
        total_files = len(analysis_results)
        passed_files = len([r for r in analysis_results if r["status"] == "PASS"])
        failed_files = len([r for r in analysis_results if r["status"] == "FAIL"])
        skipped_files = len([r for r in analysis_results if r["status"] == "SKIP"])
        
        total_queries = sum(r.get("total_queries", 0) for r in analysis_results)
        total_performance_issues = sum(len(r.get("performance_issues", [])) for r in analysis_results)

        pass_rate = (passed_files / total_files * 100) if total_files > 0 else 0

        result_message = f"""
ğŸ“Š **DML ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ**

ğŸ“‹ **ìš”ì•½:**
â€¢ ì´ íŒŒì¼: {total_files}ê°œ
â€¢ í†µê³¼: {passed_files}ê°œ ({pass_rate:.1f}%)
â€¢ ì‹¤íŒ¨: {failed_files}ê°œ
â€¢ ê±´ë„ˆëœ€: {skipped_files}ê°œ
â€¢ ì´ ì¿¼ë¦¬: {total_queries}ê°œ
â€¢ ì„±ëŠ¥ ì´ìŠˆ: {total_performance_issues}ê°œ

ğŸ“„ **í†µí•© ë³´ê³ ì„œ:** {consolidated_report_path}

ğŸ“Š **ê°œë³„ ê²°ê³¼:**"""

        for result in analysis_results:
            filename = result["filename"]
            status = result["status"]
            
            if status == "PASS":
                query_count = result.get("total_queries", 0)
                result_message += f"\nâœ… **{filename}**: í†µê³¼ ({query_count}ê°œ ì¿¼ë¦¬)"
            elif status == "FAIL":
                if "error" in result:
                    result_message += f"\nâŒ **{filename}**: ì‹¤íŒ¨ ({result['error']})"
                else:
                    issue_count = len(result.get("performance_issues", [])) + len(result.get("claude_issues", []))
                    result_message += f"\nâŒ **{filename}**: ì‹¤íŒ¨ ({issue_count}ê°œ ì´ìŠˆ)"
            else:  # SKIP
                result_message += f"\nâ­ï¸ **{filename}**: ê±´ë„ˆëœ€ ({result.get('error', '')})"

        return result_message

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦ - DDL/DML ìë™ ê°ì§€"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                sql_content = f.read()

            # SQL íƒ€ì… ê°ì§€
            sql_type = self.detect_sql_type(sql_content)
            
            if sql_type == "DML":
                # DML ì¿¼ë¦¬ì¸ ê²½ìš° ì„±ëŠ¥ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                if not database_secret:
                    return f"âŒ DML ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. database_secretì„ ì œê³µí•´ì£¼ì„¸ìš”."
                
                result = await self.execute_dml_validation_workflow(
                    sql_content, database_secret, filename
                )
                return result
            else:
                # DDL ì¿¼ë¦¬ì¸ ê²½ìš° ê¸°ì¡´ ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                result = await self.execute_validation_workflow(
                    sql_content, database_secret, filename
                )
                return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    def detect_sql_type(self, sql_content: str) -> str:
        """SQL ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ DDL/DML íƒ€ì… ê°ì§€"""
        # ì£¼ì„ ì œê±°
        content = re.sub(r'--.*$', '', sql_content, flags=re.MULTILINE)
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        content = content.strip().upper()
        
        # DDL í‚¤ì›Œë“œ ì²´í¬
        ddl_keywords = ['CREATE', 'ALTER', 'DROP', 'TRUNCATE']
        dml_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE']
        
        ddl_count = sum(1 for keyword in ddl_keywords if f'{keyword} ' in content or content.startswith(keyword))
        dml_count = sum(1 for keyword in dml_keywords if f'{keyword} ' in content or content.startswith(keyword))
        
        # DMLì´ ë” ë§ìœ¼ë©´ DMLë¡œ ë¶„ë¥˜
        if dml_count > ddl_count:
            return "DML"
        else:
            return "DDL"

    async def execute_validation_workflow(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """Edge ì—°ê²° ë°©ì‹ì˜ ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # ê²€ì¦ ìƒíƒœ ì´ˆê¸°í™”
            validation_state = {
                "ddl_content": ddl_content,
                "filename": filename,
                "database_secret": database_secret,
                "current_step": 1,
                "total_steps": (
                    7 if database_secret else 4
                ),  # DB ì—°ê²° ì—¬ë¶€ì— ë”°ë¼ ë‹¨ê³„ ìˆ˜ ì¡°ì •
                "issues": [],
                "warnings": [],
                "recommendations": [],
            }

            # 1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦
            validation_state = await self.step_1_syntax_check(validation_state)
            logger.info(f"Step 1 completed, state is None: {validation_state is None}")

            # ë¬¸ë²• ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
            if not validation_state.get("syntax_valid", False):
                return await self.generate_final_report(
                    validation_state, generate_html=False
                )

            # 2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦
            validation_state = await self.step_2_standard_check(validation_state)
            logger.info(f"Step 2 completed, state is None: {validation_state is None}")

            # 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ (database_secretì´ ìˆëŠ” ê²½ìš°ë§Œ)
            if database_secret:
                validation_state = await self.step_3_db_connection_test(
                    validation_state
                )
                logger.info(
                    f"Step 3 completed, state is None: {validation_state is None}"
                )

                # ì—°ê²° ì„±ê³µì‹œ ì¶”ê°€ ê²€ì¦ ì§„í–‰
                if validation_state and validation_state.get("db_connected", False):
                    # 4ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ê²€ì¦
                    validation_state = await self.step_4_schema_validation(
                        validation_state
                    )
                    logger.info(
                        f"Step 4 completed, state is None: {validation_state is None}"
                    )

                    # 5ë‹¨ê³„: ì œì•½ì¡°ê±´ ê²€ì¦
                    validation_state = await self.step_5_constraint_validation(
                        validation_state
                    )
                    logger.info(
                        f"Step 5 completed, state is None: {validation_state is None}"
                    )

            # 6ë‹¨ê³„: Claude AI ì¢…í•© ê²€ì¦ (ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)
            if validation_state is not None:
                validation_state = await self.step_6_claude_validation(validation_state)
            else:
                logger.error("validation_state is None before step 6")

            # 7ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± (HTML í¬í•¨)
            result = await self.generate_final_report(
                validation_state, generate_html=True
            )

            # ë¡œì»¬ ê²€ì¦ì¸ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
            if not database_secret:
                result += "\n\nğŸ” **ë¡œì»¬ ê²€ì¦ ì™„ë£Œ**\nâ€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—†ì´ ë¬¸ë²• ë° í‘œì¤€ ê·œì¹™ë§Œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\nâ€¢ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ì›í•˜ì‹œë©´ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ì„ ì§€ì •í•´ì£¼ì„¸ìš”."

            return result

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            return f"ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\në””ë²„ê·¸ ì •ë³´:\n{error_details}"

    async def prompt_for_database_selection(
        self, ddl_content: str, filename: str
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ í”„ë¡¬í”„íŠ¸"""
        try:
            # ë¡œì»¬ ê²€ì¦ì„ ìœ„í•´ íŒŒì¼ ì •ë³´ ì €ì¥
            self.pending_validation = {"ddl_content": ddl_content, "filename": filename}

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ
            secrets_list = await self.get_available_database_secrets()

            if not secrets_list:
                # ì‹œí¬ë¦¿ì´ ì—†ìœ¼ë©´ ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰
                return await self.execute_local_validation_only(ddl_content, filename)

            # ì‚¬ìš©ìì—ê²Œ ì„ íƒ ì˜µì…˜ ì œê³µ
            prompt_message = f"""ğŸ” **DDL ê²€ì¦ ì˜µì…˜ ì„ íƒ**

ğŸ“„ **íŒŒì¼:** {filename}

**ê²€ì¦ ë°©ì‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”:**

**1ï¸âƒ£ ì™„ì „ ê²€ì¦ (ê¶Œì¥)**
   â€¢ ë¬¸ë²• + í‘œì¤€ ê·œì¹™ + ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦
   â€¢ ì‹¤ì œ DB ì—°ê²°í•˜ì—¬ í…Œì´ë¸”/ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
   â€¢ ì œì•½ì¡°ê±´ ë° ì¸ë±ìŠ¤ ê²€ì¦
   â€¢ Claude AI ì¢…í•© ë¶„ì„

**2ï¸âƒ£ ë¡œì»¬ ê²€ì¦ë§Œ**
   â€¢ ë¬¸ë²• + í‘œì¤€ ê·œì¹™ ê²€ì¦
   â€¢ Claude AI ë¶„ì„
   â€¢ DB ì—°ê²° ì—†ì´ ë¹ ë¥¸ ê²€ì¦

ğŸ—„ï¸ **ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤:**
{secrets_list}

ğŸ’¡ **ì‚¬ìš©ë²•:**
   â€¢ ì™„ì „ ê²€ì¦: validate_sql_with_database ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì§€ì •
   â€¢ ë¡œì»¬ ê²€ì¦: confirm_and_execute ë„êµ¬ë¡œ 'local' ì…ë ¥"""

            return prompt_message

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰
            return await self.execute_local_validation_only(ddl_content, filename)

    async def get_available_database_secrets(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ëª¨ë“  ì‹œí¬ë¦¿ ì¡°íšŒ (í•„í„°ë§ ì—†ì´)
            secrets = self.get_secrets_by_keyword("")

            if not secrets:
                return ""

            result = ""
            for i, secret in enumerate(secrets, 1):
                result += f"   {i}. {secret}\n"

            return result

        except Exception as e:
            return ""

    async def execute_local_validation_only(
        self, ddl_content: str, filename: str
    ) -> str:
        """ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰ (DB ì—°ê²° ì—†ìŒ)"""
        try:
            # ê²€ì¦ ìƒíƒœ ì´ˆê¸°í™” (database_secret ì—†ìŒ)
            validation_state = {
                "ddl_content": ddl_content,
                "filename": filename,
                "database_secret": None,
                "current_step": 1,
                "total_steps": 4,  # ë¡œì»¬ ê²€ì¦ë§Œì´ë¯€ë¡œ 4ë‹¨ê³„
                "issues": [],
                "warnings": [],
                "recommendations": [],
            }

            # 1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦
            validation_state = await self.step_1_syntax_check(validation_state)

            # 2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦
            validation_state = await self.step_2_standard_check(validation_state)

            # 3ë‹¨ê³„: Claude AI ê²€ì¦ (DB ì •ë³´ ì—†ì´)
            validation_state = await self.step_6_claude_validation(validation_state)

            # 4ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± (HTML í¬í•¨)
            result = await self.generate_final_report(
                validation_state, generate_html=True
            )

            # ë¡œì»¬ ê²€ì¦ì„ì„ ëª…ì‹œ
            local_notice = "\n\nğŸ” **ë¡œì»¬ ê²€ì¦ ì™„ë£Œ**\nâ€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—†ì´ ë¬¸ë²• ë° í‘œì¤€ ê·œì¹™ë§Œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\nâ€¢ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ì›í•˜ì‹œë©´ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ì„ ì§€ì •í•´ì£¼ì„¸ìš”."

            return result + local_notice

        except Exception as e:
            return f"ë¡œì»¬ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def step_1_syntax_check(self, state: dict) -> dict:
        """1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦ (DDL ë° SELECT í¬í•¨)"""
        ddl_content = state["ddl_content"]
        issues = []

        # SQL íƒ€ì… ì¶”ì¶œ
        sql_type = self.extract_ddl_type(ddl_content)
        state["ddl_type"] = sql_type

        # ê¸°ë³¸ ë¬¸ë²• ê²€ì¦
        if not ddl_content.strip().endswith(";"):
            issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # SELECT ì¿¼ë¦¬ íŠ¹ë³„ ê²€ì¦
        if sql_type == "SELECT":
            # SELECT ì¿¼ë¦¬ ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if "FROM" not in ddl_content.upper():
                issues.append("SELECT ì¿¼ë¦¬ì— FROM ì ˆì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê¸°ë³¸ì ì¸ SELECT ë¬¸ë²• ê²€ì¦
            select_keywords = ["SELECT", "FROM"]
            for keyword in select_keywords:
                if keyword not in ddl_content.upper():
                    issues.append(f"SELECT ì¿¼ë¦¬ì— í•„ìˆ˜ í‚¤ì›Œë“œ '{keyword}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

        state["syntax_valid"] = len(issues) == 0
        state["syntax_issues"] = issues
        state["current_step"] = 2

        return state

    async def step_2_standard_check(self, state: dict) -> dict:
        """2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦"""
        # í˜„ì¬ëŠ” ê¸°ë³¸ êµ¬í˜„ë§Œ ì œê³µ
        state["standard_compliant"] = True
        state["standard_issues"] = []
        state["current_step"] = 3
        return state

    async def step_3_db_connection_test(self, state: dict) -> dict:
        """3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ì—°ê²° ìœ ì§€"""
        database_secret = state["database_secret"]

        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± ë° ìœ ì§€
            connection, tunnel_used = await self.get_db_connection(database_secret)

            if connection.is_connected():
                # ì—°ê²° ì •ë³´ ìˆ˜ì§‘
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db_result = cursor.fetchone()
                current_db = (
                    current_db_result[0]
                    if current_db_result and current_db_result[0]
                    else "None"
                )

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()

                # ì—°ê²° ì •ë³´ë¥¼ stateì— ì €ì¥
                state["db_connected"] = True
                state["db_connection"] = connection  # ì—°ê²° ê°ì²´ ì €ì¥
                state["tunnel_used"] = tunnel_used
                state["db_connection_info"] = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "databases": databases,
                    "tables": tables,
                }
                state["warnings"].append("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            else:
                state["db_connected"] = False
                state["issues"].append("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            state["db_connected"] = False
            state["issues"].append(
                f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}\në””ë²„ê·¸: {error_details}"
            )

        state["current_step"] = 4
        return state

    async def step_4_schema_validation(self, state: dict) -> dict:
        """4ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ê¸°ì¡´ ì—°ê²° ì‚¬ìš©, DDL ë° SELECT í¬í•¨)"""
        try:
            logger.info("ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")

            # ê¸°ì¡´ ì—°ê²° ì‚¬ìš©
            connection = state.get("db_connection")
            if not connection or not connection.is_connected():
                error_msg = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                state["issues"].append(error_msg)
                state["current_step"] = 5
                return state

            sql_type = state.get("ddl_type", "UNKNOWN")
            
            # SELECT ì¿¼ë¦¬ì¸ ê²½ìš° ë³„ë„ ê²€ì¦
            if sql_type == "SELECT":
                schema_result = await self.validate_select_query_with_connection(
                    state["ddl_content"], connection
                )
            else:
                # ê¸°ì¡´ DDL ê²€ì¦
                schema_result = await self.validate_schema_with_connection(
                    state["ddl_content"], connection
                )
            
            logger.info(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼: {schema_result}")

            if schema_result["success"]:
                schema_issues = []
                for result in schema_result["validation_results"]:
                    if result.get("issues"):
                        schema_issues.extend(result["issues"])

                state["schema_issues"] = schema_issues
                if schema_issues:
                    state["issues"].extend(schema_issues)
                else:
                    state["warnings"].append("âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ í†µê³¼")
                # ì„±ê³µí•œ ê²½ìš° ëª¨ë“  ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ì˜¤ë¥˜ ì™„ì „ ì œê±°
                filtered_issues = []
                for issue in state["issues"]:
                    if not ("ìŠ¤í‚¤ë§ˆ ê²€ì¦" in issue or "argument 7" in issue):
                        filtered_issues.append(issue)
                state["issues"] = filtered_issues
            else:
                error_msg = (
                    f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {schema_result.get('error', 'Unknown error')}"
                )
                logger.error(error_msg)
                state["issues"].append(error_msg)

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            error_msg = f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"{error_msg}\n{error_details}")
            state["issues"].append(error_msg)

        state["current_step"] = 5
        return state

    async def step_5_constraint_validation(self, state: dict) -> dict:
        """5ë‹¨ê³„: ì œì•½ì¡°ê±´ ê²€ì¦ (ê¸°ì¡´ ì—°ê²° ì‚¬ìš©, FK ì œì™¸)"""
        try:
            logger.info("ì œì•½ì¡°ê±´ ê²€ì¦ ì‹œì‘")

            # ê¸°ì¡´ ì—°ê²° ì‚¬ìš©
            connection = state.get("db_connection")
            if not connection or not connection.is_connected():
                error_msg = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                state["issues"].append(error_msg)
                state["current_step"] = 6
                return state

            constraint_result = await self.validate_constraints_with_connection(
                state["ddl_content"], connection
            )
            logger.info(f"ì œì•½ì¡°ê±´ ê²€ì¦ ê²°ê³¼: {constraint_result}")

            if constraint_result["success"]:
                constraint_issues = []
                for result in constraint_result["constraint_results"]:
                    if not result.get("valid", True):
                        constraint_issues.append(result.get("issue", "ì œì•½ì¡°ê±´ ìœ„ë°˜"))

                state["constraint_issues"] = constraint_issues
                if constraint_issues:
                    state["issues"].extend(constraint_issues)
                else:
                    state["warnings"].append("âœ… ì œì•½ì¡°ê±´ ê²€ì¦ í†µê³¼")
                # ì„±ê³µí•œ ê²½ìš° ëª¨ë“  ì œì•½ì¡°ê±´ ê´€ë ¨ ì˜¤ë¥˜ ì™„ì „ ì œê±°
                filtered_issues = []
                for issue in state["issues"]:
                    if not ("ì œì•½ì¡°ê±´ ê²€ì¦" in issue or "argument 7" in issue):
                        filtered_issues.append(issue)
                state["issues"] = filtered_issues
            else:
                error_msg = f"ì œì•½ì¡°ê±´ ê²€ì¦ ì‹¤íŒ¨: {constraint_result.get('error', 'Unknown error')}"
                logger.error(error_msg)
                state["issues"].append(error_msg)

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            error_msg = f"ì œì•½ì¡°ê±´ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"{error_msg}\n{error_details}")
            state["issues"].append(error_msg)

        state["current_step"] = 6
        return state

    async def generate_html_report(
        self,
        report_path: Path,
        filename: str,
        ddl_content: str,
        ddl_type: str,
        status: str,
        summary: str,
        issues: List[str],
        db_connection_info: Optional[Dict],
        schema_validation: Optional[Dict],
        constraint_validation: Optional[Dict],
        database_secret: Optional[str],
    ):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"

            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­ì— ë”°ë¼)
            db_info_section = ""

            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜ - Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
            claude_issues = []
            other_issues = []

            for issue in issues:
                if issue.startswith("Claude ê²€ì¦:"):
                    claude_issues.append(issue[12:].strip())  # "Claude ê²€ì¦:" ì œê±°
                else:
                    other_issues.append(issue)

            # ê¸°íƒ€ ê²€ì¦ ë¬¸ì œ ì„¹ì…˜
            other_issues_section = ""
            if other_issues:
                other_issues_section = """
                <div class="issues-section">
                    <h3>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <ul class="issues-list">
                """
                for issue in other_issues:
                    other_issues_section += f"<li>{issue}</li>"
                other_issues_section += """
                    </ul>
                </div>
                """

            # Claude ê²€ì¦ ê²°ê³¼ ì„¹ì…˜
            claude_section = ""
            if claude_issues:
                claude_section = """
                <div class="claude-section">
                    <h3>ğŸ¤– Claude AI ê²€ì¦ ê²°ê³¼</h3>
                """
                for claude_result in claude_issues:
                    # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìŠ¤íƒ€ì¼ ì ìš©
                    claude_section += f"""
                    <div class="claude-result">
                        <pre class="claude-text">{claude_result}</pre>
                    </div>
                    """
                claude_section += """
                </div>
                """

            # ì „ì²´ ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš°
            success_section = ""
            if not issues:
                success_section = """
                <div class="issues-section success">
                    <h3>âœ… ê²€ì¦ ê²°ê³¼</h3>
                    <p class="no-issues">ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</p>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SQL ê²€ì¦ ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: none;
        }}
        .claude-section {{
            margin: 30px 0;
            padding: 25px;
            border-radius: 8px;
            border: 2px solid #667eea;
            background: #f8f9ff;
            box-shadow: 0 2px 10px rgba(102, 126, 234, 0.1);
        }}
        .claude-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            font-size: 1.3em;
        }}
        .claude-result {{
            margin: 20px 0;
            padding: 0;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            box-shadow: 0 1px 5px rgba(0,0,0,0.05);
        }}
        .claude-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 800px;  /* 400pxì—ì„œ 800pxë¡œ ì¦ê°€ */
            overflow-y: auto;
            min-height: 100px;
            resize: vertical;  /* ì‚¬ìš©ìê°€ ìˆ˜ì§ìœ¼ë¡œ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥ */
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} SQL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ SQL íƒ€ì…</h4>
                    <p>{ddl_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ SQL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="info-section">
                <h3>ğŸ“Š ê²€ì¦ ê²°ê³¼</h3>
                <p style="font-size: 1.2em; font-weight: 500; color: {status_color};">{summary}</p>
            </div>
            
            {claude_section}
            {other_issues_section}
            {success_section}
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")

    async def step_6_claude_validation(self, state: dict) -> dict:
        """6ë‹¨ê³„: Claude AI ì¢…í•© ê²€ì¦ (ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)"""
        # stateê°€ Noneì¸ ê²½ìš° ë°©ì–´ ì½”ë“œ
        if state is None:
            logger.error("step_6_claude_validation: state is None")
            return {
                "current_step": 7,
                "issues": ["ì´ì „ ë‹¨ê³„ì—ì„œ ìƒíƒœ ì •ë³´ê°€ ì†ì‹¤ë˜ì—ˆìŠµë‹ˆë‹¤."],
                "warnings": [],
                "claude_issues": [],
            }

        try:
            logger.info("Claude ê²€ì¦ ì‹œì‘")

            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
            existing_analysis = {
                "syntax_issues": state.get("syntax_issues", []),
                "schema_issues": state.get("schema_issues", []),
                "constraint_issues": state.get("constraint_issues", []),
                "db_connection_info": state.get("db_connection_info", {}),
            }

            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ (DB ì—°ê²°ì´ ì„±ê³µí•œ ê²½ìš°)
            schema_info = None
            if state.get("db_connected", False) and state.get("database_secret"):
                try:
                    schema_info = await self.extract_current_schema_info(
                        state["database_secret"]
                    )
                except Exception as e:
                    logger.warning(f"Claude ê²€ì¦ìš© ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            logger.info("Claude ê²€ì¦ ì‹¤í–‰ ì¤‘...")
            # Claude ê²€ì¦ ì‹¤í–‰
            claude_result = await self.validate_with_claude(
                state["ddl_content"],
                state.get("database_secret"),
                schema_info,
                existing_analysis,
            )
            logger.info(f"Claude ê²€ì¦ ê²°ê³¼ (ì „ì²´): {claude_result}")
            print(f"ğŸ¤– Claude AI ê²€ì¦ ê²°ê³¼:\n{claude_result}\n" + "=" * 50)

            # Claude ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            claude_issues = []
            if (
                "ë¬¸ì œ" in claude_result
                or "ì˜¤ë¥˜" in claude_result
                or "ìœ„ë°˜" in claude_result
            ):
                # "ê²€ì¦ í†µê³¼"ê°€ ì•„ë‹Œ ê²½ìš° ì´ìŠˆë¡œ ì²˜ë¦¬
                if "ê²€ì¦ í†µê³¼" not in claude_result:
                    claude_issues.append(f"Claude ê²€ì¦: {claude_result}")

            state["claude_issues"] = claude_issues
            if claude_issues:
                state["issues"].extend(claude_issues)
            else:
                state["warnings"].append("âœ… Claude AI ê²€ì¦ í†µê³¼")

            logger.info("Claude ê²€ì¦ ì™„ë£Œ")

        except Exception as e:
            import traceback

            error_trace = traceback.format_exc()
            logger.error(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_trace}")
            state["issues"].append(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        state["current_step"] = 7
        return state

    async def generate_final_report(
        self, state: dict, generate_html: bool = True
    ) -> str:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        filename = state["filename"]
        ddl_content = state["ddl_content"]
        ddl_type = state.get("ddl_type", "UNKNOWN")

        # ëª¨ë“  ì´ìŠˆ ìˆ˜ì§‘ - ì„±ê³µí•œ ê²€ì¦ì˜ ì˜¤ë¥˜ëŠ” ì œì™¸
        all_issues = []
        all_issues.extend(state.get("syntax_issues", []))

        # MySQL ì—°ê²° ì˜¤ë¥˜ ì™„ì „ ì œê±°
        clean_issues = []
        for issue in state.get("issues", []):
            issue_str = str(issue)
            # MySQL ê´€ë ¨ ì˜¤ë¥˜ ëª¨ë‘ ì œê±°
            skip_keywords = [
                "argument 7 must be str or None, not bool",
                "ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: argument 7",
                "ì œì•½ì¡°ê±´ ê²€ì¦ ì‹¤íŒ¨: ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: argument 7",
            ]
            if any(keyword in issue_str for keyword in skip_keywords):
                continue
            clean_issues.append(issue)

        all_issues.extend(clean_issues)

        # ìƒíƒœ ê²°ì •
        if not state.get("syntax_valid", False):
            summary = f"âŒ ë¬¸ë²• ì˜¤ë¥˜ë¡œ ì¸í•œ ê²€ì¦ ì‹¤íŒ¨: {len(state.get('syntax_issues', []))}ê°œ"
            status = "FAIL"
        elif len(all_issues) == 0:
            summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
            status = "PASS"
        else:
            summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {len(all_issues)}ê°œ"
            status = "FAIL"

        # HTML ë³´ê³ ì„œ ìƒì„± (generate_htmlì´ Trueì¸ ê²½ìš°ë§Œ)
        if generate_html:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"

            # ìƒˆë¡œìš´ generate_html_report í•¨ìˆ˜ ì‚¬ìš©
            await self.generate_html_report(
                report_path=report_path,
                filename=filename,
                ddl_content=ddl_content,
                ddl_type=ddl_type,
                status=status,
                summary=summary,
                issues=all_issues,
                db_connection_info=state.get("db_connection_info"),
                schema_validation=state.get("schema_validation"),
                constraint_validation=state.get("constraint_validation"),
                database_secret=state.get("database_secret"),
            )

        # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
        result_message = f"""
{summary}
"""

        # HTML ë³´ê³ ì„œê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ê²½ë¡œ í‘œì‹œ
        if generate_html:
            result_message += f"""
ğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}
"""

        result_message += """
ğŸ“Š ê²€ì¦ ê²°ê³¼:
"""

        # ê° ë‹¨ê³„ë³„ ê²°ê³¼ ì¶”ê°€
        if state.get("syntax_issues"):
            result_message += f"â€¢ ë¬¸ë²• ê²€ì¦: âŒ {len(state['syntax_issues'])}ê°œ ë¬¸ì œ\n"
        else:
            result_message += "â€¢ ë¬¸ë²• ê²€ì¦: âœ… í†µê³¼\n"

        if state.get("db_connected"):
            result_message += "â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ… ì„±ê³µ\n"

            if state.get("schema_issues"):
                result_message += (
                    f"â€¢ ìŠ¤í‚¤ë§ˆ ê²€ì¦: âŒ {len(state['schema_issues'])}ê°œ ë¬¸ì œ\n"
                )
            else:
                result_message += "â€¢ ìŠ¤í‚¤ë§ˆ ê²€ì¦: âœ… í†µê³¼\n"

            if state.get("constraint_issues"):
                result_message += (
                    f"â€¢ ì œì•½ì¡°ê±´ ê²€ì¦: âŒ {len(state['constraint_issues'])}ê°œ ë¬¸ì œ\n"
                )
            else:
                result_message += "â€¢ ì œì•½ì¡°ê±´ ê²€ì¦: âœ… í†µê³¼\n"
        elif state.get("database_secret"):
            result_message += "â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âŒ ì‹¤íŒ¨\n"

        if state.get("claude_issues"):
            result_message += (
                f"â€¢ Claude AI ê²€ì¦: âŒ {len(state['claude_issues'])}ê°œ ë¬¸ì œ\n"
            )
        else:
            result_message += "â€¢ Claude AI ê²€ì¦: âœ… í†µê³¼\n"

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬
        try:
            connection = state.get("db_connection")
            if connection and connection.is_connected():
                connection.close()
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

            if state.get("tunnel_used"):
                self.cleanup_ssh_tunnel()
        except Exception as e:
            logger.warning(f"ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        return result_message

    def extract_ddl_type(self, ddl_content: str) -> str:
        """SQL íƒ€ì… ì¶”ì¶œ (DDL ë° SELECT í¬í•¨)"""
        sql_upper = ddl_content.upper().strip()
        if sql_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif sql_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif sql_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif sql_upper.startswith("DROP"):
            return "DROP"
        elif sql_upper.startswith("SELECT"):
            return "SELECT"
        else:
            return "UNKNOWN"

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH í„°ë„ ì‹œì‘ (ssh_tunnel.sh ë°©ì‹ ì‚¬ìš©, SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ)
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",  # SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            process = subprocess.run(ssh_command, capture_output=True, text=True)

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db_result = cursor.fetchone()
                current_db = (
                    current_db_result[0]
                    if current_db_result and current_db_result[0]
                    else "None"
                )

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "databases": databases,
                    "tables": tables,
                }

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"MySQL ì˜¤ë¥˜: {str(e)}"}
        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}",
                "debug": error_details,
            }

    async def validate_select_query_with_connection(
        self, select_content: str, connection
    ) -> Dict[str, Any]:
        """ê¸°ì¡´ ì—°ê²°ì„ ì‚¬ìš©í•œ SELECT ì¿¼ë¦¬ ê²€ì¦"""
        try:
            cursor = connection.cursor()
            validation_results = []

            # SELECT ì¿¼ë¦¬ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
            tables = self.extract_tables_from_select(select_content)
            
            # ê° í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            for table_name in tables:
                try:
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    
                    table_exists = cursor.fetchone()[0] > 0
                    
                    if not table_exists:
                        validation_results.append({
                            "table": table_name,
                            "query_type": "SELECT",
                            "valid": False,
                            "issues": [f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."],
                        })
                    else:
                        # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ë©´ ì»¬ëŸ¼ ê²€ì¦
                        column_issues = await self.validate_select_columns(
                            select_content, table_name, cursor
                        )
                        
                        validation_results.append({
                            "table": table_name,
                            "query_type": "SELECT",
                            "valid": len(column_issues) == 0,
                            "issues": column_issues,
                        })
                        
                except Exception as e:
                    validation_results.append({
                        "table": table_name,
                        "query_type": "SELECT",
                        "valid": False,
                        "issues": [f"í…Œì´ë¸” '{table_name}' ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"],
                    })

            # SELECT ì¿¼ë¦¬ ì‹¤í–‰ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸ (EXPLAIN ì‚¬ìš©)
            try:
                explain_query = f"EXPLAIN {select_content}"
                cursor.execute(explain_query)
                cursor.fetchall()  # ê²°ê³¼ ì†Œë¹„
                
                validation_results.append({
                    "table": "query_execution",
                    "query_type": "SELECT",
                    "valid": True,
                    "issues": [],
                    "note": "ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ê²€ì¦ í†µê³¼"
                })
                
            except Exception as e:
                validation_results.append({
                    "table": "query_execution",
                    "query_type": "SELECT",
                    "valid": False,
                    "issues": [f"ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ì˜¤ë¥˜: {str(e)}"],
                })

            cursor.close()
            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            return {"success": False, "error": f"SELECT ì¿¼ë¦¬ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def extract_tables_from_select(self, select_content: str) -> List[str]:
        """SELECT ì¿¼ë¦¬ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        import re
        
        tables = []
        
        # FROM ì ˆì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ (ê¸°ë³¸ì ì¸ íŒ¨í„´)
        from_pattern = r'FROM\s+`?(\w+)`?'
        from_matches = re.findall(from_pattern, select_content, re.IGNORECASE)
        tables.extend(from_matches)
        
        # JOIN ì ˆì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
        join_pattern = r'JOIN\s+`?(\w+)`?'
        join_matches = re.findall(join_pattern, select_content, re.IGNORECASE)
        tables.extend(join_matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        return list(set([table.lower() for table in tables]))

    async def validate_select_columns(
        self, select_content: str, table_name: str, cursor
    ) -> List[str]:
        """SELECT ì¿¼ë¦¬ì˜ ì»¬ëŸ¼ ê²€ì¦"""
        issues = []
        
        try:
            # í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )
            
            existing_columns = {row[0].lower() for row in cursor.fetchall()}
            
            # SELECT ì ˆì—ì„œ ì»¬ëŸ¼ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
            select_columns = self.extract_columns_from_select(select_content)
            
            # ê° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            for column in select_columns:
                if column != "*" and column not in existing_columns:
                    # í•¨ìˆ˜ë‚˜ í‘œí˜„ì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ì»¬ëŸ¼ëª…ë§Œ ê²€ì‚¬
                    if not self.is_function_or_expression(column):
                        issues.append(f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        
        except Exception as e:
            issues.append(f"ì»¬ëŸ¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        return issues

    def extract_columns_from_select(self, select_content: str) -> List[str]:
        """SELECT ì ˆì—ì„œ ì»¬ëŸ¼ëª… ì¶”ì¶œ (ê¸°ë³¸ì ì¸ íŒ¨í„´)"""
        import re
        
        # SELECTì™€ FROM ì‚¬ì´ì˜ ì»¬ëŸ¼ ë¶€ë¶„ ì¶”ì¶œ
        select_pattern = r'SELECT\s+(.*?)\s+FROM'
        match = re.search(select_pattern, select_content, re.IGNORECASE | re.DOTALL)
        
        if not match:
            return []
            
        columns_part = match.group(1)
        
        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
        columns = []
        for col in columns_part.split(','):
            col = col.strip()
            # AS ë³„ì¹­ ì œê±°
            if ' AS ' in col.upper():
                col = col.split(' AS ')[0].strip()
            elif ' ' in col and not self.is_function_or_expression(col):
                col = col.split()[0].strip()
            
            # ë°±í‹± ì œê±°
            col = col.strip('`')
            
            if col:
                columns.append(col.lower())
                
        return columns

    def is_function_or_expression(self, column: str) -> bool:
        """ì»¬ëŸ¼ì´ í•¨ìˆ˜ë‚˜ í‘œí˜„ì‹ì¸ì§€ í™•ì¸"""
        # í•¨ìˆ˜ í˜¸ì¶œ íŒ¨í„´ (ê´„í˜¸ í¬í•¨)
        if '(' in column and ')' in column:
            return True
            
        # ì‚°ìˆ  ì—°ì‚°ì í¬í•¨
        if any(op in column for op in ['+', '-', '*', '/', '%']):
            return True
            
        # ì§‘ê³„ í•¨ìˆ˜ë“¤
        functions = ['COUNT', 'SUM', 'AVG', 'MAX', 'MIN', 'CONCAT', 'SUBSTRING', 'DATE', 'NOW']
        for func in functions:
            if func in column.upper():
                return True
                
        return False
        """ê¸°ì¡´ ì—°ê²°ì„ ì‚¬ìš©í•œ DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ (íŒŒì¼ ë‚´ ìˆœì„œ ê³ ë ¤)"""
        try:
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed(ddl_content)
            if not ddl_info:
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

            # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ë“¤ì„ ë¯¸ë¦¬ ì¶”ì¶œ
            created_tables_in_file = set()
            for ddl_statement in ddl_info:
                if ddl_statement["type"] == "CREATE_TABLE":
                    created_tables_in_file.add(ddl_statement["table"].lower())

            cursor = connection.cursor()
            validation_results = []

            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦ (ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬)
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                if ddl_type == "CREATE_TABLE":
                    result = await self.validate_create_table(cursor, ddl_statement)
                elif ddl_type == "ALTER_TABLE":
                    # ALTER TABLE ê²€ì¦ ì‹œ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    result = await self.validate_alter_table(
                        cursor, ddl_statement, created_tables_in_file
                    )
                elif ddl_type == "CREATE_INDEX":
                    # CREATE INDEX ê²€ì¦ ì‹œ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    result = await self.validate_create_index(
                        cursor, ddl_statement, created_tables_in_file
                    )
                elif ddl_type == "DROP_TABLE":
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == "DROP_INDEX":
                    result = await self.validate_drop_index(cursor, ddl_statement)
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"],
                    }

                validation_results.append(result)

            cursor.close()

            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    async def validate_constraints_with_connection(
        self, ddl_content: str, connection
    ) -> Dict[str, Any]:
        """ê¸°ì¡´ ì—°ê²°ì„ ì‚¬ìš©í•œ ì œì•½ì¡°ê±´ ê²€ì¦ - ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸ (FK ì œì™¸)"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)
            cursor = connection.cursor()
            constraint_results = []

            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ì€ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì œì•½ì¡°ê±´ë§Œ ê²€ì¦
            # í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ì œì•½ì¡°ê±´ ê²€ì¦ë§Œ ìˆ˜í–‰
            # í–¥í›„ í•„ìš”ì‹œ PRIMARY KEY, UNIQUE ë“±ì˜ ì œì•½ì¡°ê±´ ê²€ì¦ ì¶”ê°€ ê°€ëŠ¥

            # ê¸°ë³¸ì ìœ¼ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (FK ê²€ì¦ ì œì™¸)
            constraint_results.append(
                {
                    "type": "BASIC_CONSTRAINTS",
                    "constraint": "ê¸°ë³¸ ì œì•½ì¡°ê±´ ê²€ì¦",
                    "valid": True,
                    "issue": None,
                }
            )

            cursor.close()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            return {"success": False, "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    async def validate_schema(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ (íŒŒì¼ ë‚´ ìˆœì„œ ê³ ë ¤) - ë°±ì—…ìš© í•¨ìˆ˜"""
        try:
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed(ddl_content)
            if not ddl_info:
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

            # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ë“¤ì„ ë¯¸ë¦¬ ì¶”ì¶œ
            created_tables_in_file = set()
            for ddl_statement in ddl_info:
                if ddl_statement["type"] == "CREATE_TABLE":
                    created_tables_in_file.add(ddl_statement["table"].lower())

            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            validation_results = []

            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦ (ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬)
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                if ddl_type == "CREATE_TABLE":
                    result = await self.validate_create_table(cursor, ddl_statement)
                elif ddl_type == "ALTER_TABLE":
                    # ALTER TABLE ê²€ì¦ ì‹œ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    result = await self.validate_alter_table(
                        cursor, ddl_statement, created_tables_in_file
                    )
                elif ddl_type == "CREATE_INDEX":
                    # CREATE INDEX ê²€ì¦ ì‹œ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    result = await self.validate_create_index(
                        cursor, ddl_statement, created_tables_in_file
                    )
                elif ddl_type == "DROP_TABLE":
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == "DROP_INDEX":
                    result = await self.validate_drop_index(cursor, ddl_statement)
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"],
                    }

                validation_results.append(result)

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    async def validate_constraints(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ì œì•½ì¡°ê±´ ê²€ì¦ - ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸ (FK ì œì™¸) - ë°±ì—…ìš© í•¨ìˆ˜"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)

            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            constraint_results = []

            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ì€ ì œì™¸í•˜ê³  ê¸°ë³¸ ì œì•½ì¡°ê±´ë§Œ ê²€ì¦
            constraint_results.append(
                {
                    "type": "BASIC_CONSTRAINTS",
                    "constraint": "ê¸°ë³¸ ì œì•½ì¡°ê±´ ê²€ì¦",
                    "valid": True,
                    "issue": None,
                }
            )

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ"""
        ddl_statements = []

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(
            create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE
        )

        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append(
                {
                    "type": "CREATE_TABLE",
                    "table": table_name.lower(),
                    "columns": columns_info["columns"],
                    "constraints": columns_info["constraints"],
                }
            )

        # ALTER TABLE íŒŒì‹±
        alter_patterns = [
            # ADD COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "ADD_COLUMN",
            ),
            # DROP COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?",
                "DROP_COLUMN",
            ),
            # MODIFY COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "MODIFY_COLUMN",
            ),
            # CHANGE COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)",
                "CHANGE_COLUMN",
            ),
        ]

        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == "CHANGE_COLUMN":
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "old_column": old_column.lower(),
                            "new_column": new_column.lower(),
                            "column_definition": column_def.strip(),
                        }
                    )
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "column": column_name.lower(),
                            "column_definition": (
                                column_def.strip() if column_def else None
                            ),
                        }
                    )

        # CREATE INDEX íŒŒì‹±
        create_index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)"
        )
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name, columns in index_matches:
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": [
                        col.strip().strip("`").lower() for col in columns.split(",")
                    ],
                }
            )

        # DROP TABLE íŒŒì‹±
        drop_table_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?"
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)

        for table_name in drop_table_matches:
            ddl_statements.append({"type": "DROP_TABLE", "table": table_name.lower()})

        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name in drop_index_matches:
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )

        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEì˜ ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±"""
        columns = []
        constraints = []

        # ì»¬ëŸ¼ ì •ì˜ì™€ ì œì•½ì¡°ê±´ì„ ë¶„ë¦¬
        lines = [line.strip() for line in columns_def.split(",")]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì œì•½ì¡°ê±´ í™•ì¸
            if re.match(
                r"(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)",
                line,
                re.IGNORECASE,
            ):
                constraints.append(line)
            else:
                # ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±
                column_match = re.match(
                    r"`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?", line, re.IGNORECASE
                )
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ""

                    columns.append(
                        {
                            "name": column_name,
                            "data_type": data_type,
                            "attributes": attributes.strip(),
                        }
                    )

        return {"columns": columns, "constraints": constraints}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def validate_create_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        columns = ddl_statement["columns"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists, "columns_count": len(columns)},
        }

    async def validate_alter_table(
        self, cursor, ddl_statement: Dict[str, Any], created_tables_in_file: set = None
    ) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦ (íŒŒì¼ ë‚´ ìƒì„± í…Œì´ë¸” ê³ ë ¤)"""
        table_name = ddl_statement["table"]
        alter_type = ddl_statement["alter_type"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
        created_in_file = (
            created_tables_in_file and table_name.lower() in created_tables_in_file
        )

        if not table_exists and not created_in_file:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues,
            }

        # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì˜ ê²½ìš° ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ê±´ë„ˆë›°ê³  ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        if created_in_file and not table_exists:
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": True,
                "issues": [],
                "note": f"í…Œì´ë¸” '{table_name}'ì€ ê°™ì€ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        existing_columns = {
            row[0].lower(): {
                "data_type": row[1].upper(),
                "max_length": row[2],
                "precision": row[3],
                "scale": row[4],
                "is_nullable": row[5],
            }
            for row in cursor.fetchall()
        }

        # ALTER ìœ í˜•ë³„ ê²€ì¦
        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement["column"]
            if column_name in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement["column"]
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement["column"]
            new_definition = ddl_statement["column_definition"]

            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        elif alter_type == "CHANGE_COLUMN":
            old_column = ddl_statement["old_column"]
            new_column = ddl_statement["new_column"]
            new_definition = ddl_statement["column_definition"]

            if old_column not in existing_columns:
                issues.append(f"ê¸°ì¡´ ì»¬ëŸ¼ '{old_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ìƒˆ ì»¬ëŸ¼ëª… '{new_column}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"existing_columns": list(existing_columns.keys())},
        }

    async def validate_create_index(
        self, cursor, ddl_statement: Dict[str, Any], created_tables_in_file: set = None
    ) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦ (íŒŒì¼ ë‚´ ìƒì„± í…Œì´ë¸” ê³ ë ¤)"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
        created_in_file = (
            created_tables_in_file and table_name.lower() in created_tables_in_file
        )

        if not table_exists and not created_in_file:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        elif created_in_file and not table_exists:
            # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ëœ í…Œì´ë¸”ì˜ ê²½ìš° ì¸ë±ìŠ¤ ìƒì„±ì„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            return {
                "table": table_name,
                "ddl_type": "CREATE_INDEX",
                "index_name": index_name,
                "valid": True,
                "issues": [],
                "note": f"í…Œì´ë¸” '{table_name}'ì€ ê°™ì€ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}

            for column in columns:
                if column not in existing_columns:
                    issues.append(
                        f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "columns": columns},
        }

    async def validate_drop_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists},
        }

    async def validate_drop_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if not index_exists:
                issues.append(
                    f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )

        return {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name},
        }

    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ (FK ì œì™¸)"""
        constraints = {"indexes": [], "primary_keys": []}

        # ì™¸ë˜í‚¤ëŠ” ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì œì•½ì¡°ê±´ë§Œ ì²˜ë¦¬
        # í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ì œì•½ì¡°ê±´ë§Œ ì²˜ë¦¬í•˜ê³ , í•„ìš”ì‹œ í™•ì¥ ê°€ëŠ¥

        return constraints

    async def analyze_current_schema(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„¸ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            schema_analysis = {
                "current_database": current_db,
                "tables": {},
                "indexes": {},
                "foreign_keys": {},
                "constraints": {},
            }

            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            for table_info in tables_info:
                table_name = table_info[0]
                schema_analysis["tables"][table_name] = {
                    "type": table_info[1],
                    "engine": table_info[2],
                    "rows": table_info[3],
                    "data_length": table_info[4],
                    "index_length": table_info[5],
                    "comment": table_info[6],
                    "columns": {},
                    "indexes": [],
                    "foreign_keys": [],
                }

                # ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT column_name, data_type, is_nullable, column_default,
                           column_key, extra, column_comment, character_maximum_length,
                           numeric_precision, numeric_scale
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table_name,),
                )

                columns_info = cursor.fetchall()
                for col_info in columns_info:
                    col_name = col_info[0]
                    schema_analysis["tables"][table_name]["columns"][col_name] = {
                        "data_type": col_info[1],
                        "is_nullable": col_info[2],
                        "default": col_info[3],
                        "key": col_info[4],
                        "extra": col_info[5],
                        "comment": col_info[6],
                        "max_length": col_info[7],
                        "precision": col_info[8],
                        "scale": col_info[9],
                    }

                # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT index_name, column_name, seq_in_index, non_unique,
                           index_type, comment
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table_name,),
                )

                indexes_info = cursor.fetchall()
                current_index = None
                for idx_info in indexes_info:
                    idx_name = idx_info[0]
                    if current_index != idx_name:
                        schema_analysis["tables"][table_name]["indexes"].append(
                            {
                                "name": idx_name,
                                "columns": [idx_info[1]],
                                "unique": idx_info[3] == 0,
                                "type": idx_info[4],
                                "comment": idx_info[5],
                            }
                        )
                        current_index = idx_name
                    else:
                        # ë³µí•© ì¸ë±ìŠ¤ì˜ ì¶”ê°€ ì»¬ëŸ¼
                        schema_analysis["tables"][table_name]["indexes"][-1][
                            "columns"
                        ].append(idx_info[1])

                # ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT kcu.constraint_name, kcu.column_name, kcu.referenced_table_name,
                           kcu.referenced_column_name, rc.update_rule, rc.delete_rule
                    FROM information_schema.key_column_usage kcu
                    JOIN information_schema.referential_constraints rc
                    ON kcu.constraint_name = rc.constraint_name 
                    AND kcu.table_schema = rc.constraint_schema
                    WHERE kcu.table_schema = DATABASE() AND kcu.table_name = %s
                    AND kcu.referenced_table_name IS NOT NULL
                """,
                    (table_name,),
                )

                fk_info = cursor.fetchall()
                for fk in fk_info:
                    schema_analysis["tables"][table_name]["foreign_keys"].append(
                        {
                            "constraint_name": fk[0],
                            "column": fk[1],
                            "referenced_table": fk[2],
                            "referenced_column": fk[3],
                            "update_rule": fk[4],
                            "delete_rule": fk[5],
                        }
                    )

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "schema_analysis": schema_analysis}

        except Exception as e:
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"}

    async def check_ddl_conflicts(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì  ì‚¬ì „ ê²€ì‚¬"""
        try:
            # í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë¶„ì„
            schema_result = await self.analyze_current_schema(
                database_secret, use_ssh_tunnel
            )
            if not schema_result["success"]:
                return schema_result

            schema = schema_result["schema_analysis"]
            ddl_type = self.extract_ddl_type(ddl_content)

            conflicts = []
            warnings = []
            recommendations = []

            if ddl_type == "CREATE_TABLE":
                conflicts.extend(
                    await self._check_create_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "ALTER_TABLE":
                conflicts.extend(
                    await self._check_alter_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "CREATE_INDEX":
                conflicts.extend(
                    await self._check_create_index_conflicts(ddl_content, schema)
                )
            elif ddl_type == "DROP":
                conflicts.extend(await self._check_drop_conflicts(ddl_content, schema))

            return {
                "success": True,
                "ddl_type": ddl_type,
                "conflicts": conflicts,
                "warnings": warnings,
                "recommendations": recommendations,
                "current_schema": schema,
            }

        except Exception as e:
            return {"success": False, "error": f"DDL ì¶©ëŒ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}"}

    async def _check_create_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?",
            ddl_content,
            re.IGNORECASE,
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name in schema["tables"]:
                if "IF NOT EXISTS" not in ddl_content.upper():
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”."
                    )
                else:
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSê°€ ìˆì–´ì„œ ì‹¤í–‰ì€ ë˜ì§€ë§Œ ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_alter_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """ALTER TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"ALTER\s+TABLE\s+`?(\w+)`?", ddl_content, re.IGNORECASE
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ADD COLUMN ê²€ì‚¬
            add_column_matches = re.findall(
                r"ADD\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in add_column_matches:
                if col_name in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                    )

            # DROP COLUMN ê²€ì‚¬
            drop_column_matches = re.findall(
                r"DROP\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in drop_column_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸
                    for fk in table_info["foreign_keys"]:
                        if fk["column"] == col_name:
                            conflicts.append(
                                f"ì»¬ëŸ¼ '{col_name}'ì€ ì™¸ë˜í‚¤ë¡œ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            )

            # MODIFY/CHANGE COLUMN ê²€ì‚¬
            modify_matches = re.findall(
                r"(?:MODIFY|CHANGE)\s+(?:COLUMN\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            for col_name in modify_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ìˆ˜ì •í•˜ë ¤ëŠ” ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_create_index_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE INDEX ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # ì¸ë±ìŠ¤ëª…ê³¼ í…Œì´ë¸”ëª… ì¶”ì¶œ
        index_match = re.search(
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\((.*?)\)",
            ddl_content,
            re.IGNORECASE,
        )
        if index_match:
            index_name = index_match.group(1)
            table_name = index_match.group(2)
            columns_str = index_match.group(3)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ì¸ë±ìŠ¤ ì¤‘ë³µ í™•ì¸
            for existing_index in table_info["indexes"]:
                if existing_index["name"] == index_name:
                    conflicts.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            columns = [col.strip().strip("`") for col in columns_str.split(",")]
            for col_name in columns:
                # í•¨ìˆ˜ë‚˜ í‘œí˜„ì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ì»¬ëŸ¼ëª…ë§Œ ê²€ì‚¬
                if col_name and not re.search(r"[()]", col_name):
                    if col_name not in table_info["columns"]:
                        conflicts.append(
                            f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def _check_drop_conflicts(self, ddl_content: str, schema: Dict) -> List[str]:
        """DROP ë¬¸ ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        if "DROP TABLE" in ddl_content.upper():
            # í…Œì´ë¸” ì‚­ì œ ê²€ì‚¬
            table_match = re.search(
                r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            if table_match:
                table_name = table_match.group(1)
                if table_name not in schema["tables"]:
                    if "IF EXISTS" not in ddl_content.upper():
                        conflicts.append(
                            f"ì‚­ì œí•˜ë ¤ëŠ” í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸ (ë‹¤ë¥¸ í…Œì´ë¸”ì—ì„œ ì´ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€)
                    for other_table, table_info in schema["tables"].items():
                        for fk in table_info["foreign_keys"]:
                            if fk["referenced_table"] == table_name:
                                conflicts.append(
                                    f"í…Œì´ë¸” '{table_name}'ì€ í…Œì´ë¸” '{other_table}'ì—ì„œ ì™¸ë˜í‚¤ë¡œ ì°¸ì¡°ë˜ê³  ìˆì–´ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                )

        elif "DROP INDEX" in ddl_content.upper():
            # ì¸ë±ìŠ¤ ì‚­ì œ ê²€ì‚¬
            index_match = re.search(
                r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            if index_match:
                index_name = index_match.group(1)
                table_name = index_match.group(2)

                if table_name not in schema["tables"]:
                    conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    table_info = schema["tables"][table_name]
                    index_exists = any(
                        idx["name"] == index_name for idx in table_info["indexes"]
                    )
                    if not index_exists:
                        conflicts.append(
                            f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def get_aurora_mysql_parameters(
        self,
        cluster_identifier: str,
        region: str = "ap-northeast-2",
        filter_type: str = "important",
        category: str = "all",
    ) -> str:
        """Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„° ì¡°íšŒ

        Args:
            cluster_identifier: í´ëŸ¬ìŠ¤í„° ì‹ë³„ì
            region: AWS ë¦¬ì „
            filter_type: í•„í„° íƒ€ì… (important, custom, all)
            category: íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)
        """
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
            clusters_response = rds_client.describe_db_clusters(
                DBClusterIdentifier=cluster_identifier
            )

            if not clusters_response["DBClusters"]:
                return f"âŒ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cluster_identifier}"

            cluster = clusters_response["DBClusters"][0]
            cluster_param_group = cluster.get(
                "DBClusterParameterGroup", "default.aurora-mysql8.0"
            )

            # ì¹´í…Œê³ ë¦¬ë³„ ì œëª© ì„¤ì •
            category_titles = {
                "all": "ì „ì²´",
                "security": "ë³´ì•ˆ ë° ì¸ì¦",
                "performance": "ì„±ëŠ¥ ìµœì í™”",
                "memory": "ë©”ëª¨ë¦¬ ê´€ë¦¬",
                "io": "I/O ë° ìŠ¤í† ë¦¬ì§€",
                "connection": "ì—°ê²° ê´€ë¦¬",
                "logging": "ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                "replication": "ë³µì œ ë° ë°±ì—…",
                "aurora": "Aurora íŠ¹í™” ê¸°ëŠ¥",
            }

            category_title = category_titles.get(category, category)

            result = f"""ğŸ“Š Aurora MySQL íŒŒë¼ë¯¸í„° ì •ë³´ ({category_title})

ğŸ”§ **í´ëŸ¬ìŠ¤í„° ì •ë³´:**
- í´ëŸ¬ìŠ¤í„° ID: {cluster_identifier}
- í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ê·¸ë£¹: {cluster_param_group}
- ì—”ì§„ ë²„ì „: {cluster.get('EngineVersion', 'N/A')}"""

            # í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ì¡°íšŒ
            cluster_params = await self._get_parameters(
                rds_client, cluster_param_group, "cluster", filter_type, category
            )
            if cluster_params:
                result += f"\n\nğŸ—ï¸ **í´ëŸ¬ìŠ¤í„° ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{cluster_params}"

            # ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if cluster.get("DBClusterMembers"):
                instance_id = cluster["DBClusterMembers"][0]["DBInstanceIdentifier"]
                instance_response = rds_client.describe_db_instances(
                    DBInstanceIdentifier=instance_id
                )
                if instance_response["DBInstances"]:
                    instance_param_group = instance_response["DBInstances"][0][
                        "DBParameterGroups"
                    ][0]["DBParameterGroupName"]
                    result += f"\n- ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ê·¸ë£¹: {instance_param_group}"

                    instance_params = await self._get_parameters(
                        rds_client,
                        instance_param_group,
                        "instance",
                        filter_type,
                        category,
                    )
                    if instance_params:
                        result += (
                            f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{instance_params}"
                        )
                    else:
                        if category == "all":
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— í‘œì‹œí•  íŒŒë¼ë¯¸í„° ì—†ìŒ"
                        else:
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** {category_title} ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„° ì—†ìŒ"

            return result

        except Exception as e:
            return f"âŒ Aurora íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_parameters(
        self,
        rds_client,
        param_group_name: str,
        level: str,
        filter_type: str,
        category: str,
    ) -> str:
        """íŒŒë¼ë¯¸í„° ì¡°íšŒ ë° í•„í„°ë§"""
        try:
            # íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if level == "cluster":
                response = rds_client.describe_db_cluster_parameters(
                    DBClusterParameterGroupName=param_group_name
                )
            else:
                response = rds_client.describe_db_parameters(
                    DBParameterGroupName=param_group_name
                )

            parameters = response["Parameters"]

            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒë¼ë¯¸í„° ì •ì˜ (í™•ì¥ëœ ë¶„ë¥˜)
            category_params = {
                "security": [
                    "activate_all_roles_on_login",
                    "authentication_kerberos_caseins_cmp",
                    "default_authentication_plugin",
                    "default_password_lifetime",
                    "check_proxy_users",
                    "mysql_native_password_proxy_users",
                    "sha256_password_proxy_users",
                    "validate_password_policy",
                ],
                "performance": [
                    "innodb_thread_concurrency",
                    "innodb_read_io_threads",
                    "innodb_write_io_threads",
                    "thread_cache_size",
                    "thread_stack",
                    "innodb_purge_threads",
                    "innodb_adaptive_flushing",
                    "innodb_adaptive_max_sleep_delay",
                    "innodb_concurrency_tickets",
                    "innodb_flushing_avg_loops",
                    "innodb_lru_scan_depth",
                    "innodb_max_dirty_pages_pct",
                    "innodb_max_purge_lag",
                    "innodb_max_purge_lag_delay",
                    "innodb_old_blocks_pct",
                    "innodb_old_blocks_time",
                    "innodb_parallel_read_threads",
                    "query_cache_size",
                    "query_cache_type",
                ],
                "memory": [
                    "innodb_buffer_pool_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "sort_buffer_size",
                    "read_buffer_size",
                    "read_rnd_buffer_size",
                    "join_buffer_size",
                    "innodb_log_buffer_size",
                    "key_buffer_size",
                    "innodb_buffer_pool_dump_at_shutdown",
                    "innodb_buffer_pool_load_at_startup",
                    "innodb_buffer_pool_dump_now",
                    "innodb_buffer_pool_load_now",
                    "innodb_change_buffer_max_size",
                    "bulk_insert_buffer_size",
                ],
                "io": [
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "innodb_log_file_size",
                    "innodb_io_capacity",
                    "innodb_io_capacity_max",
                    "innodb_flush_method",
                    "innodb_file_per_table",
                    "innodb_doublewrite",
                    "innodb_flush_neighbors",
                    "innodb_flush_log_at_timeout",
                    "innodb_log_compressed_pages",
                    "innodb_open_files",
                    "innodb_read_only",
                    "innodb_sort_buffer_size",
                ],
                "connection": [
                    "max_connections",
                    "max_user_connections",
                    "connect_timeout",
                    "interactive_timeout",
                    "wait_timeout",
                    "max_connect_errors",
                    "back_log",
                    "host_cache_size",
                    "max_allowed_packet",
                ],
                "logging": [
                    "general_log",
                    "slow_query_log",
                    "log_queries_not_using_indexes",
                    "long_query_time",
                    "log_slow_admin_statements",
                    "log_slow_slave_statements",
                    "general_log_file",
                    "slow_query_log_file",
                    "log_error",
                    "log_warnings",
                    "innodb_print_all_deadlocks",
                ],
                "replication": [
                    "binlog_format",
                    "expire_logs_days",
                    "max_binlog_size",
                    "binlog_cache_size",
                    "slave_net_timeout",
                    "slave_parallel_workers",
                    "binlog_checksum",
                    "binlog_group_commit_sync_delay",
                    "binlog_group_commit_sync_no_delay_count",
                    "binlog_order_commits",
                    "binlog_row_image",
                    "binlog_rows_query_log_events",
                    "binlog_stmt_cache_size",
                    "binlog_transaction_compression",
                    "binlog_backup",
                    "binlog_replication_globaldb",
                ],
                "aurora": [
                    "aurora_binlog_replication_max_yield_seconds",
                    "aurora_binlog_replication_sec_index_parallel_workers",
                    "aurora_enable_staggered_replica_restart",
                    "aurora_enhanced_binlog",
                    "aurora_full_double_precision_in_json",
                    "aurora_fwd_writer_idle_timeout",
                    "aurora_fwd_writer_max_connections_pct",
                    "aurora_in_memory_relaylog",
                    "aurora_jemalloc_background_thread",
                    "aurora_jemalloc_dirty_decay_ms",
                    "aurora_jemalloc_tcache_enabled",
                    "aurora_ml_inference_timeout",
                    "aurora_oom_response",
                    "aurora_parallel_query",
                    "aurora_read_replica_read_committed",
                    "aurora_replica_read_consistency",
                    "aurora_tmptable_enable_per_table_limit",
                    "aurora_use_vector_instructions",
                    "aurora_aurora_max_partitions_for_range",
                ],
            }

            # í•„í„°ë§ ì ìš©
            filtered_params = []

            if filter_type == "all":
                filtered_params = parameters
            elif filter_type == "custom":
                filtered_params = [p for p in parameters if p.get("Source") == "user"]
            elif filter_type == "important":
                important_params = [
                    "innodb_buffer_pool_size",
                    "max_connections",
                    "innodb_log_file_size",
                    "query_cache_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "binlog_format",
                    "character_set_server",
                    "collation_server",
                    "time_zone",
                    "aurora_parallel_query",
                    "aurora_oom_response",
                    "aws_default_s3_role",
                ]
                filtered_params = [
                    p for p in parameters if p["ParameterName"] in important_params
                ]

            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            if category != "all" and category in category_params:
                category_param_names = category_params[category]
                filtered_params = [
                    p
                    for p in filtered_params
                    if p["ParameterName"] in category_param_names
                ]

            if not filtered_params:
                return ""

            # ê²°ê³¼ í¬ë§·íŒ… (ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”)
            result = ""

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”
            if category == "all":
                # ì „ì²´ ì¡°íšŒ ì‹œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ í‘œì‹œ
                categorized_params = {}
                uncategorized_params = []

                for param in filtered_params:
                    param_name = param["ParameterName"]
                    found_category = None

                    for cat, param_list in category_params.items():
                        if param_name in param_list:
                            if cat not in categorized_params:
                                categorized_params[cat] = []
                            categorized_params[cat].append(param)
                            found_category = cat
                            break

                    if not found_category:
                        uncategorized_params.append(param)

                # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
                category_titles = {
                    "security": "ğŸ” ë³´ì•ˆ ë° ì¸ì¦",
                    "performance": "âš¡ ì„±ëŠ¥ ìµœì í™”",
                    "memory": "ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬",
                    "io": "ğŸ’¿ I/O ë° ìŠ¤í† ë¦¬ì§€",
                    "connection": "ğŸ”— ì—°ê²° ê´€ë¦¬",
                    "logging": "ğŸ“ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                    "replication": "ğŸ”„ ë³µì œ ë° ë°±ì—…",
                    "aurora": "â˜ï¸ Aurora íŠ¹í™” ê¸°ëŠ¥",
                }

                for cat in [
                    "security",
                    "performance",
                    "memory",
                    "io",
                    "connection",
                    "logging",
                    "replication",
                    "aurora",
                ]:
                    if cat in categorized_params:
                        result += f"\n{category_titles[cat]}:\n"
                        for param in categorized_params[cat]:
                            name = param["ParameterName"]
                            value = param.get("ParameterValue", "N/A")
                            source = param.get("Source", "N/A")
                            description = (
                                param.get("Description", "")[:50] + "..."
                                if len(param.get("Description", "")) > 50
                                else param.get("Description", "")
                            )

                            result += f"â€¢ {name}: {value} (Source: {source})\n"
                            if description and filter_type == "all":
                                result += f"  â””â”€ {description}\n"

                # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°
                if uncategorized_params:
                    result += f"\nğŸ”§ ê¸°íƒ€ íŒŒë¼ë¯¸í„°:\n"
                    for param in uncategorized_params:
                        name = param["ParameterName"]
                        value = param.get("ParameterValue", "N/A")
                        source = param.get("Source", "N/A")
                        description = (
                            param.get("Description", "")[:50] + "..."
                            if len(param.get("Description", "")) > 50
                            else param.get("Description", "")
                        )

                        result += f"â€¢ {name}: {value} (Source: {source})\n"
                        if description and filter_type == "all":
                            result += f"  â””â”€ {description}\n"
            else:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œ
                for param in filtered_params:
                    name = param["ParameterName"]
                    value = param.get("ParameterValue", "N/A")
                    source = param.get("Source", "N/A")
                    description = (
                        param.get("Description", "")[:50] + "..."
                        if len(param.get("Description", "")) > 50
                        else param.get("Description", "")
                    )

                    result += f"â€¢ {name}: {value} (Source: {source})\n"
                    if description and filter_type == "all":
                        result += f"  â””â”€ {description}\n"

            return result.rstrip()

        except Exception as e:
            return f"âŒ {level} íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_default_aurora_parameters(
        self, parameter_group_name: str, region: str
    ) -> str:
        """ê¸°ë³¸ Aurora íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´ ì¡°íšŒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ë“¤ ì¡°íšŒ
            param_groups_response = rds_client.describe_db_cluster_parameter_groups()

            default_group = None
            for group in param_groups_response["DBClusterParameterGroups"]:
                if "default.aurora-mysql" in group["DBClusterParameterGroupName"]:
                    default_group = group
                    break

            if not default_group:
                return (
                    f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parameter_group_name}"
                )

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë“¤ ì¡°íšŒ
            parameters_response = rds_client.describe_db_cluster_parameters(
                DBClusterParameterGroupName=default_group["DBClusterParameterGroupName"]
            )

            parameters = parameters_response["Parameters"]

            result = f"""ğŸ“Š Aurora MySQL ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì •ë³´

ğŸ”§ **íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´:**
- ê·¸ë£¹ëª…: {default_group['DBClusterParameterGroupName']}
- íŒ¨ë°€ë¦¬: {default_group['DBParameterGroupFamily']}
- ì„¤ëª…: {default_group['Description']}

ğŸ“‹ **ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì¼ë¶€):**"""

            # ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ë§Œ í‘œì‹œ
            important_params = [
                "innodb_buffer_pool_size",
                "max_connections",
                "innodb_log_file_size",
                "query_cache_size",
                "tmp_table_size",
                "max_heap_table_size",
            ]

            for param in parameters[:20]:  # ì²˜ìŒ 20ê°œë§Œ
                param_name = param["ParameterName"]
                if param_name in important_params:
                    value = param.get("ParameterValue", "N/A")
                    result += f"\nâ€¢ {param_name}: {value}"

            return result

        except Exception as e:
            return f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def create_validation_plan(
        self, filename: str, database_secret: Optional[str] = None
    ) -> Dict[str, Any]:
        """DDL ê²€ì¦ ì‹¤í–‰ ê³„íš ìƒì„±"""
        try:
            # SQL íŒŒì¼ ì¡´ì¬ í™•ì¸
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return {
                    "operation": "validate_sql_file",
                    "filename": filename,
                    "database_secret": database_secret,
                    "status": "error",
                    "error": f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}",
                    "steps": [],
                }

            # DDL ë‚´ìš© ë¯¸ë¦¬ ì½ê¸°
            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            sql_type = self.extract_ddl_type(ddl_content)

            # ê²€ì¦ ë‹¨ê³„ ì •ì˜
            steps = [
                {
                    "step": 1,
                    "name": "ë¬¸ë²• ê²€ì¦",
                    "description": "SQL ê¸°ë³¸ ë¬¸ë²• ë° êµ¬ì¡° ê²€ì¦ (DDL/SELECT í¬í•¨)",
                    "details": [
                        "ì„¸ë¯¸ì½œë¡  ëˆ„ë½ í™•ì¸",
                        "SQL êµ¬ë¬¸ êµ¬ì¡° ê²€ì¦",
                        "SELECT ì¿¼ë¦¬ ê¸°ë³¸ êµ¬ì¡° í™•ì¸",
                        "Claude AIë¥¼ í†µí•œ ê³ ê¸‰ ë¬¸ë²• ê²€ì¦",
                    ],
                },
                {
                    "step": 2,
                    "name": "í‘œì¤€ ê·œì¹™ ê²€ì¦",
                    "description": "ìŠ¤í‚¤ë§ˆ ëª…ëª… ê·œì¹™ ë° í‘œì¤€ ì¤€ìˆ˜ í™•ì¸",
                    "details": [
                        "í…Œì´ë¸”/ì»¬ëŸ¼ ëª…ëª… ê·œì¹™ ê²€ì¦",
                        "ë°ì´í„° íƒ€ì… í‘œì¤€ ì¤€ìˆ˜ í™•ì¸",
                    ],
                },
            ]

            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€ ë‹¨ê³„
            if database_secret:
                steps.extend(
                    [
                        {
                            "step": 3,
                            "name": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                            "description": f"'{database_secret}' ì‹œí¬ë¦¿ìœ¼ë¡œ DB ì—°ê²° í™•ì¸",
                            "details": [
                                "AWS Secrets Managerì—ì„œ ì—°ê²° ì •ë³´ ì¡°íšŒ",
                                "SSH í„°ë„ì„ í†µí•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                                "ì—°ê²° ìƒíƒœ ë° ê¶Œí•œ í™•ì¸",
                            ],
                        },
                        {
                            "step": 4,
                            "name": "ìŠ¤í‚¤ë§ˆ ê²€ì¦",
                            "description": "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ SQL í˜¸í™˜ì„± ê²€ì¦",
                            "details": [
                                f"{sql_type} ì‘ì—… ëŒ€ìƒ í™•ì¸",
                                "í…Œì´ë¸”/ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦",
                                "ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± í™•ì¸",
                                "SELECT ì¿¼ë¦¬ì˜ ê²½ìš° í…Œì´ë¸”/ì»¬ëŸ¼ ì°¸ì¡° ê²€ì¦",
                            ],
                        },
                        {
                            "step": 5,
                            "name": "ì œì•½ì¡°ê±´ ê²€ì¦",
                            "description": "ì™¸ë˜í‚¤, ì¸ë±ìŠ¤ ë“± ì œì•½ì¡°ê±´ ê²€ì¦",
                            "details": [
                                "ì™¸ë˜í‚¤ ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ í™•ì¸",
                                "ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€ ê²€ì¦",
                                "ì œì•½ì¡°ê±´ ì¶©ëŒ ê²€ì‚¬",
                            ],
                        },
                    ]
                )

            steps.append(
                {
                    "step": len(steps) + 1,
                    "name": "ìµœì¢… ë³´ê³ ì„œ ìƒì„±",
                    "description": "HTML í˜•ì‹ì˜ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œ ìƒì„±",
                    "details": [
                        "ê²€ì¦ ê²°ê³¼ ì¢…í•©",
                        "ë¬¸ì œì  ë° ê¶Œì¥ì‚¬í•­ ì •ë¦¬",
                        "output ë””ë ‰í† ë¦¬ì— HTML ë³´ê³ ì„œ ì €ì¥",
                    ],
                }
            )

            return {
                "operation": "validate_sql_file",
                "filename": filename,
                "database_secret": database_secret,
                "ddl_type": ddl_type,
                "ddl_content": ddl_content,  # ì‹¤ì œ DDL ë‚´ìš© í¬í•¨
                "ddl_preview": (
                    ddl_content[:200] + "..." if len(ddl_content) > 200 else ddl_content
                ),
                "steps": steps,
                "created_at": datetime.now().isoformat(),
                "status": "created",
            }

        except Exception as e:
            return {
                "operation": "validate_sql_file",
                "filename": filename,
                "database_secret": database_secret,
                "status": "error",
                "error": f"ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "steps": [],
            }

    def _format_validation_plan_display(self, plan: Dict[str, Any]) -> str:
        """ê²€ì¦ ê³„íš í‘œì‹œ í˜•ì‹ ìƒì„±"""
        if plan["status"] == "error":
            return f"âŒ **ê³„íš ìƒì„± ì‹¤íŒ¨:** {plan['error']}"

        result = f"""ğŸ¯ **ê²€ì¦ ëŒ€ìƒ:** {plan['filename']}
ğŸ“… **ê³„íš ìƒì„± ì‹œê°„:** {plan['created_at']}
ğŸ”§ **SQL íƒ€ì…:** {plan['sql_type']}
ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤:** {plan['database_secret'] or 'ì—°ê²° ì—†ìŒ (ê¸°ë³¸ ê²€ì¦ë§Œ)'}

ğŸ“ **SQL ë¯¸ë¦¬ë³´ê¸°:**
```sql
{plan['ddl_preview']}
```

ğŸ”„ **ê²€ì¦ ë‹¨ê³„ ({len(plan['steps'])}ë‹¨ê³„):**"""

        for step in plan["steps"]:
            result += f"\n\n   **{step['step']}. {step['name']}**"
            result += f"\n   â””â”€ {step['description']}"

            if step.get("details"):
                for detail in step["details"]:
                    result += f"\n      â€¢ {detail}"

        # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ë° ì£¼ì˜ì‚¬í•­
        estimated_time = "30ì´ˆ ~ 1ë¶„" if plan["database_secret"] else "10 ~ 20ì´ˆ"
        result += f"\n\nâ±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** {estimated_time}"

        if plan["database_secret"]:
            result += f"\nâš ï¸ **ì£¼ì˜ì‚¬í•­:** ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."

        return result

    def _create_validate_all_sql_plan(self, operation: str, **kwargs) -> Dict[str, Any]:
        """validate_all_sql ì‘ì—… ê³„íš ìƒì„±"""
        if operation == "validate_all_sql":
            database_secret = kwargs.get("database_secret")
            tool_name = "validate_all_sql"

            plan_steps = [
                {
                    "step": 1,
                    "action": "SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
                    "target": "sql ë””ë ‰í† ë¦¬",
                    "tool": "list_sql_files",
                },
                {
                    "step": 2,
                    "action": "íŒŒì¼ ê°œìˆ˜ í™•ì¸",
                    "target": "ìµœëŒ€ 5ê°œ ì œí•œ",
                    "tool": "internal_check",
                },
                {
                    "step": 3,
                    "action": "ê° íŒŒì¼ ìˆœì°¨ ê²€ì¦",
                    "target": "ê°œë³„ íŒŒì¼",
                    "tool": "validate_sql_file",
                },
                {
                    "step": 4,
                    "action": "ì¢…í•© ê²°ê³¼ ìƒì„±",
                    "target": "ì „ì²´ ìš”ì•½",
                    "tool": "fs_write",
                },
            ]

        elif operation == "analyze_current_schema":
            database_secret = kwargs.get("database_secret")
            tool_name = "analyze_current_schema"

            plan_steps = [
                {
                    "step": 1,
                    "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                    "target": database_secret,
                    "tool": "test_database_connection",
                },
                {
                    "step": 2,
                    "action": "í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ",
                    "target": "information_schema",
                    "tool": "mysql_query",
                },
                {
                    "step": 3,
                    "action": "ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 4,
                    "action": "ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 5,
                    "action": "ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 6,
                    "action": "ìŠ¤í‚¤ë§ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±",
                    "target": "ì¢…í•© ì •ë³´",
                    "tool": "internal_analysis",
                },
            ]

        else:
            plan_steps = [
                {
                    "step": 1,
                    "action": f"{operation} ì‹¤í–‰",
                    "target": "ê¸°ë³¸ ë™ì‘",
                    "tool": operation,
                }
            ]

        plan = {
            "operation": operation,
            "tool_name": tool_name,
            "parameters": kwargs,
            "steps": plan_steps,
            "created_at": datetime.now().isoformat(),
            "status": "created",
        }

        self.current_plan = plan
        return plan

    async def execute_with_auto_plan(self, operation: str, **kwargs) -> str:
        """ìë™ ì‹¤í–‰ ê³„íš ìƒì„± ë° í‘œì‹œ í›„ í™•ì¸"""
        try:
            # 1. ì‹¤í–‰ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)

            # ì—ëŸ¬ ì²´í¬
            if "error" in plan:
                return f"âŒ ì‹¤í–‰ ê³„íš ìƒì„± ì‹¤íŒ¨: {plan['error']}"

            plan_display = self._format_plan_display(plan)

            # 2. ì‹¤í–‰ ê³„íš í‘œì‹œ ë° í™•ì¸ ìš”ì²­
            confirmation_message = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** 
   â€¢ 'y' ë˜ëŠ” 'yes': ì‹¤í–‰ ì§„í–‰
   â€¢ 'n' ë˜ëŠ” 'no': ì‹¤í–‰ ì·¨ì†Œ

ğŸ’¡ **ì°¸ê³ :** confirm_and_execute ë„êµ¬ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

            return confirmation_message

        except Exception as e:
            return f"âŒ ì‹¤í–‰ ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def execute_with_plan(self, operation: str, **kwargs) -> str:
        """ê³„íšì— ë”°ë¥¸ ì‹¤í–‰"""
        # ê¸°ì¡´ ê³„íšì´ ìˆëŠ”ì§€ í™•ì¸
        if self.current_plan and self.current_plan["operation"] == operation:
            plan = self.current_plan
            plan_display = self._format_plan_display(plan)

            # ì‚¬ìš©ì í™•ì¸ ìš”ì²­
            confirmation = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)
"""
            return confirmation
        else:
            # ìƒˆ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)
            plan_display = self._format_plan_display(plan)

            return f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)

ğŸ’¡ **ì°¸ê³ :** 'y' ë˜ëŠ” 'yes'ë¡œ ì‘ë‹µí•˜ë©´ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.
"""

    async def create_execution_plan(self, operation: str, **kwargs) -> Dict[str, Any]:
        """ì‘ì—… ì‹¤í–‰ ê³„íš ìƒì„±"""
        try:
            logger.info(f"Creating execution plan for operation: {operation}")
            logger.info(f"Kwargs: {kwargs}")

            if operation == "validate_sql_file":
                filename = kwargs.get("filename", "")
                database_secret = kwargs.get("database_secret", "")

                # SQL íŒŒì¼ ì½ê¸°
                sql_file_path = SQL_DIR / filename
                if not sql_file_path.exists():
                    return {"error": f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"}

                with open(sql_file_path, "r", encoding="utf-8") as f:
                    ddl_content = f.read()

                ddl_type = self.extract_ddl_type(ddl_content)

                # DDL ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)
                preview = (
                    ddl_content.strip()[:100] + "..."
                    if len(ddl_content.strip()) > 100
                    else ddl_content.strip()
                )

                plan = {
                    "operation": operation,
                    "filename": filename,
                    "database_secret": database_secret,
                    "sql_type": sql_type,
                    "preview": preview,
                    "steps": [
                        "ë¬¸ë²• ê²€ì¦",
                        "í‘œì¤€ ê·œì¹™ ê²€ì¦",
                        "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                        "ìŠ¤í‚¤ë§ˆ ê²€ì¦",
                        "ì œì•½ì¡°ê±´ ê²€ì¦",
                        "ìµœì¢… ë³´ê³ ì„œ ìƒì„±",
                    ],
                    "status": "created",
                }

                return plan

            elif operation == "test_database_connection":
                database_secret = kwargs.get("database_secret", "")

                plan = {
                    "operation": operation,
                    "database_secret": database_secret,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "tool_name": "test_database_connection",
                    "steps": [
                        {
                            "step": 1,
                            "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                            "target": database_secret,
                            "tool": "test_database_connection",
                        }
                    ],
                    "status": "created",
                    "parameters": kwargs,
                }

                return plan

            elif operation == "get_schema_summary":
                database_secret = kwargs.get("database_secret", "")

                plan = {
                    "operation": operation,
                    "database_secret": database_secret,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "tool_name": "get_schema_summary",
                    "steps": [
                        {
                            "step": 1,
                            "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                            "target": database_secret,
                            "tool": "get_schema_summary",
                        },
                        {
                            "step": 2,
                            "action": "ìŠ¤í‚¤ë§ˆ ì •ë³´ ìˆ˜ì§‘",
                            "target": "ì „ì²´ ìŠ¤í‚¤ë§ˆ",
                            "tool": "get_schema_summary",
                        },
                    ],
                    "status": "created",
                    "parameters": kwargs,
                }

                return plan

            elif operation == "validate_all_sql":
                database_secret = kwargs.get("database_secret", "")

                plan = {
                    "operation": operation,
                    "database_secret": database_secret,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "tool_name": "validate_all_sql",
                    "steps": [
                        {
                            "step": 1,
                            "action": "SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
                            "target": "sql ë””ë ‰í† ë¦¬",
                            "tool": "list_sql_files",
                        },
                        {
                            "step": 2,
                            "action": "ê° íŒŒì¼ ìˆœì°¨ ê²€ì¦",
                            "target": "ìµœëŒ€ 5ê°œ íŒŒì¼",
                            "tool": "validate_sql_file",
                        },
                        {
                            "step": 3,
                            "action": "í†µí•© ë³´ê³ ì„œ ìƒì„±",
                            "target": "HTML ë³´ê³ ì„œ",
                            "tool": "generate_consolidated_html_report_with_links",
                        },
                    ],
                    "status": "created",
                    "parameters": kwargs,
                }

                logger.info(f"Created plan: {plan}")
                return plan

            logger.warning(f"Unsupported operation: {operation}")
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì…ë‹ˆë‹¤: {operation}"}

        except Exception as e:
            logger.error(f"Error creating execution plan: {str(e)}")
            return {"error": f"ì‹¤í–‰ ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}"}

    def _format_plan_display(self, plan: Dict[str, Any]) -> str:
        """ê³„íš í‘œì‹œ í˜•ì‹ ìƒì„±"""
        result = f"""ğŸ¯ **ì‘ì—…:** {plan['operation']}
ğŸ“… **ìƒì„± ì‹œê°„:** {plan['created_at']}
ğŸ”§ **ë§¤ì¹­ íˆ´:** {plan.get('tool_name', plan['operation'])}

ğŸ“ **ì‹¤í–‰ ë‹¨ê³„:**"""

        for step in plan["steps"]:
            tool_info = f" [{step.get('tool', 'internal')}]" if step.get("tool") else ""
            result += (
                f"\n   {step['step']}. {step['action']} â†’ {step['target']}{tool_info}"
            )

        if plan["parameters"]:
            result += f"\n\nâš™ï¸ **ë§¤ê°œë³€ìˆ˜:**"
            for key, value in plan["parameters"].items():
                if value:
                    result += f"\n   â€¢ {key}: {value}"

        return result

    async def confirm_and_execute(self, confirmation: str) -> str:
        """í™•ì¸ í›„ ì‹¤í–‰"""
        # ë¡œì»¬ ê²€ì¦ ìš”ì²­ ì²˜ë¦¬
        if confirmation.lower() in ["local", "2", "ë¡œì»¬"]:
            if hasattr(self, "pending_validation"):
                ddl_content = self.pending_validation["ddl_content"]
                filename = self.pending_validation["filename"]
                result = await self.execute_local_validation_only(ddl_content, filename)
                delattr(self, "pending_validation")
                return result
            else:
                return "âŒ ë¡œì»¬ ê²€ì¦í•  íŒŒì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        if not self.current_plan:
            return "âŒ ì‹¤í–‰í•  ê³„íšì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‘ì—…ì„ ìš”ì²­í•´ì£¼ì„¸ìš”."

        if confirmation.lower() not in ["y", "yes", "ì˜ˆ", "ã…‡"]:
            self.current_plan = None
            return "âŒ ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."

        # ê³„íšì— ë”°ë¼ ì‹¤ì œ ì‹¤í–‰
        plan = self.current_plan
        operation = plan["operation"]

        try:
            result = f"ğŸš€ **ì‹¤í–‰ ì‹œì‘:** {operation}\n\n"

            # DDL ê²€ì¦ ì‹¤í–‰
            if operation == "validate_sql_file":
                if plan["status"] == "error":
                    result += f"âŒ ê³„íš ì˜¤ë¥˜: {plan['error']}"
                else:
                    validation_result = await self.execute_validation_workflow(
                        plan.get("ddl_content", ""),
                        plan.get("database_secret"),
                        plan["filename"],
                    )
                    result += validation_result

            # ê¸°ì¡´ ë‹¤ë¥¸ ì‘ì—…ë“¤...
            elif operation == "test_database_connection":
                connection_result = await self.test_connection_only(
                    plan["parameters"]["database_secret"]
                )
                result += connection_result

            elif operation == "validate_all_sql":
                validation_result = await self.validate_all_sql_files(
                    plan["parameters"].get("database_secret")
                )
                result += validation_result

            elif operation == "analyze_current_schema":
                schema_result = await self.analyze_current_schema(
                    plan["parameters"]["database_secret"]
                )
                if schema_result["success"]:
                    schema = schema_result["schema_analysis"]
                    result += f"""âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ (DB: {schema['current_database']})

ğŸ“Š **ë¶„ì„ ê²°ê³¼:**
- ì´ í…Œì´ë¸” ìˆ˜: {len(schema['tables'])}ê°œ

ğŸ“‹ **í…Œì´ë¸” ìƒì„¸:**"""
                    for table_name, table_info in schema["tables"].items():
                        result += f"""
ğŸ”¹ **{table_name}** ({table_info['engine']})
   - ì»¬ëŸ¼: {len(table_info['columns'])}ê°œ
   - ì¸ë±ìŠ¤: {len(table_info['indexes'])}ê°œ  
   - ì™¸ë˜í‚¤: {len(table_info['foreign_keys'])}ê°œ
   - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""
                        if table_info["comment"]:
                            result += f"\n   - ì„¤ëª…: {table_info['comment']}"

                        # ì»¬ëŸ¼ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ“‹ **ì»¬ëŸ¼ ì •ë³´:**"
                        if table_info["columns"]:
                            for col_name, col_info in table_info["columns"].items():
                                data_type = col_info["data_type"]
                                if col_info["max_length"]:
                                    data_type += f"({col_info['max_length']})"
                                elif col_info["precision"] and col_info["scale"]:
                                    data_type += (
                                        f"({col_info['precision']},{col_info['scale']})"
                                    )
                                elif col_info["precision"]:
                                    data_type += f"({col_info['precision']})"

                                nullable = (
                                    "NULL"
                                    if col_info["is_nullable"] == "YES"
                                    else "NOT NULL"
                                )
                                key_info = (
                                    f" [{col_info['key']}]" if col_info["key"] else ""
                                )
                                extra_info = (
                                    f" {col_info['extra']}" if col_info["extra"] else ""
                                )
                                default_info = (
                                    f" DEFAULT {col_info['default']}"
                                    if col_info["default"]
                                    else ""
                                )

                                result += f"\n      â€¢ {col_name}: {data_type} {nullable}{key_info}{extra_info}{default_info}"
                                if col_info["comment"]:
                                    result += f" -- {col_info['comment']}"
                        else:
                            result += f"\n      ì»¬ëŸ¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        # ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ” **ì¸ë±ìŠ¤ ì •ë³´:**"
                        if table_info["indexes"]:
                            for idx in table_info["indexes"]:
                                unique_info = "UNIQUE " if idx["unique"] else ""
                                columns_str = ", ".join(idx["columns"])
                                result += f"\n      â€¢ {unique_info}{idx['name']} ({columns_str}) [{idx['type']}]"
                                if idx["comment"]:
                                    result += f" -- {idx['comment']}"
                        else:
                            result += f"\n      ì¸ë±ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # ì™¸ë˜í‚¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ”— **ì™¸ë˜í‚¤ ì •ë³´:**"
                        if table_info["foreign_keys"]:
                            for fk in table_info["foreign_keys"]:
                                result += f"\n      â€¢ {fk['constraint_name']}: {fk['column']} â†’ {fk['referenced_table']}.{fk['referenced_column']}"
                                result += f" (UPDATE: {fk['update_rule']}, DELETE: {fk['delete_rule']})"
                        else:
                            result += f"\n      ì™¸ë˜í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # í…Œì´ë¸” í†µê³„ ì •ë³´ ì¶”ê°€
                        if table_info["data_length"] or table_info["index_length"]:
                            result += f"\n\n   ğŸ“Š **í…Œì´ë¸” í†µê³„:**"
                            if table_info["data_length"]:
                                data_size = (
                                    table_info["data_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ë°ì´í„° í¬ê¸°: {data_size:.2f} MB"
                            if table_info["index_length"]:
                                index_size = (
                                    table_info["index_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ì¸ë±ìŠ¤ í¬ê¸°: {index_size:.2f} MB"
                else:
                    result += f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            else:
                result += f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—…: {operation}"

            # ê³„íš ì™„ë£Œ
            self.current_plan = None
            result += f"\n\nâœ… **ì‹¤í–‰ ì™„ë£Œ:** {operation}"

            return result

        except Exception as e:
            self.current_plan = None
            return f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            schema_result = await self.analyze_current_schema(database_secret)
            if not schema_result["success"]:
                return f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            schema = schema_result["schema_analysis"]

            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {schema['current_database']})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(schema['tables'])}ê°œ):"""

            for table_name, table_info in schema["tables"].items():
                column_count = len(table_info["columns"])
                index_count = len(table_info["indexes"])
                fk_count = len(table_info["foreign_keys"])

                summary += f"""
  ğŸ”¹ **{table_name}** ({table_info['engine']})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ, ì™¸ë˜í‚¤: {fk_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""

                if table_info["comment"]:
                    summary += f"\n     - ì„¤ëª…: {table_info['comment']}"

            return summary

        except Exception as e:
            return f"ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_all_sql_files(
        self, database_secret: Optional[str] = None
    ) -> str:
        """ëª¨ë“  SQL íŒŒì¼ ê²€ì¦ (ìµœëŒ€ 5ê°œ) - í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # ìµœëŒ€ 5ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
            files_to_process = sql_files[:5]
            if len(sql_files) > 5:
                logger.warning(
                    f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ìˆì§€ë§Œ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
                )

            validation_results = []
            summary_results = []
            detailed_reports = []

            for sql_file in files_to_process:
                try:
                    # ê°œë³„ íŒŒì¼ ê²€ì¦ì„ execute_validation_workflowë¡œ ì‹¤í–‰ (ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ë°©ì‹)
                    with open(sql_file, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                    result = await self.execute_validation_workflow(
                        ddl_content, database_secret, sql_file.name
                    )

                    # ê²°ê³¼ì—ì„œ ìƒíƒœ íŒŒì•… ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    syntax_valid = "ë¬¸ë²• ê²€ì¦: âœ… í†µê³¼" in result
                    db_connected = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ… ì„±ê³µ" in result
                    schema_valid = "ìŠ¤í‚¤ë§ˆ ê²€ì¦: âœ… í†µê³¼" in result
                    constraint_valid = "ì œì•½ì¡°ê±´ ê²€ì¦: âœ… í†µê³¼" in result
                    ai_valid = "Claude AI ê²€ì¦: âœ… í†µê³¼" in result

                    # ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ
                    issue_match = re.search(r"ë°œê²¬ëœ ë¬¸ì œ: (\d+)ê°œ", result)
                    issue_count = int(issue_match.group(1)) if issue_match else 0

                    # ë¬¸ë²• ì˜¤ë¥˜ ì²´í¬
                    syntax_error_match = re.search(
                        r"ë¬¸ë²• ì˜¤ë¥˜ë¡œ ì¸í•œ ê²€ì¦ ì‹¤íŒ¨: (\d+)ê°œ", result
                    )
                    if syntax_error_match:
                        issue_count = int(syntax_error_match.group(1))
                        syntax_valid = False
                        db_connected = False

                    if issue_count == 0 and "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤" in result:
                        status = "PASS"
                        issues = []
                    else:
                        status = "FAIL"
                        issues = [f"ê²€ì¦ ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ ë°œê²¬)"]

                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": ddl_content,
                            "ddl_type": self.extract_ddl_type(ddl_content),
                            "status": status,
                            "issues": issues,
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": syntax_valid,
                            "db_connected": db_connected,
                            "schema_valid": schema_valid,
                            "constraint_valid": constraint_valid,
                            "ai_valid": ai_valid,
                            "issue_count": issue_count,
                            "full_result": result,
                        }
                    )

                    # ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ê¸°ì¡´ ê°œë³„ ê²€ì¦ì—ì„œ ìƒì„±ëœ íŒŒì¼)
                    report_match = re.search(
                        r"ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: (.+\.html)", result
                    )
                    if report_match:
                        detailed_reports.append(
                            {
                                "filename": sql_file.name,
                                "report_path": report_match.group(1),
                            }
                        )

                    # ìš”ì•½ ê²°ê³¼
                    if status == "PASS":
                        summary_results.append(f"âœ… **{sql_file.name}**: í†µê³¼")
                    else:
                        summary_results.append(
                            f"âŒ **{sql_file.name}**: ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ)"
                        )

                except Exception as e:
                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": "",
                            "ddl_type": "ERROR",
                            "status": "ERROR",
                            "issues": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": False,
                            "db_connected": False,
                            "schema_valid": False,
                            "constraint_valid": False,
                            "ai_valid": False,
                            "issue_count": 1,
                            "full_result": f"ì˜¤ë¥˜: {str(e)}",
                        }
                    )
                    summary_results.append(f"âŒ **{sql_file.name}**: ì˜¤ë¥˜ - {str(e)}")

            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„± (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ í¬í•¨)
            consolidated_report_path = (
                await self.generate_consolidated_html_report_with_links(
                    validation_results, detailed_reports, database_secret
                )
            )

            # ìš”ì•½ í†µê³„
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            summary = f"""ğŸ“Š ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì™„ë£Œ

ğŸ“‹ ìš”ì•½:
â€¢ ì´ íŒŒì¼: {total_files}ê°œ
â€¢ í†µê³¼: {passed_files}ê°œ ({passed_files/total_files*100:.1f}%)
â€¢ ì‹¤íŒ¨: {failed_files}ê°œ ({failed_files/total_files*100:.1f}%)

ğŸ“„ ì¢…í•© ë³´ê³ ì„œ: {consolidated_report_path}

ğŸ“Š ê°œë³„ ê²°ê³¼:
{chr(10).join(summary_results)}"""

            if len(sql_files) > 5:
                summary += (
                    f"\n\nâš ï¸ ì „ì²´ {len(sql_files)}ê°œ íŒŒì¼ ì¤‘ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                )

            return summary

        except Exception as e:
            return f"ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_html_report_with_links(
        self,
        validation_results: List[Dict],
        detailed_reports: List[Dict],
        database_secret: str,
    ) -> str:
        """í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ê°€ í¬í•¨ëœ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files
            syntax_pass = sum(
                1 for r in validation_results if r.get("syntax_valid", False)
            )
            db_pass = sum(1 for r in validation_results if r.get("db_connected", False))
            schema_pass = sum(
                1 for r in validation_results if r.get("schema_valid", False)
            )
            constraint_pass = sum(
                1 for r in validation_results if r.get("constraint_valid", False)
            )
            ai_pass = sum(1 for r in validation_results if r.get("ai_valid", False))

            # ìƒì„¸ë³´ê³ ì„œ ë§í¬ ë§¤í•‘
            report_links = {
                report["filename"]: report["report_path"] for report in detailed_reports
            }

            # í…Œì´ë¸” í–‰ ìƒì„±
            table_rows = ""
            for i, result in enumerate(validation_results, 1):
                status_class = "success" if result["status"] == "PASS" else "error"

                # ê° ê²€ì¦ í•­ëª© ìƒíƒœ
                syntax_status = (
                    "âœ… í†µê³¼" if result.get("syntax_valid", False) else "âŒ ì‹¤íŒ¨"
                )
                db_status = (
                    "âœ… ì„±ê³µ" if result.get("db_connected", False) else "âŒ ì‹¤íŒ¨"
                )
                schema_status = (
                    "âœ… í†µê³¼" if result.get("schema_valid", False) else "âŒ ì‹¤íŒ¨"
                )
                constraint_status = (
                    "âœ… í†µê³¼" if result.get("constraint_valid", False) else "âŒ ì‹¤íŒ¨"
                )
                ai_status = "âœ… í†µê³¼" if result.get("ai_valid", False) else "âŒ ì‹¤íŒ¨"

                # ìƒì„¸ë³´ê³ ì„œ ë§í¬
                filename_cell = result["filename"]
                if result["filename"] in report_links:
                    detail_report_name = os.path.basename(
                        report_links[result["filename"]]
                    )
                    filename_cell = f'<a href="{detail_report_name}" target="_blank" class="detail-link">{result["filename"]}</a>'

                table_rows += f"""
                <tr class="{status_class}" onclick="window.open('{detail_report_name if result["filename"] in report_links else "#"}', '_blank')" style="cursor: pointer;">
                    <td>{filename_cell}</td>
                    <td class="status-cell">{syntax_status}</td>
                    <td class="status-cell">{db_status}</td>
                    <td class="status-cell">{schema_status}</td>
                    <td class="status-cell">{constraint_status}</td>
                    <td class="status-cell">{ai_status}</td>
                    <td class="issue-count">{result.get('issue_count', 0)}ê°œ</td>
                </tr>
                """

            # HTML ë³´ê³ ì„œ ìƒì„±
            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{database_secret or 'SQL'} ê²€ì¦ í†µí•© ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; border-left: 4px solid #3498db; padding-left: 15px; }}
        .summary {{ background: #ecf0f1; padding: 20px; border-radius: 8px; margin: 20px 0; }}
        .stats {{ display: flex; justify-content: space-around; margin: 20px 0; flex-wrap: wrap; }}
        .stat-box {{ text-align: center; padding: 15px; background: #fff; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); margin: 5px; min-width: 120px; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #3498db; }}
        .stat-label {{ color: #7f8c8d; margin-top: 5px; font-size: 0.9em; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #3498db; color: white; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        tr.success {{ background-color: #d5f4e6 !important; }}
        tr.error {{ background-color: #fadbd8 !important; }}
        tr:hover {{ background-color: #e8f4f8 !important; }}
        .status-cell {{ text-align: center; font-weight: bold; }}
        .issue-count {{ text-align: center; font-weight: bold; color: #e74c3c; }}
        .detail-link {{ color: #3498db; text-decoration: none; font-weight: bold; }}
        .detail-link:hover {{ text-decoration: underline; }}
        .timestamp {{ color: #7f8c8d; font-size: 0.9em; text-align: center; margin-top: 30px; }}
        .click-hint {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 10px; border-radius: 5px; margin: 10px 0; text-align: center; color: #856404; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ—„ï¸ {database_secret or 'SQL'} ê²€ì¦ í†µí•© ë³´ê³ ì„œ</h1>
        
        <div class="summary">
            <h2>ğŸ“Š ê²€ì¦ ìš”ì•½</h2>
            <div class="stats">
                <div class="stat-box">
                    <div class="stat-number">{total_files}</div>
                    <div class="stat-label">ì´ ê²€ì¦ íŒŒì¼</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{passed_files}</div>
                    <div class="stat-label">ì™„ì „ í†µê³¼</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{failed_files}</div>
                    <div class="stat-label">ì‹¤íŒ¨</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{syntax_pass}</div>
                    <div class="stat-label">ë¬¸ë²• í†µê³¼</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{db_pass}</div>
                    <div class="stat-label">DB ì—°ê²° ì„±ê³µ</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{schema_pass}</div>
                    <div class="stat-label">ìŠ¤í‚¤ë§ˆ í†µê³¼</div>
                </div>
            </div>
        </div>

        <div class="click-hint">
            ğŸ’¡ <strong>ì‚¬ìš©ë²•:</strong> ì•„ë˜ í…Œì´ë¸”ì˜ ê° í–‰ì„ í´ë¦­í•˜ë©´ í•´ë‹¹ íŒŒì¼ì˜ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        </div>

        <h2>ğŸ“‹ ìƒì„¸ ê²€ì¦ ê²°ê³¼</h2>
        
        <table>
            <thead>
                <tr>
                    <th>íŒŒì¼ëª…</th>
                    <th>ë¬¸ë²• ê²€ì¦</th>
                    <th>DB ì—°ê²°</th>
                    <th>ìŠ¤í‚¤ë§ˆ ê²€ì¦</th>
                    <th>ì œì•½ì¡°ê±´ ê²€ì¦</th>
                    <th>AI ê²€ì¦</th>
                    <th>ì´ ë¬¸ì œ ìˆ˜</th>
                </tr>
            </thead>
            <tbody>
                {table_rows}
            </tbody>
        </table>

        <h2>ğŸ“ˆ ê²€ì¦ í†µê³„</h2>
        
        <div class="summary">
            <ul>
                <li><strong>ì´ ê²€ì¦ íŒŒì¼:</strong> {total_files}ê°œ</li>
                <li><strong>ë¬¸ë²• ê²€ì¦ í†µê³¼:</strong> {syntax_pass}ê°œ ({syntax_pass/total_files*100:.1f}%)</li>
                <li><strong>ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ:</strong> {db_pass}ê°œ ({db_pass/total_files*100:.1f}%)</li>
                <li><strong>ìŠ¤í‚¤ë§ˆ ê²€ì¦ í†µê³¼:</strong> {schema_pass}ê°œ ({schema_pass/total_files*100:.1f}%)</li>
                <li><strong>ì œì•½ì¡°ê±´ ê²€ì¦ í†µê³¼:</strong> {constraint_pass}ê°œ ({constraint_pass/total_files*100:.1f}%)</li>
                <li><strong>AI ê²€ì¦ í†µê³¼:</strong> {ai_pass}ê°œ ({ai_pass/total_files*100:.1f}%)</li>
                <li><strong>ì™„ì „ í†µê³¼ íŒŒì¼:</strong> {passed_files}ê°œ ({passed_files/total_files*100:.1f}%)</li>
            </ul>
        </div>

        <h2>ğŸ¯ ê¶Œì¥ì‚¬í•­</h2>
        
        <div class="summary">
            <ul>
                <li><strong>ë¬¸ë²• ì˜¤ë¥˜ ìš°ì„  ìˆ˜ì •:</strong> ë¬¸ë²• ê²€ì¦ì— ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì„ ë¨¼ì € í•´ê²°í•˜ì„¸ìš”.</li>
                <li><strong>ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¬¸ì œ í•´ê²°:</strong> ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”/ì»¬ëŸ¼ ì°¸ì¡° ë¬¸ì œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.</li>
                <li><strong>ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì ìš©:</strong> AI ê²€ì¦ì—ì„œ ì œì•ˆí•˜ëŠ” ì„±ëŠ¥ ìµœì í™” ë° ë³´ì•ˆ ê¶Œê³ ì‚¬í•­ì„ ê²€í† í•˜ì„¸ìš”.</li>
                <li><strong>ì •ê¸°ì ì¸ ê²€ì¦:</strong> SQL íŒŒì¼ ë³€ê²½ ì‹œ ìë™í™”ëœ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ë„ì…ì„ ê²€í† í•˜ì„¸ìš”.</li>
            </ul>
        </div>

        <div class="timestamp">
            <p>ğŸ“… ë³´ê³ ì„œ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)</p>
            <p>ğŸ—„ï¸ ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤: {database_secret or 'N/A'}</p>
            <p>ğŸ”§ ê²€ì¦ ë„êµ¬: DB Assistant MCP Server v2.0</p>
        </div>
    </div>
</body>
</html>"""

            # íŒŒì¼ ì €ì¥
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_selected_sql_files(
        self, database_secret: str, sql_files: List[str]
    ) -> str:
        """ì„ íƒí•œ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  í†µí•©ë³´ê³ ì„œë¥¼ ìƒì„±"""
        try:
            if not sql_files:
                return "ê²€ì¦í•  SQL íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            # ìµœëŒ€ 10ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
            files_to_process = sql_files[:10]
            if len(sql_files) > 10:
                logger.warning(
                    f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ì§€ì •ë˜ì—ˆì§€ë§Œ ì²˜ìŒ 10ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
                )

            validation_results = []
            summary_results = []
            detailed_reports = []

            for filename in files_to_process:
                try:
                    sql_file_path = SQL_DIR / filename
                    if not sql_file_path.exists():
                        validation_results.append(
                            {
                                "filename": filename,
                                "ddl_content": "",
                                "ddl_type": "ERROR",
                                "status": "ERROR",
                                "issues": [f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"],
                                "warnings": [],
                                "db_connection_info": None,
                                "syntax_valid": False,
                                "db_connected": False,
                                "schema_valid": False,
                                "constraint_valid": False,
                                "ai_valid": False,
                                "issue_count": 1,
                                "full_result": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}",
                            }
                        )
                        summary_results.append(f"âŒ **{filename}**: íŒŒì¼ ì—†ìŒ")
                        continue

                    # íŒŒì¼ ë‚´ìš© ì½ê¸°
                    with open(sql_file_path, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                    result = await self.execute_validation_workflow(
                        ddl_content, database_secret, filename
                    )

                    # ê²°ê³¼ì—ì„œ ìƒíƒœ íŒŒì•… ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    syntax_valid = "ë¬¸ë²• ê²€ì¦: âœ… í†µê³¼" in result
                    db_connected = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ… ì„±ê³µ" in result
                    schema_valid = "ìŠ¤í‚¤ë§ˆ ê²€ì¦: âœ… í†µê³¼" in result
                    constraint_valid = "ì œì•½ì¡°ê±´ ê²€ì¦: âœ… í†µê³¼" in result
                    ai_valid = "Claude AI ê²€ì¦: âœ… í†µê³¼" in result

                    # ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ
                    issue_match = re.search(r"ë°œê²¬ëœ ë¬¸ì œ: (\d+)ê°œ", result)
                    issue_count = int(issue_match.group(1)) if issue_match else 0

                    # ë¬¸ë²• ì˜¤ë¥˜ ì²´í¬
                    syntax_error_match = re.search(
                        r"ë¬¸ë²• ì˜¤ë¥˜ë¡œ ì¸í•œ ê²€ì¦ ì‹¤íŒ¨: (\d+)ê°œ", result
                    )
                    if syntax_error_match:
                        issue_count = int(syntax_error_match.group(1))
                        syntax_valid = False
                        db_connected = False

                    if issue_count == 0 and "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤" in result:
                        status = "PASS"
                        issues = []
                    else:
                        status = "FAIL"
                        issues = [f"ê²€ì¦ ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ ë°œê²¬)"]

                    validation_results.append(
                        {
                            "filename": filename,
                            "ddl_content": ddl_content,
                            "ddl_type": self.extract_ddl_type(ddl_content),
                            "status": status,
                            "issues": issues,
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": syntax_valid,
                            "db_connected": db_connected,
                            "schema_valid": schema_valid,
                            "constraint_valid": constraint_valid,
                            "ai_valid": ai_valid,
                            "issue_count": issue_count,
                            "full_result": result,
                        }
                    )

                    # ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ê¸°ì¡´ ê°œë³„ ê²€ì¦ì—ì„œ ìƒì„±ëœ íŒŒì¼)
                    report_match = re.search(
                        r"ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: (.+\.html)", result
                    )
                    if report_match:
                        detailed_reports.append(
                            {"filename": filename, "report_path": report_match.group(1)}
                        )

                    # ìš”ì•½ ê²°ê³¼
                    if status == "PASS":
                        summary_results.append(f"âœ… **{filename}**: í†µê³¼")
                    else:
                        summary_results.append(
                            f"âŒ **{filename}**: ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ)"
                        )

                except Exception as e:
                    validation_results.append(
                        {
                            "filename": filename,
                            "ddl_content": "",
                            "ddl_type": "ERROR",
                            "status": "ERROR",
                            "issues": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": False,
                            "db_connected": False,
                            "schema_valid": False,
                            "constraint_valid": False,
                            "ai_valid": False,
                            "issue_count": 1,
                            "full_result": f"ì˜¤ë¥˜: {str(e)}",
                        }
                    )
                    summary_results.append(f"âŒ **{filename}**: ì˜¤ë¥˜ - {str(e)}")

            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„± (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ í¬í•¨)
            consolidated_report_path = (
                await self.generate_consolidated_html_report_with_links(
                    validation_results, detailed_reports, database_secret
                )
            )

            # ìš”ì•½ í†µê³„
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            summary = f"""ğŸ“Š ì„ íƒí•œ SQL íŒŒì¼ ê²€ì¦ ì™„ë£Œ

ğŸ“‹ ìš”ì•½:
â€¢ ì´ íŒŒì¼: {total_files}ê°œ
â€¢ í†µê³¼: {passed_files}ê°œ ({passed_files/total_files*100:.1f}%)
â€¢ ì‹¤íŒ¨: {failed_files}ê°œ ({failed_files/total_files*100:.1f}%)

ğŸ“„ ì¢…í•© ë³´ê³ ì„œ: {consolidated_report_path}

ğŸ“Š ê°œë³„ ê²°ê³¼:
{chr(10).join(summary_results)}"""

            if len(sql_files) > 10:
                summary += (
                    f"\n\nâš ï¸ ì „ì²´ {len(sql_files)}ê°œ íŒŒì¼ ì¤‘ ì²˜ìŒ 10ê°œë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                )

            return summary

        except Exception as e:
            return f"ì„ íƒí•œ SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def validate_multiple_sql_direct(
        self, database_secret: str, file_count: int = 10
    ) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì„ ì§ì ‘ ê²€ì¦í•˜ê³  í†µí•©ë³´ê³ ì„œë¥¼ ìƒì„± (ê³„íš ì—†ì´ ë°”ë¡œ ì‹¤í–‰)"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ íŒŒì¼ ì²˜ë¦¬ (ìµœëŒ€ 15ê°œ)
            file_count = min(file_count, 15)
            files_to_process = sql_files[:file_count]

            if len(sql_files) > file_count:
                logger.info(
                    f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ìˆì§€ë§Œ ì²˜ìŒ {file_count}ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
                )

            validation_results = []
            summary_results = []
            detailed_reports = []

            for sql_file in files_to_process:
                try:
                    # íŒŒì¼ ë‚´ìš© ì½ê¸°
                    with open(sql_file, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                    result = await self.execute_validation_workflow(
                        ddl_content, database_secret, sql_file.name
                    )

                    # ê²°ê³¼ì—ì„œ ìƒíƒœ íŒŒì•… ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    syntax_valid = "ë¬¸ë²• ê²€ì¦: âœ… í†µê³¼" in result
                    db_connected = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ… ì„±ê³µ" in result
                    schema_valid = "ìŠ¤í‚¤ë§ˆ ê²€ì¦: âœ… í†µê³¼" in result
                    constraint_valid = "ì œì•½ì¡°ê±´ ê²€ì¦: âœ… í†µê³¼" in result
                    ai_valid = "Claude AI ê²€ì¦: âœ… í†µê³¼" in result

                    # ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ
                    issue_match = re.search(r"ë°œê²¬ëœ ë¬¸ì œ: (\d+)ê°œ", result)
                    issue_count = int(issue_match.group(1)) if issue_match else 0

                    # ë¬¸ë²• ì˜¤ë¥˜ ì²´í¬
                    syntax_error_match = re.search(
                        r"ë¬¸ë²• ì˜¤ë¥˜ë¡œ ì¸í•œ ê²€ì¦ ì‹¤íŒ¨: (\d+)ê°œ", result
                    )
                    if syntax_error_match:
                        issue_count = int(syntax_error_match.group(1))
                        syntax_valid = False
                        db_connected = False

                    if issue_count == 0 and "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤" in result:
                        status = "PASS"
                        issues = []
                    else:
                        status = "FAIL"
                        issues = [f"ê²€ì¦ ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ ë°œê²¬)"]

                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": ddl_content,
                            "ddl_type": self.extract_ddl_type(ddl_content),
                            "status": status,
                            "issues": issues,
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": syntax_valid,
                            "db_connected": db_connected,
                            "schema_valid": schema_valid,
                            "constraint_valid": constraint_valid,
                            "ai_valid": ai_valid,
                            "issue_count": issue_count,
                            "full_result": result,
                        }
                    )

                    # ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ê¸°ì¡´ ê°œë³„ ê²€ì¦ì—ì„œ ìƒì„±ëœ íŒŒì¼)
                    report_match = re.search(
                        r"ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: (.+\.html)", result
                    )
                    if report_match:
                        detailed_reports.append(
                            {
                                "filename": sql_file.name,
                                "report_path": report_match.group(1),
                            }
                        )

                    # ìš”ì•½ ê²°ê³¼
                    if status == "PASS":
                        summary_results.append(f"âœ… **{sql_file.name}**: í†µê³¼")
                    else:
                        summary_results.append(
                            f"âŒ **{sql_file.name}**: ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ)"
                        )

                except Exception as e:
                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": "",
                            "ddl_type": "ERROR",
                            "status": "ERROR",
                            "issues": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": False,
                            "db_connected": False,
                            "schema_valid": False,
                            "constraint_valid": False,
                            "ai_valid": False,
                            "issue_count": 1,
                            "full_result": f"ì˜¤ë¥˜: {str(e)}",
                        }
                    )
                    summary_results.append(f"âŒ **{sql_file.name}**: ì˜¤ë¥˜ - {str(e)}")

            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„± (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ í¬í•¨)
            consolidated_report_path = (
                await self.generate_consolidated_html_report_with_links(
                    validation_results, detailed_reports, database_secret
                )
            )

            # ìš”ì•½ í†µê³„
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            summary = f"""ğŸ“Š SQL íŒŒì¼ ê²€ì¦ ì™„ë£Œ ({database_secret})

ğŸ“‹ ìš”ì•½:
â€¢ ì´ íŒŒì¼: {total_files}ê°œ
â€¢ í†µê³¼: {passed_files}ê°œ ({passed_files/total_files*100:.1f}%)
â€¢ ì‹¤íŒ¨: {failed_files}ê°œ ({failed_files/total_files*100:.1f}%)

ğŸ“„ í†µí•© ë³´ê³ ì„œ: {consolidated_report_path}

ğŸ“Š ê°œë³„ ê²°ê³¼:
{chr(10).join(summary_results)}"""

            if len(sql_files) > file_count:
                summary += f"\n\nâš ï¸ ì „ì²´ {len(sql_files)}ê°œ íŒŒì¼ ì¤‘ ì²˜ìŒ {file_count}ê°œë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."

            return summary

        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_html_report(
        self, validation_results: List[Dict], database_secret: str
    ) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì˜ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            # íŒŒì¼ë³„ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_color = "#28a745" if result["status"] == "PASS" else "#dc3545"
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"

                # Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
                claude_issues = []
                other_issues = []

                for issue in result["issues"]:
                    if issue.startswith("Claude ê²€ì¦:"):
                        claude_issues.append(issue[12:].strip())
                    else:
                        other_issues.append(issue)

                # ê¸°íƒ€ ë¬¸ì œ ì„¹ì…˜
                other_issues_html = ""
                if other_issues:
                    other_issues_html = f"""
                    <div class="issues-subsection">
                        <h5>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h5>
                        <ul>
                            {''.join(f'<li>{issue}</li>' for issue in other_issues)}
                        </ul>
                    </div>
                    """

                # Claude ê²€ì¦ ê²°ê³¼ ì„¹ì…˜
                claude_section_html = ""
                if claude_issues:
                    claude_section_html = f"""
                    <div class="claude-subsection">
                        <h5>ğŸ¤– Claude AI ê²€ì¦</h5>
                        {''.join(f'<div class="claude-result-small"><pre>{claude_result}</pre></div>' for claude_result in claude_issues)}
                    </div>
                    """

                # ì„±ê³µ ì„¹ì…˜
                success_html = ""
                if not result["issues"]:
                    success_html = '<div class="success-message">âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</div>'

                file_sections += f"""
                <div class="file-section">
                    <div class="file-header">
                        <h3>{status_icon} {i}. {result['filename']}</h3>
                        <span class="status-badge" style="background-color: {status_color};">{result['status']}</span>
                    </div>
                    
                    <div class="file-details">
                        <div class="detail-item">
                            <strong>SQL íƒ€ì…:</strong> {result['ddl_type']}
                        </div>
                        <div class="detail-item">
                            <strong>ë¬¸ì œ ìˆ˜:</strong> {len(result['issues'])}ê°œ
                        </div>
                    </div>
                    
                    <div class="sql-code-small">
                        <h4>ğŸ“ SQL ë‚´ìš©</h4>
                        <pre>{result['ddl_content']}</pre>
                    </div>
                    
                    <div class="validation-result">
                        <h4>ğŸ“Š ê²€ì¦ ê²°ê³¼</h4>
                        <pre>{result.get('full_result', 'ê²°ê³¼ ì—†ìŒ')}</pre>
                    </div>
                    
                    {claude_section_html}
                    {other_issues_html}
                    {success_html}
                </div>
                """

            # HTML ë³´ê³ ì„œ ìƒì„±
            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© SQL ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .content {{
            padding: 30px;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 30px 0;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-header {{
            background: #f8f9fa;
            padding: 20px;
            border-bottom: 1px solid #e9ecef;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .file-header h3 {{
            margin: 0;
            color: #333;
        }}
        .status-badge {{
            padding: 6px 12px;
            border-radius: 15px;
            color: white;
            font-weight: bold;
            font-size: 0.9em;
        }}
        .file-details {{
            padding: 15px 20px;
            background: #fafafa;
            border-bottom: 1px solid #e9ecef;
        }}
        .detail-item {{
            display: inline-block;
            margin-right: 30px;
            color: #666;
        }}
        .sql-code-small {{
            padding: 20px;
        }}
        .sql-code-small h4, .validation-result h4 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .sql-code-small pre, .validation-result pre {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            max-height: 200px;
            overflow-y: auto;
        }}
        .validation-result {{
            padding: 20px;
            border-top: 1px solid #e9ecef;
        }}
        .claude-subsection {{
            padding: 15px 20px;
            background: #f8f9ff;
            border-top: 1px solid #e9ecef;
        }}
        .claude-subsection h5 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .claude-result-small {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            margin: 10px 0;
        }}
        .claude-result-small pre {{
            padding: 15px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 0.9em;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
            overflow-y: auto;
        }}
        .issues-subsection {{
            padding: 15px 20px;
            border-top: 1px solid #e9ecef;
        }}
        .issues-subsection h5 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .issues-subsection ul {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-subsection li {{
            margin: 5px 0;
            color: #dc3545;
        }}
        .success-message {{
            padding: 15px 20px;
            background: #d4edda;
            color: #155724;
            border-top: 1px solid #c3e6cb;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
            }}
            .file-header {{
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© SQL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="content">
            <div class="summary-stats">
                <div class="stat-item">
                    <div class="stat-number">{total_files}</div>
                    <div class="stat-label">ì´ íŒŒì¼ ìˆ˜</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                    <div class="stat-label">í†µê³¼</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                    <div class="stat-label">ì‹¤íŒ¨</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number">{passed_files/total_files*100:.1f}%</div>
                    <div class="stat-label">ì„±ê³µë¥ </div>
                </div>
            </div>
            
            <div class="database-info">
                <h2>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´</h2>
                <p><strong>ì‹œí¬ë¦¿:</strong> {database_secret or 'N/A'}</p>
            </div>
            
            {file_sections}
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def list_aurora_mysql_clusters(self, region: str = "ap-northeast-2") -> str:
        """í˜„ì¬ ë¦¬ì „ì˜ Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # Aurora MySQL í´ëŸ¬ìŠ¤í„° ì¡°íšŒ
            response = rds_client.describe_db_clusters()

            aurora_mysql_clusters = []
            for cluster in response["DBClusters"]:
                if (
                    cluster["Engine"] == "aurora-mysql"
                    and cluster["Status"] == "available"
                ):
                    aurora_mysql_clusters.append(
                        {
                            "identifier": cluster["DBClusterIdentifier"],
                            "engine_version": cluster["EngineVersion"],
                            "endpoint": cluster.get("Endpoint", "N/A"),
                            "status": cluster["Status"],
                            "database_name": cluster.get("DatabaseName", "N/A"),
                        }
                    )

            if not aurora_mysql_clusters:
                return f"âŒ {region} ë¦¬ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            result = f"ğŸ—„ï¸ **Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡** ({region} ë¦¬ì „)\n\n"

            for i, cluster in enumerate(aurora_mysql_clusters, 1):
                result += f"**{i}. {cluster['identifier']}**\n"
                result += f"   â€¢ ì—”ì§„ ë²„ì „: {cluster['engine_version']}\n"
                result += f"   â€¢ ì—”ë“œí¬ì¸íŠ¸: {cluster['endpoint']}\n"
                result += f"   â€¢ ê¸°ë³¸ DB: {cluster['database_name']}\n"
                result += f"   â€¢ ìƒíƒœ: {cluster['status']}\n\n"

            result += "ğŸ’¡ **ì‚¬ìš©ë²•:** í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ Aurora MySQL í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_aurora_cluster(
        self, cluster_selection: str, region: str = "ap-northeast-2"
    ) -> str:
        """Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)
            response = rds_client.describe_db_clusters()

            aurora_mysql_clusters = []
            for cluster in response["DBClusters"]:
                if (
                    cluster["Engine"] == "aurora-mysql"
                    and cluster["Status"] == "available"
                ):
                    aurora_mysql_clusters.append(cluster)

            if not aurora_mysql_clusters:
                return f"âŒ {region} ë¦¬ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            selected_cluster = None

            # ë²ˆí˜¸ë¡œ ì„ íƒ
            if cluster_selection.isdigit():
                cluster_index = int(cluster_selection) - 1
                if 0 <= cluster_index < len(aurora_mysql_clusters):
                    selected_cluster = aurora_mysql_clusters[cluster_index]
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒ
                for cluster in aurora_mysql_clusters:
                    if (
                        cluster_selection.lower()
                        in cluster["DBClusterIdentifier"].lower()
                    ):
                        selected_cluster = cluster
                        break

            if not selected_cluster:
                return f"âŒ ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cluster_selection}"

            # ì„ íƒëœ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥ (ì„ì‹œë¡œ í´ë˜ìŠ¤ ë³€ìˆ˜ì— ì €ì¥)
            self.selected_cluster = {
                "identifier": selected_cluster["DBClusterIdentifier"],
                "endpoint": selected_cluster.get("Endpoint"),
                "database_name": selected_cluster.get("DatabaseName"),
                "region": region,
            }

            return f"""âœ… **Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ ì™„ë£Œ**

ğŸ—„ï¸ **ì„ íƒëœ í´ëŸ¬ìŠ¤í„°:** {selected_cluster['DBClusterIdentifier']}
ğŸŒ **ì—”ë“œí¬ì¸íŠ¸:** {selected_cluster.get('Endpoint', 'N/A')}
ğŸ“Š **ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤:** {selected_cluster.get('DatabaseName', 'N/A')}
ğŸ“ **ë¦¬ì „:** {region}

ğŸ’¡ ì´ì œ ì´ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ê²€ì¦ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        except Exception as e:
            return f"âŒ Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ ì‹¤íŒ¨: {str(e)}"

    async def list_databases(self, database_secret: str) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = cursor.fetchall()

            # ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ì œì™¸
            system_dbs = {"information_schema", "performance_schema", "mysql", "sys"}
            user_databases = [db[0] for db in databases if db[0] not in system_dbs]

            cursor.close()
            connection.close()

            if not user_databases:
                return "âŒ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡**\n\n"

            for i, db_name in enumerate(user_databases, 1):
                result += f"**{i}. {db_name}**\n"

            result += "\nğŸ’¡ **ì‚¬ìš©ë²•:** ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_database(
        self, database_secret: str, database_selection: str
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ë° ë³€ê²½"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = cursor.fetchall()

            # ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ì œì™¸
            system_dbs = {"information_schema", "performance_schema", "mysql", "sys"}
            user_databases = [db[0] for db in databases if db[0] not in system_dbs]

            if not user_databases:
                cursor.close()
                connection.close()
                return "âŒ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            selected_db = None

            # ë²ˆí˜¸ë¡œ ì„ íƒ
            if database_selection.isdigit():
                db_index = int(database_selection) - 1
                if 0 <= db_index < len(user_databases):
                    selected_db = user_databases[db_index]
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒ
                for db_name in user_databases:
                    if database_selection.lower() in db_name.lower():
                        selected_db = db_name
                        break

            if not selected_db:
                cursor.close()
                connection.close()
                return (
                    f"âŒ ì„ íƒí•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {database_selection}"
                )

            # USE ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½
            cursor.execute(f"USE `{selected_db}`")

            # ë³€ê²½ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            self.selected_database = selected_db

            cursor.close()
            connection.close()

            return f"""âœ… **ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ì™„ë£Œ**

ğŸ—„ï¸ **í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤:** {current_db}
ğŸ”„ **ë³€ê²½ ëª…ë ¹:** USE `{selected_db}`

ğŸ’¡ ì´ì œ ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹¤íŒ¨: {str(e)}"

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"

            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # íŒŒì¼ ë³µì‚¬
            import shutil

            shutil.copy2(source, target_path)

            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    async def test_connection_only(self, database_secret: str) -> str:
        """ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"""
        try:
            connection_result = await self.test_database_connection(
                database_secret, use_ssh_tunnel=True
            )

            if connection_result["success"]:
                databases_list = "\n".join(
                    [f"   - {db}" for db in connection_result.get("databases", [])]
                )
                tables_list = "\n".join(
                    [f"   - {table}" for table in connection_result.get("tables", [])]
                )

                return f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- í˜¸ìŠ¤íŠ¸: {connection_result.get('host', 'N/A')}
- í¬íŠ¸: {connection_result.get('port', 'N/A')}
- ì—°ê²° ë°©ì‹: {connection_result.get('connection_method', 'N/A')}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {connection_result.get('current_database', 'N/A')}
- ì„œë²„ ë²„ì „: {connection_result.get('server_version', 'N/A')}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**
{databases_list if databases_list else '   (ì—†ìŒ)'}

**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**
{tables_list if tables_list else '   (ì—†ìŒ)'}"""
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {connection_result['error']}"

        except Exception as e:
            return f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def analyze_slow_queries(self, database_secret: str, limit: int = 10) -> str:
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸŒ **ëŠë¦° ì¿¼ë¦¬ ë¶„ì„** (ìƒìœ„ {limit}ê°œ)\n\n"

            # Performance Schemaì—ì„œ ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
            cursor.execute(
                f"""
                SELECT 
                    DIGEST_TEXT as query_pattern,
                    COUNT_STAR as exec_count,
                    ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                    ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                    ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec,
                    ROUND(SUM_ROWS_EXAMINED/COUNT_STAR, 0) as avg_rows_examined,
                    ROUND(SUM_ROWS_SENT/COUNT_STAR, 0) as avg_rows_sent
                FROM performance_schema.events_statements_summary_by_digest 
                WHERE DIGEST_TEXT IS NOT NULL 
                AND AVG_TIMER_WAIT > 1000000000
                ORDER BY AVG_TIMER_WAIT DESC 
                LIMIT {limit}
            """
            )

            slow_queries = cursor.fetchall()

            if not slow_queries:
                result += "âœ… ëŠë¦° ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    pattern,
                    count,
                    avg_time,
                    max_time,
                    total_time,
                    avg_examined,
                    avg_sent,
                ) in enumerate(slow_queries, 1):
                    pattern_short = (
                        (pattern[:80] + "...") if len(pattern) > 80 else pattern
                    )
                    result += f"**{i}. ì¿¼ë¦¬ íŒ¨í„´:**\n```sql\n{pattern_short}\n```\n"
                    result += f"ğŸ“ˆ **í†µê³„:**\n"
                    result += f"- ì‹¤í–‰ íšŸìˆ˜: {count:,}íšŒ\n"
                    result += f"- í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}ì´ˆ\n"
                    result += f"- ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {max_time:.3f}ì´ˆ\n"
                    result += f"- ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.3f}ì´ˆ\n"
                    result += f"- í‰ê·  ê²€ì‚¬ í–‰ ìˆ˜: {avg_examined:,.0f}í–‰\n"
                    result += f"- í‰ê·  ë°˜í™˜ í–‰ ìˆ˜: {avg_sent:,.0f}í–‰\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ëŠë¦° ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_table_io_stats(
        self, database_secret: str, schema_name: Optional[str] = None
    ) -> str:
        """í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ìŠ¤í‚¤ë§ˆ ì´ë¦„ ì„¤ì •
            if not schema_name:
                cursor.execute("SELECT DATABASE()")
                schema_name = cursor.fetchone()[0]

            result = f"ğŸ’¿ **í…Œì´ë¸” I/O í†µê³„** (ìŠ¤í‚¤ë§ˆ: {schema_name})\n\n"

            # í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ
            cursor.execute(
                """
                SELECT 
                    object_name as table_name,
                    count_read,
                    count_write,
                    count_read + count_write as total_io,
                    sum_timer_read/1000000000000 as read_time_sec,
                    sum_timer_write/1000000000000 as write_time_sec,
                    (sum_timer_read + sum_timer_write)/1000000000000 as total_time_sec
                FROM performance_schema.table_io_waits_summary_by_table 
                WHERE object_schema = %s
                AND count_read + count_write > 0
                ORDER BY count_read + count_write DESC 
                LIMIT 10
            """,
                (schema_name,),
            )

            io_stats = cursor.fetchall()

            if not io_stats:
                result += "ğŸ“Š I/O í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                result += "ğŸ“Š **ìƒìœ„ 10ê°œ í…Œì´ë¸” (I/O ê¸°ì¤€):**\n\n"
                for i, (
                    table,
                    read_count,
                    write_count,
                    total_io,
                    read_time,
                    write_time,
                    total_time,
                ) in enumerate(io_stats, 1):
                    result += f"**{i}. {table}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ I/O: {total_io:,}íšŒ ({total_time:.3f}ì´ˆ)\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” I/O í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_index_usage_stats(
        self, database_secret: str, table_name: Optional[str] = None
    ) -> str:
        """ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“‡ **ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„**"
            if table_name:
                result += f" (í…Œì´ë¸”: {table_name})"
            result += "\n\n"

            # ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ
            if table_name:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND object_name = %s
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC
                """,
                    (table_name,),
                )
            else:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC 
                    LIMIT 15
                """
                )

            index_stats = cursor.fetchall()

            if not index_stats:
                result += "ğŸ“Š ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    table,
                    index,
                    read_count,
                    write_count,
                    total_usage,
                    read_time,
                    write_time,
                ) in enumerate(index_stats, 1):
                    index_display = index if index else "PRIMARY"
                    result += f"**{i}. {table}.{index_display}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ ì‚¬ìš©: {total_usage:,}íšŒ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_connection_stats(self, database_secret: str) -> str:
        """ì—°ê²° ë° ì„¸ì…˜ í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”— **ì—°ê²° ë° ì„¸ì…˜ í†µê³„**\n\n"

            # í˜„ì¬ ì—°ê²° ìƒíƒœ
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total_connections,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections,
                    SUM(CASE WHEN COMMAND = 'Sleep' THEN 1 ELSE 0 END) as idle_connections
                FROM information_schema.processlist
            """
            )

            conn_stats = cursor.fetchone()
            if conn_stats:
                result += f"ğŸ“Š **í˜„ì¬ ì—°ê²° ìƒíƒœ:**\n"
                result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n"
                result += f"- ìœ íœ´ ì—°ê²°: {conn_stats[2]}ê°œ\n\n"

            # ì‚¬ìš©ìë³„ ì—°ê²° í†µê³„
            cursor.execute(
                """
                SELECT 
                    USER as username,
                    COUNT(*) as connection_count,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_count
                FROM information_schema.processlist
                GROUP BY USER
                ORDER BY connection_count DESC
            """
            )

            user_stats = cursor.fetchall()
            if user_stats:
                result += f"ğŸ‘¥ **ì‚¬ìš©ìë³„ ì—°ê²°:**\n"
                for user, total, active in user_stats:
                    result += f"- {user}: {total}ê°œ ì—°ê²° (í™œì„±: {active}ê°œ)\n"
                result += "\n"

            # Performance Schema ìŠ¤ë ˆë“œ ì •ë³´
            cursor.execute(
                """
                SELECT 
                    type,
                    COUNT(*) as thread_count
                FROM performance_schema.threads
                GROUP BY type
            """
            )

            thread_stats = cursor.fetchall()
            if thread_stats:
                result += f"ğŸ§µ **ìŠ¤ë ˆë“œ í†µê³„:**\n"
                for thread_type, count in thread_stats:
                    result += f"- {thread_type}: {count}ê°œ\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì—°ê²° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_memory_usage(self, database_secret: str) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„**\n\n"

            # ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ)
            cursor.execute(
                """
                SELECT 
                    event_name,
                    ROUND(sum_number_of_bytes_alloc/1024/1024, 2) as allocated_mb,
                    ROUND(sum_number_of_bytes_free/1024/1024, 2) as freed_mb,
                    ROUND((sum_number_of_bytes_alloc - sum_number_of_bytes_free)/1024/1024, 2) as current_mb
                FROM performance_schema.memory_summary_global_by_event_name
                WHERE sum_number_of_bytes_alloc > 0
                ORDER BY (sum_number_of_bytes_alloc - sum_number_of_bytes_free) DESC
                LIMIT 10
            """
            )

            memory_stats = cursor.fetchall()
            if memory_stats:
                result += f"ğŸ“Š **ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ):**\n"
                for event, allocated, freed, current in memory_stats:
                    event_short = event.replace("memory/", "").replace("sql/", "")
                    result += f"- {event_short}: {current:.2f}MB (í• ë‹¹: {allocated:.2f}MB, í•´ì œ: {freed:.2f}MB)\n"
                result += "\n"

            # ì£¼ìš” ë©”ëª¨ë¦¬ ê´€ë ¨ ì‹œìŠ¤í…œ ë³€ìˆ˜
            cursor.execute(
                """
                SHOW VARIABLES WHERE Variable_name IN (
                    'innodb_buffer_pool_size',
                    'key_buffer_size',
                    'query_cache_size',
                    'tmp_table_size',
                    'max_heap_table_size',
                    'sort_buffer_size',
                    'read_buffer_size',
                    'join_buffer_size'
                )
            """
            )

            variables = cursor.fetchall()
            if variables:
                result += f"âš™ï¸ **ì£¼ìš” ë©”ëª¨ë¦¬ ì„¤ì •:**\n"
                for var_name, var_value in variables:
                    # ë°”ì´íŠ¸ ë‹¨ìœ„ë¥¼ MBë¡œ ë³€í™˜
                    if var_value.isdigit():
                        mb_value = int(var_value) / 1024 / 1024
                        result += f"- {var_name}: {mb_value:.2f}MB\n"
                    else:
                        result += f"- {var_name}: {var_value}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_lock_analysis(self, database_secret: str) -> str:
        """ë½ ìƒíƒœ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”’ **ë½ ìƒíƒœ ë¶„ì„**\n\n"

            # í˜„ì¬ ë½ ìƒíƒœ (MySQL 8.0+)
            try:
                cursor.execute(
                    """
                    SELECT 
                        object_schema,
                        object_name,
                        lock_type,
                        lock_mode,
                        lock_status,
                        thread_id
                    FROM performance_schema.data_locks
                    LIMIT 10
                """
                )

                current_locks = cursor.fetchall()
                if current_locks:
                    result += f"ğŸ” **í˜„ì¬ í™œì„± ë½ (ìƒìœ„ 10ê°œ):**\n"
                    for (
                        schema,
                        table,
                        lock_type,
                        lock_mode,
                        status,
                        thread_id,
                    ) in current_locks:
                        result += f"- {schema}.{table}: {lock_type} ({lock_mode}) - {status} [Thread: {thread_id}]\n"
                    result += "\n"
                else:
                    result += f"âœ… **í˜„ì¬ í™œì„± ë½:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **í˜„ì¬ ë½ ì •ë³´:** Performance Schema data_locks í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ë½ ëŒ€ê¸° ìƒí™©
            try:
                cursor.execute(
                    """
                    SELECT 
                        requesting_thread_id,
                        blocking_thread_id,
                        object_schema,
                        object_name,
                        lock_type
                    FROM performance_schema.data_lock_waits
                    LIMIT 10
                """
                )

                lock_waits = cursor.fetchall()
                if lock_waits:
                    result += f"â³ **ë½ ëŒ€ê¸° ìƒí™©:**\n"
                    for (
                        req_thread,
                        block_thread,
                        schema,
                        table,
                        lock_type,
                    ) in lock_waits:
                        result += f"- Thread {req_thread}ì´ Thread {block_thread}ì— ì˜í•´ ëŒ€ê¸° ì¤‘\n"
                        result += f"  ëŒ€ìƒ: {schema}.{table} ({lock_type})\n"
                    result += "\n"
                else:
                    result += f"âœ… **ë½ ëŒ€ê¸°:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **ë½ ëŒ€ê¸° ì •ë³´:** Performance Schema data_lock_waits í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ëŒ€ê¸° ì´ë²¤íŠ¸ í†µê³„
            cursor.execute(
                """
                SELECT 
                    event_name,
                    count_star as event_count,
                    ROUND(sum_timer_wait/1000000000000, 6) as total_wait_sec,
                    ROUND(avg_timer_wait/1000000000000, 6) as avg_wait_sec
                FROM performance_schema.events_waits_summary_global_by_event_name
                WHERE event_name LIKE '%lock%' 
                AND count_star > 0
                ORDER BY sum_timer_wait DESC
                LIMIT 5
            """
            )

            wait_events = cursor.fetchall()
            if wait_events:
                result += f"â±ï¸ **ë½ ê´€ë ¨ ëŒ€ê¸° ì´ë²¤íŠ¸ (ìƒìœ„ 5ê°œ):**\n"
                for event, count, total_wait, avg_wait in wait_events:
                    event_short = event.replace("wait/synch/", "").replace(
                        "wait/io/", ""
                    )
                    result += f"- {event_short}: {count:,}íšŒ (ì´ {total_wait:.3f}ì´ˆ, í‰ê·  {avg_wait:.6f}ì´ˆ)\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_replication_status(self, database_secret: str) -> str:
        """ë³µì œ ìƒíƒœ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”„ **ë³µì œ ìƒíƒœ ë¶„ì„**\n\n"

            # ë³µì œ ì—°ê²° ìƒíƒœ (Performance Schema)
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        host,
                        port,
                        user,
                        source_connection_auto_failover,
                        connection_retry_interval,
                        connection_retry_count
                    FROM performance_schema.replication_connection_configuration
                """
                )

                repl_config = cursor.fetchall()
                if repl_config:
                    result += f"âš™ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:**\n"
                    for (
                        channel,
                        host,
                        port,
                        user,
                        auto_failover,
                        retry_interval,
                        retry_count,
                    ) in repl_config:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ì†ŒìŠ¤: {host}:{port} (ì‚¬ìš©ì: {user})\n"
                        result += f"  ìë™ ì¥ì• ì¡°ì¹˜: {auto_failover}\n"
                        result += (
                            f"  ì¬ì‹œë„: {retry_count}íšŒ, ê°„ê²©: {retry_interval}ì´ˆ\n\n"
                        )
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ìƒíƒœ
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        service_state,
                        received_transaction_set,
                        last_error_message,
                        last_error_timestamp
                    FROM performance_schema.replication_connection_status
                """
                )

                repl_status = cursor.fetchall()
                if repl_status:
                    result += f"ğŸ“Š **ë³µì œ ì—°ê²° ìƒíƒœ:**\n"
                    for channel, state, trans_set, error_msg, error_time in repl_status:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if trans_set:
                            result += f"  ìˆ˜ì‹  íŠ¸ëœì­ì…˜: {trans_set[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ë§ˆì§€ë§‰ ì˜¤ë¥˜: {error_msg}\n"
                            result += f"  ì˜¤ë¥˜ ì‹œê°„: {error_time}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ì§€ì—° ì •ë³´
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        worker_id,
                        service_state,
                        last_error_message,
                        last_applied_transaction
                    FROM performance_schema.replication_applier_status_by_worker
                """
                )

                worker_status = cursor.fetchall()
                if worker_status:
                    result += f"ğŸ‘· **ë³µì œ ì›Œì»¤ ìƒíƒœ:**\n"
                    for (
                        channel,
                        worker_id,
                        state,
                        error_msg,
                        last_trans,
                    ) in worker_status:
                        result += f"- ì±„ë„: {channel}, ì›Œì»¤: {worker_id}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if last_trans:
                            result += f"  ë§ˆì§€ë§‰ ì ìš©: {last_trans[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ì˜¤ë¥˜: {error_msg}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ
            try:
                cursor.execute("SHOW MASTER STATUS")
                master_status = cursor.fetchone()
                if master_status:
                    result += f"ğŸ“ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:**\n"
                    result += f"- íŒŒì¼: {master_status[0]}\n"
                    result += f"- ìœ„ì¹˜: {master_status[1]}\n"
                    if len(master_status) > 2 and master_status[2]:
                        result += f"- ë°”ì¸ë”© DB: {master_status[2]}\n"
                    if len(master_status) > 3 and master_status[3]:
                        result += f"- ì œì™¸ DB: {master_status[3]}\n"
                else:
                    result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸:** ë¹„í™œì„±í™”\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë³µì œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def analyze_dml_performance(
        self, sql_content: str, database_secret: str, filename: str
    ) -> dict:
        """DML ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ (EXPLAIN í¬í•¨)"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            db_info = await self.get_database_info(database_secret)
            if not db_info:
                return {"error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"}

            connection = await self.connect_to_database(db_info)
            if not connection:
                return {"error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"}

            cursor = connection.cursor(dictionary=True)
            
            # SQL ë¬¸ ë¶„ë¦¬ ë° ë¶„ì„
            queries = self.extract_dml_queries(sql_content)
            if not queries:
                return {"error": "DML ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            analysis_results = []
            
            for i, query in enumerate(queries, 1):
                query_analysis = {
                    "query_number": i,
                    "query": query.strip(),
                    "query_type": self.get_query_type(query),
                    "explain_result": None,
                    "table_stats": {},
                    "index_usage": {},
                    "performance_issues": []
                }

                try:
                    # EXPLAIN ì‹¤í–‰
                    explain_query = f"EXPLAIN FORMAT=JSON {query}"
                    cursor.execute(explain_query)
                    explain_result = cursor.fetchone()
                    
                    if explain_result:
                        query_analysis["explain_result"] = explain_result
                        
                        # í…Œì´ë¸” í†µê³„ ìˆ˜ì§‘
                        tables = self.extract_tables_from_query(query)
                        for table in tables:
                            stats = await self.get_table_statistics(cursor, table)
                            query_analysis["table_stats"][table] = stats
                            
                            # ì¸ë±ìŠ¤ ì‚¬ìš© ì •ë³´
                            indexes = await self.get_table_indexes(cursor, table)
                            query_analysis["index_usage"][table] = indexes

                        # ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„
                        issues = self.analyze_explain_result(explain_result, query_analysis)
                        query_analysis["performance_issues"] = issues

                except Exception as e:
                    query_analysis["error"] = f"EXPLAIN ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"

                analysis_results.append(query_analysis)

            connection.close()
            
            return {
                "filename": filename,
                "total_queries": len(queries),
                "queries": analysis_results,
                "database_info": db_info
            }

        except Exception as e:
            return {"error": f"DML ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

    def extract_dml_queries(self, sql_content: str) -> List[str]:
        """SQL ë‚´ìš©ì—ì„œ DML ì¿¼ë¦¬ ì¶”ì¶œ"""
        # ì£¼ì„ ì œê±°
        content = re.sub(r'--.*$', '', sql_content, flags=re.MULTILINE)
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        
        # DML ì¿¼ë¦¬ íŒ¨í„´ (SELECT, UPDATE, DELETE, INSERT)
        dml_pattern = r'\b(SELECT|UPDATE|DELETE|INSERT)\b.*?(?=\b(?:SELECT|UPDATE|DELETE|INSERT)\b|$)'
        queries = re.findall(dml_pattern, content, re.IGNORECASE | re.DOTALL)
        
        # ì™„ì „í•œ ì¿¼ë¦¬ ì¶”ì¶œ
        full_queries = []
        for match in re.finditer(dml_pattern, content, re.IGNORECASE | re.DOTALL):
            query = match.group(0).strip()
            if query and len(query) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                full_queries.append(query.rstrip(';'))
        
        return full_queries

    def get_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… í™•ì¸"""
        query_upper = query.strip().upper()
        if query_upper.startswith('SELECT'):
            return 'SELECT'
        elif query_upper.startswith('UPDATE'):
            return 'UPDATE'
        elif query_upper.startswith('DELETE'):
            return 'DELETE'
        elif query_upper.startswith('INSERT'):
            return 'INSERT'
        return 'UNKNOWN'

    def extract_tables_from_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        tables = set()
        
        # FROM ì ˆì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
        from_pattern = r'\bFROM\s+([a-zA-Z_][a-zA-Z0-9_]*)'
        from_matches = re.findall(from_pattern, query, re.IGNORECASE)
        tables.update(from_matches)
        
        # JOIN ì ˆì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
        join_pattern = r'\bJOIN\s+([a-zA-Z_][a-zA-Z0-9_]*)'
        join_matches = re.findall(join_pattern, query, re.IGNORECASE)
        tables.update(join_matches)
        
        # UPDATE ë¬¸ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
        update_pattern = r'\bUPDATE\s+([a-zA-Z_][a-zA-Z0-9_]*)'
        update_matches = re.findall(update_pattern, query, re.IGNORECASE)
        tables.update(update_matches)
        
        # DELETE ë¬¸ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
        delete_pattern = r'\bDELETE\s+FROM\s+([a-zA-Z_][a-zA-Z0-9_]*)'
        delete_matches = re.findall(delete_pattern, query, re.IGNORECASE)
        tables.update(delete_matches)
        
        return list(tables)

    async def get_table_statistics(self, cursor, table_name: str) -> dict:
        """í…Œì´ë¸” í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # í…Œì´ë¸” í–‰ ìˆ˜
            cursor.execute(f"SELECT COUNT(*) as row_count FROM {table_name}")
            row_count = cursor.fetchone()['row_count']
            
            # í…Œì´ë¸” í¬ê¸° ì •ë³´
            cursor.execute(f"""
                SELECT 
                    table_rows,
                    data_length,
                    index_length,
                    (data_length + index_length) as total_size
                FROM information_schema.tables 
                WHERE table_name = '{table_name}' 
                AND table_schema = DATABASE()
            """)
            size_info = cursor.fetchone()
            
            return {
                "row_count": row_count,
                "estimated_rows": size_info['table_rows'] if size_info else 0,
                "data_size": size_info['data_length'] if size_info else 0,
                "index_size": size_info['index_length'] if size_info else 0,
                "total_size": size_info['total_size'] if size_info else 0
            }
        except Exception as e:
            return {"error": f"í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}

    async def get_table_indexes(self, cursor, table_name: str) -> dict:
        """í…Œì´ë¸” ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        try:
            cursor.execute(f"SHOW INDEX FROM {table_name}")
            indexes = cursor.fetchall()
            
            index_info = {}
            for idx in indexes:
                key_name = idx['Key_name']
                if key_name not in index_info:
                    index_info[key_name] = {
                        "columns": [],
                        "unique": idx['Non_unique'] == 0,
                        "type": idx['Index_type']
                    }
                index_info[key_name]["columns"].append(idx['Column_name'])
            
            return index_info
        except Exception as e:
            return {"error": f"ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}

    def analyze_explain_result(self, explain_result: dict, query_analysis: dict) -> List[str]:
        """EXPLAIN ê²°ê³¼ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ ì´ìŠˆ íƒì§€"""
        issues = []
        
        try:
            explain_data = json.loads(explain_result['EXPLAIN']) if isinstance(explain_result['EXPLAIN'], str) else explain_result['EXPLAIN']
            query_block = explain_data.get('query_block', {})
            
            # í…Œì´ë¸” ìŠ¤ìº” ë¶„ì„
            def analyze_table_access(table_info):
                if 'table' in table_info:
                    table = table_info['table']
                    access_type = table.get('access_type', '')
                    
                    # Full Table Scan ì²´í¬
                    if access_type == 'ALL':
                        rows_examined = table.get('rows_examined_per_scan', 0)
                        if rows_examined > 1000:
                            issues.append(f"âš ï¸ Full Table Scan ê°ì§€: {table.get('table_name', 'unknown')} ({rows_examined:,} í–‰ ìŠ¤ìº”)")
                    
                    # ì¸ë±ìŠ¤ ì‚¬ìš© ì²´í¬
                    if access_type in ['index', 'range', 'ref']:
                        key_used = table.get('key', '')
                        if key_used:
                            issues.append(f"âœ… ì¸ë±ìŠ¤ ì‚¬ìš©: {table.get('table_name', 'unknown')}.{key_used}")
                    
                    # ì„ì‹œ í…Œì´ë¸” ì‚¬ìš© ì²´í¬
                    if table.get('using_temporary_table', False):
                        issues.append(f"âš ï¸ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš©: {table.get('table_name', 'unknown')}")
                    
                    # íŒŒì¼ ì •ë ¬ ì²´í¬
                    if table.get('using_filesort', False):
                        issues.append(f"âš ï¸ íŒŒì¼ ì •ë ¬ ì‚¬ìš©: {table.get('table_name', 'unknown')}")
                
                # ì¤‘ì²©ëœ í…Œì´ë¸” ì •ë³´ ì²˜ë¦¬
                for key in ['nested_loop', 'table', 'materialized_from_subquery']:
                    if key in table_info:
                        nested = table_info[key]
                        if isinstance(nested, list):
                            for item in nested:
                                analyze_table_access(item)
                        elif isinstance(nested, dict):
                            analyze_table_access(nested)
            
            # ì¿¼ë¦¬ ë¸”ë¡ ë¶„ì„
            analyze_table_access(query_block)
            
            # ë¹„ìš© ë¶„ì„
            cost_info = query_block.get('cost_info', {})
            if cost_info:
                query_cost = cost_info.get('query_cost', 0)
                if query_cost > 1000:
                    issues.append(f"âš ï¸ ë†’ì€ ì¿¼ë¦¬ ë¹„ìš©: {query_cost:.2f}")
            
        except Exception as e:
            issues.append(f"âŒ EXPLAIN ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        
        return issues

    async def validate_dml_with_claude(
        self, analysis_result: dict, sql_content: str
    ) -> str:
        """Claudeë¥¼ ì‚¬ìš©í•œ DML ì„±ëŠ¥ ë¶„ì„"""
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ Claude ì…ë ¥ìš©ìœ¼ë¡œ í¬ë§·íŒ…
        analysis_summary = self.format_analysis_for_claude(analysis_result)
        
        prompt = f"""
ë‹¤ìŒì€ MySQL DML ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ê° ì¿¼ë¦¬ì˜ ì‹¤í–‰ ê³„íšê³¼ ì„±ëŠ¥ ì´ìŠˆë¥¼ ë¶„ì„í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.

**ë¶„ì„ ëŒ€ìƒ íŒŒì¼:** {analysis_result.get('filename', 'unknown')}
**ì´ ì¿¼ë¦¬ ìˆ˜:** {analysis_result.get('total_queries', 0)}

**ì¿¼ë¦¬ ë° ë¶„ì„ ê²°ê³¼:**
{analysis_summary}

**ì›ë³¸ SQL:**
```sql
{sql_content}
```

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ê° ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ì´ìŠˆ (ì¸ë±ìŠ¤ ëˆ„ë½, Full Table Scan ë“±)
2. ì‹¤í–‰ ê³„íš ìµœì í™” ë°©ì•ˆ
3. ì¸ë±ìŠ¤ ì¶”ê°€/ìˆ˜ì • ê¶Œì¥ì‚¬í•­
4. ì¿¼ë¦¬ ë¦¬íŒ©í† ë§ ì œì•ˆ
5. ì „ì²´ì ì¸ ì„±ëŠ¥ ê°œì„  ë°©í–¥

**ì‘ë‹µ í˜•ì‹:**
- ë¬¸ì œê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ì¸ ì´ìŠˆì™€ í•´ê²°ë°©ì•ˆ ì œì‹œ
- ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ì„±ëŠ¥ ë¶„ì„ í†µê³¼" ì‘ë‹µ
"""

        try:
            claude_input = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4000,
                "messages": [{"role": "user", "content": prompt}]
            })

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            
            if hasattr(response, 'get'):
                response_body = json.loads(response.get('body').read())
            else:
                response_body = json.loads(response['body'].read())

            content = response_body.get("content", [])
            if content and len(content) > 0:
                first_content = content[0]
                if hasattr(first_content, 'get'):
                    return first_content.get('text', str(first_content))
                else:
                    return str(first_content)
            else:
                return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"
                
        except Exception as e:
            logger.error(f"Claude DML ë¶„ì„ ì˜¤ë¥˜: {e}")
            return f"Claude ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def format_analysis_for_claude(self, analysis_result: dict) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ Claude ì…ë ¥ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted = ""
        
        for query_info in analysis_result.get('queries', []):
            formatted += f"\n**ì¿¼ë¦¬ #{query_info['query_number']} ({query_info['query_type']})**\n"
            formatted += f"```sql\n{query_info['query']}\n```\n"
            
            # EXPLAIN ê²°ê³¼
            if query_info.get('explain_result'):
                formatted += "**ì‹¤í–‰ ê³„íš:**\n"
                try:
                    explain_data = json.loads(query_info['explain_result']['EXPLAIN']) if isinstance(query_info['explain_result']['EXPLAIN'], str) else query_info['explain_result']['EXPLAIN']
                    formatted += f"```json\n{json.dumps(explain_data, indent=2, ensure_ascii=False)}\n```\n"
                except:
                    formatted += f"{query_info['explain_result']}\n"
            
            # í…Œì´ë¸” í†µê³„
            if query_info.get('table_stats'):
                formatted += "**í…Œì´ë¸” í†µê³„:**\n"
                for table, stats in query_info['table_stats'].items():
                    formatted += f"- {table}: {stats.get('row_count', 0):,} í–‰, {stats.get('total_size', 0):,} bytes\n"
            
            # ì¸ë±ìŠ¤ ì •ë³´
            if query_info.get('index_usage'):
                formatted += "**ì¸ë±ìŠ¤ ì •ë³´:**\n"
                for table, indexes in query_info['index_usage'].items():
                    formatted += f"- {table}: {list(indexes.keys())}\n"
            
            # ì„±ëŠ¥ ì´ìŠˆ
            if query_info.get('performance_issues'):
                formatted += "**ê°ì§€ëœ ì´ìŠˆ:**\n"
                for issue in query_info['performance_issues']:
                    formatted += f"- {issue}\n"
            
            formatted += "\n" + "="*50 + "\n"
        
        return formatted

    async def validate_with_claude(
        self,
        ddl_content: str,
        database_secret: str = None,
        schema_info: dict = None,
        existing_analysis: dict = None,
    ) -> str:
        """
        Claude cross-region í”„ë¡œíŒŒì¼ì„ í™œìš©í•œ DDL ê²€ì¦ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ë° ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)
        """

        # ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆœì„œ ê³ ë ¤)

        if schema_info:
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            schema_text = []
            if isinstance(schema_info, dict):
                for key, value in schema_info.items():
                    schema_text.append(f"{key}: {value}")
            else:
                schema_text.append(str(schema_info))

            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if existing_analysis:
                schema_text.append(f"ê¸°ì¡´ ë¶„ì„ ê²°ê³¼: {existing_analysis}")

            schema_context = f"""
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì‹¤í–‰ ìˆœì„œë³„):
{chr(10).join(schema_text)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ DDLì˜ ì ì ˆì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. íŒŒì¼ ë‚´ì—ì„œ ë¨¼ì € ìƒì„±ëœ í…Œì´ë¸”ì€ ì´í›„ ALTER/INDEX ì‘ì—…ì—ì„œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
2. ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€
3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”/ì¸ë±ìŠ¤ì— ëŒ€í•œ DROP ì‹œë„
4. ì‹¤í–‰ ìˆœì„œìƒ ë…¼ë¦¬ì  ì˜¤ë¥˜
"""
        else:
            schema_context = """
ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        prompt = f"""
        ë‹¤ìŒ SQL ë¬¸ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {schema_context}

        ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
        1. ë¬¸ë²• ì˜¤ë¥˜ (DDL ë° SELECT ì¿¼ë¦¬ ëª¨ë‘)
        2. í‘œì¤€ ê·œì¹™ ìœ„ë°˜
        3. ì„±ëŠ¥ ë¬¸ì œ (SELECT ì¿¼ë¦¬ì˜ ê²½ìš° ì¸ë±ìŠ¤ ì‚¬ìš©, JOIN ìµœì í™” ë“±)
        4. ìŠ¤í‚¤ë§ˆ ì¶©ëŒ (í…Œì´ë¸”/ì»¬ëŸ¼/ì¸ë±ìŠ¤ ì¤‘ë³µ ë“±)
        5. ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± ë¬¸ì œ
        6. SELECT ì¿¼ë¦¬ì˜ ê²½ìš°: í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€, ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€, JOIN ì¡°ê±´ ì ì ˆì„±
        
        ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì°¸ê³ í•˜ë˜, ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì¶”ê°€ ê²€ì¦ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        ë¬¸ì œê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•´ì£¼ì„¸ìš”. ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ê²€ì¦ í†µê³¼"ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        claude_input = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,  # í† í° ìˆ˜ë¥¼ 4ë°°ë¡œ ì¦ê°€
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            }
        )

        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile í˜¸ì¶œ
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            # responseê°€ dictì¸ì§€ í™•ì¸í•˜ê³  body ì¶”ì¶œ
            if isinstance(response, dict) and "body" in response:
                response_body = json.loads(response["body"].read())
            else:
                logger.error(f"Unexpected response format: {type(response)}")
                return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ - ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ êµ¬ì¡°"

            content = response_body.get("content", [])
            if isinstance(content, list) and len(content) > 0:
                first_content = content[0]
                if isinstance(first_content, dict):
                    return first_content.get("text", "")
                else:
                    return str(first_content)
            else:
                return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"
        except Exception as e:
            logger.warning(
                f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}"
            )
            # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id, body=claude_input
                )
                # responseê°€ dictì¸ì§€ í™•ì¸í•˜ê³  body ì¶”ì¶œ
                if isinstance(response, dict) and "body" in response:
                    response_body = json.loads(response["body"].read())
                else:
                    logger.error(f"Unexpected response format: {type(response)}")
                    return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ - ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ êµ¬ì¡°"

                content = response_body.get("content", [])
                if isinstance(content, list) and len(content) > 0:
                    first_content = content[0]
                    if isinstance(first_content, dict):
                        return first_content.get("text", "")
                    else:
                        return str(first_content)
                else:
                    return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"
            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def extract_current_schema_info(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘: database_secret={database_secret}")
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            logger.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}")

            schema_info = {"tables": [], "columns": {}, "indexes": {}}

            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            cursor.execute(
                """
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """
            )
            tables = [row[0] for row in cursor.fetchall()]
            schema_info["tables"] = tables
            logger.info(f"ë°œê²¬ëœ í…Œì´ë¸”: {tables}")

            # ê° í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT column_name, data_type, character_maximum_length, 
                           numeric_precision, numeric_scale, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table,),
                )

                columns = []
                for col_row in cursor.fetchall():
                    col_info = {
                        "name": col_row[0],
                        "data_type": col_row[1],
                        "max_length": col_row[2],
                        "precision": col_row[3],
                        "scale": col_row[4],
                        "nullable": col_row[5] == "YES",
                        "default": col_row[6],
                    }
                    columns.append(col_info)

                schema_info["columns"][table] = columns

            # ê° í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT index_name, column_name, seq_in_index, non_unique
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table,),
                )

                indexes = {}
                for idx_row in cursor.fetchall():
                    idx_name = idx_row[0]
                    col_name = idx_row[1]
                    seq = idx_row[2]
                    non_unique = idx_row[3]

                    if idx_name not in indexes:
                        indexes[idx_name] = {"columns": [], "unique": non_unique == 0}

                    indexes[idx_name]["columns"].append(col_name)

                schema_info["indexes"][table] = indexes

            cursor.close()
            connection.close()

            return schema_info

        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}


# MCP ì„œë²„ ì„¤ì •
server = Server("ddl-qcli-validator")
ddl_validator = DDLValidationQCLIServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ Aurora MySQL ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì„¸ë¡œë¡œ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="validate_all_sql",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤ (ìµœëŒ€ 5ê°œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="validate_selected_sql_files",
            description="ì„ íƒí•œ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  í†µí•©ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "sql_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ê²€ì¦í•  SQL íŒŒì¼ëª… ëª©ë¡ (ìµœëŒ€ 10ê°œ)",
                    },
                },
                "required": ["database_secret", "sql_files"],
            },
        ),
        types.Tool(
            name="validate_multiple_sql_direct",
            description="ì—¬ëŸ¬ SQL íŒŒì¼ì„ ì§ì ‘ ê²€ì¦í•˜ê³  í†µí•©ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ê³„íš ì—†ì´ ë°”ë¡œ ì‹¤í–‰)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "file_count": {
                        "type": "integer",
                        "description": "ê²€ì¦í•  íŒŒì¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 15)",
                        "default": 10,
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="analyze_current_schema",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="check_ddl_conflicts",
            description="SQL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì ì„ ì‚¬ì „ ê²€ì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "ddl_content": {"type": "string", "description": "ê²€ì‚¬í•  SQL ë¬¸"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["ddl_content", "database_secret"],
            },
        ),
        types.Tool(
            name="get_aurora_mysql_parameters",
            description="Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                    },
                    "filter_type": {
                        "type": "string",
                        "description": "í•„í„° íƒ€ì… (important: ì£¼ìš” íŒŒë¼ë¯¸í„°ë§Œ, custom: ì‚¬ìš©ì ì •ì˜ë§Œ, all: ì „ì²´)",
                        "enum": ["important", "custom", "all"],
                        "default": "important",
                    },
                    "category": {
                        "type": "string",
                        "description": "íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)",
                        "enum": [
                            "all",
                            "security",
                            "performance",
                            "memory",
                            "io",
                            "connection",
                            "logging",
                            "replication",
                            "aurora",
                        ],
                        "default": "all",
                    },
                },
                "required": ["cluster_identifier"],
            },
        ),
        types.Tool(
            name="create_execution_plan",
            description="ì‘ì—… ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "operation": {"type": "string", "description": "ì‹¤í–‰í•  ì‘ì—…ëª…"},
                    "parameters": {"type": "object", "description": "ì‘ì—… ë§¤ê°œë³€ìˆ˜"},
                },
                "required": ["operation"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="confirm_and_execute",
            description="ê³„íš í™•ì¸ í›„ ì‹¤í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "confirmation": {
                        "type": "string",
                        "description": "ì‹¤í–‰ í™•ì¸ (y/yes/n/no)",
                    }
                },
                "required": ["confirmation"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="analyze_slow_queries",
            description="ëŠë¦° ì¿¼ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "limit": {
                        "type": "integer",
                        "description": "ì¡°íšŒí•  ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)",
                        "default": 10,
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_io_stats",
            description="í…Œì´ë¸”ë³„ I/O í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "schema_name": {
                        "type": "string",
                        "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: í˜„ì¬ DB)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_index_usage_stats",
            description="ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "í…Œì´ë¸” ì´ë¦„ (ì„ íƒì‚¬í•­, ì „ì²´ ì¡°íšŒì‹œ ìƒëµ)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_connection_stats",
            description="ì—°ê²° ë° ì„¸ì…˜ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_memory_usage",
            description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_lock_analysis",
            description="ë½ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_replication_status",
            description="ë³µì œ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="list_aurora_mysql_clusters",
            description="í˜„ì¬ ë¦¬ì „ì˜ Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    }
                },
            },
        ),
        types.Tool(
            name="select_aurora_cluster",
            description="Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_selection": {
                        "type": "string",
                        "description": "í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                },
                "required": ["cluster_selection"],
            },
        ),
        types.Tool(
            name="list_databases",
            description="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="select_database",
            description="ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë³€ê²½í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "database_selection"],
            },
        ),
        types.Tool(
            name="validate_sql_with_database",
            description="ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì§€ì •í•˜ì—¬ SQL íŒŒì¼ì„ ì™„ì „ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["filename", "database_secret"],
            },
        ),
        types.Tool(
            name="validate_dml_performance",
            description="DML ì¿¼ë¦¬(SELECT, UPDATE, DELETE, INSERT)ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ë¶„ì„í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["filename", "database_secret"],
            },
        ),
        types.Tool(
            name="validate_multiple_dml_files",
            description="ì—¬ëŸ¬ DML íŒŒì¼ì„ ì¼ê´„ ì„±ëŠ¥ ë¶„ì„í•˜ê³  í†µí•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "sql_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ë¶„ì„í•  DML SQL íŒŒì¼ëª… ëª©ë¡ (ìµœëŒ€ 10ê°œ)",
                    },
                },
                "required": ["database_secret", "sql_files"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await ddl_validator.list_sql_files()
        elif name == "list_database_secrets":
            result = await ddl_validator.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "validate_sql_file":
            # database_secretì´ ì—†ìœ¼ë©´ ì„ íƒ ì˜µì…˜ ì œê³µ
            filename = arguments["filename"]
            database_secret = arguments.get("database_secret")

            if not database_secret:
                # SQL íŒŒì¼ ì½ê¸°
                sql_file_path = SQL_DIR / filename
                if not sql_file_path.exists():
                    result = f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
                else:
                    with open(sql_file_path, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ í”„ë¡¬í”„íŠ¸ í‘œì‹œ
                    result = await ddl_validator.prompt_for_database_selection(
                        ddl_content, filename
                    )
            else:
                # ê¸°ì¡´ ê³„íš ìƒì„± ë°©ì‹
                plan = await ddl_validator.create_validation_plan(
                    filename, database_secret
                )
                plan_display = ddl_validator._format_validation_plan_display(plan)

                result = f"""ğŸ“‹ **DDL ê²€ì¦ ì‹¤í–‰ ê³„íš:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ê²€ì¦ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**
   â€¢ 'y' ë˜ëŠ” 'yes': ê²€ì¦ ì‹¤í–‰
   â€¢ 'n' ë˜ëŠ” 'no': ê²€ì¦ ì·¨ì†Œ

ğŸ’¡ **ì°¸ê³ :** confirm_and_execute ë„êµ¬ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

                ddl_validator.current_plan = plan
        elif name == "test_database_connection":
            result = await ddl_validator.execute_with_auto_plan(
                "test_database_connection", database_secret=arguments["database_secret"]
            )
        elif name == "validate_all_sql":
            # ìë™ ê³„íš ìƒì„± ë° ì‹¤í–‰
            result = await ddl_validator.execute_with_auto_plan(
                "validate_all_sql", database_secret=arguments.get("database_secret")
            )
        elif name == "validate_selected_sql_files":
            # ì„ íƒí•œ SQL íŒŒì¼ë“¤ ê²€ì¦
            result = await ddl_validator.validate_selected_sql_files(
                arguments["database_secret"], arguments["sql_files"]
            )
        elif name == "validate_multiple_sql_direct":
            # ì—¬ëŸ¬ SQL íŒŒì¼ ì§ì ‘ ê²€ì¦
            result = await ddl_validator.validate_multiple_sql_direct(
                arguments["database_secret"], arguments.get("file_count", 10)
            )
        elif name == "copy_sql_to_directory":
            result = await ddl_validator.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "analyze_current_schema":
            result = await ddl_validator.execute_with_auto_plan(
                "analyze_current_schema", database_secret=arguments["database_secret"]
            )
        elif name == "check_ddl_conflicts":
            result = await ddl_validator.execute_with_auto_plan(
                "check_ddl_conflicts",
                ddl_content=arguments["ddl_content"],
                database_secret=arguments["database_secret"],
            )
        elif name == "get_schema_summary":
            result = await ddl_validator.execute_with_auto_plan(
                "get_schema_summary", database_secret=arguments["database_secret"]
            )
        elif name == "get_aurora_mysql_parameters":
            result = await ddl_validator.get_aurora_mysql_parameters(
                cluster_identifier=arguments["cluster_identifier"],
                region=arguments.get("region", "ap-northeast-2"),
                filter_type=arguments.get("filter_type", "important"),
                category=arguments.get("category", "all"),
            )
        elif name == "create_execution_plan":
            plan = await ddl_validator.create_execution_plan(
                arguments["operation"], **arguments.get("parameters", {})
            )
            result = ddl_validator._format_plan_display(plan)
        elif name == "confirm_and_execute":
            result = await ddl_validator.confirm_and_execute(arguments["confirmation"])
        elif name == "get_performance_metrics":
            result = await ddl_validator.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "analyze_slow_queries":
            result = await ddl_validator.analyze_slow_queries(
                arguments["database_secret"], arguments.get("limit", 10)
            )
        elif name == "get_table_io_stats":
            result = await ddl_validator.get_table_io_stats(
                arguments["database_secret"], arguments.get("schema_name")
            )
        elif name == "get_index_usage_stats":
            result = await ddl_validator.get_index_usage_stats(
                arguments["database_secret"], arguments.get("table_name")
            )
        elif name == "get_connection_stats":
            result = await ddl_validator.get_connection_stats(
                arguments["database_secret"]
            )
        elif name == "get_memory_usage":
            result = await ddl_validator.get_memory_usage(arguments["database_secret"])
        elif name == "get_lock_analysis":
            result = await ddl_validator.get_lock_analysis(arguments["database_secret"])
        elif name == "get_replication_status":
            result = await ddl_validator.get_replication_status(
                arguments["database_secret"]
            )
        elif name == "list_aurora_mysql_clusters":
            result = await ddl_validator.list_aurora_mysql_clusters(
                arguments.get("region", "ap-northeast-2")
            )
        elif name == "select_aurora_cluster":
            result = await ddl_validator.select_aurora_cluster(
                arguments["cluster_selection"],
                arguments.get("region", "ap-northeast-2"),
            )
        elif name == "list_databases":
            result = await ddl_validator.list_databases(arguments["database_secret"])
        elif name == "select_database":
            result = await ddl_validator.select_database(
                arguments["database_secret"], arguments["database_selection"]
            )
        elif name == "validate_sql_with_database":
            result = await ddl_validator.execute_validation_workflow(
                ddl_content="",  # íŒŒì¼ì—ì„œ ì½ì–´ì˜¬ ì˜ˆì •
                database_secret=arguments["database_secret"],
                filename=arguments["filename"],
            )
            # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì½ì–´ì„œ ì²˜ë¦¬í•´ì•¼ í•¨
            sql_file_path = SQL_DIR / arguments["filename"]
            if sql_file_path.exists():
                with open(sql_file_path, "r", encoding="utf-8") as f:
                    ddl_content = f.read()
                result = await ddl_validator.execute_validation_workflow(
                    ddl_content, arguments["database_secret"], arguments["filename"]
                )
            else:
                result = f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arguments['filename']}"
        elif name == "validate_dml_performance":
            # DML ì„±ëŠ¥ ë¶„ì„
            sql_file_path = SQL_DIR / arguments["filename"]
            if sql_file_path.exists():
                with open(sql_file_path, "r", encoding="utf-8") as f:
                    sql_content = f.read()
                result = await ddl_validator.execute_dml_validation_workflow(
                    sql_content, arguments["database_secret"], arguments["filename"]
                )
            else:
                result = f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arguments['filename']}"
        elif name == "validate_multiple_dml_files":
            # ì—¬ëŸ¬ DML íŒŒì¼ ì¼ê´„ ì„±ëŠ¥ ë¶„ì„
            result = await ddl_validator.validate_multiple_dml_files(
                arguments["database_secret"], arguments["sql_files"]
            )
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="ddl-qcli-validator",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())

#!/usr/bin/env python3
"""
DB Assistant Amazon Q CLI MCP ì„œë²„ 
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import sys 

import boto3
from botocore.exceptions import ClientError
try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

# ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pandas as pd
    import numpy as np
    import matplotlib
    matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ matplotlib ì‚¬ìš©
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.impute import SimpleImputer
    ANALYSIS_AVAILABLE = True
except ImportError:
    ANALYSIS_AVAILABLE = False

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"
DATA_DIR = CURRENT_DIR / "data"

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)

class DBAssistantMCPServer:
    def __init__(self):
        self.bedrock_client = boto3.client(
            "bedrock-runtime", region_name="us-west-2", verify=False
        )
        self.knowledge_base_id = "0WQUBRHVR8"
        self.selected_database = None
        self.current_plan = None
        # ë¶„ì„ ê´€ë ¨ ì´ˆê¸°í™”
        self.cloudwatch = None
        self.default_metrics = [
            'CPUUtilization', 'DatabaseConnections', 'DBLoad', 'DBLoadCPU', 
            'DBLoadNonCPU', 'FreeableMemory', 'ReadIOPS', 'WriteIOPS',
            'ReadLatency', 'WriteLatency', 'NetworkReceiveThroughput',
            'NetworkTransmitThroughput', 'BufferCacheHitRatio'
        ]

    def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            
            all_secrets = []
            next_token = None
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()
                
                all_secrets.extend([secret["Name"] for secret in response["SecretList"]])
                
                if 'NextToken' not in response:
                    break
                next_token = response['NextToken']
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time
            
            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            
            # SSH í„°ë„ ì‹œì‘
            ssh_command = [
                "ssh",
                "-F", "/dev/null",
                "-o", "UserKnownHostsFile=/dev/null",
                "-o", "StrictHostKeyChecking=no",
                "-i", "/Users/heungh/test.pem",
                "-f", "-N",
                "-L", f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255"
            ]
            
            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")
            
            process = subprocess.run(ssh_command, capture_output=True, text=True)
            
            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)
            
            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            import subprocess
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def get_db_connection(self, database_secret: str, selected_database: str = None, use_ssh_tunnel: bool = True):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        if mysql is None:
            raise Exception("mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        session = boto3.session.Session()
        client = session.client(
            service_name="secretsmanager",
            region_name="ap-northeast-2",
            verify=False,
        )
        get_secret_value_response = client.get_secret_value(SecretId=database_secret)
        secret = get_secret_value_response["SecretString"]
        db_config = json.loads(secret)
        
        connection_config = None
        tunnel_used = False
        
        # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        database_name = selected_database or db_config.get('dbname', db_config.get('database'))
        
        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(db_config.get('host')):
                connection_config = {
                    'host': 'localhost',
                    'port': 3307,
                    'user': db_config.get('username'),
                    'password': db_config.get('password'),
                    'database': database_name,
                    'connection_timeout': 10
                }
                tunnel_used = True
        
        if not connection_config:
            connection_config = {
                'host': db_config.get('host'),
                'port': db_config.get('port', 3306),
                'user': db_config.get('username'),
                'password': db_config.get('password'),
                'database': database_name,
                'connection_timeout': 10
            }
        
        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            secrets = self.get_secrets_by_keyword(keyword)
            if not secrets:
                return f"'{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤." if keyword else "ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
            
            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:\n{secret_list}"
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def test_database_connection(self, database_secret: str, use_ssh_tunnel: bool = True) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel)
            
            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]
                
                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]
                
                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]
                
                cursor.close()
                connection.close()
                
                result = f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- ì„œë²„ ë²„ì „: {db_info}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}
- ì—°ê²° ë°©ì‹: {'SSH Tunnel' if tunnel_used else 'Direct'}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**"""
                for db in databases:
                    if db not in ['information_schema', 'performance_schema', 'mysql', 'sys']:
                        result += f"\n   - {db}"
                
                if tables:
                    result += f"\n\n**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**"
                    for table in tables:
                        result += f"\n   - {table}"
                
                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                
                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                
        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ MySQL ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"

    async def list_databases(self, database_secret: str, use_ssh_tunnel: bool = True) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            if mysql is None:
                raise Exception("mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=database_secret)
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)
            
            connection_config = None
            tunnel_used = False
            
            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get('host')):
                    connection_config = {
                        'host': 'localhost',
                        'port': 3307,
                        'user': db_config.get('username'),
                        'password': db_config.get('password'),
                        'connection_timeout': 10
                    }
                    tunnel_used = True
            
            if not connection_config:
                connection_config = {
                    'host': db_config.get('host'),
                    'port': db_config.get('port', 3306),
                    'user': db_config.get('username'),
                    'password': db_config.get('password'),
                    'connection_timeout': 10
                }
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´ ì—°ê²°
            connection = mysql.connector.connect(**connection_config)
            cursor = connection.cursor()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = [db[0] for db in cursor.fetchall() if db[0] not in ['information_schema', 'performance_schema', 'mysql', 'sys']]
            
            cursor.close()
            connection.close()
            
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            result = "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:\n\n"
            for i, db in enumerate(databases, 1):
                result += f"{i}. {db}\n"
            result += f"\nì´ {len(databases)}ê°œì˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤."
            result += "\n\nğŸ’¡ íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•˜ë ¤ë©´ ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            
            return result
            
        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_database(self, database_secret: str, database_selection: str, use_ssh_tunnel: bool = True) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ (USE ëª…ë ¹ì–´ ì‹¤í–‰)"""
        try:
            # ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìœ íš¨ì„± ê²€ì¦
            db_list_result = await self.list_databases(database_secret, use_ssh_tunnel)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì—ì„œ ì‹¤ì œ DB ì´ë¦„ë“¤ ì¶”ì¶œ
            lines = db_list_result.split('\n')
            databases = []
            for line in lines:
                if line.strip() and line[0].isdigit() and '. ' in line:
                    db_name = line.split('. ', 1)[1]
                    databases.append(db_name)
            
            selected_db = None
            
            # ë²ˆí˜¸ë¡œ ì„ íƒí•œ ê²½ìš°
            if database_selection.isdigit():
                index = int(database_selection) - 1
                if 0 <= index < len(databases):
                    selected_db = databases[index]
                else:
                    return f"âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(databases)} ë²”ìœ„ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.\n\n{db_list_result}"
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒí•œ ê²½ìš°
                if database_selection in databases:
                    selected_db = database_selection
                else:
                    return f"âŒ '{database_selection}' ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n{db_list_result}"
            
            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ë¡œ USE ëª…ë ¹ì–´ ì‹¤í–‰
            connection, tunnel_used = await self.get_db_connection(database_secret, None, use_ssh_tunnel)
            
            if connection.is_connected():
                cursor = connection.cursor()
                
                # USE ëª…ë ¹ì–´ ì‹¤í–‰
                cursor.execute(f"USE `{selected_db}`")
                
                # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]
                
                cursor.close()
                connection.close()
                
                # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                self.selected_database = selected_db
                
                result = f"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{selected_db}' ì„ íƒ ì™„ë£Œ!\n\n"
                result += f"ğŸ”— í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: {current_db}\n"
                result += f"ğŸ’¡ ì´ì œ ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ìŠ¤í‚¤ë§ˆ ë¶„ì„ì´ë‚˜ SQL ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                
                return result
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì˜¤ë¥˜: {e}")
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°í•´ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database)
            cursor = connection.cursor()
            
            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            
            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute("""
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """)
            
            tables_info = cursor.fetchall()
            
            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {current_db})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(tables_info)}ê°œ):"""
            
            for table_info in tables_info:
                table_name = table_info[0]
                table_type = table_info[1]
                engine = table_info[2]
                rows = table_info[3] or 0
                comment = table_info[6] or ""
                
                # ì»¬ëŸ¼ ìˆ˜ ì¡°íšŒ
                cursor.execute("""
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """, (table_name,))
                column_count = cursor.fetchone()[0]
                
                # ì¸ë±ìŠ¤ ìˆ˜ ì¡°íšŒ
                cursor.execute("""
                    SELECT COUNT(DISTINCT index_name) FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """, (table_name,))
                index_count = cursor.fetchone()[0]
                
                summary += f"""
  ğŸ”¹ **{table_name}** ({engine})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {rows:,}"""
                
                if comment:
                    summary += f"\n     - ì„¤ëª…: {comment}"
            
            cursor.close()
            connection.close()
            
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            return summary
            
        except Exception as e:
            return f"âŒ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def get_table_schema(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database)
            cursor = connection.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """, (table_name,))
            
            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            cursor.execute("""
                SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT, 
                       COLUMN_COMMENT, COLUMN_KEY, EXTRA
                FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY ORDINAL_POSITION
            """, (table_name,))
            
            columns = cursor.fetchall()
            
            result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ìŠ¤í‚¤ë§ˆ ì •ë³´**\n\n"
            result += f"ğŸ“Š **ì»¬ëŸ¼ ëª©ë¡** ({len(columns)}ê°œ):\n"
            
            for col in columns:
                col_name, data_type, is_nullable, default_val, comment, key, extra = col
                
                result += f"\nğŸ”¹ **{col_name}**\n"
                result += f"   - íƒ€ì…: {data_type}\n"
                result += f"   - NULL í—ˆìš©: {'ì˜ˆ' if is_nullable == 'YES' else 'ì•„ë‹ˆì˜¤'}\n"
                
                if default_val is not None:
                    result += f"   - ê¸°ë³¸ê°’: {default_val}\n"
                
                if key:
                    key_type = {"PRI": "ê¸°ë³¸í‚¤", "UNI": "ê³ ìœ í‚¤", "MUL": "ì¸ë±ìŠ¤"}.get(key, key)
                    result += f"   - í‚¤ íƒ€ì…: {key_type}\n"
                
                if extra:
                    result += f"   - ì¶”ê°€ ì†ì„±: {extra}\n"
                
                if comment:
                    result += f"   - ì„¤ëª…: {comment}\n"
            
            cursor.close()
            connection.close()
            
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            return result
            
        except Exception as e:
            return f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_table_index(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database)
            cursor = connection.cursor()
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """, (table_name,))
            
            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            cursor.execute("""
                SELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, NON_UNIQUE, 
                       INDEX_TYPE, CARDINALITY, NULLABLE, INDEX_COMMENT
                FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY INDEX_NAME, SEQ_IN_INDEX
            """, (table_name,))
            
            indexes = cursor.fetchall()
            
            if not indexes:
                result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\nâŒ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\n"
                
                # ì¸ë±ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                index_groups = {}
                for idx in indexes:
                    idx_name = idx[0]
                    if idx_name not in index_groups:
                        index_groups[idx_name] = []
                    index_groups[idx_name].append(idx)
                
                result += f"ğŸ“Š **ì¸ë±ìŠ¤ ëª©ë¡** ({len(index_groups)}ê°œ):\n"
                
                for idx_name, idx_cols in index_groups.items():
                    first_col = idx_cols[0]
                    is_unique = "ê³ ìœ " if first_col[3] == 0 else "ì¼ë°˜"
                    idx_type = first_col[4]
                    comment = first_col[7] or ""
                    
                    result += f"\nğŸ”¹ **{idx_name}** ({is_unique} ì¸ë±ìŠ¤)\n"
                    result += f"   - íƒ€ì…: {idx_type}\n"
                    
                    # ì»¬ëŸ¼ ëª©ë¡
                    columns = [f"{col[1]}" for col in idx_cols]
                    result += f"   - ì»¬ëŸ¼: {', '.join(columns)}\n"
                    
                    if comment:
                        result += f"   - ì„¤ëª…: {comment}\n"
            
            cursor.close()
            connection.close()
            
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            return result
            
        except Exception as e:
            return f"âŒ í…Œì´ë¸” ì¸ë±ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_performance_metrics(self, database_secret: str, metric_type: str = "all") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database)
            cursor = connection.cursor()
            
            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"
            
            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute("""
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """)
                
                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (pattern, count, avg_time, max_time, total_time) in enumerate(query_stats, 1):
                        pattern_short = (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"
            
            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """)
                
                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"
            
            cursor.close()
            connection.close()
            
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            return result
            
        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    # === DDL ê²€ì¦ ê´€ë ¨ ë©”ì„œë“œ ===
    
    async def validate_sql_file(self, filename: str, database_secret: Optional[str] = None) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, 'r', encoding='utf-8') as f:
                ddl_content = f.read()

            result = await self.validate_ddl(ddl_content, database_secret, filename)
            return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def validate_ddl(self, ddl_content: str, database_secret: Optional[str], filename: str) -> str:
        """DDL ê²€ì¦ ì‹¤í–‰"""
        try:
            # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
            debug_log_path = OUTPUT_DIR / f"debug_log_{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            def debug_log(message):
                with open(debug_log_path, 'a', encoding='utf-8') as f:
                    f.write(f"{datetime.now().strftime('%H:%M:%S')} - {message}\n")
                    f.flush()
            
            debug_log(f"validate_ddl ì‹œì‘ - íŒŒì¼: {filename}")
            debug_log(f"DDL ë‚´ìš©: {ddl_content.strip()}")
            
            issues = []
            db_connection_info = None
            schema_validation = None
            constraint_validation = None
            
            # 1. ê¸°ë³¸ ë¬¸ë²• ê²€ì¦
            if not ddl_content.strip().endswith(";"):
                issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ ì‹¤íŒ¨")
            else:
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ í†µê³¼")
            
            # 2. DDL íƒ€ì… í™•ì¸
            ddl_type = self.extract_ddl_type(ddl_content)
            debug_log(f"DDL íƒ€ì…: {ddl_type}")
            
            # 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ (database_secretì´ ì œê³µëœ ê²½ìš°)
            if database_secret:
                debug_log(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘: {database_secret}")
                try:
                    db_connection_info = await self.test_database_connection_for_validation(database_secret)
                    debug_log(f"DB ì—°ê²° ê²°ê³¼: {db_connection_info['success']}")
                    
                    if not db_connection_info["success"]:
                        issues.append(f"DB ì—°ê²° ì‹¤íŒ¨: {db_connection_info['error']}")
                        debug_log(f"DB ì—°ê²° ì‹¤íŒ¨: {db_connection_info['error']}")
                    else:
                        debug_log(f"DB ì—°ê²° ì„±ê³µ, DDL íƒ€ì… ì²´í¬: {ddl_type}")
                        # DDL êµ¬ë¬¸ì— ëŒ€í•´ì„œë§Œ ìŠ¤í‚¤ë§ˆ/ì œì•½ì¡°ê±´ ê²€ì¦ ìˆ˜í–‰
                        if ddl_type in ["CREATE_TABLE", "ALTER_TABLE", "CREATE_INDEX", "DROP"]:
                            debug_log("ìŠ¤í‚¤ë§ˆ ê²€ì¦ ëŒ€ìƒ DDL íƒ€ì…")
                            # 4. ìŠ¤í‚¤ë§ˆ ê²€ì¦
                            try:
                                debug_log("ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")
                                schema_validation = await self.validate_schema_with_debug(ddl_content, database_secret, debug_log)
                                debug_log(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: success={schema_validation['success']}")
                                
                                if schema_validation["success"]:
                                    debug_log(f"ê²€ì¦ ê²°ê³¼ ê°œìˆ˜: {len(schema_validation['validation_results'])}")
                                    for i, result in enumerate(schema_validation["validation_results"]):
                                        debug_log(f"ê²°ê³¼ [{i}]: {result}")
                                        if result.get("issues") and len(result["issues"]) > 0:
                                            debug_log(f"ì´ìŠˆ ë°œê²¬: {result['issues']}")
                                            issues.extend([f"ìŠ¤í‚¤ë§ˆ ê²€ì¦: {issue}" for issue in result["issues"]])
                                        else:
                                            debug_log("ì´ìŠˆ ì—†ìŒ")
                                else:
                                    issues.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {schema_validation['error']}")
                                    debug_log(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {schema_validation['error']}")
                            except Exception as e:
                                logger.error(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
                                issues.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                debug_log(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜ˆì™¸: {e}")
                        else:
                            # SELECT, SHOW ë“±ì˜ ì¿¼ë¦¬ë¬¸ì€ ê¸°ë³¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰
                            logger.info(f"ì¿¼ë¦¬ë¬¸ ({ddl_type}) ê°ì§€: ìŠ¤í‚¤ë§ˆ/ì œì•½ì¡°ê±´ ê²€ì¦ ê±´ë„ˆëœ€")
                            debug_log(f"ì¿¼ë¦¬ë¬¸ ê°ì§€, ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê±´ë„ˆëœ€: {ddl_type}")
                            
                except Exception as e:
                    logger.error(f"DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
                    issues.append(f"DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    debug_log(f"DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜ˆì™¸: {e}")
            else:
                debug_log("database_secret ì—†ìŒ, DB ê²€ì¦ ê±´ë„ˆëœ€")
            
            # ê²€ì¦ ì™„ë£Œ
            debug_log(f"ìµœì¢… ì´ìŠˆ ê°œìˆ˜: {len(issues)}")
            debug_log(f"ì´ìŠˆ ëª©ë¡: {issues}")
            
            # ê²°ê³¼ ìƒì„±
            if not issues:
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
            else:
                summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ"
                status = "FAIL"
            
            debug_log(f"ìµœì¢… ìƒíƒœ: {status}, ìš”ì•½: {summary}")
            
            # ë³´ê³ ì„œ ìƒì„± (HTML í˜•ì‹)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"
            
            # HTML ë³´ê³ ì„œ ìƒì„±
            await self.generate_html_report(report_path, filename, ddl_content, ddl_type, 
                                          status, summary, issues, db_connection_info, 
                                          schema_validation, constraint_validation, database_secret)
            
            return f"{summary}\n\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}\nğŸ” ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"
            
        except Exception as e:
            return f"DDL ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def extract_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ì¶”ì¶œ"""
        # ì£¼ì„ê³¼ ë¹ˆ ì¤„ì„ ì œê±°í•˜ê³  ì‹¤ì œ DDL êµ¬ë¬¸ë§Œ ì¶”ì¶œ
        lines = ddl_content.strip().split('\n')
        ddl_lines = []
        
        for line in lines:
            line = line.strip()
            # ì£¼ì„ ë¼ì¸ì´ë‚˜ ë¹ˆ ë¼ì¸ ê±´ë„ˆë›°ê¸°
            if line and not line.startswith('--') and not line.startswith('#'):
                ddl_lines.append(line)
        
        if not ddl_lines:
            return "UNKNOWN"
        
        # ì²« ë²ˆì§¸ ìœ íš¨í•œ DDL ë¼ì¸ìœ¼ë¡œ íƒ€ì… íŒë‹¨
        ddl_upper = ' '.join(ddl_lines).upper().strip()
        
        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP TABLE"):
            return "DROP TABLE"
        elif ddl_upper.startswith("DROP INDEX"):
            return "DROP INDEX"
        elif ddl_upper.startswith("USE "):
            return "USE"
        elif ddl_upper.startswith("SHOW "):
            return "SHOW"
        elif ddl_upper.startswith("SELECT"):
            return "SELECT"
        else:
            return "UNKNOWN"

    def detect_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ê°ì§€"""
        ddl_upper = ddl_content.upper().strip()
        
        if ddl_upper.startswith('CREATE TABLE'):
            return 'CREATE_TABLE'
        elif ddl_upper.startswith('ALTER TABLE'):
            return 'ALTER_TABLE'
        elif ddl_upper.startswith('DROP TABLE'):
            return 'DROP_TABLE'
        elif ddl_upper.startswith('CREATE INDEX'):
            return 'CREATE_INDEX'
        elif ddl_upper.startswith('DROP INDEX'):
            return 'DROP_INDEX'
        elif ddl_upper.startswith('INSERT'):
            return 'INSERT'
        elif ddl_upper.startswith('UPDATE'):
            return 'UPDATE'
        elif ddl_upper.startswith('DELETE'):
            return 'DELETE'
        elif ddl_upper.startswith('SELECT'):
            return 'SELECT'
        else:
            return 'UNKNOWN'

    async def validate_with_claude(self, ddl_content: str) -> str:
        """
        Claude cross-region í”„ë¡œíŒŒì¼ì„ í™œìš©í•œ DDL ê²€ì¦ (Sonnet 4 â†’ 3.7 fallback)
        """
        prompt = f"""
        ë‹¤ìŒ DDL ë¬¸ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        ë¬¸ë²• ì˜¤ë¥˜, í‘œì¤€ ê·œì¹™ ìœ„ë°˜, ì„±ëŠ¥ ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
        ë¬¸ì œê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•´ì£¼ì„¸ìš”. ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ê²€ì¦ í†µê³¼"ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        claude_input = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1024,
            "messages": [
                {"role": "user", "content": [{"type": "text", "text": prompt}]}
            ],
            "temperature": 0.3,
        })
        
        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile í˜¸ì¶œ
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id,
                body=claude_input
            )
            response_body = json.loads(response.get("body").read())
            return response_body.get("content", [{}])[0].get("text", "")
        except Exception as e:
            logger.warning(f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}")
            # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id,
                    body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                return response_body.get("content", [{}])[0].get("text", "")
            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def test_database_connection_for_validation(self, database_secret: str, use_ssh_tunnel: bool = True) -> Dict[str, Any]:
        """ê²€ì¦ìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel)
            
            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]
                
                cursor.close()
                connection.close()
                
                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "host": "localhost" if tunnel_used else "remote",
                    "port": 3307 if tunnel_used else 3306
                }
                
                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                
                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {
                    "success": False,
                    "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }
                
        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"MySQL ì˜¤ë¥˜: {str(e)}"
            }
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"
            }

    async def validate_schema_with_debug(self, ddl_content: str, database_secret: str, debug_log, use_ssh_tunnel: bool = True) -> Dict[str, Any]:
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        try:
            debug_log("validate_schema ì‹œì‘")
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed_with_debug(ddl_content, debug_log)
            debug_log(f"íŒŒì‹±ëœ DDL ì •ë³´: {ddl_info}")
            
            if not ddl_info:
                debug_log("DDL íŒŒì‹± ì‹¤íŒ¨")
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel)
            cursor = connection.cursor()
            debug_log(f"DB ì—°ê²° ì„±ê³µ, í„°ë„ ì‚¬ìš©: {tunnel_used}")
            
            validation_results = []
            
            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement['type']
                table_name = ddl_statement['table']
                debug_log(f"ê²€ì¦ ì¤‘: {ddl_type} on {table_name}")
                
                if ddl_type == 'CREATE_TABLE':
                    debug_log("CREATE_TABLE ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_create_table_with_debug(cursor, ddl_statement, debug_log)
                    debug_log(f"CREATE_TABLE ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == 'ALTER_TABLE':
                    debug_log("ALTER_TABLE ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_alter_table_with_debug(cursor, ddl_statement, debug_log)
                    debug_log(f"ALTER_TABLE ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == 'CREATE_INDEX':
                    result = await self.validate_create_index(cursor, ddl_statement)
                elif ddl_type == 'DROP_TABLE':
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == 'CREATE_INDEX':
                    debug_log("CREATE_INDEX ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_create_index_with_debug(cursor, ddl_statement, debug_log)
                    debug_log(f"CREATE_INDEX ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == 'DROP_INDEX':
                    debug_log("DROP_INDEX ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_drop_index_with_debug(cursor, ddl_statement, debug_log)
                    debug_log(f"DROP_INDEX ê²€ì¦ ê²°ê³¼: {result}")
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"]
                    }
                
                validation_results.append(result)
                debug_log(f"ê²€ì¦ ê²°ê³¼ ì¶”ê°€ë¨: {result}")
            
            cursor.close()
            connection.close()
            
            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            debug_log(f"validate_schema ì™„ë£Œ, ê²°ê³¼ ê°œìˆ˜: {len(validation_results)}")
            return {
                "success": True,
                "validation_results": validation_results
            }
            
        except Exception as e:
            debug_log(f"validate_schema ì˜ˆì™¸: {e}")
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"
            }

    def parse_ddl_detailed_with_debug(self, ddl_content: str, debug_log) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ (ë””ë²„ê·¸ ë²„ì „)"""
        ddl_statements = []
        
        debug_log(f"DDL íŒŒì‹± ì‹œì‘: {repr(ddl_content)}")
        
        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?'
        create_table_matches = re.findall(create_table_pattern, ddl_content, re.IGNORECASE)
        
        debug_log(f"CREATE TABLE íŒŒì‹± - ê²°ê³¼: {create_table_matches}")
        
        for table_name in create_table_matches:
            ddl_statements.append({
                'type': 'CREATE_TABLE',
                'table': table_name.lower()
            })
            debug_log(f"CREATE TABLE êµ¬ë¬¸ ì¶”ê°€ë¨: {table_name}")
        
        # ALTER TABLE íŒŒì‹±
        alter_table_pattern = r'ALTER\s+TABLE\s+`?(\w+)`?'
        alter_table_matches = re.findall(alter_table_pattern, ddl_content, re.IGNORECASE)
        
        debug_log(f"ALTER TABLE íŒŒì‹± - ê²°ê³¼: {alter_table_matches}")
        
        for table_name in alter_table_matches:
            ddl_statements.append({
                'type': 'ALTER_TABLE',
                'table': table_name.lower()
            })
            debug_log(f"ALTER TABLE êµ¬ë¬¸ ì¶”ê°€ë¨: {table_name}")
        
        # CREATE INDEX íŒŒì‹±
        create_index_pattern = r'CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\((.*?)\)'
        create_index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)
        
        debug_log(f"CREATE INDEX íŒŒì‹± - ê²°ê³¼: {create_index_matches}")
        
        for index_name, table_name, columns in create_index_matches:
            ddl_statements.append({
                'type': 'CREATE_INDEX',
                'table': table_name.lower(),
                'index_name': index_name.lower(),
                'columns': columns.strip()
            })
            debug_log(f"CREATE INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {table_name}({columns})")

        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r'DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?'
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)
        
        debug_log(f"DROP INDEX íŒŒì‹± - íŒ¨í„´: {drop_index_pattern}")
        debug_log(f"DROP INDEX íŒŒì‹± - ê²°ê³¼: {drop_index_matches}")
        
        for index_name, table_name in drop_index_matches:
            ddl_statements.append({
                'type': 'DROP_INDEX',
                'table': table_name.lower(),
                'index_name': index_name.lower()
            })
            debug_log(f"DROP INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {table_name}")
        
        debug_log(f"ì „ì²´ íŒŒì‹± ê²°ê³¼: {len(ddl_statements)}ê°œ êµ¬ë¬¸")
        for i, stmt in enumerate(ddl_statements):
            debug_log(f"  [{i}] {stmt['type']}: {stmt}")
        
        return ddl_statements

    async def validate_create_index_with_debug(self, cursor, ddl_statement: Dict[str, Any], debug_log) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement['table']
        index_name = ddl_statement['index_name']
        columns = ddl_statement['columns']
        
        debug_log(f"CREATE INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}, columns={columns}")
        
        issues = []
        
        # 1. í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            cursor.execute("SHOW TABLES LIKE %s", (table_name,))
            table_exists = cursor.fetchone() is not None
            debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
            
            if not table_exists:
                issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # 2. ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                cursor.execute("SHOW INDEX FROM `{}`".format(table_name))
                existing_indexes = cursor.fetchall()
                existing_index_names = [idx[2] for idx in existing_indexes]  # Key_name
                debug_log(f"ê¸°ì¡´ ì¸ë±ìŠ¤: {existing_index_names}")
                
                if index_name in existing_index_names:
                    issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                
                # 3. ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute("DESCRIBE `{}`".format(table_name))
                table_columns = cursor.fetchall()
                existing_columns = [col[0] for col in table_columns]  # Field name
                debug_log(f"í…Œì´ë¸” ì»¬ëŸ¼: {existing_columns}")
                
                # ì¸ë±ìŠ¤ ì»¬ëŸ¼ íŒŒì‹± (email, name ë“±)
                index_columns = [col.strip().strip('`') for col in columns.split(',')]
                debug_log(f"ì¸ë±ìŠ¤ ì»¬ëŸ¼: {index_columns}")
                
                for col in index_columns:
                    if col not in existing_columns:
                        issues.append(f"ì»¬ëŸ¼ '{col}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        
        except Exception as e:
            debug_log(f"CREATE INDEX ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            issues.append(f"CREATE INDEX ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        result = {
            'valid': len(issues) == 0,
            'issues': issues,
            'table_name': table_name,
            'index_name': index_name,
            'columns': columns
        }
        
        debug_log(f"CREATE INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}")
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")
        
        return result

    async def validate_drop_index_with_debug(self, cursor, ddl_statement: Dict[str, Any], debug_log) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement['table']
        index_name = ddl_statement['index_name']
        
        debug_log(f"DROP INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}")
        
        issues = []
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” '{table_name}' ì¡´ì¬ ì—¬ë¶€: {table_exists}")
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            debug_log(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """, (table_name, index_name))
            
            index_exists = cursor.fetchone()[0] > 0
            debug_log(f"ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬ ì—¬ë¶€: {index_exists}")
            
            if not index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                debug_log(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")
        
        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "index_name": index_name,
                "table_exists": table_exists
            }
        } #test
        debug_log(f"DROP INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}")
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")
        
        return result

    async def validate_create_table_with_debug(self, cursor, ddl_statement: Dict[str, Any], debug_log) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement['table']
        
        debug_log(f"CREATE TABLE ê²€ì¦ ì‹œì‘: table={table_name}")
        
        issues = []
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
        
        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        result = {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists
            }
        }
        
        debug_log(f"CREATE TABLE ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}")
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")
        
        return result

    async def validate_alter_table_with_debug(self, cursor, ddl_statement: Dict[str, Any], debug_log) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement['table']
        
        debug_log(f"ALTER TABLE ê²€ì¦ ì‹œì‘: table={table_name}")
        
        issues = []
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        result = {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists
            }
        }
        
        debug_log(f"ALTER TABLE ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}")
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")
        
        return result
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        try:
            print(f"[DEBUG] validate_schema ì‹œì‘")
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed(ddl_content)
            print(f"[DEBUG] íŒŒì‹±ëœ DDL ì •ë³´: {ddl_info}")
            
            if not ddl_info:
                print(f"[DEBUG] DDL íŒŒì‹± ì‹¤íŒ¨")
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel)
            cursor = connection.cursor()
            print(f"[DEBUG] DB ì—°ê²° ì„±ê³µ, í„°ë„ ì‚¬ìš©: {tunnel_used}")
            
            validation_results = []
            
            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement['type']
                table_name = ddl_statement['table']
                print(f"[DEBUG] ê²€ì¦ ì¤‘: {ddl_type} on {table_name}")
                
                if ddl_type == 'CREATE_TABLE':
                    result = await self.validate_create_table(cursor, ddl_statement)
                elif ddl_type == 'ALTER_TABLE':
                    result = await self.validate_alter_table(cursor, ddl_statement)
                elif ddl_type == 'CREATE_INDEX':
                    result = await self.validate_create_index(cursor, ddl_statement)
                elif ddl_type == 'DROP_TABLE':
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == 'DROP_INDEX':
                    print(f"[DEBUG] DROP_INDEX ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_drop_index(cursor, ddl_statement)
                    print(f"[DEBUG] DROP_INDEX ê²€ì¦ ê²°ê³¼: {result}")
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"]
                    }
                
                validation_results.append(result)
                print(f"[DEBUG] ê²€ì¦ ê²°ê³¼ ì¶”ê°€ë¨: {result}")
            
            cursor.close()
            connection.close()
            
            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            print(f"[DEBUG] validate_schema ì™„ë£Œ, ê²°ê³¼ ê°œìˆ˜: {len(validation_results)}")
            return {
                "success": True,
                "validation_results": validation_results
            }
            
        except Exception as e:
            print(f"[DEBUG] validate_schema ì˜ˆì™¸: {e}")
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"
            }

    async def validate_constraints(self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True) -> Dict[str, Any]:
        """ì œì•½ì¡°ê±´ ê²€ì¦ - FK, ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)
            
            connection, tunnel_used = await self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel)
            cursor = connection.cursor()
            
            constraint_results = []
            
            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ê²€ì¦
            if constraints_info.get('foreign_keys'):
                for fk in constraints_info['foreign_keys']:
                    # ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    cursor.execute("""
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """, (fk['referenced_table'],))
                    
                    ref_table_exists = cursor.fetchone()[0] > 0
                    
                    if ref_table_exists:
                        # ì°¸ì¡° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        cursor.execute("""
                            SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_schema = DATABASE() 
                            AND table_name = %s AND column_name = %s
                        """, (fk['referenced_table'], fk['referenced_column']))
                        
                        ref_column_exists = cursor.fetchone()[0] > 0
                        
                        constraint_results.append({
                            "type": "FOREIGN_KEY",
                            "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                            "valid": ref_column_exists,
                            "issue": None if ref_column_exists else f"ì°¸ì¡° ì»¬ëŸ¼ '{fk['referenced_table']}.{fk['referenced_column']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        })
                    else:
                        constraint_results.append({
                            "type": "FOREIGN_KEY",
                            "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                            "valid": False,
                            "issue": f"ì°¸ì¡° í…Œì´ë¸” '{fk['referenced_table']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        })
            
            cursor.close()
            connection.close()
            
            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()
            
            return {
                "success": True,
                "constraint_results": constraint_results
            }
            
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"
            }

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ"""
        ddl_statements = []
        
        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE)
        
        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append({
                'type': 'CREATE_TABLE',
                'table': table_name.lower(),
                'columns': columns_info['columns'],
                'constraints': columns_info['constraints']
            })
        
        # ALTER TABLE íŒŒì‹±
        alter_patterns = [
            # ADD COLUMN
            (r'ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)', 'ADD_COLUMN'),
            # DROP COLUMN
            (r'ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?', 'DROP_COLUMN'),
            # MODIFY COLUMN
            (r'ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)', 'MODIFY_COLUMN'),
            # CHANGE COLUMN
            (r'ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)', 'CHANGE_COLUMN')
        ]
        
        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == 'CHANGE_COLUMN':
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append({
                        'type': 'ALTER_TABLE',
                        'table': table_name.lower(),
                        'alter_type': alter_type,
                        'old_column': old_column.lower(),
                        'new_column': new_column.lower(),
                        'column_definition': column_def.strip()
                    })
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append({
                        'type': 'ALTER_TABLE',
                        'table': table_name.lower(),
                        'alter_type': alter_type,
                        'column': column_name.lower(),
                        'column_definition': column_def.strip() if column_def else None
                    })
        
        # CREATE INDEX íŒŒì‹±
        create_index_pattern = r'CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)'
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)
        
        for index_name, table_name, columns in index_matches:
            ddl_statements.append({
                'type': 'CREATE_INDEX',
                'table': table_name.lower(),
                'index_name': index_name.lower(),
                'columns': [col.strip().strip('`').lower() for col in columns.split(',')]
            })
        
        # DROP TABLE íŒŒì‹±
        drop_table_pattern = r'DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?'
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)
        
        for table_name in drop_table_matches:
            ddl_statements.append({
                'type': 'DROP_TABLE',
                'table': table_name.lower()
            })
        
        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r'DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?'
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)
        
        print(f"[DEBUG] DROP INDEX íŒŒì‹± - íŒ¨í„´: {drop_index_pattern}")
        print(f"[DEBUG] DROP INDEX íŒŒì‹± - ì…ë ¥: {repr(ddl_content)}")
        print(f"[DEBUG] DROP INDEX íŒŒì‹± - ê²°ê³¼: {drop_index_matches}")
        
        for index_name, table_name in drop_index_matches:
            ddl_statements.append({
                'type': 'DROP_INDEX',
                'table': table_name.lower(),
                'index_name': index_name.lower()
            })
            print(f"[DEBUG] DROP INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {table_name}")
        
        print(f"[DEBUG] ì „ì²´ íŒŒì‹± ê²°ê³¼: {len(ddl_statements)}ê°œ êµ¬ë¬¸")
        for i, stmt in enumerate(ddl_statements):
            print(f"[DEBUG]   [{i}] {stmt['type']}: {stmt}")
        
        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEì˜ ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±"""
        columns = []
        constraints = []
        
        # ì»¬ëŸ¼ ì •ì˜ì™€ ì œì•½ì¡°ê±´ì„ ë¶„ë¦¬
        lines = [line.strip() for line in columns_def.split(',')]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì œì•½ì¡°ê±´ í™•ì¸
            if re.match(r'(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)', line, re.IGNORECASE):
                constraints.append(line)
            else:
                # ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±
                column_match = re.match(r'`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?', line, re.IGNORECASE)
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ''
                    
                    columns.append({
                        'name': column_name,
                        'data_type': data_type,
                        'attributes': attributes.strip()
                    })
        
        return {
            'columns': columns,
            'constraints': constraints
        }

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ"""
        constraints = {
            'foreign_keys': [],
            'indexes': [],
            'primary_keys': []
        }
        
        # ì™¸ë˜í‚¤ íŒ¨í„´ ë§¤ì¹­
        fk_pattern = r'FOREIGN\s+KEY\s*\(`?(\w+)`?\)\s*REFERENCES\s+`?(\w+)`?\s*\(`?(\w+)`?\)'
        fk_matches = re.findall(fk_pattern, ddl_content, re.IGNORECASE)
        
        for column, ref_table, ref_column in fk_matches:
            constraints['foreign_keys'].append({
                'column': column,
                'referenced_table': ref_table,
                'referenced_column': ref_column
            })
        
        return constraints
        
    async def validate_create_table(self, cursor, ddl_statement: Dict[str, Any]) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement['table']
        columns = ddl_statement['columns']
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        issues = []
        
        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "columns_count": len(columns)
            }
        }

    async def validate_alter_table(self, cursor, ddl_statement: Dict[str, Any]) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement['table']
        alter_type = ddl_statement['alter_type']
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        issues = []
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues
            }
        
        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        existing_columns = {row[0].lower(): {
            'data_type': row[1].upper(),
            'max_length': row[2],
            'precision': row[3],
            'scale': row[4],
            'is_nullable': row[5]
        } for row in cursor.fetchall()}
        
        # ALTER ìœ í˜•ë³„ ê²€ì¦
        if alter_type == 'ADD_COLUMN':
            column_name = ddl_statement['column']
            if column_name in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        elif alter_type == 'DROP_COLUMN':
            column_name = ddl_statement['column']
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        elif alter_type == 'MODIFY_COLUMN':
            column_name = ddl_statement['column']
            new_definition = ddl_statement['column_definition']
            
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result['valid']:
                    issues.extend(validation_result['issues'])
        
        elif alter_type == 'CHANGE_COLUMN':
            old_column = ddl_statement['old_column']
            new_column = ddl_statement['new_column']
            new_definition = ddl_statement['column_definition']
            
            if old_column not in existing_columns:
                issues.append(f"ê¸°ì¡´ ì»¬ëŸ¼ '{old_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ìƒˆ ì»¬ëŸ¼ëª… '{new_column}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result['valid']:
                    issues.extend(validation_result['issues'])
        
        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "existing_columns": list(existing_columns.keys())
            }
        }

    async def validate_create_index(self, cursor, ddl_statement: Dict[str, Any]) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement['table']
        index_name = ddl_statement['index_name']
        columns = ddl_statement['columns']
        
        issues = []
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """, (table_name, index_name))
            
            index_exists = cursor.fetchone()[0] > 0
            
            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            
            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute("""
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """, (table_name,))
            
            existing_columns = {row[0].lower() for row in cursor.fetchall()}
            
            for column in columns:
                if column not in existing_columns:
                    issues.append(f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "index_name": index_name,
                "columns": columns
            }
        }

    async def validate_drop_table(self, cursor, ddl_statement: Dict[str, Any]) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement['table']
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        issues = []
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {
                "table_exists": table_exists
            }
        }

    async def validate_drop_index(self, cursor, ddl_statement: Dict[str, Any]) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement['table']
        index_name = ddl_statement['index_name']
        
        print(f"[DEBUG] DROP INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}")
        
        issues = []
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """, (table_name,))
        
        table_exists = cursor.fetchone()[0] > 0
        print(f"[DEBUG] í…Œì´ë¸” '{table_name}' ì¡´ì¬ ì—¬ë¶€: {table_exists}")
        
        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"[DEBUG] í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """, (table_name, index_name))
            
            index_exists = cursor.fetchone()[0] > 0
            print(f"[DEBUG] ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬ ì—¬ë¶€: {index_exists}")
            
            if not index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"[DEBUG] ì¸ë±ìŠ¤ '{index_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")
        
        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "index_name": index_name,
                "table_exists": table_exists
            }
        }
        
        print(f"[DEBUG] DROP INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}")
        print(f"[DEBUG] ìµœì¢… ê²°ê³¼: {result}")
        
        return result

    def validate_column_type_change(self, existing_column: Dict[str, Any], new_definition: str) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []
        
        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column['data_type']
        
        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (['VARCHAR', 'CHAR', 'TEXT'], ['INT', 'BIGINT', 'DECIMAL', 'FLOAT', 'DOUBLE']),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (['INT', 'BIGINT', 'DECIMAL', 'FLOAT', 'DOUBLE'], ['VARCHAR', 'CHAR']),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (['DATE', 'DATETIME', 'TIMESTAMP'], ['INT', 'VARCHAR', 'CHAR']),
        ]
        
        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info['type'] in to_types:
                issues.append(f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ['VARCHAR', 'CHAR'] and new_type_info['type'] in ['VARCHAR', 'CHAR']:
            existing_length = existing_column['max_length']
            new_length = new_type_info['length']
            
            if existing_length and new_length and new_length < existing_length:
                issues.append(f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == 'DECIMAL' and new_type_info['type'] == 'DECIMAL':
            existing_precision = existing_column['precision']
            existing_scale = existing_column['scale']
            new_precision = new_type_info['precision']
            new_scale = new_type_info['scale']
            
            if (existing_precision and new_precision and new_precision < existing_precision) or \
               (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return {
            'valid': len(issues) == 0,
            'issues': issues
        }

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r'(\w+)(?:\(([^)]+)\))?', data_type_str.upper())
        if not type_match:
            return {'type': data_type_str.upper(), 'length': None, 'precision': None, 'scale': None}
        
        base_type = type_match.group(1)
        params = type_match.group(2)
        
        result = {'type': base_type, 'length': None, 'precision': None, 'scale': None}
        
        if params:
            if ',' in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(',')]
                result['precision'] = int(parts[0]) if parts[0].isdigit() else None
                result['scale'] = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result['length'] = int(params) if params.isdigit() else None
        
        return result

    async def generate_html_report(self, report_path: Path, filename: str, ddl_content: str, 
                                 ddl_type: str, status: str, summary: str, issues: List[str],
                                 db_connection_info: Optional[Dict], schema_validation: Optional[Dict],
                                 constraint_validation: Optional[Dict], database_secret: Optional[str]):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"
            
            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­ì— ë”°ë¼)
            db_info_section = ""
            
            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜
            issues_section = ""
            if issues:
                issues_section = """
                <div class="issues-section">
                    <h3>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <ul class="issues-list">
                """
                for issue in issues:
                    issues_section += f"<li>{issue}</li>"
                issues_section += """
                    </ul>
                </div>
                """
            else:
                issues_section = """
                <div class="issues-section success">
                    <h3>âœ… ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <p class="no-issues">ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>
                </div>
                """
            
            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DDL ê²€ì¦ ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: none;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} DDL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ DDL íƒ€ì…</h4>
                    <p>{ddl_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ DDL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="info-section">
                <h3>ğŸ“Š ê²€ì¦ ê²°ê³¼</h3>
                <p style="font-size: 1.2em; font-weight: 500; color: {status_color};">{summary}</p>
            </div>
            
            {issues_section}
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
                
        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")

    async def generate_consolidated_html_report(self, validation_results: List[Dict], database_secret: str) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì˜ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename
            
            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r['status'] == 'PASS')
            failed_files = total_files - passed_files
            
            # íŒŒì¼ë³„ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_icon = "âœ…" if result['status'] == 'PASS' else "âŒ"
                status_class = "success" if result['status'] == 'PASS' else "error"
                
                issues_html = ""
                if result['issues']:
                    issues_html = "<ul class='issues-list'>"
                    for issue in result['issues']:
                        issues_html += f"<li>{issue}</li>"
                    issues_html += "</ul>"
                else:
                    issues_html = "<p class='no-issues'>ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>"
                
                file_sections += f"""
                <div class="file-section {status_class}">
                    <h3>{status_icon} {i}. {result['filename']}</h3>
                    <div class="file-details">
                        <div class="file-info">
                            <span><strong>DDL íƒ€ì…:</strong> {result['ddl_type']}</span>
                            <span><strong>ìƒíƒœ:</strong> {result['status']}</span>
                            <span><strong>ë¬¸ì œ ìˆ˜:</strong> {len(result['issues'])}ê°œ</span>
                        </div>
                        <div class="sql-code">
{result['ddl_content']}
                        </div>
                        {f'<div class="issues-section"><h4>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h4>{issues_html}</div>' if result['issues'] else ''}
                    </div>
                </div>
                """
            
            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© DDL ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 20px 30px;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-section.success {{
            border-left: 4px solid #28a745;
        }}
        .file-section.error {{
            border-left: 4px solid #dc3545;
        }}
        .file-section h3 {{
            margin: 0;
            padding: 15px 20px;
            background: #f8f9fa;
            border-bottom: 1px solid #e9ecef;
        }}
        .file-details {{
            padding: 20px;
        }}
        .file-info {{
            display: flex;
            gap: 20px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }}
        .file-info span {{
            background: #e9ecef;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: none;
            font-size: 0.9em;
        }}
        .issues-section {{
            margin-top: 15px;
            padding: 15px;
            background: #fff5f5;
            border: 1px solid #fed7d7;
            border-radius: 6px;
        }}
        .issues-section h4 {{
            margin: 0 0 10px 0;
            color: #c53030;
        }}
        .issues-list {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
            color: #c53030;
        }}
        .no-issues {{
            color: #38a169;
            margin: 0;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
                margin: 20px;
            }}
            .file-section {{
                margin: 20px 15px;
            }}
            .file-info {{
                flex-direction: column;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© DDL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ë°ì´í„°ë² ì´ìŠ¤: {database_secret}</p>
            <p>ê²€ì¦ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
        
        <div class="summary-stats">
            <div class="stat-item">
                <div class="stat-number">{total_files}</div>
                <div class="stat-label">ì´ íŒŒì¼ ìˆ˜</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                <div class="stat-label">í†µê³¼</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                <div class="stat-label">ì‹¤íŒ¨</div>
            </div>
            <div class="stat-item">
                <div class="stat-number">{round(passed_files/total_files*100) if total_files > 0 else 0}%</div>
                <div class="stat-label">ì„±ê³µë¥ </div>
            </div>
        </div>
        
        {file_sections}
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>"""
            
            # íŒŒì¼ ì €ì¥
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            return str(report_path)
            
        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_all_sql_files(self, database_secret: Optional[str] = None) -> str:
        """ëª¨ë“  SQL íŒŒì¼ ê²€ì¦ ë° í†µí•© ë³´ê³ ì„œ ìƒì„± (ìµœëŒ€ 5ê°œ)"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ìµœëŒ€ 5ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
            files_to_process = sql_files[:5]
            if len(sql_files) > 5:
                logger.warning(f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ìˆì§€ë§Œ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
            validation_results = []
            summary_results = []
            
            for sql_file in files_to_process:
                try:
                    # ê°œë³„ íŒŒì¼ ê²€ì¦ (ë³´ê³ ì„œ ìƒì„± ì—†ì´)
                    ddl_content = sql_file.read_text(encoding='utf-8')
                    ddl_type = self.detect_ddl_type(ddl_content)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ê²€ì¦
                    db_connection_info = None
                    issues = []
                    
                    if database_secret:
                        db_connection_info = await self.test_db_connection(database_secret)
                        if db_connection_info and db_connection_info.get("success"):
                            # ìŠ¤í‚¤ë§ˆ ê²€ì¦
                            schema_validation = await self.validate_schema(ddl_content, database_secret)
                            if schema_validation and not schema_validation.get("valid", True):
                                issues.extend(schema_validation.get("issues", []))
                            
                            # ì œì•½ì¡°ê±´ ê²€ì¦
                            constraint_validation = await self.validate_constraints(ddl_content, database_secret)
                            if constraint_validation and not constraint_validation.get("valid", True):
                                issues.extend(constraint_validation.get("issues", []))
                    
                    status = "PASS" if not issues else "FAIL"
                    
                    # ê²°ê³¼ ì €ì¥
                    validation_results.append({
                        'filename': sql_file.name,
                        'ddl_content': ddl_content,
                        'ddl_type': ddl_type,
                        'status': status,
                        'issues': issues
                    })
                    
                    summary_results.append(f"**{sql_file.name}**: {'âœ… í†µê³¼' if status == 'PASS' else f'âŒ ì‹¤íŒ¨ ({len(issues)}ê°œ ë¬¸ì œ)'}")
                    
                except Exception as e:
                    validation_results.append({
                        'filename': sql_file.name,
                        'ddl_content': f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}",
                        'ddl_type': "UNKNOWN",
                        'status': "FAIL",
                        'issues': [f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}"]
                    })
                    summary_results.append(f"**{sql_file.name}**: âŒ ê²€ì¦ ì‹¤íŒ¨ - {str(e)}")
            
            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„±
            if validation_results:
                report_path = await self.generate_consolidated_html_report(validation_results, database_secret or "N/A")
                
                # í†µê³„ ê³„ì‚°
                total_files = len(validation_results)
                passed_files = sum(1 for r in validation_results if r['status'] == 'PASS')
                failed_files = total_files - passed_files
                
                summary = f"ğŸ“Š ì´ {total_files}ê°œ íŒŒì¼ ê²€ì¦ ì™„ë£Œ"
                if len(sql_files) > 5:
                    summary += f" (ì „ì²´ {len(sql_files)}ê°œ ì¤‘ 5ê°œ ì²˜ë¦¬)"
                
                summary += f"\nâ€¢ í†µê³¼: {passed_files}ê°œ ({round(passed_files/total_files*100)}%)"
                summary += f"\nâ€¢ ì‹¤íŒ¨: {failed_files}ê°œ ({round(failed_files/total_files*100)}%)"
                summary += f"\n\nğŸ“„ í†µí•© ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}"
                
                return f"{summary}\n\n" + "\n".join(summary_results)
            else:
                return "ê²€ì¦í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def copy_sql_file(self, source_path: str, target_name: Optional[str] = None) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"
            
            if not source.suffix.lower() == '.sql':
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"
            
            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith('.sql'):
                    target_name += '.sql'
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name
            
            # íŒŒì¼ ë³µì‚¬
            import shutil
            shutil.copy2(source, target_path)
            
            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"
            
        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    # === ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œ ===
    
    def setup_cloudwatch_client(self, region_name: str = 'us-east-1'):
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            self.cloudwatch = boto3.client('cloudwatch', region_name=region_name)
            return True
        except Exception as e:
            logger.error(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def collect_db_metrics(self, db_instance_identifier: str, hours: int = 24, 
                               metrics: Optional[List[str]] = None, region: str = 'us-east-1') -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandas numpy matplotlib scikit-learnì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        try:
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            if not metrics:
                metrics = self.default_metrics

            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            data = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
            for metric in metrics:
                try:
                    response = self.cloudwatch.get_metric_statistics(
                        Namespace='AWS/RDS',
                        MetricName=metric,
                        Dimensions=[
                            {
                                'Name': 'DBInstanceIdentifier',
                                'Value': db_instance_identifier
                            },
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5ë¶„ ê°„ê²©
                        Statistics=['Average']
                    )
                    
                    # ì‘ë‹µì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    for point in response['Datapoints']:
                        data.append({
                            'Timestamp': point['Timestamp'].replace(tzinfo=None),
                            'Metric': metric,
                            'Value': point['Average']
                        })
                except Exception as e:
                    logger.error(f"ë©”íŠ¸ë¦­ {metric} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

            if not data:
                return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            df = df.sort_values('Timestamp')

            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            pivot_df = df.pivot(index='Timestamp', columns='Metric', values='Value')

            # CSV íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = DATA_DIR / f'database_metrics_{db_instance_identifier}_{timestamp}.csv'
            pivot_df.to_csv(csv_file)

            return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {csv_file}"

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def analyze_metric_correlation(self, csv_file: str, target_metric: str = 'CPUUtilization', 
                                       top_n: int = 10) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"íƒ€ê²Ÿ ë©”íŠ¸ë¦­ '{target_metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ìƒê´€ ë¶„ì„
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(target_metric, errors='ignore')
            top_correlations = target_correlations.nlargest(top_n)

            # ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
            result = f"ğŸ“Š {target_metric}ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ {top_n}ê°œ ë©”íŠ¸ë¦­:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"â€¢ {metric}: {correlation:.4f}\n"

            # ì‹œê°í™”
            plt.figure(figsize=(12, 6))
            top_correlations.plot(kind='bar')
            plt.title(f'Top {top_n} Metrics Correlated with {target_metric}')
            plt.xlabel('Metrics')
            plt.ylabel('Correlation Coefficient')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'correlation_analysis_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"\nğŸ“ˆ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def detect_metric_outliers(self, csv_file: str, std_threshold: float = 2.0) -> str:
        """ì•„ì›ƒë¼ì´ì–´ íƒì§€"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            result = f"ğŸš¨ ì•„ì›ƒë¼ì´ì–´ íƒì§€ ê²°ê³¼ (ì„ê³„ê°’: Â±{std_threshold}Ïƒ):\n\n"

            outlier_summary = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ì•„ì›ƒë¼ì´ì–´ íƒì§€
            for column in df.columns:
                series = df[column]
                mean = series.mean()
                std = series.std()
                lower_bound = mean - std_threshold * std
                upper_bound = mean + std_threshold * std
                
                outliers = series[(series < lower_bound) | (series > upper_bound)]
                
                if not outliers.empty:
                    result += f"âš ï¸ {column} ë©”íŠ¸ë¦­ì˜ ì•„ì›ƒë¼ì´ì–´ ({len(outliers)}ê°œ):\n"
                    result += f"   ì •ìƒ ë²”ìœ„: {lower_bound:.2f} ~ {upper_bound:.2f}\n"
                    
                    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ í‘œì‹œ
                    for i, (timestamp, value) in enumerate(outliers.items()):
                        if i >= 5:
                            result += f"   ... ë° {len(outliers) - 5}ê°œ ë”\n"
                            break
                        result += f"   â€¢ {timestamp}: {value:.2f}\n"
                    result += "\n"
                    
                    outlier_summary.append({
                        'metric': column,
                        'count': len(outliers),
                        'percentage': (len(outliers) / len(series)) * 100
                    })
                else:
                    result += f"âœ… {column}: ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ\n"

            # ìš”ì•½ ì •ë³´
            if outlier_summary:
                result += "\nğŸ“‹ ì•„ì›ƒë¼ì´ì–´ ìš”ì•½:\n"
                for summary in outlier_summary:
                    result += f"â€¢ {summary['metric']}: {summary['count']}ê°œ ({summary['percentage']:.1f}%)\n"

            return result

        except Exception as e:
            return f"ì•„ì›ƒë¼ì´ì–´ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def perform_regression_analysis(self, csv_file: str, predictor_metric: str, 
                                        target_metric: str = 'CPUUtilization') -> str:
        """íšŒê·€ ë¶„ì„ ìˆ˜í–‰"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)

            # í•„ìš”í•œ ë©”íŠ¸ë¦­ í™•ì¸
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ë°ì´í„° ì¤€ë¹„
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN ê°’ ì²˜ë¦¬
            imputer = SimpleImputer(strategy='mean')
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± (2ì°¨)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_poly_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê³„ìˆ˜ ì¶œë ¥
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ({predictor_metric} â†’ {target_metric}):\n\n"
            result += f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥:\n"
            result += f"â€¢ Mean Squared Error: {mse:.4f}\n"
            result += f"â€¢ R-squared Score: {r2:.4f}\n\n"
            
            result += f"ğŸ”¢ ë‹¤í•­ íšŒê·€ ëª¨ë¸ (2ì°¨):\n"
            result += f"y = {coefficients[1]:.4f}xÂ² + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # í•´ì„ ì¶”ê°€
            result += "í•´ì„:\n"
            if r2 > 0.7:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"
            elif r2 > 0.5:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ì¤‘ê°„ ì˜ˆì¸¡ ì •í™•ë„)\n"
            else:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë‚®ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"

            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            plt.figure(figsize=(12, 8))
            
            # ì‚°ì ë„
            plt.subplot(2, 1, 1)
            plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='ì‹¤ì œ ë°ì´í„°')
            
            # ì˜ˆì¸¡ ê³¡ì„ ì„ ìœ„í•œ ì •ë ¬ëœ ë°ì´í„°
            X_plot = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)
            X_plot_poly = poly_features.transform(X_plot)
            y_plot_pred = model.predict(X_plot_poly)
            
            plt.plot(X_plot, y_plot_pred, color='red', linewidth=2, label='ì˜ˆì¸¡ ëª¨ë¸')
            plt.title(f'{predictor_metric} vs {target_metric} íšŒê·€ ë¶„ì„')
            plt.xlabel(predictor_metric)
            plt.ylabel(target_metric)
            plt.legend()
            plt.grid(True, alpha=0.3)

            # ì”ì°¨ í”Œë¡¯
            plt.subplot(2, 1, 2)
            residuals = y_test - y_pred
            plt.scatter(y_pred, residuals, color='green', alpha=0.6)
            plt.axhline(y=0, color='red', linestyle='--')
            plt.title('ì”ì°¨ í”Œë¡¯ (Residual Plot)')
            plt.xlabel('ì˜ˆì¸¡ê°’')
            plt.ylabel('ì”ì°¨')
            plt.grid(True, alpha=0.3)

            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'regression_analysis_{predictor_metric}_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"íšŒê·€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def list_data_files(self) -> str:
        """ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ“ ë°ì´í„° íŒŒì¼ ëª©ë¡:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"â€¢ {file.name}\n"
                result += f"  í¬ê¸°: {file_size:,} bytes\n"
                result += f"  ìˆ˜ì •ì¼: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            
            result = f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ({csv_file}):\n\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë©”íŠ¸ë¦­ ìˆ˜: {len(df.columns)}ê°œ\n\n"
            
            result += "ğŸ“Š ë©”íŠ¸ë¦­ ëª©ë¡:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}ê°œ ë°ì´í„°)\n"

            # ê¸°ë³¸ í†µê³„
            result += f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# MCP ì„œë²„ ì„¤ì •
server = Server("db-assistant-mcp-server")
db_assistant = DBAssistantMCPServer()

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)"
                    }
                }
            }
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="list_databases",
            description="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="select_database",
            description="íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ (USE ëª…ë ¹ì–´ ì‹¤í–‰). ë¨¼ì € list_databasesë¡œ ëª©ë¡ì„ í™•ì¸í•œ í›„ ë²ˆí˜¸ë‚˜ ì´ë¦„ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”.",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ì„ íƒí•  ë°ì´í„°ë² ì´ìŠ¤ (ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„)"
                    }
                },
                "required": ["database_secret", "database_selection"]
            }
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="get_table_schema",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„"
                    }
                },
                "required": ["database_secret", "table_name"]
            }
        ),
        types.Tool(
            name="get_table_index",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„"
                    }
                },
                "required": ["database_secret", "table_name"]
            }
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì"
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­)"
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)",
                        "default": "us-east-1"
                    }
                },
                "required": ["db_instance_identifier"]
            }
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ìƒìœ„ Nê°œ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: 10)",
                        "default": 10
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="ë©”íŠ¸ë¦­ ë°ì´í„°ì—ì„œ ì•„ì›ƒë¼ì´ì–´ë¥¼ íƒì§€í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "std_threshold": {
                        "type": "number",
                        "description": "í‘œì¤€í¸ì°¨ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2.0)",
                        "default": 2.0
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="ë©”íŠ¸ë¦­ ê°„ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "predictor_metric": {
                        "type": "string",
                        "description": "ì˜ˆì¸¡ ë³€ìˆ˜ ë©”íŠ¸ë¦­"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    }
                },
                "required": ["csv_file", "predictor_metric"]
            }
        ),
        types.Tool(
            name="list_data_files",
            description="ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {
                        "type": "string",
                        "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"
                    },
                    "database_secret": {
                        "type": "string", 
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)"
                    }
                },
                "required": ["filename"]
            }
        ),
        types.Tool(
            name="validate_all_sql",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤ (ìµœëŒ€ 5ê°œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)"
                    }
                }
            }
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ"
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)"
                    }
                },
                "required": ["source_path"]
            }
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV íŒŒì¼ì˜ ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ìš”ì•½í•  CSV íŒŒì¼ëª…"
                    }
                },
                "required": ["csv_file"]
            }
        ),
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await db_assistant.list_sql_files()
        elif name == "list_database_secrets":
            result = await db_assistant.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "test_database_connection":
            result = await db_assistant.test_database_connection(
                arguments["database_secret"]
            )
        elif name == "list_databases":
            result = await db_assistant.list_databases(
                arguments["database_secret"]
            )
        elif name == "select_database":
            result = await db_assistant.select_database(
                arguments["database_secret"],
                arguments["database_selection"]
            )
        elif name == "get_schema_summary":
            result = await db_assistant.get_schema_summary(
                arguments["database_secret"]
            )
        elif name == "get_table_schema":
            result = await db_assistant.get_table_schema(
                arguments["database_secret"],
                arguments["table_name"]
            )
        elif name == "get_table_index":
            result = await db_assistant.get_table_index(
                arguments["database_secret"],
                arguments["table_name"]
            )
        elif name == "get_performance_metrics":
            result = await db_assistant.get_performance_metrics(
                arguments["database_secret"],
                arguments.get("metric_type", "all")
            )
        elif name == "collect_db_metrics":
            result = await db_assistant.collect_db_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", "us-east-1")
            )
        elif name == "analyze_metric_correlation":
            result = await db_assistant.analyze_metric_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10)
            )
        elif name == "detect_metric_outliers":
            result = await db_assistant.detect_metric_outliers(
                arguments["csv_file"],
                arguments.get("std_threshold", 2.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_assistant.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization")
            )
        elif name == "list_data_files":
            result = await db_assistant.list_data_files()
        elif name == "validate_sql_file":
            result = await db_assistant.validate_sql_file(
                arguments["filename"],
                arguments.get("database_secret")
            )
        elif name == "validate_all_sql":
            result = await db_assistant.validate_all_sql_files(
                arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await db_assistant.copy_sql_file(
                arguments["source_path"],
                arguments.get("target_name")
            )
        elif name == "get_metric_summary":
            result = await db_assistant.get_metric_summary(arguments["csv_file"])
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"
        
        return [types.TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="db-assistant-mcp-server",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e

if __name__ == "__main__":
    asyncio.run(main())

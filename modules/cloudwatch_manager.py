"""
CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ëª¨ë“ˆ

CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ë¶„ì„, ì´ìƒ ì§•í›„ íƒì§€ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
ëŒ€ë¶€ë¶„ì˜ CloudWatch API í˜¸ì¶œì€ Lambdaë¡œ ì˜¤í”„ë¡œë“œë˜ë©°, ì´ ëª¨ë“ˆì€ ë°ì´í„° ë³€í™˜ ë° ë¶„ì„ì„ ë‹´ë‹¹
"""

import json
import logging
import boto3
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from pathlib import Path

# ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pandas as pd
    import numpy as np
    ANALYSIS_AVAILABLE = True
except ImportError:
    ANALYSIS_AVAILABLE = False

logger = logging.getLogger(__name__)

# ë””ë ‰í† ë¦¬ ê²½ë¡œ
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)


class CloudWatchManager:
    """
    CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤

    Lambdaë¥¼ í†µí•´ CloudWatch APIë¥¼ í˜¸ì¶œí•˜ê³ , ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """

    def __init__(self, region: str = 'ap-northeast-2', lambda_client=None):
        """
        Args:
            region: AWS ë¦¬ì „
            lambda_client: LambdaClient ì¸ìŠ¤í„´ìŠ¤ (Lambda í˜¸ì¶œìš©)
        """
        self.region = region
        self.cloudwatch = None
        self.lambda_client = lambda_client
        self.current_instance_class = "r5.large"  # ê¸°ë³¸ê°’

        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ëª©ë¡
        self.default_metrics = [
            "CPUUtilization",
            "DatabaseConnections",
            "FreeableMemory",
            "ReadLatency",
            "WriteLatency",
            "ReadIOPS",
            "WriteIOPS",
        ]

        logger.info(f"CloudWatchManager ì´ˆê¸°í™” ì™„ë£Œ - ë¦¬ì „: {region}")

    def setup_cloudwatch_client(self, region_name: str = None) -> bool:
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            if region_name is None:
                region_name = self.region
            self.cloudwatch = boto3.client("cloudwatch", region_name=region_name)
            logger.info(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ - ë¦¬ì „: {region_name}")
            return True
        except Exception as e:
            logger.error(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def collect_db_metrics(
        self,
        db_instance_identifier: str,
        hours: int = 24,
        metrics: Optional[List[str]] = None,
        region: str = None,
    ) -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

        Args:
            db_instance_identifier: DB ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì
            hours: ìˆ˜ì§‘ ê¸°ê°„ (ì‹œê°„)
            metrics: ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡
            region: AWS ë¦¬ì „

        Returns:
            str: ìˆ˜ì§‘ ê²°ê³¼ ë©”ì‹œì§€
        """
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandas numpy scikit-learnì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."

        if region is None:
            region = self.region

        try:
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            # Lambda í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜
            if not self.lambda_client:
                return "âŒ Lambda í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            # í´ëŸ¬ìŠ¤í„° í™•ì¸ ë° ì¸ìŠ¤í„´ìŠ¤ identifier ë³€í™˜ (Lambda ì‚¬ìš©)
            try:
                logger.info(f"Lambdaë¡œ RDS ì •ë³´ ì¡°íšŒ: {db_instance_identifier}")
                rds_info = await self.lambda_client._call_lambda('get-rds-cluster-info', {
                    'identifier': db_instance_identifier,
                    'region': region
                })

                if rds_info['type'] == 'cluster':
                    # í´ëŸ¬ìŠ¤í„°ì¸ ê²½ìš° ì²« ë²ˆì§¸ writer ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                    original_id = db_instance_identifier
                    for member in rds_info['members']:
                        if member['is_writer']:
                            db_instance_identifier = member['identifier']
                            self.current_instance_class = member.get('instance_class', 'r5.large')
                            break
                    logger.info(
                        f"í´ëŸ¬ìŠ¤í„° {original_id}ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ {db_instance_identifier}ë¡œ ë³€í™˜"
                    )
                else:
                    # ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš°
                    self.current_instance_class = rds_info.get('instance_class', 'r5.large')
                    logger.info(f"ì¸ìŠ¤í„´ìŠ¤ í´ë˜ìŠ¤: {self.current_instance_class}")

            except Exception as e:
                logger.warning(f"RDS ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ì‚¬ìš©): {str(e)}")
                self.current_instance_class = "r5.large"  # ê¸°ë³¸ê°’

            if not metrics:
                metrics = self.default_metrics

            # CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Lambda ì‚¬ìš©)
            logger.info(f"Lambdaë¡œ CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘: {db_instance_identifier}")
            try:
                metrics_result = await self.lambda_client._call_lambda('get-cloudwatch-metrics-raw', {
                    'instance_identifier': db_instance_identifier,
                    'metrics': metrics,
                    'hours': hours,
                    'region': region,
                    'period': 300
                })

                # Lambdaì—ì„œ ë°›ì€ ë°ì´í„°ë¥¼ pandas í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                data = []
                for point in metrics_result.get('metrics_data', []):
                    # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    timestamp = datetime.fromisoformat(point['timestamp'].replace('Z', '+00:00'))
                    data.append({
                        "Timestamp": timestamp.replace(tzinfo=None),
                        "Metric": point['metric'],
                        "Value": point['average']  # í‰ê· ê°’ ì‚¬ìš©
                    })

                logger.info(f"Lambdaì—ì„œ {len(data)}ê°œ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì§‘")

            except Exception as e:
                logger.error(f"Lambda ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                data = []

            if not data:
                return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            df = df.sort_values("Timestamp")

            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            pivot_df = df.pivot(index="Timestamp", columns="Metric", values="Value")

            # CSV íŒŒì¼ë¡œ ì €ì¥ (ì„ì‹œ)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"database_metrics_{db_instance_identifier}_{timestamp}.csv"
            csv_file = DATA_DIR / csv_filename
            pivot_df.to_csv(csv_file)

            # S3ì— ì—…ë¡œë“œ
            s3_bucket = "db-assistant-query-results-dev"
            s3_key = f"metrics/{db_instance_identifier}/{csv_filename}"

            try:
                s3_client = boto3.client('s3', region_name=region)

                # CSV íŒŒì¼ ì—…ë¡œë“œ
                s3_client.upload_file(str(csv_file), s3_bucket, s3_key)
                logger.info(f"CSV íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{s3_bucket}/{s3_key}")

                # Pre-signed URL ìƒì„± (7ì¼ ìœ íš¨)
                presigned_url = s3_client.generate_presigned_url(
                    'get_object',
                    Params={'Bucket': s3_bucket, 'Key': s3_key},
                    ExpiresIn=604800  # 7ì¼ = 604800ì´ˆ
                )

                # ë¡œì»¬ íŒŒì¼ ìœ ì§€ (ë¶„ì„ìš©)
                logger.info(f"ë¡œì»¬ CSV íŒŒì¼ ìœ ì§€: {csv_file} (ë¶„ì„ìš©)")

                return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ S3 ì €ì¥ ìœ„ì¹˜: s3://{s3_bucket}/{s3_key}\nğŸ“ ë¡œì»¬ ì €ì¥: {csv_file}\nğŸ”— ë‹¤ìš´ë¡œë“œ URL (7ì¼ ìœ íš¨): {presigned_url}"

            except Exception as s3_error:
                logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ íŒŒì¼ ìœ ì§€: {s3_error}")
                return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ ë¡œì»¬ ì €ì¥: {csv_file}\nâš ï¸  S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(s3_error)}"

        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def analyze_metric_correlation(
        self, csv_file: str, target_metric: str = "CPUUtilization", top_n: int = 10
    ) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„

        Args:
            csv_file: ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ
            target_metric: ëŒ€ìƒ ë©”íŠ¸ë¦­
            top_n: í‘œì‹œí•  ìƒìœ„ Nê°œ

        Returns:
            str: ë¶„ì„ ê²°ê³¼
        """
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"íƒ€ê²Ÿ ë©”íŠ¸ë¦­ '{target_metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ìƒê´€ ë¶„ì„
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(
                target_metric, errors="ignore"
            )
            top_correlations = target_correlations.nlargest(top_n)

            # ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
            result = f"ğŸ“Š {target_metric}ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ {top_n}ê°œ ë©”íŠ¸ë¦­:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"â€¢ {metric}: {correlation:.4f}\n"

            return result

        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

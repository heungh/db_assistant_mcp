#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ MCP ì„œë²„
CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ìƒê´€ê´€ê³„ ë¶„ì„, ì•„ì›ƒë¼ì´ì–´ íƒì§€, íšŒê·€ ë¶„ì„ ê¸°ëŠ¥ ì œê³µ
ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°, ì„±ëŠ¥ ë¶„ì„, AI ê¸°ë°˜ ì§„ë‹¨ ê¸°ëŠ¥ í†µí•©
"""

import asyncio
import json
import os
import re
import subprocess
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

import boto3
from botocore.exceptions import ClientError
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ matplotlib ì‚¬ìš©
import matplotlib.pyplot as plt
try:
    import seaborn as sns
except ImportError:
    sns = None
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
DATA_DIR = CURRENT_DIR / "data"

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)

class AnalyzeDBServer:
    def __init__(self):
        # ê¸°ë³¸ ë¦¬ì „ ì„¤ì •
        self.default_region = "ap-northeast-2"
        
        # CloudWatch í´ë¼ì´ì–¸íŠ¸
        self.cloudwatch = None
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸ (Claude AI í†µí•©) - Cross Region Inference
        self.bedrock_client = boto3.client(
            "bedrock-runtime", region_name="us-east-1", verify=False
        )
        self.bedrock_agent_client = boto3.client(
            "bedrock-agent-runtime", region_name="us-east-1", verify=False
        )
        self.knowledge_base_id = "0WQUBRHVR8"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
        self.shared_connection = None
        self.shared_cursor = None
        self.tunnel_used = False
        self.selected_database = None
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì„¤ì •
        self.default_metrics = [
            'CPUUtilization', 'DatabaseConnections', 'DBLoad', 'DBLoadCPU', 
            'DBLoadNonCPU', 'FreeableMemory', 'ReadIOPS', 'WriteIOPS',
            'ReadLatency', 'WriteLatency', 'NetworkReceiveThroughput',
            'NetworkTransmitThroughput', 'BufferCacheHitRatio'
        ]
    
    def get_secret(self, secret_name):
        """AWS Secrets Managerì—ì„œ ì‹œí¬ë¦¿ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            return json.loads(get_secret_value_response["SecretString"])
        except Exception as e:
            logger.error(f"ì‹œí¬ë¦¿ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH í„°ë„ ì‹œì‘
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")
            process = subprocess.run(ssh_command, capture_output=True, text=True)
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def get_db_connection(self, database_secret: str, selected_database: str = None, use_ssh_tunnel: bool = True):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        if mysql is None:
            raise Exception("mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        db_config = self.get_secret(database_secret)
        if not db_config:
            raise Exception(f"ì‹œí¬ë¦¿ {database_secret}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        connection_config = None
        tunnel_used = False

        database_name = selected_database or db_config.get("dbname", db_config.get("database"))
        if database_name is not None:
            database_name = str(database_name)

        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(db_config.get("host")):
                connection_config = {
                    "host": "localhost",
                    "port": 3307,
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "database": database_name,
                    "connection_timeout": 10,
                }
                tunnel_used = True

        if not connection_config:
            connection_config = {
                "host": db_config.get("host"),
                "port": db_config.get("port", 3306),
                "user": db_config.get("username"),
                "password": db_config.get("password"),
                "database": database_name,
                "connection_timeout": 10,
            }

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def setup_shared_connection(self, database_secret: str, selected_database: str = None, use_ssh_tunnel: bool = True):
        """ê³µìš© DB ì—°ê²° ì„¤ì •"""
        try:
            if self.shared_connection and self.shared_connection.is_connected():
                logger.info("ì´ë¯¸ í™œì„±í™”ëœ ê³µìš© ì—°ê²°ì´ ìˆìŠµë‹ˆë‹¤.")
                return True

            self.shared_connection, self.tunnel_used = self.get_db_connection(
                database_secret, selected_database, use_ssh_tunnel
            )

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_cursor = self.shared_connection.cursor()
                logger.info(f"ê³µìš© DB ì—°ê²° ì„¤ì • ì™„ë£Œ (í„°ë„: {self.tunnel_used})")
                return True
            else:
                logger.error("ê³µìš© DB ì—°ê²° ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"ê³µìš© DB ì—°ê²° ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    async def query_knowledge_base(self, query: str, analysis_type: str = "performance") -> str:
        """Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ"""
        try:
            # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ì¿¼ë¦¬ ì¡°ì •
            if analysis_type == "performance":
                kb_query = f"Aurora MySQL ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ {query}"
            elif analysis_type == "troubleshooting":
                kb_query = f"Aurora MySQL ë¬¸ì œ í•´ê²° ê°€ì´ë“œ {query}"
            else:
                kb_query = f"ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ {query}"
            
            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": kb_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {
                        "numberOfResults": 3
                    }
                }
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            knowledge_content = []
            for result in response.get("retrievalResults", []):
                content = result.get("content", {}).get("text", "")
                if content:
                    knowledge_content.append(content)
            
            if knowledge_content:
                return "\n\n".join(knowledge_content)
            else:
                return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.warning(f"Knowledge Base ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return "Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def analyze_with_claude(self, metrics_data: str, scenario_context: str = "", analysis_type: str = "performance") -> str:
        """Claude AIë¥¼ í™œìš©í•œ ì„±ëŠ¥ ë¶„ì„"""
        try:
            # Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ
            knowledge_context = ""
            try:
                knowledge_info = await self.query_knowledge_base(scenario_context, analysis_type)
                if knowledge_info and knowledge_info != "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                    knowledge_context = f"""
**ì°¸ê³  ê°€ì´ë“œë¼ì¸:**
{knowledge_info}
"""
            except Exception as e:
                logger.warning(f"Knowledge Base ì¡°íšŒ ì‹¤íŒ¨: {e}")

            prompt = f"""ë‹¹ì‹ ì€ Aurora MySQL ì„±ëŠ¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë°œê²¬ë‚´ìš©ê³¼ ë¦¬ë·°ê³ ë ¤ì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”.

{knowledge_context}

**ë¶„ì„í•  ë©”íŠ¸ë¦­ ë°ì´í„°:**
{metrics_data}

**ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸:**
{scenario_context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

## ë°œê²¬ë‚´ìš©
- ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ë¶„ì„
- ë¹„ì •ìƒì ì¸ íŒ¨í„´ì´ë‚˜ ì„ê³„ê°’ ì´ˆê³¼ í•­ëª©
- ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ë¶„ì„

## ë¦¬ë·°ê³ ë ¤ì‚¬í•­
- ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ í•­ëª©
- ì¤‘ì¥ê¸°ì  ìµœì í™” ë°©ì•ˆ
- ëª¨ë‹ˆí„°ë§ ê°•í™”ê°€ í•„ìš”í•œ ì˜ì—­
- êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ ì œì‹œ

ë¶„ì„ì€ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            claude_input = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2000,
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            })

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 inference profile í˜¸ì¶œ
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                analysis_result = response_body.get("content", [{}])[0].get("text", "")
            except Exception as e:
                logger.warning(f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}")
                # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
                try:
                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    response_body = json.loads(response.get("body").read())
                    analysis_result = response_body.get("content", [{}])[0].get("text", "")
                except Exception as e:
                    logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                    analysis_result = f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            # ë³´ê³ ì„œ ì œëª© ì—…ë°ì´íŠ¸ (Insight -> ë°œê²¬ë‚´ìš©, Recommendations -> ë¦¬ë·°ê³ ë ¤ì‚¬í•­)
            analysis_result = self.update_report_titles(analysis_result)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Claude ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def get_database_performance_metrics(self, database_secret: str, metric_type: str = "all") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel=True)
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute("""
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """)

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (pattern, count, avg_time, max_time, total_time) in enumerate(query_stats, 1):
                        pattern_short = (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """)

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            if metric_type in ["all", "io"]:
                # InnoDB ìƒíƒœ ì •ë³´
                cursor.execute("SHOW ENGINE INNODB STATUS")
                innodb_status = cursor.fetchone()
                if innodb_status:
                    status_text = innodb_status[2]
                    # ê°„ë‹¨í•œ íŒŒì‹±ìœ¼ë¡œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                    if "Buffer pool hit rate" in status_text:
                        import re
                        hit_rate_match = re.search(r'Buffer pool hit rate (\d+)', status_text)
                        if hit_rate_match:
                            hit_rate = hit_rate_match.group(1)
                            result += f"ğŸ’¾ **ë²„í¼ í’€ íˆíŠ¸ìœ¨:** {hit_rate}/1000\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´"""
        try:
            connection, tunnel_used = self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel=True)
            cursor = connection.cursor()

            result = f"ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½**\n\n"

            # ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            result += f"**í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤:** {current_db}\n\n"

            # í…Œì´ë¸” ëª©ë¡ê³¼ ê¸°ë³¸ ì •ë³´
            cursor.execute("""
                SELECT 
                    TABLE_NAME,
                    ENGINE,
                    TABLE_ROWS,
                    ROUND(((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024), 2) AS size_mb
                FROM information_schema.TABLES 
                WHERE TABLE_SCHEMA = DATABASE()
                ORDER BY size_mb DESC
            """)

            tables = cursor.fetchall()
            if tables:
                result += f"ğŸ“‹ **í…Œì´ë¸” ëª©ë¡ ({len(tables)}ê°œ):**\n"
                for table_name, engine, rows, size_mb in tables:
                    result += f"- {table_name} ({engine}, {rows:,}í–‰, {size_mb}MB)\n"
                result += "\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def diagnose_connection_issues(self, database_secret: str) -> str:
        """ì‹œë‚˜ë¦¬ì˜¤ 1: ì—°ê²° ìˆ˜ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨"""
        try:
            connection, tunnel_used = self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel=True)
            cursor = connection.cursor()

            result = "ğŸ” **ì—°ê²° ìˆ˜ ê¸‰ì¦ ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨**\n\n"

            # 1ë‹¨ê³„: ì—°ê²° ìƒíƒœ í™•ì¸
            cursor.execute("""
                SELECT 
                    SUBSTRING_INDEX(host, ':', 1) as client_ip,
                    COUNT(*) as connection_count,
                    command,
                    state
                FROM information_schema.processlist 
                GROUP BY client_ip, command, state
                ORDER BY connection_count DESC
                LIMIT 10
            """)

            connections = cursor.fetchall()
            if connections:
                result += "ğŸ“Š **í´ë¼ì´ì–¸íŠ¸ë³„ ì—°ê²° í˜„í™©:**\n"
                for client_ip, count, command, state in connections:
                    result += f"- {client_ip}: {count}ê°œ ì—°ê²° ({command}, {state})\n"
                result += "\n"

            # 2ë‹¨ê³„: ì¥ì‹œê°„ ì‹¤í–‰ ì¿¼ë¦¬ í™•ì¸
            cursor.execute("""
                SELECT 
                    id, user, host, db, command, time, state, 
                    LEFT(info, 100) as query_preview
                FROM information_schema.processlist 
                WHERE time > 30 AND command != 'Sleep'
                ORDER BY time DESC
                LIMIT 5
            """)

            long_queries = cursor.fetchall()
            if long_queries:
                result += "â° **ì¥ì‹œê°„ ì‹¤í–‰ ì¿¼ë¦¬:**\n"
                for query_id, user, host, db, command, time_sec, state, query in long_queries:
                    result += f"- ID {query_id}: {user}@{host} ({time_sec}ì´ˆ)\n"
                    result += f"  ì¿¼ë¦¬: {query}\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            # Claude AI ë¶„ì„ ìš”ì²­
            ai_analysis = await self.analyze_with_claude(
                result, 
                "ì—°ê²° ìˆ˜ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ìƒí™© ë¶„ì„", 
                "troubleshooting"
            )
            result += f"\n{ai_analysis}"

            return result

        except Exception as e:
            return f"âŒ ì—°ê²° ì´ìŠˆ ì§„ë‹¨ ì‹¤íŒ¨: {str(e)}"

    async def diagnose_io_bottleneck(self, database_secret: str) -> str:
        """ì‹œë‚˜ë¦¬ì˜¤ 2: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‘ì—…ìœ¼ë¡œ ì¸í•œ I/O ë³‘ëª© ì§„ë‹¨"""
        try:
            connection, tunnel_used = self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel=True)
            cursor = connection.cursor()

            result = "ğŸ” **I/O ë³‘ëª© í˜„ìƒ ì§„ë‹¨**\n\n"

            # InnoDB ìƒíƒœ í™•ì¸
            cursor.execute("SHOW ENGINE INNODB STATUS")
            innodb_status = cursor.fetchone()
            
            if innodb_status:
                status_text = innodb_status[2]
                result += "ğŸ“Š **InnoDB ìƒíƒœ ë¶„ì„:**\n"
                
                # ë¡œê·¸ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ
                import re
                log_match = re.search(r'Log sequence number (\d+)', status_text)
                if log_match:
                    result += f"- Log sequence number: {log_match.group(1)}\n"
                
                # ë²„í¼ í’€ íˆíŠ¸ìœ¨ ì¶”ì¶œ
                hit_rate_match = re.search(r'Buffer pool hit rate (\d+) / (\d+)', status_text)
                if hit_rate_match:
                    hit_rate = int(hit_rate_match.group(1)) / int(hit_rate_match.group(2)) * 100
                    result += f"- Buffer pool hit rate: {hit_rate:.2f}%\n"
                
                result += "\n"

            # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëŒ€ìš©ëŸ‰ ì‘ì—… í™•ì¸
            cursor.execute("""
                SELECT 
                    id, user, host, db, command, time, state,
                    LEFT(info, 200) as query_preview
                FROM information_schema.processlist 
                WHERE (info LIKE '%INSERT%' OR info LIKE '%UPDATE%' OR info LIKE '%DELETE%')
                AND time > 10
                ORDER BY time DESC
                LIMIT 5
            """)

            batch_queries = cursor.fetchall()
            if batch_queries:
                result += "ğŸ”„ **ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‘ì—…:**\n"
                for query_id, user, host, db, command, time_sec, state, query in batch_queries:
                    result += f"- ID {query_id}: {user}@{host} ({time_sec}ì´ˆ)\n"
                    result += f"  ì‘ì—…: {query}\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            # Claude AI ë¶„ì„ ìš”ì²­
            ai_analysis = await self.analyze_with_claude(
                result, 
                "ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‘ì—…ìœ¼ë¡œ ì¸í•œ I/O ë³‘ëª© ìƒí™© ë¶„ì„", 
                "troubleshooting"
            )
            result += f"\n{ai_analysis}"

            return result

        except Exception as e:
            return f"âŒ I/O ë³‘ëª© ì§„ë‹¨ ì‹¤íŒ¨: {str(e)}"

    async def diagnose_memory_pressure(self, database_secret: str) -> str:
        """ì‹œë‚˜ë¦¬ì˜¤ 3: ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨"""
        try:
            connection, tunnel_used = self.get_db_connection(database_secret, self.selected_database, use_ssh_tunnel=True)
            cursor = connection.cursor()

            result = "ğŸ” **ë©”ëª¨ë¦¬ ë¶€ì¡± ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨**\n\n"

            # ë©”ëª¨ë¦¬ ê´€ë ¨ ìƒíƒœ ë³€ìˆ˜ í™•ì¸
            cursor.execute("""
                SHOW STATUS WHERE Variable_name IN (
                    'Innodb_buffer_pool_pages_total',
                    'Innodb_buffer_pool_pages_free',
                    'Innodb_buffer_pool_pages_dirty',
                    'Innodb_buffer_pool_read_requests',
                    'Innodb_buffer_pool_reads'
                )
            """)

            memory_stats = cursor.fetchall()
            if memory_stats:
                result += "ğŸ’¾ **ë©”ëª¨ë¦¬ ìƒíƒœ:**\n"
                stats_dict = {name: value for name, value in memory_stats}
                
                total_pages = int(stats_dict.get('Innodb_buffer_pool_pages_total', 0))
                free_pages = int(stats_dict.get('Innodb_buffer_pool_pages_free', 0))
                dirty_pages = int(stats_dict.get('Innodb_buffer_pool_pages_dirty', 0))
                
                if total_pages > 0:
                    used_pages = total_pages - free_pages
                    result += f"- ì´ ë²„í¼ í’€ í˜ì´ì§€: {total_pages:,}\n"
                    result += f"- ì‚¬ìš© ì¤‘ í˜ì´ì§€: {used_pages:,} ({used_pages/total_pages*100:.1f}%)\n"
                    result += f"- ë”í‹° í˜ì´ì§€: {dirty_pages:,} ({dirty_pages/total_pages*100:.1f}%)\n"
                
                read_requests = int(stats_dict.get('Innodb_buffer_pool_read_requests', 0))
                reads = int(stats_dict.get('Innodb_buffer_pool_reads', 0))
                
                if read_requests > 0:
                    hit_rate = (1 - reads/read_requests) * 100
                    result += f"- ë²„í¼ í’€ íˆíŠ¸ìœ¨: {hit_rate:.2f}%\n"
                
                result += "\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            # Claude AI ë¶„ì„ ìš”ì²­
            ai_analysis = await self.analyze_with_claude(
                result, 
                "ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ìƒí™© ë¶„ì„", 
                "troubleshooting"
            )
            result += f"\n{ai_analysis}"

            return result

        except Exception as e:
            return f"âŒ ë©”ëª¨ë¦¬ ì••ë°• ì§„ë‹¨ ì‹¤íŒ¨: {str(e)}"

    async def dynamic_troubleshooting_workflow(self, database_secret: str, user_question: str) -> str:
        """ë™ì  ì›Œí¬í”Œë¡œìš°: ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¥¸ ë§ì¶¤í˜• ì§„ë‹¨"""
        try:
            # ë¨¼ì € Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ
            knowledge_info = await self.query_knowledge_base(user_question, "troubleshooting")
            
            # ì§ˆë¬¸ ë¶„ì„ì„ í†µí•œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜
            question_lower = user_question.lower()
            
            if any(keyword in question_lower for keyword in ['ì—°ê²°', 'connection', 'íƒ€ì„ì•„ì›ƒ', 'timeout']):
                diagnosis_result = await self.diagnose_connection_issues(database_secret)
                scenario_type = "ì—°ê²° ë¬¸ì œ"
            elif any(keyword in question_lower for keyword in ['ëŠë¦¼', 'slow', 'io', 'ë””ìŠ¤í¬', 'disk']):
                diagnosis_result = await self.diagnose_io_bottleneck(database_secret)
                scenario_type = "I/O ë³‘ëª©"
            elif any(keyword in question_lower for keyword in ['ë©”ëª¨ë¦¬', 'memory', 'ë²„í¼', 'buffer']):
                diagnosis_result = await self.diagnose_memory_pressure(database_secret)
                scenario_type = "ë©”ëª¨ë¦¬ ë¶€ì¡±"
            else:
                # ì¼ë°˜ì ì¸ ì„±ëŠ¥ ë¶„ì„
                performance_metrics = await self.get_database_performance_metrics(database_secret)
                diagnosis_result = performance_metrics
                scenario_type = "ì¼ë°˜ ì„±ëŠ¥ ë¶„ì„"

            # Claude AIì—ê²Œ ì¢…í•© ë¶„ì„ ìš”ì²­
            comprehensive_analysis = await self.analyze_with_claude(
                f"ì‚¬ìš©ì ì§ˆë¬¸: {user_question}\n\nì§„ë‹¨ ê²°ê³¼:\n{diagnosis_result}",
                f"{scenario_type} ìƒí™©ì—ì„œì˜ ì¢…í•©ì  ë¶„ì„ ë° í•´ê²°ë°©ì•ˆ ì œì‹œ",
                "troubleshooting"
            )

            result = f"""ğŸ¤– **ë™ì  ì›Œí¬í”Œë¡œìš° ë¶„ì„ ê²°ê³¼**

**ì‚¬ìš©ì ì§ˆë¬¸:** {user_question}
**ë¶„ë¥˜ëœ ì‹œë‚˜ë¦¬ì˜¤:** {scenario_type}

**Knowledge Base ì°¸ê³  ì •ë³´:**
{knowledge_info}

**ì§„ë‹¨ ê²°ê³¼:**
{diagnosis_result}

**ì¢…í•© ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­:**
{comprehensive_analysis}
"""
            return result

        except Exception as e:
            return f"âŒ ë™ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"

    def save_analysis_history(self, analysis_result: str):
        """ë¶„ì„ ê²°ê³¼ë¥¼ íˆìŠ¤í† ë¦¬ íŒŒì¼ì— ì €ì¥"""
        try:
            history_file = CURRENT_DIR / "analyze_db_history.md"
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            with open(history_file, "a", encoding="utf-8") as f:
                f.write(f"\n## {timestamp}\n\n")
                f.write(analysis_result)
                f.write("\n\n---\n")
            
            logger.info(f"ë¶„ì„ ê²°ê³¼ê°€ {history_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def update_report_titles(self, content: str) -> str:
        """ë³´ê³ ì„œì˜ Insight íƒ€ì´í‹€ì„ ë°œê²¬ë‚´ìš©, ë¦¬ë·°ê³ ë ¤ì‚¬í•­ìœ¼ë¡œ ë³€ê²½"""
        try:
            # Insight ê´€ë ¨ ì œëª©ë“¤ì„ í•œêµ­ì–´ë¡œ ë³€ê²½
            content = re.sub(r'## Insights?', '## ë°œê²¬ë‚´ìš©', content, flags=re.IGNORECASE)
            content = re.sub(r'## Key Insights?', '## ë°œê²¬ë‚´ìš©', content, flags=re.IGNORECASE)
            content = re.sub(r'## Analysis Insights?', '## ë°œê²¬ë‚´ìš©', content, flags=re.IGNORECASE)
            
            # Recommendationsë¥¼ ë¦¬ë·°ê³ ë ¤ì‚¬í•­ìœ¼ë¡œ ë³€ê²½
            content = re.sub(r'## Recommendations?', '## ë¦¬ë·°ê³ ë ¤ì‚¬í•­', content, flags=re.IGNORECASE)
            content = re.sub(r'## Key Recommendations?', '## ë¦¬ë·°ê³ ë ¤ì‚¬í•­', content, flags=re.IGNORECASE)
            content = re.sub(r'## Action Items?', '## ë¦¬ë·°ê³ ë ¤ì‚¬í•­', content, flags=re.IGNORECASE)
            
            return content
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ì œëª© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return content

    async def list_database_secrets(self, keyword: str = None) -> str:
        """AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ (ë‹¤ì¤‘ ë¦¬ì „ ì§€ì›)"""
        try:
            regions = [self.default_region, "us-east-1", "us-west-2"]
            all_secrets = []
            
            for region in regions:
                try:
                    session = boto3.session.Session()
                    client = session.client(
                        service_name="secretsmanager",
                        region_name=region,
                        verify=False,
                    )
                    
                    # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
                    next_token = None
                    while True:
                        if next_token:
                            response = client.list_secrets(NextToken=next_token)
                        else:
                            response = client.list_secrets()
                        
                        for secret in response.get("SecretList", []):
                            secret["Region"] = region  # ë¦¬ì „ ì •ë³´ ì¶”ê°€
                            all_secrets.append(secret)
                        
                        if "NextToken" not in response:
                            break
                        next_token = response["NextToken"]
                        
                except Exception as e:
                    logger.warning(f"ë¦¬ì „ {region}ì—ì„œ ì‹œí¬ë¦¿ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue
            
            result = "ğŸ” **ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:**\n\n"
            
            filtered_secrets = []
            for secret in all_secrets:
                secret_name = secret.get("Name", "")
                if keyword:
                    if keyword.lower() in secret_name.lower():
                        filtered_secrets.append(secret)
                else:
                    # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì‹œí¬ë¦¿ë§Œ í•„í„°ë§
                    if any(db_keyword in secret_name.lower() for db_keyword in ['db', 'database', 'mysql', 'rds', 'aurora']):
                        filtered_secrets.append(secret)
            
            if filtered_secrets:
                for i, secret in enumerate(filtered_secrets, 1):
                    result += f"{i}. **{secret['Name']}** ({secret['Region']})\n"
                    if secret.get('Description'):
                        result += f"   - ì„¤ëª…: {secret['Description']}\n"
                    result += f"   - ìƒì„±ì¼: {secret.get('CreatedDate', 'N/A')}\n\n"
            else:
                result += "ì¡°ê±´ì— ë§ëŠ” ì‹œí¬ë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            
            return result
            
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def setup_cloudwatch_client(self, region_name: str = None):
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            if region_name is None:
                region_name = self.default_region
            self.cloudwatch = boto3.client('cloudwatch', region_name=region_name)
            return True
        except Exception as e:
            logger.error(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def collect_metrics(self, db_instance_identifier: str, hours: int = 24, 
                            metrics: Optional[List[str]] = None, region: str = None) -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            if region is None:
                region = self.default_region
                
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            if not metrics:
                metrics = self.default_metrics

            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            data = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
            for metric in metrics:
                try:
                    response = self.cloudwatch.get_metric_statistics(
                        Namespace='AWS/RDS',
                        MetricName=metric,
                        Dimensions=[
                            {
                                'Name': 'DBInstanceIdentifier',
                                'Value': db_instance_identifier
                            },
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5ë¶„ ê°„ê²©
                        Statistics=['Average']
                    )
                    
                    # ì‘ë‹µì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    for point in response['Datapoints']:
                        data.append({
                            'Timestamp': point['Timestamp'].replace(tzinfo=None),
                            'Metric': metric,
                            'Value': point['Average']
                        })
                except Exception as e:
                    logger.error(f"ë©”íŠ¸ë¦­ {metric} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

            if not data:
                return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            df = df.sort_values('Timestamp')

            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            pivot_df = df.pivot(index='Timestamp', columns='Metric', values='Value')

            # CSV íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = DATA_DIR / f'database_metrics_{db_instance_identifier}_{timestamp}.csv'
            pivot_df.to_csv(csv_file)

            return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {csv_file}"

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def analyze_correlation(self, csv_file: str, target_metric: str = 'CPUUtilization', 
                                top_n: int = 10) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"íƒ€ê²Ÿ ë©”íŠ¸ë¦­ '{target_metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ìƒê´€ ë¶„ì„
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(target_metric, errors='ignore')
            top_correlations = target_correlations.nlargest(top_n)

            # ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
            result = f"ğŸ“Š {target_metric}ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ {top_n}ê°œ ë©”íŠ¸ë¦­:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"â€¢ {metric}: {correlation:.4f}\n"

            # ì‹œê°í™”
            plt.figure(figsize=(12, 6))
            top_correlations.plot(kind='bar')
            plt.title(f'Top {top_n} Metrics Correlated with {target_metric}')
            plt.xlabel('Metrics')
            plt.ylabel('Correlation Coefficient')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'correlation_analysis_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"\nğŸ“ˆ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def detect_outliers(self, csv_file: str, std_threshold: float = 2.0) -> str:
        """ì•„ì›ƒë¼ì´ì–´ íƒì§€"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•œ í†µê³„ ê³„ì‚°
            stats = df.agg(['mean', 'max', 'min', 'std'])

            result = "ğŸ“Š ê° ë©”íŠ¸ë¦­ì˜ í†µê³„:\n"
            result += stats.to_string() + "\n\n"

            result += f"ğŸš¨ ì•„ì›ƒë¼ì´ì–´ íƒì§€ ê²°ê³¼ (ì„ê³„ê°’: Â±{std_threshold}Ïƒ):\n\n"

            outlier_summary = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ì•„ì›ƒë¼ì´ì–´ íƒì§€
            for column in df.columns:
                series = df[column]
                mean = series.mean()
                std = series.std()
                lower_bound = mean - std_threshold * std
                upper_bound = mean + std_threshold * std
                
                outliers = series[(series < lower_bound) | (series > upper_bound)]
                
                if not outliers.empty:
                    result += f"âš ï¸ {column} ë©”íŠ¸ë¦­ì˜ ì•„ì›ƒë¼ì´ì–´ ({len(outliers)}ê°œ):\n"
                    result += f"   ì •ìƒ ë²”ìœ„: {lower_bound:.2f} ~ {upper_bound:.2f}\n"
                    
                    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ í‘œì‹œ
                    for i, (timestamp, value) in enumerate(outliers.items()):
                        if i >= 5:
                            result += f"   ... ë° {len(outliers) - 5}ê°œ ë”\n"
                            break
                        result += f"   â€¢ {timestamp}: {value:.2f}\n"
                    result += "\n"
                    
                    outlier_summary.append({
                        'metric': column,
                        'count': len(outliers),
                        'percentage': (len(outliers) / len(series)) * 100
                    })
                else:
                    result += f"âœ… {column}: ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ\n"

            # ìš”ì•½ ì •ë³´
            if outlier_summary:
                result += "\nğŸ“‹ ì•„ì›ƒë¼ì´ì–´ ìš”ì•½:\n"
                for summary in outlier_summary:
                    result += f"â€¢ {summary['metric']}: {summary['count']}ê°œ ({summary['percentage']:.1f}%)\n"

            return result

        except Exception as e:
            return f"ì•„ì›ƒë¼ì´ì–´ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def perform_regression_analysis(self, csv_file: str, predictor_metric: str, 
                                        target_metric: str = 'CPUUtilization') -> str:
        """íšŒê·€ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)

            # í•„ìš”í•œ ë©”íŠ¸ë¦­ í™•ì¸
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ë°ì´í„° ì¤€ë¹„
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN ê°’ ì²˜ë¦¬
            imputer = SimpleImputer(strategy='mean')
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± (2ì°¨)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_poly_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê³„ìˆ˜ ì¶œë ¥
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ({predictor_metric} â†’ {target_metric}):\n\n"
            result += f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥:\n"
            result += f"â€¢ Mean Squared Error: {mse:.4f}\n"
            result += f"â€¢ R-squared Score: {r2:.4f}\n\n"
            
            result += f"ğŸ”¢ ë‹¤í•­ íšŒê·€ ëª¨ë¸ (2ì°¨):\n"
            result += f"y = {coefficients[1]:.4f}xÂ² + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            plt.figure(figsize=(12, 8))
            
            # ì‚°ì ë„
            plt.subplot(2, 1, 1)
            plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='ì‹¤ì œ ë°ì´í„°')
            
            # ì˜ˆì¸¡ ê³¡ì„ ì„ ìœ„í•œ ì •ë ¬ëœ ë°ì´í„°
            X_plot = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)
            X_plot_poly = poly_features.transform(X_plot)
            y_plot_pred = model.predict(X_plot_poly)
            
            plt.plot(X_plot, y_plot_pred, color='red', linewidth=2, label='ì˜ˆì¸¡ ëª¨ë¸')
            plt.title(f'{predictor_metric} vs {target_metric} íšŒê·€ ë¶„ì„')
            plt.xlabel(predictor_metric)
            plt.ylabel(target_metric)
            plt.legend()
            plt.grid(True, alpha=0.3)

            # ì”ì°¨ í”Œë¡¯
            plt.subplot(2, 1, 2)
            residuals = y_test - y_pred
            plt.scatter(y_pred, residuals, color='green', alpha=0.6)
            plt.axhline(y=0, color='red', linestyle='--')
            plt.title('ì”ì°¨ í”Œë¡¯ (Residual Plot)')
            plt.xlabel('ì˜ˆì¸¡ê°’')
            plt.ylabel('ì”ì°¨')
            plt.grid(True, alpha=0.3)

            plt.tight_layout()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = OUTPUT_DIR / f'regression_analysis_{predictor_metric}_{target_metric}_{timestamp}.png'
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()

            result += f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"íšŒê·€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def list_data_files(self) -> str:
        """ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ“ ë°ì´í„° íŒŒì¼ ëª©ë¡:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"â€¢ {file.name}\n"
                result += f"  í¬ê¸°: {file_size:,} bytes\n"
                result += f"  ìˆ˜ì •ì¼: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            
            result = f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ({csv_file}):\n\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë©”íŠ¸ë¦­ ìˆ˜: {len(df.columns)}ê°œ\n\n"
            
            result += "ğŸ“Š ë©”íŠ¸ë¦­ ëª©ë¡:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}ê°œ ë°ì´í„°)\n"

            # ê¸°ë³¸ í†µê³„
            result += f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def generate_correlation_report(self, csv_file: str, target_metrics: List[str] = None) -> str:
        """ìƒê´€ê´€ê³„ ìƒì„¸ ë¶„ì„ ë° HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith('/'):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col='Timestamp', parse_dates=True)
            df = df.dropna()

            # ê¸°ë³¸ íƒ€ê²Ÿ ë©”íŠ¸ë¦­ ì„¤ì •
            if target_metrics is None:
                target_metrics = ['CPUUtilization', 'DatabaseConnections', 'WriteIOPS', 'ReadIOPS']
            
            # ì¡´ì¬í•˜ëŠ” ë©”íŠ¸ë¦­ë§Œ í•„í„°ë§
            available_metrics = [m for m in target_metrics if m in df.columns]
            if not available_metrics:
                return f"ì§€ì •ëœ íƒ€ê²Ÿ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = OUTPUT_DIR / f'correlation_detailed_report_{timestamp}.html'
            
            # ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨ ìˆ˜í–‰
            performance_analysis = self._analyze_performance_issues(df)
            outlier_analysis = self._detect_outliers_for_report(df)
            
            # HTML ë¦¬í¬íŠ¸ ìƒì„±
            html_content = self._generate_correlation_html(df, available_metrics, csv_file, performance_analysis, outlier_analysis)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(html_content)

            # ìš”ì•½ ê²°ê³¼ ìƒì„±
            correlation_matrix = df.corr()
            result = f"ğŸ“Š ìƒê´€ê´€ê³„ ìƒì„¸ ë¶„ì„ ì™„ë£Œ\n\n"
            result += f"ğŸ“ ë¶„ì„ íŒŒì¼: {csv_file}\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë¶„ì„ ë©”íŠ¸ë¦­: {len(available_metrics)}ê°œ\n\n"

            # ê° íƒ€ê²Ÿ ë©”íŠ¸ë¦­ë³„ ìƒìœ„ ìƒê´€ê´€ê³„
            for target in available_metrics:
                target_corr = correlation_matrix[target].abs().drop(target, errors='ignore')
                top_3 = target_corr.nlargest(3)
                result += f"ğŸ¯ {target} ìƒìœ„ ìƒê´€ê´€ê³„:\n"
                for metric, corr in top_3.items():
                    result += f"  â€¢ {metric}: {corr:.4f}\n"
                result += "\n"

            result += f"ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}\n"
            result += f"ğŸ“Š ë¦¬í¬íŠ¸ì—ëŠ” íˆíŠ¸ë§µ, ì‚°ì ë„, ì‹œê³„ì—´ ë¶„ì„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."

            return result

        except Exception as e:
            return f"ìƒê´€ê´€ê³„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _analyze_performance_issues(self, df: pd.DataFrame) -> dict:
        """ì„±ëŠ¥ ë¬¸ì œ ë¶„ì„"""
        analysis = {
            'cpu_spikes': [],
            'connection_issues': [],
            'io_bottlenecks': [],
            'memory_pressure': [],
            'key_findings': []
        }
        
        try:
            # CPU ê¸‰ì¦ íƒì§€
            if 'CPUUtilization' in df.columns:
                cpu_mean = df['CPUUtilization'].mean()
                cpu_std = df['CPUUtilization'].std()
                cpu_threshold = cpu_mean + 2 * cpu_std
                cpu_spikes = df[df['CPUUtilization'] > cpu_threshold]
                
                for idx, row in cpu_spikes.iterrows():
                    analysis['cpu_spikes'].append({
                        'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                        'value': row['CPUUtilization'],
                        'severity': 'High' if row['CPUUtilization'] > cpu_mean + 3 * cpu_std else 'Medium'
                    })
            
            # ì—°ê²° ìˆ˜ ë¬¸ì œ íƒì§€
            if 'DatabaseConnections' in df.columns:
                conn_mean = df['DatabaseConnections'].mean()
                conn_std = df['DatabaseConnections'].std()
                conn_threshold = conn_mean + 2 * conn_std
                conn_spikes = df[df['DatabaseConnections'] > conn_threshold]
                
                for idx, row in conn_spikes.iterrows():
                    analysis['connection_issues'].append({
                        'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                        'value': row['DatabaseConnections'],
                        'severity': 'High' if row['DatabaseConnections'] > conn_mean + 3 * conn_std else 'Medium'
                    })
            
            # I/O ë³‘ëª© íƒì§€
            if 'ReadIOPS' in df.columns and 'WriteIOPS' in df.columns:
                read_mean = df['ReadIOPS'].mean()
                read_std = df['ReadIOPS'].std()
                write_mean = df['WriteIOPS'].mean()
                write_std = df['WriteIOPS'].std()
                
                read_spikes = df[df['ReadIOPS'] > read_mean + 2 * read_std]
                write_spikes = df[df['WriteIOPS'] > write_mean + 2 * write_std]
                
                for idx, row in read_spikes.iterrows():
                    analysis['io_bottlenecks'].append({
                        'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                        'type': 'Read',
                        'value': row['ReadIOPS'],
                        'severity': 'High' if row['ReadIOPS'] > read_mean + 3 * read_std else 'Medium'
                    })
                
                for idx, row in write_spikes.iterrows():
                    analysis['io_bottlenecks'].append({
                        'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                        'type': 'Write',
                        'value': row['WriteIOPS'],
                        'severity': 'High' if row['WriteIOPS'] > write_mean + 3 * write_std else 'Medium'
                    })
            
            # ë©”ëª¨ë¦¬ ì••ë°• íƒì§€
            if 'FreeableMemory' in df.columns:
                mem_mean = df['FreeableMemory'].mean()
                mem_std = df['FreeableMemory'].std()
                mem_threshold = mem_mean - 2 * mem_std
                mem_pressure = df[df['FreeableMemory'] < mem_threshold]
                
                for idx, row in mem_pressure.iterrows():
                    analysis['memory_pressure'].append({
                        'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                        'value': row['FreeableMemory'],
                        'severity': 'High' if row['FreeableMemory'] < mem_mean - 3 * mem_std else 'Medium'
                    })
            
            # ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„±
            if analysis['cpu_spikes']:
                analysis['key_findings'].append(f"CPU ì‚¬ìš©ë¥  ê¸‰ì¦ {len(analysis['cpu_spikes'])}íšŒ íƒì§€")
            if analysis['connection_issues']:
                analysis['key_findings'].append(f"ì—°ê²° ìˆ˜ ê¸‰ì¦ {len(analysis['connection_issues'])}íšŒ íƒì§€")
            if analysis['io_bottlenecks']:
                analysis['key_findings'].append(f"I/O ë³‘ëª© í˜„ìƒ {len(analysis['io_bottlenecks'])}íšŒ íƒì§€")
            if analysis['memory_pressure']:
                analysis['key_findings'].append(f"ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© {len(analysis['memory_pressure'])}íšŒ íƒì§€")
                
        except Exception as e:
            analysis['error'] = str(e)
        
        return analysis
    
    def _detect_outliers_for_report(self, df: pd.DataFrame, std_threshold: float = 2.0) -> dict:
        """ë¦¬í¬íŠ¸ìš© ì•„ì›ƒë¼ì´ì–´ íƒì§€"""
        outliers = {}
        
        try:
            for column in df.columns:
                if df[column].dtype in ['float64', 'int64']:
                    mean_val = df[column].mean()
                    std_val = df[column].std()
                    
                    if std_val > 0:
                        upper_bound = mean_val + std_threshold * std_val
                        lower_bound = mean_val - std_threshold * std_val
                        
                        outlier_data = df[(df[column] > upper_bound) | (df[column] < lower_bound)]
                        
                        if len(outlier_data) > 0:
                            outliers[column] = {
                                'count': len(outlier_data),
                                'percentage': (len(outlier_data) / len(df)) * 100,
                                'values': []
                            }
                            
                            # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                            for idx, row in outlier_data.head(5).iterrows():
                                outliers[column]['values'].append({
                                    'timestamp': idx.strftime('%Y-%m-%d %H:%M:%S'),
                                    'value': row[column],
                                    'deviation': abs(row[column] - mean_val) / std_val
                                })
        except Exception as e:
            outliers['error'] = str(e)
        
        return outliers

    def _generate_correlation_html(self, df: pd.DataFrame, target_metrics: List[str], csv_file: str, 
                                 performance_analysis: dict, outlier_analysis: dict) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„ HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        correlation_matrix = df.corr()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(12, 10))
        if sns is not None:
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        else:
            im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
            plt.colorbar(im, shrink=0.8)
            for i in range(len(correlation_matrix.columns)):
                for j in range(len(correlation_matrix.columns)):
                    plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                            ha='center', va='center', fontsize=8)
            plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)
            plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
        
        plt.title('Correlation Matrix Heatmap')
        plt.tight_layout()
        heatmap_file = f'correlation_heatmap_{timestamp}.png'
        plt.savefig(OUTPUT_DIR / heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()

        # ì‹œê³„ì—´ ì°¨íŠ¸ ìƒì„± (ëª¨ë“  ë©”íŠ¸ë¦­)
        plt.figure(figsize=(20, 12))
        metrics_to_plot = list(df.columns)[:12]  # ìµœëŒ€ 12ê°œ ë©”íŠ¸ë¦­
        rows = 4
        cols = 3
        
        for i, metric in enumerate(metrics_to_plot):
            plt.subplot(rows, cols, i+1)
            plt.plot(df.index, df[metric], label=metric, linewidth=1, alpha=0.8)
            plt.title(f'{metric}', fontsize=10)
            plt.xlabel('Time', fontsize=8)
            plt.ylabel('Value', fontsize=8)
            plt.xticks(rotation=45, fontsize=8)
            plt.yticks(fontsize=8)
            plt.grid(True, alpha=0.3)
            
            # ì•„ì›ƒë¼ì´ì–´ í‘œì‹œ
            if metric in outlier_analysis:
                for outlier in outlier_analysis[metric]['values'][:3]:
                    outlier_time = pd.to_datetime(outlier['timestamp'])
                    if outlier_time in df.index:
                        plt.scatter(outlier_time, outlier['value'], color='red', s=50, alpha=0.7)
        
        plt.tight_layout()
        timeseries_file = f'correlation_timeseries_{timestamp}.png'
        plt.savefig(OUTPUT_DIR / timeseries_file, dpi=300, bbox_inches='tight')
        plt.close()

        # ì„±ëŠ¥ ë¬¸ì œ ì‹œê°í™”
        plt.figure(figsize=(15, 10))
        
        # CPUì™€ DBLoad ê´€ê³„
        if 'CPUUtilization' in df.columns and 'DBLoad' in df.columns:
            plt.subplot(2, 2, 1)
            plt.scatter(df['CPUUtilization'], df['DBLoad'], alpha=0.6)
            plt.xlabel('CPU Utilization (%)')
            plt.ylabel('DB Load')
            plt.title('CPU vs DB Load')
            plt.grid(True, alpha=0.3)
        
        # ì—°ê²° ìˆ˜ì™€ ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰
        if 'DatabaseConnections' in df.columns and 'NetworkTransmitThroughput' in df.columns:
            plt.subplot(2, 2, 2)
            plt.scatter(df['DatabaseConnections'], df['NetworkTransmitThroughput'], alpha=0.6)
            plt.xlabel('Database Connections')
            plt.ylabel('Network Transmit Throughput')
            plt.title('Connections vs Network')
            plt.grid(True, alpha=0.3)
        
        # I/O íŒ¨í„´
        if 'ReadIOPS' in df.columns and 'WriteIOPS' in df.columns:
            plt.subplot(2, 2, 3)
            plt.scatter(df['ReadIOPS'], df['WriteIOPS'], alpha=0.6)
            plt.xlabel('Read IOPS')
            plt.ylabel('Write IOPS')
            plt.title('Read vs Write IOPS')
            plt.grid(True, alpha=0.3)
        
        # ë©”ëª¨ë¦¬ì™€ CPU
        if 'FreeableMemory' in df.columns and 'CPUUtilization' in df.columns:
            plt.subplot(2, 2, 4)
            plt.scatter(df['FreeableMemory'], df['CPUUtilization'], alpha=0.6)
            plt.xlabel('Freeable Memory')
            plt.ylabel('CPU Utilization (%)')
            plt.title('Memory vs CPU')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        scatter_file = f'performance_scatter_{timestamp}.png'
        plt.savefig(OUTPUT_DIR / scatter_file, dpi=300, bbox_inches='tight')
        plt.close()

        # HTML í…œí”Œë¦¿
        html_template = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ - {csv_file}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #e0e0e0; border-radius: 8px; }}
        .alert {{ padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .alert-danger {{ background-color: #f8d7da; border-color: #f5c6cb; color: #721c24; }}
        .alert-warning {{ background-color: #fff3cd; border-color: #ffeaa7; color: #856404; }}
        .alert-info {{ background-color: #d1ecf1; border-color: #bee5eb; color: #0c5460; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ padding: 15px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #007bff; }}
        .performance-card {{ padding: 15px; background: #fff5f5; border-radius: 8px; border-left: 4px solid #dc3545; }}
        .correlation-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .correlation-table th, .correlation-table td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        .correlation-table th {{ background-color: #f8f9fa; font-weight: bold; }}
        .high-corr {{ color: #dc3545; font-weight: bold; }}
        .medium-corr {{ color: #fd7e14; font-weight: bold; }}
        .low-corr {{ color: #28a745; }}
        .chart-container {{ text-align: center; margin: 20px 0; }}
        .chart-container img {{ max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
        .summary-stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .stat-box {{ text-align: center; padding: 15px; background: #e3f2fd; border-radius: 8px; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #1976d2; }}
        .stat-label {{ font-size: 14px; color: #666; }}
        .issue-list {{ list-style: none; padding: 0; }}
        .issue-item {{ padding: 10px; margin: 5px 0; background: #fff; border-left: 4px solid #dc3545; border-radius: 4px; }}
        .issue-high {{ border-left-color: #dc3545; }}
        .issue-medium {{ border-left-color: #fd7e14; }}
        .timestamp {{ font-family: monospace; color: #666; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸</h1>
            <p>íŒŒì¼: {csv_file} | ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>

        <div class="section">
            <h2>ğŸ“ˆ ë°ì´í„° ê°œìš”</h2>
            <div class="summary-stats">
                <div class="stat-box">
                    <div class="stat-number">{len(df)}</div>
                    <div class="stat-label">ë°ì´í„° í¬ì¸íŠ¸</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{len(df.columns)}</div>
                    <div class="stat-label">ë©”íŠ¸ë¦­ ìˆ˜</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{len(target_metrics)}</div>
                    <div class="stat-label">ë¶„ì„ ëŒ€ìƒ</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{(df.index.max() - df.index.min()).days + 1}</div>
                    <div class="stat-label">ë¶„ì„ ì¼ìˆ˜</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>ğŸš¨ ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨</h2>
        """

        # ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨ ê²°ê³¼ ì¶”ê°€
        if performance_analysis.get('key_findings'):
            html_template += '<div class="alert alert-danger"><h3>ì£¼ìš” ë°œê²¬ì‚¬í•­</h3><ul>'
            for finding in performance_analysis['key_findings']:
                html_template += f'<li>{finding}</li>'
            html_template += '</ul></div>'

        html_template += '<div class="metric-grid">'

        # CPU ê¸‰ì¦ ì´ë²¤íŠ¸
        if performance_analysis.get('cpu_spikes'):
            html_template += '''
                <div class="performance-card">
                    <h3>ğŸ”¥ CPU ì‚¬ìš©ë¥  ê¸‰ì¦</h3>
                    <ul class="issue-list">
            '''
            for spike in performance_analysis['cpu_spikes'][:5]:
                severity_class = 'issue-high' if spike['severity'] == 'High' else 'issue-medium'
                html_template += f'''
                    <li class="issue-item {severity_class}">
                        <div class="timestamp">{spike['timestamp']}</div>
                        <div>CPU: {spike['value']:.2f}% ({spike['severity']})</div>
                    </li>
                '''
            html_template += '</ul></div>'

        # ì—°ê²° ìˆ˜ ë¬¸ì œ
        if performance_analysis.get('connection_issues'):
            html_template += '''
                <div class="performance-card">
                    <h3>ğŸ”— ì—°ê²° ìˆ˜ ê¸‰ì¦</h3>
                    <ul class="issue-list">
            '''
            for issue in performance_analysis['connection_issues'][:5]:
                severity_class = 'issue-high' if issue['severity'] == 'High' else 'issue-medium'
                html_template += f'''
                    <li class="issue-item {severity_class}">
                        <div class="timestamp">{issue['timestamp']}</div>
                        <div>ì—°ê²° ìˆ˜: {issue['value']:.0f} ({issue['severity']})</div>
                    </li>
                '''
            html_template += '</ul></div>'

        # I/O ë³‘ëª©
        if performance_analysis.get('io_bottlenecks'):
            html_template += '''
                <div class="performance-card">
                    <h3>ğŸ’¾ I/O ë³‘ëª© í˜„ìƒ</h3>
                    <ul class="issue-list">
            '''
            for bottleneck in performance_analysis['io_bottlenecks'][:5]:
                severity_class = 'issue-high' if bottleneck['severity'] == 'High' else 'issue-medium'
                html_template += f'''
                    <li class="issue-item {severity_class}">
                        <div class="timestamp">{bottleneck['timestamp']}</div>
                        <div>{bottleneck['type']} IOPS: {bottleneck['value']:.2f} ({bottleneck['severity']})</div>
                    </li>
                '''
            html_template += '</ul></div>'

        # ë©”ëª¨ë¦¬ ì••ë°•
        if performance_analysis.get('memory_pressure'):
            html_template += '''
                <div class="performance-card">
                    <h3>ğŸ§  ë©”ëª¨ë¦¬ ì••ë°•</h3>
                    <ul class="issue-list">
            '''
            for pressure in performance_analysis['memory_pressure'][:5]:
                severity_class = 'issue-high' if pressure['severity'] == 'High' else 'issue-medium'
                html_template += f'''
                    <li class="issue-item {severity_class}">
                        <div class="timestamp">{pressure['timestamp']}</div>
                        <div>ì—¬ìœ  ë©”ëª¨ë¦¬: {pressure['value']:,.0f} bytes ({pressure['severity']})</div>
                    </li>
                '''
            html_template += '</ul></div>'

        html_template += '</div></div>'

        # ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ ì„¹ì…˜
        html_template += '''
        <div class="section">
            <h2>ğŸ“Š ì•„ì›ƒë¼ì´ì–´ ë¶„ì„</h2>
            <div class="metric-grid">
        '''

        for metric, data in outlier_analysis.items():
            if metric != 'error' and data['count'] > 0:
                html_template += f'''
                    <div class="metric-card">
                        <h3>{metric}</h3>
                        <p>ì•„ì›ƒë¼ì´ì–´: {data['count']}ê°œ ({data['percentage']:.1f}%)</p>
                        <ul class="issue-list">
                '''
                for outlier in data['values']:
                    html_template += f'''
                        <li class="issue-item">
                            <div class="timestamp">{outlier['timestamp']}</div>
                            <div>ê°’: {outlier['value']:.4f} (í¸ì°¨: {outlier['deviation']:.2f}Ïƒ)</div>
                        </li>
                    '''
                html_template += '</ul></div>'

        html_template += '</div></div>'

        # ì°¨íŠ¸ ì„¹ì…˜ë“¤
        html_template += f'''
        <div class="section">
            <h2>ğŸ”¥ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ</h2>
            <div class="chart-container">
                <img src="{heatmap_file}" alt="Correlation Heatmap">
            </div>
            <p>íˆíŠ¸ë§µì—ì„œ ë¹¨ê°„ìƒ‰ì€ ì–‘ì˜ ìƒê´€ê´€ê³„, íŒŒë€ìƒ‰ì€ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ê°•í•©ë‹ˆë‹¤.</p>
        </div>

        <div class="section">
            <h2>ğŸ“Š ì „ì²´ ë©”íŠ¸ë¦­ ì‹œê³„ì—´ ë¶„ì„</h2>
            <div class="chart-container">
                <img src="{timeseries_file}" alt="Time Series Analysis">
            </div>
            <p>ëª¨ë“  ë©”íŠ¸ë¦­ì˜ ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë¹¨ê°„ ì ì€ ì•„ì›ƒë¼ì´ì–´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</p>
        </div>

        <div class="section">
            <h2>ğŸ¯ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„</h2>
            <div class="chart-container">
                <img src="{scatter_file}" alt="Performance Scatter Plots">
            </div>
            <p>ì£¼ìš” ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‚°ì ë„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.</p>
        </div>

        <div class="section">
            <h2>ğŸ¯ ì£¼ìš” ìƒê´€ê´€ê³„ ë¶„ì„</h2>
            <div class="metric-grid">
        '''

        # ê° íƒ€ê²Ÿ ë©”íŠ¸ë¦­ë³„ ìƒê´€ê´€ê³„ ì¹´ë“œ ìƒì„±
        for target in target_metrics:
            if target in df.columns:
                target_corr = correlation_matrix[target].abs().drop(target, errors='ignore')
                top_5 = target_corr.nlargest(5)
                
                html_template += f'''
                    <div class="metric-card">
                        <h3>{target}</h3>
                        <table class="correlation-table">
                            <tr><th>ë©”íŠ¸ë¦­</th><th>ìƒê´€ê³„ìˆ˜</th><th>ê°•ë„</th></tr>
                '''
                
                for metric, corr in top_5.items():
                    if corr >= 0.8:
                        corr_class = "high-corr"
                        strength = "ë§¤ìš° ê°•í•¨"
                    elif corr >= 0.5:
                        corr_class = "medium-corr"
                        strength = "ë³´í†µ"
                    else:
                        corr_class = "low-corr"
                        strength = "ì•½í•¨"
                    
                    html_template += f'''
                            <tr>
                                <td>{metric}</td>
                                <td class="{corr_class}">{corr:.4f}</td>
                                <td>{strength}</td>
                            </tr>
                    '''
                
                html_template += '''
                        </table>
                    </div>
                '''

        html_template += '''
            </div>
        </div>

        <div class="section">
            <h2>ğŸ’¡ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­</h2>
            <div class="metric-grid">
        '''

        # ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        high_correlations = []
        for target in target_metrics:
            if target in df.columns:
                target_corr = correlation_matrix[target].abs().drop(target, errors='ignore')
                high_corr_metrics = target_corr[target_corr >= 0.8]
                if len(high_corr_metrics) > 0:
                    high_correlations.append((target, high_corr_metrics))

        if high_correlations:
            html_template += '''
                <div class="metric-card">
                    <h3>ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­</h3>
                    <ul>
            '''
            for target, corr_metrics in high_correlations[:3]:
                html_template += f"<li><strong>{target}</strong>ëŠ” {len(corr_metrics)}ê°œ ë©”íŠ¸ë¦­ê³¼ ê°•í•œ ìƒê´€ê´€ê³„</li>"
            html_template += '''
                    </ul>
                </div>
                <div class="metric-card">
                    <h3>âš¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­</h3>
                    <ul>
                        <li>ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì„ í•¨ê»˜ ëª¨ë‹ˆí„°ë§</li>
                        <li>CPU ì‚¬ìš©ë¥ ê³¼ ì—°ê´€ëœ ë©”íŠ¸ë¦­ë“¤ì˜ ì„ê³„ê°’ ì„¤ì •</li>
                        <li>ì—°ê²° ìˆ˜ ì¦ê°€ ì‹œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ëª¨ë‹ˆí„°ë§ ê°•í™”</li>
                        <li>I/O ë©”íŠ¸ë¦­ê³¼ ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ì§€í‘œ ì—°ê³„ ë¶„ì„</li>
                        <li>ì•„ì›ƒë¼ì´ì–´ ë°œìƒ ì‹œì ì˜ ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ ë¶„ì„</li>
                        <li>Performance Insightsë¥¼ í†µí•œ ìƒì„¸ ì¿¼ë¦¬ ë¶„ì„</li>
                    </ul>
                </div>
            '''

        html_template += '''
            </div>
        </div>
    </div>
</body>
</html>
        '''

        return html_template
        """ìƒê´€ê´€ê³„ ë¶„ì„ HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        correlation_matrix = df.corr()
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(12, 10))
        if sns is not None:
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        else:
            # seabornì´ ì—†ì„ ê²½ìš° matplotlibìœ¼ë¡œ ëŒ€ì²´
            im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
            plt.colorbar(im, shrink=0.8)
            # ìƒê´€ê³„ìˆ˜ ê°’ì„ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
            for i in range(len(correlation_matrix.columns)):
                for j in range(len(correlation_matrix.columns)):
                    plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                            ha='center', va='center', fontsize=8)
            plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)
            plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
        
        plt.title('Correlation Matrix Heatmap')
        plt.tight_layout()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        heatmap_file = f'correlation_heatmap_{timestamp}.png'
        plt.savefig(OUTPUT_DIR / heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()

        # ì‹œê³„ì—´ ì°¨íŠ¸ ìƒì„±
        plt.figure(figsize=(15, 8))
        for i, metric in enumerate(target_metrics[:4]):  # ìµœëŒ€ 4ê°œë§Œ í‘œì‹œ
            plt.subplot(2, 2, i+1)
            plt.plot(df.index, df[metric], label=metric, linewidth=1)
            plt.title(f'{metric} Time Series')
            plt.xlabel('Time')
            plt.ylabel('Value')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        timeseries_file = f'correlation_timeseries_{timestamp}.png'
        plt.savefig(OUTPUT_DIR / timeseries_file, dpi=300, bbox_inches='tight')
        plt.close()

        # HTML í…œí”Œë¦¿
        html_template = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸ - {csv_file}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #e0e0e0; border-radius: 8px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ padding: 15px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #007bff; }}
        .correlation-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .correlation-table th, .correlation-table td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        .correlation-table th {{ background-color: #f8f9fa; font-weight: bold; }}
        .high-corr {{ color: #dc3545; font-weight: bold; }}
        .medium-corr {{ color: #fd7e14; font-weight: bold; }}
        .low-corr {{ color: #28a745; }}
        .chart-container {{ text-align: center; margin: 20px 0; }}
        .chart-container img {{ max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
        .summary-stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .stat-box {{ text-align: center; padding: 15px; background: #e3f2fd; border-radius: 8px; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #1976d2; }}
        .stat-label {{ font-size: 14px; color: #666; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸</h1>
            <p>íŒŒì¼: {csv_file} | ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>

        <div class="section">
            <h2>ğŸ“ˆ ë°ì´í„° ê°œìš”</h2>
            <div class="summary-stats">
                <div class="stat-box">
                    <div class="stat-number">{len(df)}</div>
                    <div class="stat-label">ë°ì´í„° í¬ì¸íŠ¸</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{len(df.columns)}</div>
                    <div class="stat-label">ë©”íŠ¸ë¦­ ìˆ˜</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{len(target_metrics)}</div>
                    <div class="stat-label">ë¶„ì„ ëŒ€ìƒ</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">{df.index.max() - df.index.min()}</div>
                    <div class="stat-label">ë¶„ì„ ê¸°ê°„</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ”¥ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ</h2>
            <div class="chart-container">
                <img src="{heatmap_file}" alt="Correlation Heatmap">
            </div>
            <p>íˆíŠ¸ë§µì—ì„œ ë¹¨ê°„ìƒ‰ì€ ì–‘ì˜ ìƒê´€ê´€ê³„, íŒŒë€ìƒ‰ì€ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ê°•í•©ë‹ˆë‹¤.</p>
        </div>

        <div class="section">
            <h2>ğŸ“Š ì‹œê³„ì—´ ë¶„ì„</h2>
            <div class="chart-container">
                <img src="{timeseries_file}" alt="Time Series Analysis">
            </div>
            <p>ì£¼ìš” ë©”íŠ¸ë¦­ë“¤ì˜ ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>
        </div>

        <div class="section">
            <h2>ğŸ¯ ì£¼ìš” ìƒê´€ê´€ê³„ ë¶„ì„</h2>
            <div class="metric-grid">
        """

        # ê° íƒ€ê²Ÿ ë©”íŠ¸ë¦­ë³„ ìƒê´€ê´€ê³„ ì¹´ë“œ ìƒì„±
        for target in target_metrics:
            target_corr = correlation_matrix[target].abs().drop(target, errors='ignore')
            top_5 = target_corr.nlargest(5)
            
            html_template += f"""
                <div class="metric-card">
                    <h3>{target}</h3>
                    <table class="correlation-table">
                        <tr><th>ë©”íŠ¸ë¦­</th><th>ìƒê´€ê³„ìˆ˜</th><th>ê°•ë„</th></tr>
            """
            
            for metric, corr in top_5.items():
                if corr >= 0.8:
                    corr_class = "high-corr"
                    strength = "ë§¤ìš° ê°•í•¨"
                elif corr >= 0.5:
                    corr_class = "medium-corr"
                    strength = "ë³´í†µ"
                else:
                    corr_class = "low-corr"
                    strength = "ì•½í•¨"
                
                html_template += f"""
                        <tr>
                            <td>{metric}</td>
                            <td class="{corr_class}">{corr:.4f}</td>
                            <td>{strength}</td>
                        </tr>
                """
            
            html_template += """
                    </table>
                </div>
            """

        html_template += """
            </div>
        </div>

        <div class="section">
            <h2>ğŸ’¡ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­</h2>
            <div class="metric-grid">
        """

        # ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        high_correlations = []
        for target in target_metrics:
            target_corr = correlation_matrix[target].abs().drop(target, errors='ignore')
            high_corr_metrics = target_corr[target_corr >= 0.8]
            if len(high_corr_metrics) > 0:
                high_correlations.append((target, high_corr_metrics))

        if high_correlations:
            html_template += """
                <div class="metric-card">
                    <h3>ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­</h3>
                    <ul>
            """
            for target, corr_metrics in high_correlations[:3]:
                html_template += f"<li><strong>{target}</strong>ëŠ” {len(corr_metrics)}ê°œ ë©”íŠ¸ë¦­ê³¼ ê°•í•œ ìƒê´€ê´€ê³„</li>"
            html_template += """
                    </ul>
                </div>
                <div class="metric-card">
                    <h3>âš¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­</h3>
                    <ul>
                        <li>ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì„ í•¨ê»˜ ëª¨ë‹ˆí„°ë§</li>
                        <li>CPU ì‚¬ìš©ë¥ ê³¼ ì—°ê´€ëœ ë©”íŠ¸ë¦­ë“¤ì˜ ì„ê³„ê°’ ì„¤ì •</li>
                        <li>ì—°ê²° ìˆ˜ ì¦ê°€ ì‹œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ëª¨ë‹ˆí„°ë§ ê°•í™”</li>
                        <li>I/O ë©”íŠ¸ë¦­ê³¼ ìŠ¤í† ë¦¬ì§€ ì„±ëŠ¥ ì§€í‘œ ì—°ê³„ ë¶„ì„</li>
                    </ul>
                </div>
            """

        html_template += """
            </div>
        </div>
    </div>
</body>
</html>
        """

        return html_template


# MCP ì„œë²„ ì„¤ì •
server = Server("analyze-db")

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì„±ëŠ¥ ë¶„ì„ ë„êµ¬
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)"
                    }
                }
            }
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="get_database_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, connection, io)",
                        "default": "all"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="get_schema_summary",
            description="ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        # ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì§„ë‹¨ ë„êµ¬
        types.Tool(
            name="diagnose_connection_issues",
            description="ì—°ê²° ìˆ˜ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="diagnose_io_bottleneck",
            description="I/O ë³‘ëª© í˜„ìƒì„ ì§„ë‹¨í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="diagnose_memory_pressure",
            description="ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    }
                },
                "required": ["database_secret"]
            }
        ),
        types.Tool(
            name="dynamic_troubleshooting_workflow",
            description="ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¥¸ ë™ì  ì›Œí¬í”Œë¡œìš°ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ì œë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„"
                    },
                    "user_question": {
                        "type": "string",
                        "description": "ì‚¬ìš©ìì˜ ë¬¸ì œ ìƒí™© ì§ˆë¬¸"
                    }
                },
                "required": ["database_secret", "user_question"]
            }
        ),
        # CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ë„êµ¬
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì"
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­)"
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)",
                        "default": "us-east-1"
                    }
                },
                "required": ["db_instance_identifier"]
            }
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ìƒìœ„ Nê°œ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: 10)",
                        "default": 10
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="generate_correlation_report",
            description="ìƒê´€ê´€ê³„ ìƒì„¸ ë¶„ì„ ë° HTML ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "target_metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ë¶„ì„í•  íƒ€ê²Ÿ ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: CPUUtilization, DatabaseConnections, WriteIOPS, ReadIOPS)"
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="ë©”íŠ¸ë¦­ ë°ì´í„°ì—ì„œ ì•„ì›ƒë¼ì´ì–´ë¥¼ íƒì§€í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "std_threshold": {
                        "type": "number",
                        "description": "í‘œì¤€í¸ì°¨ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2.0)",
                        "default": 2.0
                    }
                },
                "required": ["csv_file"]
            }
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="ë©”íŠ¸ë¦­ ê°„ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"
                    },
                    "predictor_metric": {
                        "type": "string",
                        "description": "ì˜ˆì¸¡ ë³€ìˆ˜ ë©”íŠ¸ë¦­"
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization"
                    }
                },
                "required": ["csv_file", "predictor_metric"]
            }
        ),
        types.Tool(
            name="list_data_files",
            description="ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV íŒŒì¼ì˜ ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {
                        "type": "string",
                        "description": "ìš”ì•½í•  CSV íŒŒì¼ëª…"
                    }
                },
                "required": ["csv_file"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        db_server = AnalyzeDBServer()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì„±ëŠ¥ ë¶„ì„ ë„êµ¬
        if name == "list_database_secrets":
            result = await db_server.list_database_secrets(arguments.get("keyword"))
        elif name == "test_database_connection":
            connection, tunnel_used = db_server.get_db_connection(arguments["database_secret"])
            if connection.is_connected():
                db_info = connection.get_server_info()
                result = f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ! ì„œë²„ ë²„ì „: {db_info}"
                connection.close()
                if tunnel_used:
                    db_server.cleanup_ssh_tunnel()
            else:
                result = "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
        elif name == "get_database_performance_metrics":
            result = await db_server.get_database_performance_metrics(
                arguments["database_secret"],
                arguments.get("metric_type", "all")
            )
        elif name == "get_schema_summary":
            result = await db_server.get_schema_summary(arguments["database_secret"])
        
        # ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì§„ë‹¨ ë„êµ¬
        elif name == "diagnose_connection_issues":
            result = await db_server.diagnose_connection_issues(arguments["database_secret"])
            db_server.save_analysis_history(result)
        elif name == "diagnose_io_bottleneck":
            result = await db_server.diagnose_io_bottleneck(arguments["database_secret"])
            db_server.save_analysis_history(result)
        elif name == "diagnose_memory_pressure":
            result = await db_server.diagnose_memory_pressure(arguments["database_secret"])
            db_server.save_analysis_history(result)
        elif name == "dynamic_troubleshooting_workflow":
            result = await db_server.dynamic_troubleshooting_workflow(
                arguments["database_secret"],
                arguments["user_question"]
            )
            db_server.save_analysis_history(result)
        
        # CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ ë„êµ¬
        elif name == "collect_db_metrics":
            result = await db_server.collect_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", db_server.default_region)
            )
        elif name == "analyze_metric_correlation":
            result = await db_server.analyze_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10)
            )
        elif name == "detect_metric_outliers":
            result = await db_server.detect_outliers(
                arguments["csv_file"],
                arguments.get("std_threshold", 2.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_server.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization")
            )
        elif name == "list_data_files":
            result = await db_server.list_data_files()
        elif name == "get_metric_summary":
            result = await db_server.get_metric_summary(arguments["csv_file"])
        elif name == "generate_correlation_report":
            result = await db_server.generate_correlation_report(
                arguments["csv_file"],
                arguments.get("target_metrics")
            )
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"
        
        return [types.TextContent(type="text", text=result)]
    except Exception as e:
        error_msg = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return [types.TextContent(type="text", text=error_msg)]

async def main():
    # Stdin/stdoutë¥¼ í†µí•œ MCP ì„œë²„ ì‹¤í–‰
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="analyze-db",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )

if __name__ == "__main__":
    asyncio.run(main())
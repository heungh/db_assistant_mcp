#!/usr/bin/env python3
"""
DB Assistant Amazon Q CLI MCP ì„œë²„
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import sys

import boto3
from botocore.exceptions import ClientError

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

# ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
ANALYSIS_AVAILABLE = False
try:
    import pandas as pd
    import numpy as np
    import sqlparse
    import matplotlib

    matplotlib.use("Agg")  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ matplotlib ì‚¬ìš©
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.impute import SimpleImputer

    ANALYSIS_AVAILABLE = True
except ImportError:
    sqlparse = None

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


def create_session_log(operation_name: str = "operation"):
    """ì‘ì—…ë³„ ì„¸ì…˜ ë¡œê·¸ íŒŒì¼ ìƒì„± ë° ë¡œê·¸ í•¨ìˆ˜ ë°˜í™˜"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"ddl_validation_{operation_name}_{timestamp}.log"
    log_path = Path("logs") / log_filename

    # logs ë””ë ‰í† ë¦¬ ìƒì„±
    log_path.parent.mkdir(exist_ok=True)

    def log_message(level: str, message: str):
        """ì„¸ì…˜ ë¡œê·¸ íŒŒì¼ì— ë©”ì‹œì§€ ì‘ì„±"""
        timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp_str} - {operation_name} - {level} - {message}\n"

        with open(log_path, "a", encoding="utf-8") as f:
            f.write(log_entry)

        # ì½˜ì†”ì—ë„ ì¶œë ¥
        print(f"[{level}] {message}")

    # ì´ˆê¸° ë¡œê·¸ ì‘ì„±
    log_message("INFO", f"ìƒˆ ì‘ì—… ì„¸ì…˜ ì‹œì‘: {operation_name} - ë¡œê·¸ íŒŒì¼: {log_path}")

    return log_message, str(log_path)


# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"
DATA_DIR = CURRENT_DIR / "data"
LOGS_DIR = CURRENT_DIR / "logs"

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)


class DBAssistantMCPServer:
    def __init__(self):
        try:
            logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œì‘")
            self.bedrock_client = boto3.client(
                "bedrock-runtime", region_name="us-west-2", verify=False
            )
            logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ - ë¦¬ì „: us-west-2")

            # Bedrock ì ‘ê·¼ ê¶Œí•œ í…ŒìŠ¤íŠ¸
            try:
                # ê°„ë‹¨í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒë¡œ ê¶Œí•œ í…ŒìŠ¤íŠ¸
                bedrock_control = boto3.client(
                    "bedrock", region_name="us-west-2", verify=False
                )
                logger.info("Bedrock ì„œë¹„ìŠ¤ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ ì¤‘...")
                # ì‹¤ì œ ê¶Œí•œ í…ŒìŠ¤íŠ¸ëŠ” ëª¨ë¸ í˜¸ì¶œ ì‹œ ìˆ˜í–‰
                logger.info("Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ")
            except Exception as perm_e:
                logger.warning(
                    f"Bedrock ê¶Œí•œ ì‚¬ì „ í™•ì¸ ì‹¤íŒ¨ (ëª¨ë¸ í˜¸ì¶œ ì‹œ ì¬ì‹œë„): {perm_e}"
                )

        except Exception as e:
            logger.error(f"Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        self.knowledge_base_id = "0WQUBRHVR8"
        self.selected_database = None
        self.current_plan = None

        # ê³µìš© DB ì—°ê²° ë³€ìˆ˜ (ì—°ê²° ì¬ì‚¬ìš©ì„ ìœ„í•´)
        self.shared_connection = None
        self.shared_cursor = None
        self.tunnel_used = False

        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        self.PERFORMANCE_THRESHOLDS = {
            "max_rows_scan": 10_000_000,  # 1ì²œë§Œ í–‰ ì´ìƒ ìŠ¤ìº” ì‹œ ì‹¤íŒ¨
            "table_scan_ratio": 0.1,  # í…Œì´ë¸”ì˜ 10% ì´ìƒ ìŠ¤ìº” ì‹œ ê²½ê³ 
            "critical_rows_scan": 50_000_000,  # 5ì²œë§Œ í–‰ ì´ìƒ ìŠ¤ìº” ì‹œ ì‹¬ê°í•œ ë¬¸ì œ
        }

        # ë¶„ì„ ê´€ë ¨ ì´ˆê¸°í™”
        self.cloudwatch = None
        self.default_metrics = [
            "CPUUtilization",
            "DatabaseConnections",
            "DBLoad",
            "DBLoadCPU",
            "DBLoadNonCPU",
            "FreeableMemory",
            "ReadIOPS",
            "WriteIOPS",
            "ReadLatency",
            "WriteLatency",
            "NetworkReceiveThroughput",
            "NetworkTransmitThroughput",
            "BufferCacheHitRatio",
        ]

        # ê¸°ë³¸ ë¦¬ì „ ì„¤ì •
        self.default_region = self.get_default_region()

        # Knowledge Base í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.bedrock_agent_client = boto3.client(
            "bedrock-agent-runtime", region_name="us-east-1", verify=False
        )

    def get_default_region(self) -> str:
        """í˜„ì¬ AWS í”„ë¡œíŒŒì¼ì˜ ê¸°ë³¸ ë¦¬ì „ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.Session()
            return session.region_name or "ap-northeast-2"
        except Exception:
            return "ap-northeast-2"

    def parse_table_name(self, full_table_name: str) -> tuple:
        """í…Œì´ë¸”ëª…ì—ì„œ ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª…ì„ ë¶„ë¦¬"""
        if "." in full_table_name:
            schema, table = full_table_name.split(".", 1)
            return schema.strip("`"), table.strip("`")
        return None, full_table_name.strip("`")

    async def query_knowledge_base(self, query: str, sql_type: str) -> str:
        """Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ"""
        try:
            # SQL íƒ€ì…ì— ë”°ë¥¸ ì¿¼ë¦¬ ì¡°ì •
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

            if sql_type in ddl_types:
                # DDLì¸ ê²½ìš° ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ ì¡°íšŒ
                kb_query = f"ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ {query}"
            elif sql_type in dql_types:
                # DQLì¸ ê²½ìš° Aurora MySQL ìµœì í™” ê°€ì´ë“œ ì¡°íšŒ
                kb_query = f"Aurora MySQL ìµœì í™” ê°€ì´ë“œ {query}"
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ ì¡°íšŒ
                kb_query = f"ë°ì´í„°ë² ì´ìŠ¤ ë„ë©”ì¸ ê´€ë¦¬ ê·œì¹™ {query}"

            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": kb_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": 3}
                },
            )

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            knowledge_content = []
            for result in response.get("retrievalResults", []):
                content = result.get("content", {}).get("text", "")
                if content:
                    knowledge_content.append(content)

            if knowledge_content:
                return "\n\n".join(knowledge_content)
            else:
                return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.warning(f"Knowledge Base ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return "Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH í„°ë„ ì‹œì‘
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            process = subprocess.run(ssh_command, capture_output=True, text=True)

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            import subprocess

            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def validate_individual_ddl_statements(
        self, sql_content: str, cursor, debug_log, cte_tables: List[str]
    ):
        """ê°œë³„ DDL êµ¬ë¬¸ ê²€ì¦ - CREATE TABLE/INDEX ê°ê° ê²€ì¦"""
        debug_log("ğŸ”¥ğŸ”¥ğŸ”¥ validate_individual_ddl_statements í•¨ìˆ˜ ì‹œì‘ ğŸ”¥ğŸ”¥ğŸ”¥")
        result = {"issues": []}

        try:
            # DDL êµ¬ë¬¸ íŒŒì‹±
            ddl_statements = self.parse_ddl_statements(sql_content)
            debug_log(f"íŒŒì‹±ëœ DDL êµ¬ë¬¸ ìˆ˜: {len(ddl_statements)}")

            # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸” ëª©ë¡ ì¶”ì¶œ
            tables_created_in_file = set()
            for ddl in ddl_statements:
                if ddl["type"] == "CREATE_TABLE" and ddl["table"] not in cte_tables:
                    tables_created_in_file.add(ddl["table"])
            debug_log(f"íŒŒì¼ ë‚´ ìƒì„± í…Œì´ë¸”: {list(tables_created_in_file)}")

            for i, ddl in enumerate(ddl_statements):
                debug_log(
                    f"DDL [{i}] ê²€ì¦ ì‹œì‘: {ddl['type']} - {ddl.get('table', ddl.get('index_name', 'unknown'))}"
                )

                if ddl["type"] == "CREATE_TABLE":
                    table_name = ddl["table"]

                    # CTE aliasëŠ” ìŠ¤í‚µ
                    if table_name in cte_tables:
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name}ì€ CTE aliasì´ë¯€ë¡œ ìŠ¤í‚µ"
                        )
                        continue

                    # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)
                    schema, actual_table = self.parse_table_name(table_name)
                    if schema:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    exists = cursor.fetchone()[0] > 0

                    if exists:
                        issue = f"CREATE TABLE ì‹¤íŒ¨: í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                        result["issues"].append(issue)
                        debug_log(f"CREATE TABLE [{i}] - {table_name} ì‹¤íŒ¨: ì´ë¯¸ ì¡´ì¬")
                    else:
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name} ì„±ê³µ: ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
                        )

                elif ddl["type"] == "CREATE_INDEX":
                    table_name = ddl["table"]
                    index_name = ddl["index_name"]
                    columns = ddl["columns"]

                    # CTE alias í…Œì´ë¸”ì€ ìŠ¤í‚µ
                    if table_name in cte_tables:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}ì€ CTE aliasì´ë¯€ë¡œ ìŠ¤í‚µ"
                        )
                        continue

                    # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (DB + íŒŒì¼ ë‚´ ìƒì„±) - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
                    schema, actual_table = self.parse_table_name(table_name)
                    if schema:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    table_exists_in_db = cursor.fetchone()[0] > 0
                    table_created_in_file = actual_table in tables_created_in_file

                    if not table_exists_in_db and not table_created_in_file:
                        issue = f"CREATE INDEX ì‹¤íŒ¨: í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        result["issues"].append(issue)
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} ì‹¤íŒ¨: í…Œì´ë¸” ì—†ìŒ"
                        )
                        continue

                    # í…Œì´ë¸”ì´ íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” ê²½ìš°, ì¸ë±ìŠ¤ ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ
                    if table_created_in_file:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} ì„±ê³µ: íŒŒì¼ ë‚´ ìƒì„± í…Œì´ë¸”"
                        )
                        continue

                    # ê¸°ì¡´ í…Œì´ë¸”ì˜ ì¤‘ë³µ ì¸ë±ìŠ¤ í™•ì¸
                    cursor.execute(f"SHOW INDEX FROM `{table_name}`")
                    existing_indexes = cursor.fetchall()

                    # ë™ì¼í•œ ì»¬ëŸ¼ì— ëŒ€í•œ ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                    duplicate_found = False
                    for existing_idx in existing_indexes:
                        if existing_idx[4] == columns:  # Column_name
                            issue = f"CREATE INDEX ì‹¤íŒ¨: í…Œì´ë¸” '{table_name}'ì˜ ì»¬ëŸ¼ '{columns}'ì— ì´ë¯¸ ì¸ë±ìŠ¤ '{existing_idx[2]}'ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
                            result["issues"].append(issue)
                            debug_log(
                                f"CREATE INDEX [{i}] - {table_name}.{index_name} ì‹¤íŒ¨: ì¤‘ë³µ ì¸ë±ìŠ¤"
                            )
                            duplicate_found = True
                            break

                    if not duplicate_found:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} ì„±ê³µ: ì¤‘ë³µ ì—†ìŒ"
                        )

            debug_log(
                f"ğŸ”¥ğŸ”¥ğŸ”¥ validate_individual_ddl_statements ì™„ë£Œ: {len(result['issues'])}ê°œ ì´ìŠˆ ğŸ”¥ğŸ”¥ğŸ”¥"
            )
            return result

        except Exception as e:
            debug_log(f"ê°œë³„ DDL ê²€ì¦ ì˜¤ë¥˜: {e}")
            result["issues"].append(f"DDL ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return result

    def extract_successful_created_tables(
        self, sql_content: str, issues: List[str]
    ) -> List[str]:
        """ì„±ê³µí•œ CREATE TABLEë§Œ ì¶”ì¶œ (ì‹¤íŒ¨í•œ ê²ƒì€ ì œì™¸)"""
        created_tables = self.extract_created_tables(sql_content)
        successful_tables = []

        for table in created_tables:
            # issuesì—ì„œ í•´ë‹¹ í…Œì´ë¸”ì˜ CREATE TABLE ì‹¤íŒ¨ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            table_failed = any(
                f"í…Œì´ë¸” '{table}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤" in issue for issue in issues
            )
            if not table_failed:
                successful_tables.append(table)

        return successful_tables

    def validate_dml_columns_with_context(
        self, sql_content: str, cursor, debug_log, available_tables: List[str]
    ):
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ DML ì»¬ëŸ¼ ê²€ì¦"""
        # ê¸°ì¡´ validate_dml_columns ë¡œì§ì„ ì‚¬ìš©í•˜ë˜, available_tablesë¥¼ ê³ ë ¤
        # ì´ í•¨ìˆ˜ëŠ” ê¸°ì¡´ í•¨ìˆ˜ë¥¼ í™•ì¥í•œ ë²„ì „ì…ë‹ˆë‹¤
        return self.validate_dml_columns(sql_content, cursor, debug_log)

    def parse_ddl_statements(self, sql_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ íŒŒì‹±í•˜ì—¬ ê°œë³„ êµ¬ë¬¸ìœ¼ë¡œ ë¶„ë¦¬"""
        statements = []

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = (
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\("
        )
        for match in re.finditer(create_table_pattern, sql_content, re.IGNORECASE):
            statements.append({"type": "CREATE_TABLE", "table": match.group(1)})

        # CREATE INDEX íŒŒì‹±
        create_index_pattern = r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(\s*`?(\w+)`?"
        for match in re.finditer(create_index_pattern, sql_content, re.IGNORECASE):
            statements.append(
                {
                    "type": "CREATE_INDEX",
                    "index_name": match.group(1),
                    "table": match.group(2),
                    "columns": match.group(3),
                }
            )

        return statements

    async def execute_explain_with_cursor(self, sql_content: str, cursor, debug_log):
        """EXPLAIN ì‹¤í–‰ (ì»¤ì„œ ì‚¬ìš©)"""
        result = {"issues": [], "explain_data": None}

        try:
            if cursor is None:
                debug_log("ì»¤ì„œê°€ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return result

            # EXPLAIN ì‹¤í–‰
            explain_query = f"EXPLAIN {sql_content.strip().rstrip(';')}"
            debug_log(f"EXPLAIN ì¿¼ë¦¬: {explain_query}")

            cursor.execute(explain_query)
            explain_data = cursor.fetchall()
            result["explain_data"] = explain_data

            # EXPLAIN ê²°ê³¼ëŠ” ë¬¸ìì—´ë¡œë§Œ ì €ì¥

            debug_log("EXPLAIN ì‹¤í–‰ ì™„ë£Œ")
            return result

        except Exception as e:
            debug_log(f"EXPLAIN ì‹¤í–‰ ì˜ˆì™¸: {e}")
            result["issues"].append(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return result

    def check_performance_issues(self, explain_data, query_content, debug_log):
        """EXPLAIN ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬"""
        debug_log("ğŸ”ğŸ”ğŸ” check_performance_issues í•¨ìˆ˜ ì‹œì‘ ğŸ”ğŸ”ğŸ”")
        performance_issues = []

        # ìŠ¹ì¸ëœ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì¿¼ë¦¬ ì²´í¬
        batch_approval_patterns = [
            r"ëŒ€ìš©ëŸ‰\s*ë°°ì¹˜.*ìŠ¹ì¸",
            r"ë°°ì¹˜.*ìŠ¹ì¸.*ë°›ìŒ",
            r"ìŠ¹ì¸.*ëŒ€ìš©ëŸ‰",
            r"approved.*batch",
            r"batch.*approved",
        ]

        is_approved_batch = False
        for pattern in batch_approval_patterns:
            if re.search(pattern, query_content, re.IGNORECASE):
                is_approved_batch = True
                debug_log(f"ìŠ¹ì¸ëœ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì¿¼ë¦¬ë¡œ ì¸ì‹: {pattern}")
                break

        debug_log(f"EXPLAIN ë°ì´í„° í–‰ ìˆ˜: {len(explain_data)}")
        for idx, row in enumerate(explain_data):
            debug_log(f"EXPLAIN í–‰ {idx}: {row}")
            if len(row) >= 10:  # EXPLAIN ê²°ê³¼ êµ¬ì¡° í™•ì¸
                rows_examined = row[9] if row[9] is not None else 0
                debug_log(f"ê²€ì‚¬í•  í–‰ ìˆ˜: {rows_examined}")

                if rows_examined >= self.PERFORMANCE_THRESHOLDS["critical_rows_scan"]:
                    if is_approved_batch:
                        issue = f"âš ï¸ ê²½ê³ : ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰) - ìŠ¹ì¸ëœ ë°°ì¹˜ ì‘ì—…"
                        performance_issues.append(issue)
                        debug_log(f"ìŠ¹ì¸ëœ ë°°ì¹˜ - ê²½ê³  ì¶”ê°€: {issue}")
                    else:
                        issue = f"âŒ ì‹¤íŒ¨: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ì „ì²´ ìŠ¤ìº” ({rows_examined:,}í–‰)"
                        performance_issues.append(issue)
                        debug_log(f"ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ì‹¤íŒ¨ ì¶”ê°€: {issue}")

                elif rows_examined >= self.PERFORMANCE_THRESHOLDS["max_rows_scan"]:
                    if is_approved_batch:
                        issue = f"âš ï¸ ê²½ê³ : ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰) - ìŠ¹ì¸ëœ ë°°ì¹˜ ì‘ì—…"
                        performance_issues.append(issue)
                        debug_log(f"ìŠ¹ì¸ëœ ë°°ì¹˜ - ê²½ê³  ì¶”ê°€: {issue}")
                    else:
                        issue = f"âŒ ì‹¤íŒ¨: ì„±ëŠ¥ ë¬¸ì œ - ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìŠ¤ìº” ({rows_examined:,}í–‰)"
                        performance_issues.append(issue)
                        debug_log(f"ì„±ëŠ¥ ë¬¸ì œ - ì‹¤íŒ¨ ì¶”ê°€: {issue}")

        debug_log(
            f"ğŸ”ğŸ”ğŸ” check_performance_issues ì™„ë£Œ - ì´ìŠˆ: {performance_issues}, ìŠ¹ì¸: {is_approved_batch} ğŸ”ğŸ”ğŸ”"
        )
        return performance_issues, is_approved_batch

    async def execute_explain_individual_queries(
        self, sql_content: str, cursor, debug_log
    ):
        """ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬í•˜ì—¬ EXPLAIN ì‹¤í–‰ - CREATE êµ¬ë¬¸ ê³ ë ¤"""
        result = {"issues": [], "explain_data": [], "performance_issues": []}

        try:
            if cursor is None:
                debug_log("ì»¤ì„œê°€ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return result

            # í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸” ì¶”ì¶œ
            created_tables = self.extract_created_tables(sql_content)
            debug_log(f"í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”: {created_tables}")

            # SQL íŒŒì¼ì„ ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬
            if sqlparse:
                statements = sqlparse.split(sql_content)
            else:
                # sqlparseê°€ ì—†ìœ¼ë©´ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬
                statements = [
                    stmt.strip() for stmt in sql_content.split(";") if stmt.strip()
                ]

            debug_log(f"ì´ {len(statements)}ê°œì˜ ê°œë³„ ì¿¼ë¦¬ë¡œ ë¶„ë¦¬")

            for i, stmt in enumerate(statements):
                if not stmt.strip():
                    continue

                # ì£¼ì„ ì œê±° (ë¼ì¸ ì£¼ì„ê³¼ ë¸”ë¡ ì£¼ì„ ëª¨ë‘)
                cleaned_stmt = re.sub(
                    r"--.*$", "", stmt, flags=re.MULTILINE
                )  # ë¼ì¸ ì£¼ì„ ì œê±°
                cleaned_stmt = re.sub(
                    r"/\*.*?\*/", "", cleaned_stmt, flags=re.DOTALL
                )  # ë¸”ë¡ ì£¼ì„ ì œê±°
                cleaned_stmt = cleaned_stmt.strip()
                debug_log(
                    f"ì¿¼ë¦¬ {i+1} ì •ë¦¬ í›„: {repr(cleaned_stmt[:100])}"
                )  # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
                if not cleaned_stmt:
                    continue

                # DDL/DML/ê´€ë¦¬ ëª…ë ¹ì–´ëŠ” EXPLAIN ìŠ¤í‚µ
                ddl_pattern = re.match(
                    r"^\s*(CREATE|ALTER|DROP|RENAME)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )
                dml_pattern = re.match(
                    r"^\s*(INSERT|UPDATE|DELETE)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )
                admin_pattern = re.match(
                    r"^\s*(SHOW|DESCRIBE|DESC|USE|SET|EXPLAIN)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )

                if ddl_pattern:
                    debug_log(
                        f"ğŸ”¥ğŸ”¥ğŸ”¥ ì¿¼ë¦¬ {i+1}: DDL êµ¬ë¬¸ì´ë¯€ë¡œ EXPLAIN ìŠ¤í‚µ ({ddl_pattern.group(1).upper()}) ğŸ”¥ğŸ”¥ğŸ”¥"
                    )
                    continue
                elif dml_pattern:
                    debug_log(
                        f"ğŸ”¥ğŸ”¥ğŸ”¥ ì¿¼ë¦¬ {i+1}: DML êµ¬ë¬¸ì´ë¯€ë¡œ EXPLAIN ìŠ¤í‚µ ({dml_pattern.group(1).upper()}) ğŸ”¥ğŸ”¥ğŸ”¥"
                    )
                    continue
                elif admin_pattern:
                    debug_log(
                        f"ğŸ”¥ğŸ”¥ğŸ”¥ ì¿¼ë¦¬ {i+1}: ê´€ë¦¬ ëª…ë ¹ì–´ì´ë¯€ë¡œ EXPLAIN ìŠ¤í‚µ ({admin_pattern.group(1).upper()}) ğŸ”¥ğŸ”¥ğŸ”¥"
                    )
                    continue

                # ì¿¼ë¦¬ì—ì„œ ì°¸ì¡°í•˜ëŠ” í…Œì´ë¸” ì¶”ì¶œ
                query_tables = self.extract_table_names(cleaned_stmt)

                # ìƒˆë¡œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸
                references_new_table = any(
                    table in created_tables for table in query_tables
                )

                if references_new_table:
                    debug_log(
                        f"ì¿¼ë¦¬ {i+1}: ìƒˆë¡œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ë¯€ë¡œ EXPLAIN ìŠ¤í‚µ - í…Œì´ë¸”: {[t for t in query_tables if t in created_tables]}"
                    )
                    continue

                try:
                    # ê° ì¿¼ë¦¬ì— ëŒ€í•´ EXPLAIN ì‹¤í–‰
                    explain_query = f"EXPLAIN {cleaned_stmt}"
                    debug_log(f"ê°œë³„ ì¿¼ë¦¬ {i+1} EXPLAIN: {cleaned_stmt[:100]}...")
                    debug_log(
                        f"ğŸš¨ğŸš¨ğŸš¨ ì„±ëŠ¥ ê²€ì‚¬ ì½”ë“œ ë²„ì „ í™•ì¸ - ì„ê³„ê°’: {self.PERFORMANCE_THRESHOLDS} ğŸš¨ğŸš¨ğŸš¨"
                    )

                    cursor.execute(explain_query)
                    explain_data = cursor.fetchall()

                    # ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬
                    debug_log(
                        f"ğŸ” ì„±ëŠ¥ ê²€ì‚¬ ì‹œì‘ - ì¿¼ë¦¬ {i+1}, EXPLAIN í–‰ ìˆ˜: {len(explain_data)}"
                    )
                    perf_issues, is_approved = self.check_performance_issues(
                        explain_data, cleaned_stmt, debug_log
                    )
                    debug_log(
                        f"ğŸ” ì„±ëŠ¥ ê²€ì‚¬ ì™„ë£Œ - ì´ìŠˆ: {perf_issues}, ìŠ¹ì¸ë¨: {is_approved}"
                    )

                    if perf_issues:
                        result["performance_issues"].extend(perf_issues)
                        debug_log(f"âš ï¸ ì„±ëŠ¥ ì´ìŠˆ ì¶”ê°€ë¨: {perf_issues}")
                        # ìŠ¹ì¸ë˜ì§€ ì•Šì€ ëŒ€ìš©ëŸ‰ ìŠ¤ìº”ì€ ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
                        if not is_approved and any(
                            "âŒ ì‹¤íŒ¨" in issue for issue in perf_issues
                        ):
                            result["issues"].extend(perf_issues)
                            debug_log(f"âŒ ì„±ëŠ¥ ì´ìŠˆë¥¼ ì˜¤ë¥˜ë¡œ ì²˜ë¦¬: {perf_issues}")

                    result["explain_data"].append(
                        {
                            "query_index": i + 1,
                            "query": (
                                cleaned_stmt[:200] + "..."
                                if len(cleaned_stmt) > 200
                                else cleaned_stmt
                            ),
                            "explain_result": explain_data,
                        }
                    )
                    debug_log(f"ê°œë³„ ì¿¼ë¦¬ {i+1} EXPLAIN ì„±ê³µ")

                except Exception as e:
                    error_msg = f"ì¿¼ë¦¬ {i+1} EXPLAIN ì˜¤ë¥˜: {str(e)}"
                    debug_log(error_msg)
                    result["issues"].append(error_msg)

                    # ì»¬ëŸ¼ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ì˜¤ë¥˜ì¸ ê²½ìš° ìƒì„¸ ì •ë³´ ì¶”ê°€
                    if "Unknown column" in str(e):
                        result["issues"].append(
                            f"ì¿¼ë¦¬ {i+1}ì—ì„œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ì„ ì°¸ì¡°í•©ë‹ˆë‹¤: {cleaned_stmt[:100]}..."
                        )

            debug_log(
                f"ê°œë³„ ì¿¼ë¦¬ EXPLAIN ì™„ë£Œ: {len(result['explain_data'])}ê°œ ì„±ê³µ, {len(result['issues'])}ê°œ ì˜¤ë¥˜"
            )
            return result

        except Exception as e:
            debug_log(f"ê°œë³„ ì¿¼ë¦¬ EXPLAIN ì „ì²´ ì˜ˆì™¸: {e}")
            result["issues"].append(f"ê°œë³„ ì¿¼ë¦¬ EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return result

    async def test_individual_query_validation(
        self, database_secret: str, filename: str
    ) -> str:
        """ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        try:
            # SQL íŒŒì¼ ì½ê¸°
            sql_file_path = os.path.join("sql", filename)
            if not os.path.exists(sql_file_path):
                return f"âŒ SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                sql_content = f.read()

            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            if not connection:
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

            cursor = connection.cursor()

            # ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ ì‹¤í–‰
            result = await self.execute_explain_individual_queries(
                sql_content, cursor, print
            )

            # ì—°ê²° ì •ë¦¬
            cursor.close()
            connection.close()
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return f"âœ… ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ ì™„ë£Œ\nì„±ê³µ: {len(result['explain_data'])}ê°œ\nì˜¤ë¥˜: {len(result['issues'])}ê°œ\nìƒì„¸: {result}"

        except Exception as e:
            return f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    async def validate_schema_with_cursor(self, ddl_content: str, cursor):
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ì»¤ì„œ ì‚¬ìš©) - ë‹¤ì¤‘ CREATE êµ¬ë¬¸ ê³ ë ¤"""
        result = {"valid": True, "issues": []}

        try:
            if cursor is None:
                result["valid"] = False
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return result

            # í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ë“¤ ì¶”ì¶œ
            created_tables = self.extract_created_tables(ddl_content)

            # DDL íƒ€ì…ì— ë”°ë¥¸ ê²€ì¦
            ddl_type = self.extract_ddl_type(ddl_content)

            if ddl_type == "CREATE_TABLE":
                # ê° CREATE TABLE êµ¬ë¬¸ì— ëŒ€í•´ ê²€ì¦
                for table_name in created_tables:
                    cursor.execute("SHOW TABLES LIKE %s", (table_name,))
                    if cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                        )

            elif ddl_type == "ALTER_TABLE":
                # í…Œì´ë¸” ë³€ê²½ ê²€ì¦
                table_name = self.extract_table_name_from_alter(ddl_content)
                if table_name:
                    cursor.execute("SHOW TABLES LIKE %s", (table_name,))
                    if not cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

            return result

        except Exception as e:
            result["valid"] = False
            result["issues"].append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return result

    async def validate_constraints_with_cursor(self, ddl_content: str, cursor):
        """ì œì•½ì¡°ê±´ ê²€ì¦ (ì»¤ì„œ ì‚¬ìš©)"""
        result = {"valid": True, "issues": []}

        try:
            if cursor is None:
                result["valid"] = False
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return result

            # ì™¸ë˜í‚¤ ê²€ì¦
            foreign_keys = self.extract_foreign_keys(ddl_content)
            for fk in foreign_keys:
                ref_table = fk.get("referenced_table")
                if ref_table:
                    cursor.execute("SHOW TABLES LIKE %s", (ref_table,))
                    if not cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"ì°¸ì¡° í…Œì´ë¸” '{ref_table}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

            return result

        except Exception as e:
            result["valid"] = False
            result["issues"].append(f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return result

    async def validate_table_existence(self, sql_content: str, connection, debug_log):
        """í…Œì´ë¸” ì¡´ì¬ì„± ê²€ì¦"""
        result = {"issues": [], "tables_checked": []}

        try:
            if connection is None:
                debug_log("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
                return result

            cursor = connection.cursor()
            if cursor is None:
                debug_log("ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                return result

            # SQLì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
            tables = self.extract_table_names(sql_content)
            debug_log(f"ì¶”ì¶œëœ í…Œì´ë¸”ëª…: {tables}")

            for table in tables:
                result["tables_checked"].append(table)

                try:
                    # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    cursor.execute("SHOW TABLES LIKE %s", (table,))
                    exists = cursor.fetchone()

                    if not exists:
                        issue = f"í…Œì´ë¸” '{table}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        result["issues"].append(issue)
                        debug_log(f"í…Œì´ë¸” ì¡´ì¬ì„± ê²€ì¦ ì‹¤íŒ¨: {table}")
                    else:
                        debug_log(f"í…Œì´ë¸” ì¡´ì¬ì„± ê²€ì¦ í†µê³¼: {table}")
                except Exception as table_check_error:
                    debug_log(f"í…Œì´ë¸” {table} ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {table_check_error}")
                    result["issues"].append(
                        f"í…Œì´ë¸” '{table}' ê²€ì¦ ì˜¤ë¥˜: {str(table_check_error)}"
                    )

            cursor.close()

        except Exception as e:
            debug_log(f"í…Œì´ë¸” ì¡´ì¬ì„± ê²€ì¦ ì˜¤ë¥˜: {e}")
            result["issues"].append(f"í…Œì´ë¸” ì¡´ì¬ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}")

        return result

    def extract_table_name_from_alter(self, ddl_content: str) -> str:
        """ALTER TABLE êµ¬ë¬¸ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # ALTER TABLE íŒ¨í„´
        alter_pattern = r"ALTER\s+TABLE\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        match = re.search(alter_pattern, sql_clean, re.IGNORECASE)

        if match:
            return match.group(1)
        return None

    def extract_created_tables(self, sql_content: str) -> List[str]:
        """í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE TABLE íŒ¨í„´ - ë” ì •í™•í•œ ë§¤ì¹­
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)

        # ìœ íš¨í•œ í…Œì´ë¸”ëª…ë§Œ í•„í„°ë§ (SQL í‚¤ì›Œë“œ ì œì™¸)
        sql_keywords = {
            "and",
            "or",
            "not",
            "in",
            "on",
            "as",
            "is",
            "if",
            "by",
            "to",
            "from",
            "where",
            "select",
            "insert",
            "update",
            "delete",
        }
        for table in create_matches:
            if table.lower() not in sql_keywords and len(table) > 1:
                tables.add(table)

        return list(tables)

    def extract_created_indexes(self, sql_content: str) -> List[str]:
        """í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” ì¸ë±ìŠ¤ëª… ì¶”ì¶œ"""
        indexes = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE INDEX íŒ¨í„´
        index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+ON"
        )
        index_matches = re.findall(index_pattern, sql_clean, re.IGNORECASE)
        indexes.update(index_matches)

        return list(indexes)

    def extract_cte_tables(self, sql_content: str) -> List[str]:
        """WITHì ˆì˜ CTE(Common Table Expression) í…Œì´ë¸”ëª… ì¶”ì¶œ"""
        cte_tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITH RECURSIVE íŒ¨í„´ (ê°€ì¥ ì¼ë°˜ì )
        recursive_with_pattern = (
            r"WITH\s+(?:RECURSIVE\s+)?([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        )
        recursive_matches = re.findall(recursive_with_pattern, sql_clean, re.IGNORECASE)
        cte_tables.update(recursive_matches)

        # ì¶”ê°€ CTE í…Œì´ë¸”ë“¤ (ì‰¼í‘œ í›„)
        additional_cte_pattern = r",\s*([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        additional_matches = re.findall(
            additional_cte_pattern, sql_clean, re.IGNORECASE
        )
        cte_tables.update(additional_matches)

        return list(cte_tables)

    def extract_foreign_keys(self, ddl_content: str) -> List[Dict[str, str]]:
        """DDLì—ì„œ ì™¸ë˜í‚¤ ì •ë³´ ì¶”ì¶œ"""
        foreign_keys = []

        # ì£¼ì„ ì œê±°
        ddl_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        ddl_clean = re.sub(r"/\*.*?\*/", "", ddl_clean, flags=re.DOTALL)

        # FOREIGN KEY íŒ¨í„´ ë§¤ì¹­
        fk_pattern = r"FOREIGN\s+KEY\s*\(\s*([^)]+)\s*\)\s*REFERENCES\s+([^\s(]+)\s*\(\s*([^)]+)\s*\)"
        matches = re.finditer(fk_pattern, ddl_clean, re.IGNORECASE)

        for match in matches:
            column = match.group(1).strip().strip("`")
            ref_table = match.group(2).strip().strip("`")
            ref_column = match.group(3).strip().strip("`")

            foreign_keys.append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return foreign_keys

    def extract_table_names(self, sql_content: str) -> List[str]:
        """SQLì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ (WITHì ˆ CTE í…Œì´ë¸” ì œì™¸)"""
        tables = set()

        # ì£¼ì„ ì œê±°
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITHì ˆì˜ CTE í…Œì´ë¸”ë“¤ ì¶”ì¶œ
        cte_tables = set(self.extract_cte_tables(sql_content))

        # MySQL í‚¤ì›Œë“œë“¤ (í…Œì´ë¸”ëª…ì´ ì•„ë‹Œ ê²ƒë“¤)
        mysql_keywords = {
            "CURRENT_TIMESTAMP",
            "NOW",
            "NULL",
            "TRUE",
            "FALSE",
            "DEFAULT",
            "AUTO_INCREMENT",
            "PRIMARY",
            "KEY",
            "UNIQUE",
            "INDEX",
            "FOREIGN",
            "REFERENCES",
            "ON",
            "DELETE",
            "UPDATE",
            "CASCADE",
            "SET",
            "RESTRICT",
            "NO",
            "ACTION",
            "CHECK",
            "CONSTRAINT",
            "ENUM",
            "VARCHAR",
            "INT",
            "DECIMAL",
            "DATETIME",
            "TIMESTAMP",
            "TEXT",
            "BOOLEAN",
            "TINYINT",
            "SMALLINT",
            "MEDIUMINT",
            "BIGINT",
            "FLOAT",
            "DOUBLE",
            "CHAR",
            "BINARY",
            "VARBINARY",
            "BLOB",
            "TINYBLOB",
            "MEDIUMBLOB",
            "LONGBLOB",
            "TINYTEXT",
            "MEDIUMTEXT",
            "LONGTEXT",
            "DATE",
            "TIME",
            "YEAR",
        }

        # CREATE TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)
        for schema, table in create_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # ALTER TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        alter_pattern = r"ALTER\s+TABLE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        alter_matches = re.findall(alter_pattern, sql_clean, re.IGNORECASE)
        for schema, table in alter_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # DROP TABLE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        drop_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?"
        drop_matches = re.findall(drop_pattern, sql_clean, re.IGNORECASE)
        for schema, table in drop_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # FROM íŒ¨í„´ (SELECT, DELETE) - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        from_pattern = r"\bFROM\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|WHERE|ORDER|GROUP|LIMIT|JOIN|INNER|LEFT|RIGHT|FULL|CROSS)"
        from_matches = re.findall(from_pattern, sql_clean, re.IGNORECASE)
        for schema, table in from_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # JOIN íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        join_pattern = r"\b(?:INNER\s+|LEFT\s+|RIGHT\s+|FULL\s+|CROSS\s+)?JOIN\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|ON)"
        join_matches = re.findall(join_pattern, sql_clean, re.IGNORECASE)
        for schema, table in join_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # UPDATE íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        update_pattern = r"\bUPDATE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|SET)"
        update_matches = re.findall(update_pattern, sql_clean, re.IGNORECASE)
        for schema, table in update_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # INSERT INTO íŒ¨í„´ - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        insert_pattern = r"\bINSERT\s+INTO\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|\()"
        insert_matches = re.findall(insert_pattern, sql_clean, re.IGNORECASE)
        for schema, table in insert_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        return list(tables)

    async def execute_explain(self, sql_content: str, connection, debug_log):
        """EXPLAIN ì‹¤í–‰ ë° ë¶„ì„"""
        result = {"issues": [], "explain_data": None}

        try:
            if connection is None:
                debug_log("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ Noneì…ë‹ˆë‹¤")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
                return result

            cursor = connection.cursor(dictionary=True)
            if cursor is None:
                debug_log("ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                result["issues"].append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œ ìƒì„± ì‹¤íŒ¨")
                return result

            # ì£¼ì„ ì œê±°í•˜ê³  ì‹¤ì œ SQLë§Œ ì¶”ì¶œ
            sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
            sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)
            sql_clean = sql_clean.strip()

            if sql_clean.endswith(";"):
                sql_clean = sql_clean[:-1]

            explain_sql = f"EXPLAIN {sql_clean}"
            debug_log(f"EXPLAIN ì‹¤í–‰: {explain_sql}")

            cursor.execute(explain_sql)
            explain_result = cursor.fetchall()
            result["explain_data"] = explain_result

            debug_log(f"EXPLAIN ê²°ê³¼: {explain_result}")

            # EXPLAIN ê²°ê³¼ëŠ” ë¬¸ìì—´ë¡œë§Œ ì €ì¥
            cursor.close()

        except Exception as e:
            debug_log(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            result["issues"].append(f"EXPLAIN ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")

        return result

    def get_db_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
        db_instance_identifier: str = None,
    ):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        if mysql is None:
            raise Exception(
                "mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            )

        # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        session = boto3.session.Session()
        client = session.client(
            service_name="secretsmanager",
            region_name="ap-northeast-2",
            verify=False,
        )
        get_secret_value_response = client.get_secret_value(SecretId=database_secret)
        secret = get_secret_value_response["SecretString"]
        db_config = json.loads(secret)

        connection_config = None
        tunnel_used = False

        # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        database_name = selected_database or db_config.get(
            "dbname", db_config.get("database")
        )
        # database_nameì´ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¬¸ìì—´ë¡œ ë³€í™˜
        if database_name is not None:
            database_name = str(database_name)

        # db_instance_identifierê°€ ì œê³µë˜ë©´ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
        host = db_config.get("host")
        if db_instance_identifier:
            # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€ê²½
            if ".cluster-" in host:
                # aurora-cluster.cluster-xxx.region.rds.amazonaws.com -> instance-id.xxx.region.rds.amazonaws.com
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    # cluster- ë¶€ë¶„ì„ ì œê±°í•˜ê³  ì¸ìŠ¤í„´ìŠ¤ IDë¡œ êµì²´
                    host_parts[1] = host_parts[1].replace("cluster-", "")
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"
            else:
                # ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš° ì¸ìŠ¤í„´ìŠ¤ IDë¡œ êµì²´
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"

        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(host):
                connection_config = {
                    "host": "localhost",
                    "port": 3307,
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "database": database_name,
                    "connection_timeout": 10,
                }
                tunnel_used = True

        if not connection_config:
            connection_config = {
                "host": host,
                "port": db_config.get("port", 3306),
                "user": db_config.get("username"),
                "password": db_config.get("password"),
                "database": database_name,
                "connection_timeout": 10,
            }

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def setup_shared_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
        db_instance_identifier: str = None,
    ):
        """ê³µìš© DB ì—°ê²° ì„¤ì • (í•œ ë²ˆë§Œ í˜¸ì¶œ)"""
        try:
            if self.shared_connection and self.shared_connection.is_connected():
                logger.info("ì´ë¯¸ í™œì„±í™”ëœ ê³µìš© ì—°ê²°ì´ ìˆìŠµë‹ˆë‹¤.")
                return True

            self.shared_connection, self.tunnel_used = self.get_db_connection(
                database_secret,
                selected_database,
                use_ssh_tunnel,
                db_instance_identifier,
            )

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_cursor = self.shared_connection.cursor()
                # ì—°ê²°ëœ í˜¸ìŠ¤íŠ¸ ì •ë³´ ë¡œê¹…
                host_info = f"ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}" if db_instance_identifier else "í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸"
                logger.info(f"ê³µìš© DB ì—°ê²° ì„¤ì • ì™„ë£Œ - {host_info} (í„°ë„: {self.tunnel_used})")
                return True
            else:
                logger.error("ê³µìš© DB ì—°ê²° ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"ê³µìš© DB ì—°ê²° ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def cleanup_shared_connection(self):
        """ê³µìš© DB ì—°ê²° ì •ë¦¬"""
        try:
            if self.shared_cursor:
                self.shared_cursor.close()
                self.shared_cursor = None
                logger.info("ê³µìš© ì»¤ì„œ ë‹«ê¸° ì™„ë£Œ")

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_connection.close()
                self.shared_connection = None
                logger.info("ê³µìš© DB ì—°ê²° ë‹«ê¸° ì™„ë£Œ")

            if self.tunnel_used:
                self.cleanup_ssh_tunnel()
                self.tunnel_used = False
                logger.info("SSH í„°ë„ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ê³µìš© ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def get_shared_cursor(self):
        """ê³µìš© ì»¤ì„œ ë°˜í™˜"""
        if self.shared_cursor is None:
            logger.error(
                "ê³µìš© ì»¤ì„œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup_shared_connection()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )
            return None
        return self.shared_cursor

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            secrets = self.get_secrets_by_keyword(keyword)
            if not secrets:
                return (
                    f"'{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                    if keyword
                    else "ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                )

            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:\n{secret_list}"
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- ì„œë²„ ë²„ì „: {db_info}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}
- ì—°ê²° ë°©ì‹: {'SSH Tunnel' if tunnel_used else 'Direct'}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**"""
                for db in databases:
                    if db not in [
                        "information_schema",
                        "performance_schema",
                        "mysql",
                        "sys",
                    ]:
                        result += f"\n   - {db}"

                if tables:
                    result += f"\n\n**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**"
                    for table in tables:
                        result += f"\n   - {table}"

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ MySQL ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"

    async def list_databases(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            if mysql is None:
                raise Exception("mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)

            connection_config = None
            tunnel_used = False

            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get("host")):
                    connection_config = {
                        "host": "localhost",
                        "port": 3307,
                        "user": db_config.get("username"),
                        "password": db_config.get("password"),
                        "connection_timeout": 10,
                    }
                    tunnel_used = True

            if not connection_config:
                connection_config = {
                    "host": db_config.get("host"),
                    "port": db_config.get("port", 3306),
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "connection_timeout": 10,
                }

            # ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´ ì—°ê²°
            connection = mysql.connector.connect(**connection_config)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = [
                db[0]
                for db in cursor.fetchall()
                if db[0]
                not in ["information_schema", "performance_schema", "mysql", "sys"]
            ]

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            result = "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:\n\n"
            for i, db in enumerate(databases, 1):
                result += f"{i}. {db}\n"
            result += f"\nì´ {len(databases)}ê°œì˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤."
            result += "\n\nğŸ’¡ íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•˜ë ¤ë©´ ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_database(
        self, database_secret: str, database_selection: str, use_ssh_tunnel: bool = True
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ (USE ëª…ë ¹ì–´ ì‹¤í–‰)"""
        try:
            # ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìœ íš¨ì„± ê²€ì¦
            db_list_result = await self.list_databases(database_secret, use_ssh_tunnel)

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì—ì„œ ì‹¤ì œ DB ì´ë¦„ë“¤ ì¶”ì¶œ
            lines = db_list_result.split("\n")
            databases = []
            for line in lines:
                if line.strip() and line[0].isdigit() and ". " in line:
                    db_name = line.split(". ", 1)[1]
                    databases.append(db_name)

            selected_db = None

            # ë²ˆí˜¸ë¡œ ì„ íƒí•œ ê²½ìš°
            if database_selection.isdigit():
                index = int(database_selection) - 1
                if 0 <= index < len(databases):
                    selected_db = databases[index]
                else:
                    return f"âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(databases)} ë²”ìœ„ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.\n\n{db_list_result}"
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒí•œ ê²½ìš°
                if database_selection in databases:
                    selected_db = database_selection
                else:
                    return f"âŒ '{database_selection}' ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n{db_list_result}"

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ë¡œ USE ëª…ë ¹ì–´ ì‹¤í–‰
            connection, tunnel_used = self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )

            if connection.is_connected():
                cursor = connection.cursor()

                # USE ëª…ë ¹ì–´ ì‹¤í–‰
                cursor.execute(f"USE `{selected_db}`")

                # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                cursor.close()
                connection.close()

                # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                self.selected_database = selected_db

                result = f"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{selected_db}' ì„ íƒ ì™„ë£Œ!\n\n"
                result += f"ğŸ”— í˜„ì¬ í™œì„± ë°ì´í„°ë² ì´ìŠ¤: {current_db}\n"
                result += f"ğŸ’¡ ì´ì œ ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ìŠ¤í‚¤ë§ˆ ë¶„ì„ì´ë‚˜ SQL ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì˜¤ë¥˜: {e}")
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°í•´ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {current_db})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(tables_info)}ê°œ):"""

            for table_info in tables_info:
                table_name = table_info[0]
                table_type = table_info[1]
                engine = table_info[2]
                rows = table_info[3] or 0
                comment = table_info[6] or ""

                # ì»¬ëŸ¼ ìˆ˜ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                column_count = cursor.fetchone()[0]

                # ì¸ë±ìŠ¤ ìˆ˜ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT COUNT(DISTINCT index_name) FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                index_count = cursor.fetchone()[0]

                summary += f"""
  ğŸ”¹ **{table_name}** ({engine})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {rows:,}"""

                if comment:
                    summary += f"\n     - ì„¤ëª…: {comment}"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return summary

        except Exception as e:
            return f"âŒ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def get_table_schema(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            cursor.execute(
                """
                SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT, 
                       COLUMN_COMMENT, COLUMN_KEY, EXTRA
                FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY ORDINAL_POSITION
            """,
                (table_name,),
            )

            columns = cursor.fetchall()

            result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ìŠ¤í‚¤ë§ˆ ì •ë³´**\n\n"
            result += f"ğŸ“Š **ì»¬ëŸ¼ ëª©ë¡** ({len(columns)}ê°œ):\n"

            for col in columns:
                col_name, data_type, is_nullable, default_val, comment, key, extra = col

                result += f"\nğŸ”¹ **{col_name}**\n"
                result += f"   - íƒ€ì…: {data_type}\n"
                result += (
                    f"   - NULL í—ˆìš©: {'ì˜ˆ' if is_nullable == 'YES' else 'ì•„ë‹ˆì˜¤'}\n"
                )

                if default_val is not None:
                    result += f"   - ê¸°ë³¸ê°’: {default_val}\n"

                if key:
                    key_type = {"PRI": "ê¸°ë³¸í‚¤", "UNI": "ê³ ìœ í‚¤", "MUL": "ì¸ë±ìŠ¤"}.get(
                        key, key
                    )
                    result += f"   - í‚¤ íƒ€ì…: {key_type}\n"

                if extra:
                    result += f"   - ì¶”ê°€ ì†ì„±: {extra}\n"

                if comment:
                    result += f"   - ì„¤ëª…: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_table_index(self, database_secret: str, table_name: str) -> str:
        """íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"âŒ í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            cursor.execute(
                """
                SELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, NON_UNIQUE, 
                       INDEX_TYPE, CARDINALITY, NULLABLE, INDEX_COMMENT
                FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY INDEX_NAME, SEQ_IN_INDEX
            """,
                (table_name,),
            )

            indexes = cursor.fetchall()

            if not indexes:
                result = (
                    f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\nâŒ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
                )
            else:
                result = f"ğŸ“‹ **í…Œì´ë¸” '{table_name}' ì¸ë±ìŠ¤ ì •ë³´**\n\n"

                # ì¸ë±ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                index_groups = {}
                for idx in indexes:
                    idx_name = idx[0]
                    if idx_name not in index_groups:
                        index_groups[idx_name] = []
                    index_groups[idx_name].append(idx)

                result += f"ğŸ“Š **ì¸ë±ìŠ¤ ëª©ë¡** ({len(index_groups)}ê°œ):\n"

                for idx_name, idx_cols in index_groups.items():
                    first_col = idx_cols[0]
                    is_unique = "ê³ ìœ " if first_col[3] == 0 else "ì¼ë°˜"
                    idx_type = first_col[4]
                    comment = first_col[7] or ""

                    result += f"\nğŸ”¹ **{idx_name}** ({is_unique} ì¸ë±ìŠ¤)\n"
                    result += f"   - íƒ€ì…: {idx_type}\n"

                    # ì»¬ëŸ¼ ëª©ë¡
                    columns = [f"{col[1]}" for col in idx_cols]
                    result += f"   - ì»¬ëŸ¼: {', '.join(columns)}\n"

                    if comment:
                        result += f"   - ì„¤ëª…: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” ì¸ë±ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    # === DDL ê²€ì¦ ê´€ë ¨ ë©”ì„œë“œ ===

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            result = await self.validate_ddl(ddl_content, database_secret, filename)
            return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def validate_ddl(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """DDL/DML ê²€ì¦ ì‹¤í–‰ (ì—°ê²° ì¬ì‚¬ìš© íŒ¨í„´ ì ìš©)"""
        try:
            # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
            debug_log_path = (
                LOGS_DIR
                / f"debug_log_{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            )

            def debug_log(message):
                with open(debug_log_path, "a", encoding="utf-8") as f:
                    f.write(f"{datetime.now().strftime('%H:%M:%S')} - {message}\n")
                    f.flush()

            debug_log(f"validate_ddl ì‹œì‘ - íŒŒì¼: {filename}")
            debug_log(f"SQL ë‚´ìš©: {ddl_content.strip()}")

            issues = []
            db_connection_info = None
            schema_validation = None
            claude_analysis_result = None  # Claude ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
            constraint_validation = None
            explain_result = None

            # ë³€ìˆ˜ ì´ˆê¸°í™”
            dml_column_issues = []

            # 1. ê¸°ë³¸ ë¬¸ë²• ê²€ì¦ - ê°œì„ ëœ ì„¸ë¯¸ì½œë¡  ê²€ì¦
            semicolon_valid = self.validate_semicolon_usage(ddl_content)
            if not semicolon_valid:
                issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ ì‹¤íŒ¨")
            else:
                debug_log("ì„¸ë¯¸ì½œë¡  ê²€ì¦ í†µê³¼")

            # 2. SQL íƒ€ì… í™•ì¸
            sql_type = self.extract_ddl_type(ddl_content, debug_log)
            debug_log(f"SQL íƒ€ì…: {sql_type}")

            # 3. SQL íƒ€ì…ì— ë”°ë¥¸ ê²€ì¦ ë¶„ê¸°
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT", "MIXED_SELECT"]
            skip_types = ["SHOW", "SET", "USE"]  # ìŠ¤í‚µí•  SQL íƒ€ì…

            if database_secret:
                try:
                    debug_log("ê³µìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • ì‹œì‘")

                    # ê³µìš© ì—°ê²°ì´ ì—†ìœ¼ë©´ ì„¤ì •
                    if (
                        not self.shared_connection
                        or not self.shared_connection.is_connected()
                    ):
                        if not self.setup_shared_connection(
                            database_secret, self.selected_database
                        ):
                            debug_log("ê³µìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                            issues.append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            debug_log("ê³µìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

                    # ê³µìš© ì»¤ì„œ ì‚¬ìš©
                    cursor = self.get_shared_cursor()
                    if cursor:
                        debug_log("ê³µìš© ì»¤ì„œ ì‚¬ìš© ì‹œì‘")

                        # WITHì ˆ í™•ì¸
                        cte_tables = self.extract_cte_tables(ddl_content)
                        has_with_clause = len(cte_tables) > 0
                        debug_log(f"WITHì ˆ CTE í…Œì´ë¸”: {cte_tables}")
                        debug_log(f"WITHì ˆ ì¡´ì¬ ì—¬ë¶€: {has_with_clause}")

                        # SQL íƒ€ì…ë³„ ê²€ì¦ ë¶„ê¸°
                        if sql_type in skip_types:
                            debug_log(
                                f"SQL íƒ€ì… ìŠ¤í‚µ: {sql_type} (SHOW/SET/USE êµ¬ë¬¸ì€ ê²€ì¦í•˜ì§€ ì•ŠìŒ)"
                            )

                        # DDL ê²€ì¦
                        elif sql_type in ddl_types:
                            debug_log(f"DDL ê²€ì¦ ìˆ˜í–‰: {sql_type}")
                            debug_log("=== ìƒˆë¡œìš´ ê°œë³„ DDL ê²€ì¦ ë¡œì§ ì‹œì‘ ===")

                            # ê°œë³„ DDL êµ¬ë¬¸ ê²€ì¦ (WITHì ˆê³¼ ê´€ê³„ì—†ì´ ì‹¤í–‰)
                            ddl_validation = (
                                await self.validate_individual_ddl_statements(
                                    ddl_content, cursor, debug_log, cte_tables
                                )
                            )
                            debug_log(
                                f"ê°œë³„ DDL ê²€ì¦ ì™„ë£Œ: {len(ddl_validation['issues'])}ê°œ ì´ìŠˆ"
                            )
                            if ddl_validation["issues"]:
                                issues.extend(ddl_validation["issues"])

                        # DQL(DML) ê²€ì¦ - MIXED_SELECT í¬í•¨
                        elif sql_type in dql_types:
                            debug_log(f"DQL ê²€ì¦ ìˆ˜í–‰: {sql_type}")

                            # MIXED_SELECTì¸ ê²½ìš° DDLê³¼ DML ëª¨ë‘ ê²€ì¦
                            if sql_type == "MIXED_SELECT":
                                debug_log("=== í˜¼í•© SQL íŒŒì¼ ê²€ì¦ ì‹œì‘ ===")

                                # 1. DDL êµ¬ë¬¸ ê²€ì¦
                                debug_log("í˜¼í•© íŒŒì¼ ë‚´ DDL êµ¬ë¬¸ ê²€ì¦ ì‹œì‘")
                                ddl_validation = (
                                    await self.validate_individual_ddl_statements(
                                        ddl_content, cursor, debug_log, cte_tables
                                    )
                                )
                                debug_log(
                                    f"í˜¼í•© íŒŒì¼ DDL ê²€ì¦ ì™„ë£Œ: {len(ddl_validation['issues'])}ê°œ ì´ìŠˆ"
                                )
                                if ddl_validation["issues"]:
                                    issues.extend(ddl_validation["issues"])

                            # DML ê²€ì¦ (CTE aliasëŠ” ìŠ¤í‚µí•˜ë˜, ì‹¤ì œ í…Œì´ë¸”ì€ ê²€ì¦)
                            debug_log("ê°œë³„ ì¿¼ë¦¬ EXPLAIN í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘")
                            explain_result = (
                                await self.execute_explain_individual_queries(
                                    ddl_content, cursor, debug_log
                                )
                            )
                            debug_log(
                                f"ê°œë³„ ì¿¼ë¦¬ EXPLAIN í•¨ìˆ˜ í˜¸ì¶œ ì™„ë£Œ: {explain_result}"
                            )
                            if explain_result["issues"]:
                                issues.extend(explain_result["issues"])

                            # ì„±ëŠ¥ ì´ìŠˆ ì²˜ë¦¬
                            if (
                                "performance_issues" in explain_result
                                and explain_result["performance_issues"]
                            ):
                                debug_log(
                                    f"ì„±ëŠ¥ ì´ìŠˆ ë°œê²¬: {explain_result['performance_issues']}"
                                )
                                # ì„±ëŠ¥ ì´ìŠˆê°€ ìˆìœ¼ë©´ ì „ì²´ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                                for perf_issue in explain_result["performance_issues"]:
                                    if "âŒ ì‹¤íŒ¨" in perf_issue:
                                        issues.append(perf_issue)

                        else:
                            debug_log(f"ì•Œ ìˆ˜ ì—†ëŠ” SQL íƒ€ì…: {sql_type}")

                    else:
                        debug_log("ê³µìš© ì»¤ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        issues.append("ë°ì´í„°ë² ì´ìŠ¤ ì»¤ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    debug_log(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì˜¤ë¥˜: {e}")
                    issues.append(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")

            # 3.5. DML ì¿¼ë¦¬ì˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
            dml_column_issues = []
            if database_secret:  # sql_type ì¡°ê±´ ì œê±°í•˜ì—¬ ëª¨ë“  ê²½ìš°ì— ì‹¤í–‰
                try:
                    debug_log("DML ì»¬ëŸ¼ ê²€ì¦ ì‹œì‘")
                    cursor = self.get_shared_cursor()
                    if cursor:
                        # ì„±ê³µí•œ CREATE TABLEë§Œ ì¶”ì¶œ (ì‹¤íŒ¨í•œ ê²ƒì€ ì œì™¸)
                        successful_created_tables = (
                            self.extract_successful_created_tables(ddl_content, issues)
                        )
                        debug_log(f"ì„±ê³µí•œ CREATE TABLE: {successful_created_tables}")

                        # CTE alias ì¶”ì¶œ
                        cte_tables = self.extract_cte_tables(ddl_content)
                        debug_log(f"CTE alias í…Œì´ë¸”: {cte_tables}")

                        # DML ì»¬ëŸ¼ ê²€ì¦ (ì„±ê³µí•œ í…Œì´ë¸”ê³¼ CTE aliasëŠ” ìŠ¤í‚µ)
                        available_tables = successful_created_tables + cte_tables
                        dml_validation_result = self.validate_dml_columns_with_context(
                            ddl_content, cursor, debug_log, available_tables
                        )
                        debug_log(f"DML ê²€ì¦ ê²°ê³¼: {dml_validation_result}")
                        if dml_validation_result["queries_with_issues"] > 0:
                            for query_result in dml_validation_result["results"]:
                                for issue in query_result["issues"]:
                                    dml_column_issues.append(
                                        {
                                            "type": "COLUMN_NOT_EXISTS",
                                            "severity": "ERROR",
                                            "message": issue["message"],
                                            "table": issue["table"],
                                            "column": issue["column"],
                                            "query_type": sql_type,
                                        }
                                    )
                                    debug_log(f"DML ì»¬ëŸ¼ ì˜¤ë¥˜ ë°œê²¬: {issue['message']}")

                        debug_log(
                            f"DML ì»¬ëŸ¼ ê²€ì¦ ì™„ë£Œ: {len(dml_column_issues)}ê°œ ì´ìŠˆ ë°œê²¬"
                        )
                    else:
                        debug_log("DML ì»¬ëŸ¼ ê²€ì¦ ê±´ë„ˆëœ€: ê³µìš© ì»¤ì„œ ì—†ìŒ")
                except Exception as e:
                    debug_log(f"DML ì»¬ëŸ¼ ê²€ì¦ ì˜¤ë¥˜: {e}")
                    import traceback

                    debug_log(f"DML ì»¬ëŸ¼ ê²€ì¦ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")

            # 4. Claudeë¥¼ í†µí•œ ê²€ì¦
            try:
                debug_log("Claude ê²€ì¦ ì‹œì‘")
                # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ
                relevant_schema_info = None
                explain_info_str = None

                if database_secret:
                    try:
                        relevant_schema_info = await self.extract_relevant_schema_info(
                            ddl_content, database_secret, debug_log
                        )
                        debug_log(f"ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {relevant_schema_info}")

                        # EXPLAIN ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë³„ë„ ì²˜ë¦¬
                        if explain_result:
                            explain_info_str = (
                                f"EXPLAIN ë¶„ì„ ê²°ê³¼: {str(explain_result)}"
                            )
                            debug_log(
                                f"EXPLAIN ì •ë³´ ë¬¸ìì—´ ë³€í™˜ ì™„ë£Œ: {explain_info_str}"
                            )

                    except Exception as e:
                        debug_log(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±
                schema_validation_summary = self.create_schema_validation_summary(
                    issues, dml_column_issues
                )
                debug_log(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ìš”ì•½ ìƒì„±: {schema_validation_summary}")

                claude_result = await self.validate_with_claude(
                    ddl_content,
                    database_secret,
                    relevant_schema_info,
                    explain_info_str,
                    sql_type,
                    schema_validation_summary,
                )
                debug_log(f"Claude ê²€ì¦ ê²°ê³¼: {claude_result}")
                debug_log(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì „ë‹¬ë¨: {relevant_schema_info is not None}")

                # Claude ê²°ê³¼ë¥¼ í•­ìƒ ì €ì¥ (ì„±ê³µ/ì‹¤íŒ¨ ìƒê´€ì—†ì´)
                claude_analysis_result = claude_result

                # Claude ì‘ë‹µ ë¶„ì„ - ë” ì—„ê²©í•œ ê²€ì¦
                if "ì˜¤ë¥˜:" in claude_result or "ì¡´ì¬í•˜ì§€ ì•Š" in claude_result:
                    issues.append(f"Claude ê²€ì¦: {claude_result}")
                    debug_log("Claude ê²€ì¦ì—ì„œ ì˜¤ë¥˜ ë°œê²¬")
                elif claude_result.startswith("ê²€ì¦ í†µê³¼"):
                    debug_log("Claude ê²€ì¦ í†µê³¼")
                else:
                    debug_log("Claude ê²€ì¦ ì™„ë£Œ")

            except Exception as e:
                logger.error(f"Claude ê²€ì¦ ì˜¤ë¥˜: {e}")
                issues.append(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                debug_log(f"Claude ê²€ì¦ ì˜ˆì™¸: {e}")
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ Claude ê²°ê³¼ ì„¤ì •
                claude_analysis_result = f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

            # DML ì»¬ëŸ¼ ì´ìŠˆë¥¼ ê¸°ì¡´ ì´ìŠˆ ëª©ë¡ì— ì¶”ê°€
            if dml_column_issues:
                for dml_issue in dml_column_issues:
                    issues.append(dml_issue["message"])
                debug_log(
                    f"DML ì»¬ëŸ¼ ì´ìŠˆ {len(dml_column_issues)}ê°œë¥¼ ìµœì¢… ì´ìŠˆ ëª©ë¡ì— ì¶”ê°€"
                )

            # ê²€ì¦ ì™„ë£Œ
            debug_log(f"ìµœì¢… ì´ìŠˆ ê°œìˆ˜: {len(issues)}")
            debug_log(f"ì´ìŠˆ ëª©ë¡: {issues}")

            # Claude ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ìƒíƒœ ê²°ì •
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("ê²€ì¦ í†µê³¼")
            )

            # ê²°ê³¼ ìƒì„± - Claude ê²€ì¦ì´ ì„±ê³µì´ë©´ ìš°ì„ ì ìœ¼ë¡œ PASS ì²˜ë¦¬
            if claude_success and not any(
                "ì˜¤ë¥˜:" in issue or "ì‹¤íŒ¨" in issue or "ì¡´ì¬í•˜ì§€ ì•Š" in issue
                for issue in issues
            ):
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
                debug_log("Claude ê²€ì¦ ì„±ê³µìœ¼ë¡œ ìµœì¢… ìƒíƒœë¥¼ PASSë¡œ ì„¤ì •")
            elif not issues:
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
                status = "PASS"
            else:
                # ì„±ëŠ¥ ë¬¸ì œì™€ ê¸°íƒ€ ë¬¸ì œ ë¶„ë¥˜
                performance_issues = [
                    issue for issue in issues if "ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ" in str(issue)
                ]
                claude_issues = [
                    issue for issue in issues if "Claude ê²€ì¦:" in str(issue)
                ]
                other_issues = [
                    issue
                    for issue in issues
                    if issue not in performance_issues and issue not in claude_issues
                ]

                # ë¬¸ì œ ìš”ì•½ ìƒì„±
                problem_parts = []
                if performance_issues:
                    unique_performance = len(
                        set(str(issue) for issue in performance_issues)
                    )
                    if unique_performance == 1:
                        problem_parts.append("ì„±ëŠ¥ ë¬¸ì œ")
                    else:
                        problem_parts.append(f"ì„±ëŠ¥ ë¬¸ì œ {unique_performance}ê±´")

                if claude_issues:
                    problem_parts.append("AI ë¶„ì„ ë¬¸ì œ")

                if other_issues:
                    problem_parts.append(f"ê¸°íƒ€ ë¬¸ì œ {len(other_issues)}ê±´")

                if (
                    len(problem_parts) == 1
                    and "ì„±ëŠ¥ ë¬¸ì œ" in problem_parts[0]
                    and not other_issues
                    and not claude_issues
                ):
                    summary = "âŒ ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ ë°œê²¬"
                else:
                    summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {', '.join(problem_parts)}"

                status = "FAIL"

            debug_log(f"ìµœì¢… ìƒíƒœ: {status}, ìš”ì•½: {summary}")

            # ë³´ê³ ì„œ ìƒì„± (HTML í˜•ì‹)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"

            # HTML ë³´ê³ ì„œ ìƒì„±
            debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            debug_log(f"dml_column_issues ê°’: {dml_column_issues}")
            debug_log(f"report_path ê°’: {report_path}")
            try:
                await self.generate_html_report(
                    report_path,
                    filename,
                    ddl_content,
                    sql_type,
                    status,
                    summary,
                    issues,
                    db_connection_info,
                    schema_validation,
                    constraint_validation,
                    database_secret,
                    explain_result,
                    claude_analysis_result,  # Claude ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    dml_column_issues,  # DML ì»¬ëŸ¼ ì´ìŠˆ ì¶”ê°€
                )
                debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            except Exception as html_error:
                debug_log(f"HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {html_error}")
                import traceback

                debug_log(f"HTML ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

            # ê³µìš© ì—°ê²° ì •ë¦¬ (Claude ê²€ì¦ ì™„ë£Œ í›„)
            debug_log("ê³µìš© ì—°ê²° ì •ë¦¬ ì‹œì‘")
            self.cleanup_shared_connection()
            debug_log("ê³µìš© ì—°ê²° ì •ë¦¬ ì™„ë£Œ")

            return f"{summary}\n\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}\nğŸ” ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"

        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì—°ê²° ì •ë¦¬
            try:
                self.cleanup_shared_connection()
            except:
                pass
            return f"SQL ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def validate_semicolon_usage(self, ddl_content: str) -> bool:
        """ê°œì„ ëœ ì„¸ë¯¸ì½œë¡  ê²€ì¦ - ë…ë¦½ì ì¸ ë¬¸ì¥ì€ ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©"""
        content = ddl_content.strip()

        # ë¹ˆ ë‚´ìš©ì€ í†µê³¼
        if not content:
            return True

        # ì£¼ì„ ì œê±°í•˜ê³  ì‹¤ì œ SQL êµ¬ë¬¸ë§Œ ì¶”ì¶œ
        lines = content.split("\n")
        sql_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith("--") and not line.startswith("/*"):
                sql_lines.append(line)

        if not sql_lines:
            return True

        # ì‹¤ì œ SQL êµ¬ë¬¸ ê²°í•©
        actual_sql = " ".join(sql_lines).strip()

        # ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° (ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
        statements = [stmt.strip() for stmt in actual_sql.split(";") if stmt.strip()]

        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë…ë¦½ì ì¸ ë‹¨ì¼ êµ¬ë¬¸ì¸ì§€ í™•ì¸
        if len(statements) == 1:
            # ë‹¨ì¼ êµ¬ë¬¸ì¸ ê²½ìš° ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©
            single_stmt = statements[0].upper().strip()

            # SET, USE, SHOW ë“± ë…ë¦½ì ì¸ êµ¬ë¬¸ë“¤ì€ ì„¸ë¯¸ì½œë¡  ì—†ì–´ë„ í—ˆìš©
            independent_keywords = [
                "SET SESSION",
                "SET GLOBAL",
                "SET @",
                "SET @@",
                "USE ",
                "SHOW ",
                "DESCRIBE ",
                "DESC ",
                "EXPLAIN ",
                "SELECT ",
                "INSERT ",
                "UPDATE ",
                "DELETE ",
                "CREATE TABLE",
                "CREATE INDEX",
                "ALTER TABLE",
                "DROP TABLE",
            ]

            for keyword in independent_keywords:
                if single_stmt.startswith(keyword):
                    return True

        # ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ì„ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ ì„¸ë¯¸ì½œë¡ ì´ ìˆì–´ì•¼ í•¨
        return content.endswith(";")

    def extract_ddl_type(self, ddl_content: str, debug_log=None) -> str:
        """í˜¼í•© SQL íŒŒì¼ íƒ€ì… ì¶”ì¶œ - SELECT ì¿¼ë¦¬ê°€ ë§ìœ¼ë©´ MIXED_SELECTë¡œ ë¶„ë¥˜"""
        import re

        # ì£¼ì„ê³¼ ë¹ˆ ì¤„ì„ ì œê±°í•˜ê³  ì‹¤ì œ êµ¬ë¬¸ë§Œ ì¶”ì¶œ
        # ë¨¼ì € /* */ ìŠ¤íƒ€ì¼ ì£¼ì„ì„ ì „ì²´ì ìœ¼ë¡œ ì œê±°
        ddl_content = re.sub(r"/\*.*?\*/", "", ddl_content, flags=re.DOTALL)

        lines = ddl_content.strip().split("\n")
        ddl_lines = []

        for line in lines:
            line = line.strip()
            # ì£¼ì„ ë¼ì¸ì´ë‚˜ ë¹ˆ ë¼ì¸ ê±´ë„ˆë›°ê¸°
            if line and not line.startswith("--") and not line.startswith("#"):
                ddl_lines.append(line)

        if not ddl_lines:
            return "UNKNOWN"

        # ì „ì²´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ êµ¬ë¬¸ íƒ€ì…ë³„ ê°œìˆ˜ ê³„ì‚°
        full_content = " ".join(ddl_lines).upper()

        # ê°œë³„ êµ¬ë¬¸ë“¤ì„ ë¶„ì„
        statements = []
        for line in ddl_lines:
            line_upper = line.upper().strip()
            if line_upper and not line_upper.startswith("/*"):
                statements.append(line_upper)

        # êµ¬ë¬¸ íƒ€ì…ë³„ ê°œìˆ˜ ê³„ì‚°
        type_counts = {
            "SELECT": 0,
            "INSERT": 0,
            "UPDATE": 0,
            "DELETE": 0,
            "CREATE_TABLE": 0,
            "ALTER_TABLE": 0,
            "CREATE_INDEX": 0,
            "DROP_TABLE": 0,
            "DROP_INDEX": 0,
            "RENAME": 0,
        }

        # ê° êµ¬ë¬¸ ë¶„ì„ - ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬ëœ ì‹¤ì œ êµ¬ë¬¸ ë‹¨ìœ„ë¡œ ê³„ì‚°
        sql_statements = [
            stmt.strip() for stmt in ddl_content.split(";") if stmt.strip()
        ]

        for stmt in sql_statements:
            stmt_upper = stmt.upper().strip()

            # /* */ ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±°
            stmt_upper = re.sub(r"/\*.*?\*/", "", stmt_upper, flags=re.DOTALL)

            # -- ìŠ¤íƒ€ì¼ ì£¼ì„ ì œê±°
            stmt_lines = [
                line.strip()
                for line in stmt_upper.split("\n")
                if line.strip() and not line.strip().startswith("--")
            ]
            if not stmt_lines:
                continue

            stmt_clean = " ".join(stmt_lines).strip()

            if stmt_clean.startswith("SELECT"):
                type_counts["SELECT"] += 1
            elif stmt_clean.startswith("INSERT"):
                type_counts["INSERT"] += 1
            elif stmt_clean.startswith("UPDATE"):
                type_counts["UPDATE"] += 1
            elif stmt_clean.startswith("DELETE"):
                type_counts["DELETE"] += 1
            elif stmt_clean.startswith("CREATE TABLE"):
                type_counts["CREATE_TABLE"] += 1
            elif stmt_clean.startswith("ALTER TABLE") or re.search(
                r"\bALTER\s+TABLE\b", stmt_clean
            ):
                type_counts["ALTER_TABLE"] += 1
            elif stmt_clean.startswith("CREATE INDEX"):
                type_counts["CREATE_INDEX"] += 1
            elif stmt_clean.startswith("DROP TABLE"):
                type_counts["DROP_TABLE"] += 1
            elif stmt_clean.startswith("DROP INDEX"):
                type_counts["DROP_INDEX"] += 1
            elif stmt_clean.startswith("RENAME TABLE") or re.search(
                r"\bRENAME\s+TABLE\b", stmt_clean
            ):
                type_counts["RENAME"] += 1

        # ì´ êµ¬ë¬¸ ìˆ˜
        total_statements = sum(type_counts.values())

        # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
        if debug_log:
            debug_log(f"DEBUG - êµ¬ë¬¸ ê°œìˆ˜: {type_counts}")
            debug_log(f"DEBUG - ì´ êµ¬ë¬¸: {total_statements}")

        # SELECT ì¿¼ë¦¬ê°€ 50% ì´ìƒì´ë©´ MIXED_SELECTë¡œ ë¶„ë¥˜
        if (
            type_counts["SELECT"] > 0
            and type_counts["SELECT"] >= total_statements * 0.5
        ):
            if debug_log:
                debug_log("DEBUG - MIXED_SELECTë¡œ ë¶„ë¥˜ë¨")
            return "MIXED_SELECT"

        # ê¸°ì¡´ ìš°ì„ ìˆœìœ„ ë¡œì§ (DDL ìš°ì„ )
        if type_counts["CREATE_TABLE"] > 0:
            return "CREATE_TABLE"
        elif type_counts["ALTER_TABLE"] > 0 or type_counts["RENAME"] > 0:
            return "ALTER_TABLE"
        elif type_counts["CREATE_INDEX"] > 0:
            return "CREATE_INDEX"
        elif type_counts["DROP_TABLE"] > 0:
            return "DROP_TABLE"
        elif type_counts["DROP_INDEX"] > 0:
            return "DROP_INDEX"
        elif type_counts["SELECT"] > 0:
            return "SELECT"
        elif type_counts["INSERT"] > 0:
            return "INSERT"
        elif type_counts["UPDATE"] > 0:
            return "UPDATE"
        elif type_counts["DELETE"] > 0:
            return "DELETE"

        # ê¸°íƒ€ êµ¬ë¬¸ ì²˜ë¦¬
        if any(stmt.startswith("SHOW ") for stmt in statements):
            return "SHOW"
        elif any(stmt.startswith("SET ") for stmt in statements):
            return "SET"
        elif any(stmt.startswith("USE ") for stmt in statements):
            return "USE"
        else:
            return "UNKNOWN"

    def detect_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ê°ì§€"""
        ddl_upper = ddl_content.upper().strip()

        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("DROP TABLE"):
            return "DROP_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP INDEX"):
            return "DROP_INDEX"
        elif ddl_upper.startswith("INSERT"):
            return "INSERT"
        elif ddl_upper.startswith("UPDATE"):
            return "UPDATE"
        elif ddl_upper.startswith("DELETE"):
            return "DELETE"
        elif ddl_upper.startswith("SELECT"):
            return "SELECT"
        else:
            return "UNKNOWN"

    def create_schema_validation_summary(
        self, issues: list, dml_column_issues: list
    ) -> str:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ Claudeì—ê²Œ ì „ë‹¬í•  í˜•íƒœë¡œ ìƒì„±"""
        if not issues and not dml_column_issues:
            return "ìŠ¤í‚¤ë§ˆ ê²€ì¦: ëª¨ë“  ê²€ì¦ í†µê³¼"

        summary_parts = []
        if issues:
            summary_parts.append(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¬¸ì œì  ({len(issues)}ê°œ):")
            for i, issue in enumerate(issues, 1):  # ëª¨ë“  ë¬¸ì œ í‘œì‹œ
                summary_parts.append(f"  {i}. {issue}")

        if dml_column_issues:
            summary_parts.append(f"ì»¬ëŸ¼ ê²€ì¦ ë¬¸ì œì  ({len(dml_column_issues)}ê°œ):")
            for i, issue in enumerate(dml_column_issues, 1):  # ëª¨ë“  ë¬¸ì œ í‘œì‹œ
                summary_parts.append(f"  {i}. {issue}")

        return "\n".join(summary_parts)

    async def validate_with_claude(
        self,
        ddl_content: str,
        database_secret: str = None,
        schema_info: dict = None,
        explain_info: str = None,
        sql_type: str = None,
        schema_validation_summary: str = None,
    ) -> str:
        """
        Claude cross-region í”„ë¡œíŒŒì¼ì„ í™œìš©í•œ DDL ê²€ì¦ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨)
        """
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ê³  database_secretì´ ìˆìœ¼ë©´ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ
        if schema_info is None and database_secret:
            try:
                schema_info = await self.extract_current_schema_info(database_secret)
            except Exception as e:
                logger.warning(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                schema_info = {}

        # ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆœì„œ ê³ ë ¤)
        schema_context = ""
        if schema_info:
            schema_details = []

            # ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ì²˜ë¦¬
            sorted_items = sorted(
                schema_info.items(), key=lambda x: x[1].get("order", 0)
            )

            for key, info in sorted_items:
                order = info.get("order", 0)

                if info["type"] == "table":
                    table_name = info.get("table_name", key)

                    if "columns" in info:
                        # ALTER TABLE ì¼€ì´ìŠ¤
                        columns_info = [
                            f"{col['name']}({col['data_type']})"
                            for col in info["columns"]
                        ]
                        schema_details.append(f"[{order}] ALTER TABLE '{table_name}':")
                        schema_details.append(f"  - DBì— ì¡´ì¬: {info['exists']}")
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - íŒŒì¼ ë‚´ ìƒì„±ë¨: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - ìœ íš¨ ì¡´ì¬: {info.get('effective_exists', info['exists'])}"
                        )

                        if info["exists"] and columns_info:
                            schema_details.append(
                                f"  - ê¸°ì¡´ ì»¬ëŸ¼: {', '.join(columns_info)}"
                            )

                        if info.get("alter_type") and info.get("target_column"):
                            schema_details.append(
                                f"  - ALTER ì‘ì—…: {info['alter_type']} {info['target_column']}"
                            )
                    else:
                        # CREATE/DROP TABLE ì¼€ì´ìŠ¤
                        action = (
                            "CREATE"
                            if "CREATE" in key or info.get("exists") == False
                            else "DROP"
                        )
                        schema_details.append(
                            f"[{order}] {action} TABLE '{table_name}': DBì¡´ì¬={info['exists']}"
                        )

                elif info["type"] == "index":
                    table_name = info.get("table_name", key.split(".")[0])
                    index_name = info["index_name"]

                    if "duplicate_column_indexes" in info:
                        # CREATE INDEX ì¼€ì´ìŠ¤
                        schema_details.append(
                            f"[{order}] CREATE INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - í…Œì´ë¸” DBì¡´ì¬: {info['table_exists']}"
                        )
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - í…Œì´ë¸” íŒŒì¼ë‚´ìƒì„±: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - í…Œì´ë¸” ìœ íš¨ì¡´ì¬: {info.get('effective_exists', info['table_exists'])}"
                        )
                        schema_details.append(
                            f"  - ìƒì„±í•  ì»¬ëŸ¼: {', '.join(info['target_columns'])}"
                        )

                        # DBì˜ ì¤‘ë³µ ì¸ë±ìŠ¤
                        if info["duplicate_column_indexes"]:
                            schema_details.append("  - DBì˜ ë™ì¼ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤:")
                            for dup_idx in info["duplicate_column_indexes"]:
                                schema_details.append(
                                    f"    * {dup_idx['name']} ({dup_idx['columns']}) - {'UNIQUE' if dup_idx['unique'] else 'NON-UNIQUE'}"
                                )

                        # íŒŒì¼ ë‚´ ì¤‘ë³µ ì¸ë±ìŠ¤
                        if info.get("file_duplicate_indexes"):
                            schema_details.append("  - íŒŒì¼ ë‚´ ë™ì¼ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤:")
                            for dup_idx in info["file_duplicate_indexes"]:
                                schema_details.append(
                                    f"    * [{dup_idx['order']}] {dup_idx['name']} ({','.join(dup_idx['columns'])})"
                                )

                        if not info["duplicate_column_indexes"] and not info.get(
                            "file_duplicate_indexes"
                        ):
                            schema_details.append("  - ì¤‘ë³µ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤ ì—†ìŒ")
                    else:
                        # DROP INDEX ì¼€ì´ìŠ¤
                        schema_details.append(
                            f"[{order}] DROP INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - í…Œì´ë¸” ì¡´ì¬: {info['table_exists']}"
                        )
                        schema_details.append(
                            f"  - ì¸ë±ìŠ¤ ì¡´ì¬: {info['index_exists']}"
                        )

            if schema_details:
                schema_context = f"""
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì‹¤í–‰ ìˆœì„œë³„):
{chr(10).join(schema_details)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ DDLì˜ ì ì ˆì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. íŒŒì¼ ë‚´ì—ì„œ ë¨¼ì € ìƒì„±ëœ í…Œì´ë¸”ì€ ì´í›„ ALTER/INDEX ì‘ì—…ì—ì„œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
2. ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€
3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”/ì¸ë±ìŠ¤ì— ëŒ€í•œ DROP ì‹œë„
4. ì‹¤í–‰ ìˆœì„œìƒ ë…¼ë¦¬ì  ì˜¤ë¥˜
"""
            else:
                schema_context = """
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
        else:
            schema_context = """
ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        # EXPLAIN ì •ë³´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        explain_context = ""
        if explain_info:
            explain_context = f"""
EXPLAIN ë¶„ì„ ê²°ê³¼:
{explain_info}

ìœ„ EXPLAIN ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì„±ëŠ¥ìƒ ë¬¸ì œê°€ ìˆëŠ”ì§€ë„ í•¨ê»˜ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì°¸ê³ : DDL êµ¬ë¬¸(CREATE, ALTER, DROP ë“±)ì— ëŒ€í•´ì„œëŠ” EXPLAINì„ ì‹¤í–‰í•˜ì§€ ì•Šìœ¼ë©°, 
SELECT, UPDATE, DELETE, INSERT ë“±ì˜ DML êµ¬ë¬¸ì— ëŒ€í•´ì„œë§Œ EXPLAIN ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        schema_validation_context = ""
        schema_has_errors = False
        if schema_validation_summary:
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            schema_has_errors = (
                "ì˜¤ë¥˜" in schema_validation_summary
                or "ì‹¤íŒ¨" in schema_validation_summary
                or "ì¡´ì¬í•˜ì§€ ì•Š" in schema_validation_summary
                or "ì´ë¯¸ ì¡´ì¬" in schema_validation_summary
            )

            schema_validation_context = f"""
ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼:
{schema_validation_summary}

ìœ„ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì¢…í•©ì ì¸ íŒë‹¨ì„ í•´ì£¼ì„¸ìš”.
ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ëœ ê²½ìš°, ì‹¤íŒ¨ë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ê³  ì™œ ë¬¸ì œê°€ ë‚˜ì™”ëŠ”ì§€ë„ ì„¤ëª…í•˜ë©´ì„œ ê²€ì¦í•´ì£¼ì„¸ìš”.
"""

        # Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ ì¡°íšŒ
        knowledge_context = ""
        try:
            knowledge_info = await self.query_knowledge_base(ddl_content, sql_type)
            if knowledge_info and knowledge_info != "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                knowledge_context = f"""
Knowledge Base ì°¸ê³  ì •ë³´:
{knowledge_info}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê²€ì¦ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
"""
        except Exception as e:
            logger.warning(f"Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

        # DDLê³¼ DQLì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ë¶„
        ddl_types = [
            "CREATE_TABLE",
            "ALTER_TABLE",
            "CREATE_INDEX",
            "DROP_TABLE",
            "DROP_INDEX",
        ]
        dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

        if sql_type in ddl_types:
            # DDL ê²€ì¦ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ DDL ë¬¸ì„ Aurora MySQL ë¬¸ë²•ìœ¼ë¡œ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {schema_context}

        {schema_validation_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ë§Œ í™•ì¸í•˜ì„¸ìš”.

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "DDLì´ Aurora MySQLì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ ë°˜ë“œì‹œ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì„±ëŠ¥ ê°œì„ ì´ë‚˜ ëª¨ë²” ì‚¬ë¡€ëŠ” "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”  
        3. ì‹¤í–‰ì„ ë§‰ëŠ” ì‹¬ê°í•œ ë¬¸ë²• ì˜¤ë¥˜ë§Œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”

        **ì˜ˆì‹œ:**
        - ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ìš°: "ê²€ì¦ í†µê³¼"
        - ê°œì„ ì  ìˆëŠ” ê²½ìš°: "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: NULL ì†ì„±ì„ ëª…ì‹œí•˜ë©´ ë” ëª…í™•í•©ë‹ˆë‹¤)"
        - ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°: "ì˜¤ë¥˜: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•©ë‹ˆë‹¤"

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        elif sql_type in dql_types:
            # DQL ê²€ì¦ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ DQL(DML) ì¿¼ë¦¬ë¥¼ Aurora MySQLì—ì„œ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {explain_context}

        {schema_validation_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        1. Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        2. ì„±ëŠ¥ìƒ ë¬¸ì œê°€ ìˆëŠ”ì§€ ë¶„ì„
        3. ì¸ë±ìŠ¤ ì‚¬ìš© íš¨ìœ¨ì„± ê²€í† 
        4. **ì¤‘ìš”: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìœ¼ë©´ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬**

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì„±ëŠ¥ ë¬¸ì œ ì‹¤íŒ¨ ê¸°ì¤€:**
        ë‹¤ìŒê³¼ ê°™ì€ ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ë°œê²¬ë˜ë©´ ë°˜ë“œì‹œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”:
        - 1ì²œë§Œ í–‰ ì´ìƒì˜ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì— ëŒ€í•œ ì „ì²´ í…Œì´ë¸” ìŠ¤ìº” (Full Table Scan)
        - WHERE ì ˆ ì—†ëŠ” ëŒ€ìš©ëŸ‰ í…Œì´ë¸” UPDATE/DELETE
        - ì¸ë±ìŠ¤ ì—†ëŠ” ëŒ€ìš©ëŸ‰ í…Œì´ë¸” JOIN
        - ì¹´ë””ë„ë¦¬í‹°ê°€ ë§¤ìš° ë†’ì€ GROUP BYë‚˜ ORDER BY ì‘ì—…
        - ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ëŠ” ë³µì¡í•œ ì„œë¸Œì¿¼ë¦¬

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "ì¿¼ë¦¬ê°€ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìœ¼ë©´ "ì˜¤ë¥˜: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - ..."ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”
        3. ê²½ë¯¸í•œ ì„±ëŠ¥ ê°œì„ ì ë§Œ ìˆìœ¼ë©´ "ê²€ì¦ í†µê³¼ (ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”
        4. ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ëŠ” "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”

        **ì˜ˆì‹œ:**
        - ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì„±ëŠ¥ ë¬¸ì œ ì—†ìŒ: "ê²€ì¦ í†µê³¼"
        - ê²½ë¯¸í•œ ì„±ëŠ¥ ê°œì„ ì : "ê²€ì¦ í†µê³¼ (ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­: ì¸ë±ìŠ¤ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”)"
        - ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ: "ì˜¤ë¥˜: ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ - 1ì²œë§Œ í–‰ ì´ìƒ í…Œì´ë¸”ì˜ ì „ì²´ ìŠ¤ìº”ìœ¼ë¡œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ë¶ˆê°€"
        - ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°: "ì˜¤ë¥˜: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•©ë‹ˆë‹¤"

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""
        ë‹¤ìŒ SQL ë¬¸ì„ Aurora MySQL ë¬¸ë²•ìœ¼ë¡œ ê²€ì¦í•´ì£¼ì„¸ìš”: 

        {ddl_content}

        {schema_context}

        {explain_context}

        {schema_validation_context}

        {knowledge_context}

        **ê²€ì¦ ê¸°ì¤€:**
        Aurora MySQL 8.0ì—ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ë§Œ í™•ì¸í•˜ì„¸ìš”. 

        **ì¤‘ìš”: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬**
        {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ê¸°íƒ€ ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹¤íŒ¨ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”." if schema_has_errors else ""}

        **ì‘ë‹µ ê·œì¹™:**
        1. {"ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ëœ ê²½ìš° ë°˜ë“œì‹œ 'ì˜¤ë¥˜:'ë¡œ ì‹œì‘í•˜ì—¬ ì‹¤íŒ¨ë¡œ í‰ê°€í•˜ì„¸ìš”" if schema_has_errors else "SQLì´ Aurora MySQLì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ ë°˜ë“œì‹œ 'ê²€ì¦ í†µê³¼'ë¡œ ì‹œì‘í•˜ì„¸ìš”"}
        2. ì„±ëŠ¥ ê°œì„ ì´ë‚˜ ëª¨ë²” ì‚¬ë¡€ëŠ” "ê²€ì¦ í†µê³¼ (ê¶Œì¥ì‚¬í•­: ...)"ë¡œ í‘œì‹œí•˜ì„¸ìš”  
        3. ì‹¤í–‰ì„ ë§‰ëŠ” ì‹¬ê°í•œ ë¬¸ë²• ì˜¤ë¥˜ë§Œ "ì˜¤ë¥˜:"ë¡œ ì‹œì‘í•˜ì„¸ìš”
        4. ê¶Œì¥ì‚¬í•­ì„ ì œì•ˆí• ë•Œ, Aurora MySQL 8.0 ì´ ì•„ë‹Œ ê¸°ëŠ¥ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ê¶Œì¥í•˜ì§€ ë§ˆì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì¿¼ë¦¬ìºì‹œ ê¸°ëŠ¥ì€ 8.0ë¶€í„° ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ mysqlê¸°ëŠ¥ì„ ê¶Œì¥í•˜ê±°ë‚˜ ê±°ë¡ í•˜ì§€ ë§ˆì„¸ìš”.

        ë°˜ë“œì‹œ ìœ„ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """

        claude_input = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,  # í† í° ìˆ˜ë¥¼ 4ë°°ë¡œ ì¦ê°€
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            }
        )

        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile í˜¸ì¶œ
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            response_body = json.loads(response.get("body").read())
            return response_body.get("content", [{}])[0].get("text", "")
        except Exception as e:
            logger.warning(
                f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}"
            )
            # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                return response_body.get("content", [{}])[0].get("text", "")
            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def generate_performance_recommendations_with_claude(
        self,
        metrics_summary: str,
        correlation_analysis: str,
        outliers_analysis: str,
        slow_queries: str,
        memory_queries: str,
        cpu_queries: str,
        temp_queries: str,
        database_secret: str = None,
    ) -> Dict[str, Any]:
        """
        Claudeë¥¼ í™œìš©í•˜ì—¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ê³¼ ì¿¼ë¦¬ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê¶Œì¥ì‚¬í•­ ìƒì„±
        """
        try:
            # Knowledge Baseì—ì„œ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ ì¡°íšŒ
            knowledge_context = ""
            try:
                knowledge_info = await self.query_knowledge_base(
                    "database performance optimization recommendations", "PERFORMANCE"
                )
                if knowledge_info and knowledge_info != "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.":
                    knowledge_context = f"""
Knowledge Base ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ:
{knowledge_info}

ìœ„ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
            except Exception as e:
                logger.warning(f"Knowledge Base ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

            prompt = f"""
ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì í™” ê¶Œì¥ì‚¬í•­ê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ìƒì„±í•´ì£¼ì„¸ìš”:

**ë©”íŠ¸ë¦­ ìš”ì•½:**
{metrics_summary}

**ìƒê´€ê´€ê³„ ë¶„ì„:**
{correlation_analysis}

**ì´ìƒ ì§•í›„ ë¶„ì„:**
{outliers_analysis}

**ëŠë¦° ì¿¼ë¦¬ ë¶„ì„:**
{slow_queries}

**ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬:**
{memory_queries}

**CPU ì§‘ì•½ì  ì¿¼ë¦¬:**
{cpu_queries}

**ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬:**
{temp_queries}

{knowledge_context}

**ìš”êµ¬ì‚¬í•­:**
1. ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”
2. ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ì„ ì œì•ˆí•˜ì„¸ìš”
3. ê° ê¶Œì¥ì‚¬í•­ì— ëŒ€í•´ ì˜ˆìƒ íš¨ê³¼ì™€ êµ¬í˜„ ë‚œì´ë„ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ì•¡ì…˜ ì•„ì´í…œì€ ë‹´ë‹¹ì, ì˜ˆìƒ ì†Œìš”ì‹œê°„, ìš°ì„ ìˆœìœ„ë¥¼ í¬í•¨í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
    "immediate_improvements": [
        {{
            "category": "ëª¨ë‹ˆí„°ë§/ì„±ëŠ¥/ìš©ëŸ‰ê³„íš",
            "title": "êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­ ì œëª©",
            "description": "ìƒì„¸ ì„¤ëª…",
            "items": ["êµ¬ì²´ì ì¸ ì‹¤í–‰ í•­ëª©1", "êµ¬ì²´ì ì¸ ì‹¤í–‰ í•­ëª©2"],
            "expected_impact": "ì˜ˆìƒ íš¨ê³¼",
            "difficulty": "ë‚®ìŒ/ì¤‘ê°„/ë†’ìŒ"
        }}
    ],
    "action_items": [
        {{
            "priority": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ",
            "item": "êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ",
            "estimated_time": "ì˜ˆìƒ ì†Œìš”ì‹œê°„",
            "assignee": "ë‹´ë‹¹ì ì—­í• ",
            "rationale": "ì´ ì•¡ì…˜ì´ í•„ìš”í•œ ì´ìœ "
        }}
    ]
}}

ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë°œê²¬ëœ ë¬¸ì œì ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”.
"""

            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4096,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 í˜¸ì¶œ
            try:
                logger.info(f"Claude Sonnet 4 í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸ID: {sonnet_4_model_id}")
                logger.debug(f"ì…ë ¥ ë°ì´í„° í¬ê¸°: {len(claude_input)} bytes")

                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                logger.info("Claude Sonnet 4 ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

                response_body = json.loads(response.get("body").read())
                logger.debug(f"ì‘ë‹µ ë³¸ë¬¸ íŒŒì‹± ì™„ë£Œ: {list(response_body.keys())}")

                claude_response = response_body.get("content", [{}])[0].get("text", "")
                logger.info(
                    f"Claude ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(claude_response)} characters"
                )
                logger.debug(f"Claude ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {claude_response[:200]}...")

                # JSON íŒŒì‹± ì‹œë„ - ë¨¼ì € ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í™•ì¸
                try:
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                    import re

                    markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                    markdown_match = re.search(
                        markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                    )

                    if markdown_match:
                        json_content = markdown_match.group(1).strip()
                        logger.info(
                            f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ, ê¸¸ì´: {len(json_content)}"
                        )
                        parsed_result = json.loads(json_content)
                    else:
                        # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì§ì ‘ íŒŒì‹± ì‹œë„
                        logger.info("ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ìŒ, ì§ì ‘ JSON íŒŒì‹± ì‹œë„")
                        parsed_result = json.loads(claude_response)

                    logger.info("Claude ì‘ë‹µ JSON íŒŒì‹± ì„±ê³µ")
                    logger.debug(f"íŒŒì‹±ëœ ê²°ê³¼ í‚¤: {list(parsed_result.keys())}")

                    # í•„ìš”í•œ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if isinstance(parsed_result, dict) and (
                        "immediate_improvements" in parsed_result
                        or "action_items" in parsed_result
                    ):
                        improvements_count = len(
                            parsed_result.get("immediate_improvements", [])
                        )
                        actions_count = len(parsed_result.get("action_items", []))
                        logger.info(
                            f"ìœ íš¨í•œ ê¶Œì¥ì‚¬í•­ íŒŒì‹± ì™„ë£Œ: {improvements_count}ê°œ ê°œì„ ì‚¬í•­, {actions_count}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                        )
                        return parsed_result
                    else:
                        logger.warning("íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                        return self._get_default_recommendations()

                except json.JSONDecodeError as json_err:
                    logger.error(f"Claude Sonnet 4 ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
                    logger.info("í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
                    # í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                    parsed_result = self._parse_claude_text_response(claude_response)
                    if parsed_result:
                        logger.info("í…ìŠ¤íŠ¸ íŒŒì‹± ì„±ê³µ")
                        return parsed_result
                    logger.error(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ ë‚´ìš©: {claude_response}")
                    return self._get_default_recommendations()

            except Exception as e:
                logger.error(
                    f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ - ì—ëŸ¬ íƒ€ì…: {type(e).__name__}"
                )
                logger.error(f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
                if hasattr(e, "response"):
                    logger.error(
                        f"AWS ì‘ë‹µ ì½”ë“œ: {e.response.get('Error', {}).get('Code', 'Unknown')}"
                    )
                    logger.error(
                        f"AWS ì‘ë‹µ ë©”ì‹œì§€: {e.response.get('Error', {}).get('Message', 'Unknown')}"
                    )

                # Claude 3.7 Sonnet í˜¸ì¶œ (fallback)
                try:
                    logger.info(
                        f"Claude 3.7 Sonnet fallback ì‹œì‘ - ëª¨ë¸ID: {sonnet_3_7_model_id}"
                    )

                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    logger.info("Claude 3.7 Sonnet ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")

                    response_body = json.loads(response.get("body").read())
                    claude_response = response_body.get("content", [{}])[0].get(
                        "text", ""
                    )
                    logger.info(
                        f"Claude 3.7 ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(claude_response)} characters"
                    )

                    try:
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                        import re

                        markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                        markdown_match = re.search(
                            markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                        )

                        if markdown_match:
                            json_content = markdown_match.group(1).strip()
                            logger.info(
                                f"Claude 3.7 ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ, ê¸¸ì´: {len(json_content)}"
                            )
                            parsed_result = json.loads(json_content)
                        else:
                            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì§ì ‘ íŒŒì‹± ì‹œë„
                            logger.info(
                                "Claude 3.7 ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ìŒ, ì§ì ‘ JSON íŒŒì‹± ì‹œë„"
                            )
                            parsed_result = json.loads(claude_response)

                        logger.info("Claude 3.7 ì‘ë‹µ JSON íŒŒì‹± ì„±ê³µ")

                        # í•„ìš”í•œ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if isinstance(parsed_result, dict) and (
                            "immediate_improvements" in parsed_result
                            or "action_items" in parsed_result
                        ):
                            improvements_count = len(
                                parsed_result.get("immediate_improvements", [])
                            )
                            actions_count = len(parsed_result.get("action_items", []))
                            logger.info(
                                f"Claude 3.7 ìœ íš¨í•œ ê¶Œì¥ì‚¬í•­ íŒŒì‹± ì™„ë£Œ: {improvements_count}ê°œ ê°œì„ ì‚¬í•­, {actions_count}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                            )
                            return parsed_result
                        else:
                            logger.warning("Claude 3.7 íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                            return self._get_default_recommendations()

                    except json.JSONDecodeError as json_err:
                        logger.error(f"Claude 3.7 ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
                        logger.info("Claude 3.7 í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
                        # í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
                        parsed_result = self._parse_claude_text_response(
                            claude_response
                        )
                        if parsed_result:
                            logger.info("Claude 3.7 í…ìŠ¤íŠ¸ íŒŒì‹± ì„±ê³µ")
                            return parsed_result
                        logger.error(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ ë‚´ìš©: {claude_response}")
                        return self._get_default_recommendations()

                except Exception as fallback_e:
                    logger.error(
                        f"Claude 3.7 Sonnet fallback ì‹¤íŒ¨ - ì—ëŸ¬ íƒ€ì…: {type(fallback_e).__name__}"
                    )
                    logger.error(
                        f"Claude 3.7 Sonnet fallback ì‹¤íŒ¨ - ì—ëŸ¬ ë©”ì‹œì§€: {str(fallback_e)}"
                    )
                    if hasattr(fallback_e, "response"):
                        logger.error(
                            f"AWS fallback ì‘ë‹µ ì½”ë“œ: {fallback_e.response.get('Error', {}).get('Code', 'Unknown')}"
                        )
                        logger.error(
                            f"AWS fallback ì‘ë‹µ ë©”ì‹œì§€: {fallback_e.response.get('Error', {}).get('Message', 'Unknown')}"
                        )
                    return self._get_default_recommendations()

        except Exception as e:
            logger.error(
                f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ì—ëŸ¬ íƒ€ì…: {type(e).__name__}"
            )
            logger.error(f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            logger.error(
                f"ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ - ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True
            )
            return self._get_default_recommendations()

    def _parse_claude_text_response(
        self, text_response: str
    ) -> Optional[Dict[str, Any]]:
        """Claude í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            import re

            logger.info(f"Claude ì‘ë‹µ íŒŒì‹± ì‹œì‘, ì‘ë‹µ ê¸¸ì´: {len(text_response)}")
            logger.debug(f"ì‘ë‹µ ì‹œì‘ ë¶€ë¶„: {text_response[:200]}")

            # ë¨¼ì € ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ
            markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
            markdown_match = re.search(
                markdown_pattern, text_response, re.DOTALL | re.IGNORECASE
            )

            if markdown_match:
                json_content = markdown_match.group(1).strip()
                logger.info(
                    f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ, ê¸¸ì´: {len(json_content)}"
                )
                logger.debug(f"ì¶”ì¶œëœ JSON ì‹œì‘ ë¶€ë¶„: {json_content[:200]}")
                try:
                    parsed = json.loads(json_content)
                    if isinstance(parsed, dict) and (
                        "immediate_improvements" in parsed or "action_items" in parsed
                    ):
                        logger.info(
                            f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ JSON íŒŒì‹± ì„±ê³µ - ê°œì„ ì‚¬í•­: {len(parsed.get('immediate_improvements', []))}ê°œ, ì•¡ì…˜ì•„ì´í…œ: {len(parsed.get('action_items', []))}ê°œ"
                        )
                        return parsed
                    else:
                        logger.warning("íŒŒì‹±ëœ JSONì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŒ")
                except json.JSONDecodeError as e:
                    logger.error(f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.debug(
                        f"íŒŒì‹± ì‹¤íŒ¨í•œ JSON ë‚´ìš© (ì²˜ìŒ 500ì): {json_content[:500]}"
                    )
            else:
                logger.warning("ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ íŒ¨í„´ë“¤ ì‹œë„
            logger.info("ëŒ€ì²´ JSON íŒ¨í„´ ë§¤ì¹­ ì‹œë„")
            json_patterns = [
                r'(\{[^{}]*"immediate_improvements"[^{}]*\})',
                r'(\{.*?"immediate_improvements".*?\})',
                r'(\{.*?"action_items".*?\})',
                r"(\{.*?\})",
            ]

            for i, pattern in enumerate(json_patterns):
                logger.debug(f"íŒ¨í„´ {i+1} ì‹œë„: {pattern}")
                matches = re.findall(pattern, text_response, re.DOTALL | re.IGNORECASE)
                logger.debug(f"íŒ¨í„´ {i+1}ì—ì„œ {len(matches)}ê°œ ë§¤ì¹˜ ë°œê²¬")
                for j, match in enumerate(matches):
                    try:
                        parsed = json.loads(match)
                        if isinstance(parsed, dict) and (
                            "immediate_improvements" in parsed
                            or "action_items" in parsed
                        ):
                            logger.info(f"JSON íŒ¨í„´ ë§¤ì¹­ ì„±ê³µ: íŒ¨í„´ {i+1}, ë§¤ì¹˜ {j+1}")
                            return parsed
                    except json.JSONDecodeError as e:
                        logger.debug(f"íŒ¨í„´ {i+1}, ë§¤ì¹˜ {j+1} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue

            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
            logger.info("êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
            return self._extract_from_structured_text(text_response)

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _extract_from_structured_text(self, text: str) -> Optional[Dict[str, Any]]:
        """êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
        try:
            import re

            result = {"immediate_improvements": [], "action_items": []}

            # ê°œì„ ì‚¬í•­ ì„¹ì…˜ ì°¾ê¸°
            improvements_pattern = r'"immediate_improvements":\s*\[(.*?)\]'
            improvements_match = re.search(improvements_pattern, text, re.DOTALL)

            if improvements_match:
                improvements_text = improvements_match.group(1)
                # ê° ê°œì„ ì‚¬í•­ íŒŒì‹±
                item_pattern = r"\{([^{}]+)\}"
                for item_match in re.finditer(item_pattern, improvements_text):
                    item_text = item_match.group(1)
                    improvement = self._parse_improvement_item(item_text)
                    if improvement:
                        result["immediate_improvements"].append(improvement)

            # ì•¡ì…˜ ì•„ì´í…œ ì„¹ì…˜ ì°¾ê¸°
            actions_pattern = r'"action_items":\s*\[(.*?)\]'
            actions_match = re.search(actions_pattern, text, re.DOTALL)

            if actions_match:
                actions_text = actions_match.group(1)
                # ê° ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹±
                for item_match in re.finditer(item_pattern, actions_text):
                    item_text = item_match.group(1)
                    action = self._parse_action_item(item_text)
                    if action:
                        result["action_items"].append(action)

            if result["immediate_improvements"] or result["action_items"]:
                return result

            return None

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _parse_improvement_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """ê°œì„ ì‚¬í•­ ì•„ì´í…œ íŒŒì‹±"""
        try:
            import re

            improvement = {}

            # ê° í•„ë“œ ì¶”ì¶œ
            fields = {
                "category": r'"category":\s*"([^"]+)"',
                "title": r'"title":\s*"([^"]+)"',
                "description": r'"description":\s*"([^"]+)"',
                "expected_impact": r'"expected_impact":\s*"([^"]+)"',
                "difficulty": r'"difficulty":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    improvement[field] = match.group(1)

            # items ë°°ì—´ ì¶”ì¶œ
            items_pattern = r'"items":\s*\[(.*?)\]'
            items_match = re.search(items_pattern, item_text, re.DOTALL)
            if items_match:
                items_text = items_match.group(1)
                items = re.findall(r'"([^"]+)"', items_text)
                improvement["items"] = items

            return improvement if improvement else None

        except Exception as e:
            logger.error(f"ê°œì„ ì‚¬í•­ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _parse_action_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹±"""
        try:
            import re

            action = {}

            # ê° í•„ë“œ ì¶”ì¶œ
            fields = {
                "priority": r'"priority":\s*"([^"]+)"',
                "item": r'"item":\s*"([^"]+)"',
                "estimated_time": r'"estimated_time":\s*"([^"]+)"',
                "assignee": r'"assignee":\s*"([^"]+)"',
                "rationale": r'"rationale":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    action[field] = match.group(1)

            return action if action else None

        except Exception as e:
            logger.error(f"ì•¡ì…˜ ì•„ì´í…œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _generate_recommendations_html(self, recommendations: Dict[str, Any]) -> str:
        """Claude ê¶Œì¥ì‚¬í•­ì„ HTMLë¡œ ë³€í™˜"""
        try:
            html_parts = []

            # ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­
            html_parts.append("<h4>ğŸš€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­</h4>")

            improvements = recommendations.get("immediate_improvements", [])
            if not improvements:
                html_parts.append(
                    '<div class="info-box">í˜„ì¬ ë¶„ì„ëœ ë°ì´í„°ì—ì„œëŠ” ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</div>'
                )
            else:
                for improvement in improvements:
                    category = improvement.get("category", "ê¸°íƒ€")
                    title = improvement.get("title", "ê°œì„ ì‚¬í•­")
                    description = improvement.get("description", "")
                    items = improvement.get("items", [])
                    expected_impact = improvement.get("expected_impact", "")
                    difficulty = improvement.get("difficulty", "ì¤‘ê°„")

                    html_parts.append(
                        f"""
                    <div class="recommendation">
                        <strong>{category}: {title}</strong>
                        <p style="margin: 10px 0; color: #666;">{description}</p>
                        <ul style="margin-top: 10px; margin-left: 20px;">
                    """
                    )

                    for item in items:
                        html_parts.append(f"<li>{item}</li>")

                    html_parts.append(
                        f"""
                        </ul>
                        <div style="margin-top: 10px; font-size: 0.9em;">
                            <span style="color: #27ae60;"><strong>ì˜ˆìƒ íš¨ê³¼:</strong> {expected_impact}</span> | 
                            <span style="color: #3498db;"><strong>êµ¬í˜„ ë‚œì´ë„:</strong> {difficulty}</span>
                        </div>
                    </div>
                    """
                    )

            # ì•¡ì…˜ ì•„ì´í…œ í…Œì´ë¸”
            html_parts.append("<h4>ğŸ“‹ ì•¡ì…˜ ì•„ì´í…œ</h4>")

            actions = recommendations.get("action_items", [])
            if not actions:
                html_parts.append(
                    '<div class="info-box">í˜„ì¬ ë¶„ì„ëœ ë°ì´í„°ì—ì„œëŠ” íŠ¹ë³„í•œ ì•¡ì…˜ ì•„ì´í…œì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</div>'
                )
            else:
                html_parts.append(
                    """
                    <table class="table">
                        <thead>
                            <tr>
                                <th>ìš°ì„ ìˆœìœ„</th>
                                <th>í•­ëª©</th>
                                <th>ì˜ˆìƒ ì†Œìš”ì‹œê°„</th>
                                <th>ë‹´ë‹¹ì</th>
                                <th>ê·¼ê±°</th>
                            </tr>
                        </thead>
                        <tbody>
                """
                )

                for action in actions:
                    priority = action.get("priority", "ì¤‘ê°„")
                    item = action.get("item", "ì•¡ì…˜ ì•„ì´í…œ")
                    estimated_time = action.get("estimated_time", "ë¯¸ì •")
                    assignee = action.get("assignee", "ë‹´ë‹¹ì")
                    rationale = action.get("rationale", "")

                    # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ìŠ¤íƒ€ì¼ í´ë˜ìŠ¤
                    priority_class = {
                        "ë†’ìŒ": "status-critical",
                        "ì¤‘ê°„": "status-warning",
                        "ë‚®ìŒ": "status-good",
                    }.get(priority, "status-warning")

                    html_parts.append(
                        f"""
                            <tr>
                                <td><span class="{priority_class}">{priority}</span></td>
                                <td>{item}</td>
                                <td>{estimated_time}</td>
                                <td>{assignee}</td>
                                <td style="font-size: 0.9em; color: #666;">{rationale}</td>
                            </tr>
                    """
                    )

                html_parts.append(
                    """
                        </tbody>
                    </table>
                """
                )

            return "".join(html_parts)

        except Exception as e:
            logger.error(f"HTML ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f'<div class="issue">ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}</div>'

    def _get_default_recommendations(self) -> Dict[str, Any]:
        """Claude í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ ë°˜í™˜"""
        return {
            "immediate_improvements": [
                {
                    "category": "ëª¨ë‹ˆí„°ë§",
                    "title": "ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                    "description": "ê¸°ë³¸ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•",
                    "items": [
                        "CloudWatch ì•ŒëŒ ì„¤ì •",
                        "Performance Insights í™œì„±í™”",
                        "ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê·¸ ì •ê¸° ë¶„ì„",
                    ],
                    "expected_impact": "ì„±ëŠ¥ ë¬¸ì œ ì¡°ê¸° ë°œê²¬",
                    "difficulty": "ë‚®ìŒ",
                }
            ],
            "action_items": [
                {
                    "priority": "ë†’ìŒ",
                    "item": "CloudWatch ì•ŒëŒ ì„¤ì •",
                    "estimated_time": "1ì¼",
                    "assignee": "DBA",
                    "rationale": "ì„±ëŠ¥ ë¬¸ì œ ì¡°ê¸° ê°ì§€ë¥¼ ìœ„í•´ í•„ìš”",
                }
            ],
        }

    async def extract_current_schema_info(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘: database_secret={database_secret}")
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            logger.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}")

            schema_info = {"tables": [], "columns": {}, "indexes": {}}

            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            cursor.execute(
                """
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """
            )
            tables = [row[0] for row in cursor.fetchall()]
            schema_info["tables"] = tables
            logger.info(f"ë°œê²¬ëœ í…Œì´ë¸”: {tables}")

            # ê° í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT column_name, data_type, character_maximum_length, 
                           numeric_precision, numeric_scale, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table,),
                )

                columns = []
                for col_row in cursor.fetchall():
                    col_info = {
                        "name": col_row[0],
                        "data_type": col_row[1],
                        "max_length": col_row[2],
                        "precision": col_row[3],
                        "scale": col_row[4],
                        "is_nullable": col_row[5],
                        "default_value": col_row[6],
                    }
                    columns.append(col_info)

                schema_info["columns"][table] = columns

                # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT index_name, column_name, non_unique, seq_in_index
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table,),
                )

                indexes = {}
                for idx_row in cursor.fetchall():
                    idx_name = idx_row[0]
                    col_name = idx_row[1]
                    is_unique = idx_row[2] == 0

                    if idx_name not in indexes:
                        indexes[idx_name] = {"columns": [], "unique": is_unique}
                    indexes[idx_name]["columns"].append(col_name)

                schema_info["indexes"][table] = indexes

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            logger.info(
                f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(schema_info['tables'])}ê°œ í…Œì´ë¸”, {len(schema_info['columns'])}ê°œ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´"
            )
            return schema_info

        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            if "tunnel_used" in locals() and tunnel_used:
                self.cleanup_ssh_tunnel()
            return {}

    async def extract_relevant_schema_info(
        self,
        ddl_content: str,
        database_secret: str,
        debug_log,
        use_ssh_tunnel: bool = True,
    ) -> Dict[str, Any]:
        """DDL ìœ í˜•ì— ë”°ë¼ ê´€ë ¨ëœ ìŠ¤í‚¤ë§ˆ ì •ë³´ë§Œ ì¶”ì¶œ (ê³µìš© ì»¤ì„œ ì‚¬ìš©)"""
        try:
            # DDL ìœ í˜• íŒŒì‹±
            ddl_info = self.parse_ddl_detailed_with_debug(ddl_content, debug_log)
            if not ddl_info:
                return {}

            # ê³µìš© ì»¤ì„œ ì‚¬ìš©
            cursor = self.get_shared_cursor()
            if cursor is None:
                debug_log("ê³µìš© ì»¤ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ - ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ë‹¨")
                return {}

            relevant_info = {}

            # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ë“¤ì„ ì¶”ì 
            created_tables_in_file = set()
            created_indexes_in_file = {}  # table_name -> [index_info]

            debug_log(f"ì´ {len(ddl_info)}ê°œì˜ DDL êµ¬ë¬¸ ì²˜ë¦¬ ì‹œì‘")

            for i, ddl_statement in enumerate(ddl_info):
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                debug_log(f"ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ: {ddl_type} for {table_name}")

                if ddl_type == "CREATE_TABLE":
                    # CREATE TABLE: í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    try:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (table_name,),
                        )
                        result = cursor.fetchone()
                        table_exists = result[0] > 0 if result else False

                        relevant_info[f"{i}_{table_name}"] = {
                            "exists": table_exists,
                            "type": "table",
                            "order": i,
                            "table_name": table_name,
                        }

                        # íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ë¡œ ì¶”ê°€
                        created_tables_in_file.add(table_name)
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name} ì¡´ì¬: {table_exists}, íŒŒì¼ ë‚´ ìƒì„± ì¶”ê°€"
                        )
                    except Exception as e:
                        debug_log(f"CREATE TABLE ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
                        table_exists = False

                elif ddl_type == "ALTER_TABLE":
                    # ALTER TABLE: í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ + ì»¬ëŸ¼ ì •ë³´ (íŒŒì¼ ë‚´ ìƒì„± ê³ ë ¤)
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    # íŒŒì¼ ë‚´ì—ì„œ ì´ë¯¸ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    created_in_file = table_name in created_tables_in_file
                    effective_exists = table_exists or created_in_file

                    columns_info = []
                    if table_exists:  # ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
                        cursor.execute(
                            """
                            SELECT column_name, data_type, character_maximum_length, 
                                   numeric_precision, numeric_scale, is_nullable
                            FROM information_schema.columns 
                            WHERE table_schema = DATABASE() AND table_name = %s
                            ORDER BY ordinal_position
                        """,
                            (table_name,),
                        )

                        for col_row in cursor.fetchall():
                            columns_info.append(
                                {
                                    "name": col_row[0],
                                    "data_type": col_row[1],
                                    "max_length": col_row[2],
                                    "precision": col_row[3],
                                    "scale": col_row[4],
                                    "is_nullable": col_row[5],
                                }
                            )

                    relevant_info[f"{i}_{table_name}"] = {
                        "exists": table_exists,
                        "created_in_file": created_in_file,
                        "effective_exists": effective_exists,
                        "type": "table",
                        "columns": columns_info,
                        "alter_type": ddl_statement.get("alter_type", "GENERAL"),
                        "target_column": ddl_statement.get("column"),
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"ALTER TABLE [{i}] - {table_name} DBì¡´ì¬: {table_exists}, íŒŒì¼ë‚´ìƒì„±: {created_in_file}, ìœ íš¨ì¡´ì¬: {effective_exists}"
                    )

                elif ddl_type == "CREATE_INDEX":
                    # CREATE INDEX: í…Œì´ë¸” ì¡´ì¬ + ì¸ë±ìŠ¤ ì •ë³´ + ë™ì¼ ì»¬ëŸ¼ êµ¬ì„± ì¸ë±ìŠ¤ í™•ì¸ (íŒŒì¼ ë‚´ ìƒì„± ê³ ë ¤)
                    index_name = ddl_statement.get("index_name")
                    columns_str = ddl_statement.get("columns", "")
                    target_columns = [
                        col.strip().strip("`").lower() for col in columns_str.split(",")
                    ]

                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    # íŒŒì¼ ë‚´ì—ì„œ ì´ë¯¸ ìƒì„±ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                    created_in_file = table_name in created_tables_in_file
                    effective_exists = table_exists or created_in_file

                    existing_indexes = []
                    duplicate_column_indexes = []
                    file_duplicate_indexes = []

                    # DBì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ í™•ì¸
                    if table_exists:
                        cursor.execute(
                            """
                            SELECT index_name, GROUP_CONCAT(column_name ORDER BY seq_in_index) as columns,
                                   non_unique
                            FROM information_schema.statistics 
                            WHERE table_schema = DATABASE() AND table_name = %s
                            GROUP BY index_name, non_unique
                        """,
                            (table_name,),
                        )

                        target_columns_sorted = ",".join(sorted(target_columns))

                        for idx_row in cursor.fetchall():
                            idx_name = idx_row[0]
                            idx_columns = idx_row[1].lower()
                            is_unique = idx_row[2] == 0

                            existing_indexes.append(
                                {
                                    "name": idx_name,
                                    "columns": idx_columns,
                                    "unique": is_unique,
                                }
                            )

                            # ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„± í™•ì¸
                            if idx_columns == target_columns_sorted:
                                duplicate_column_indexes.append(
                                    {
                                        "name": idx_name,
                                        "columns": idx_columns,
                                        "unique": is_unique,
                                    }
                                )

                    # íŒŒì¼ ë‚´ì—ì„œ ì´ë¯¸ ìƒì„±ëœ ì¸ë±ìŠ¤ì™€ ì¤‘ë³µ í™•ì¸
                    target_columns_sorted = ",".join(sorted(target_columns))
                    if table_name in created_indexes_in_file:
                        for file_idx in created_indexes_in_file[table_name]:
                            if file_idx["columns_sorted"] == target_columns_sorted:
                                file_duplicate_indexes.append(file_idx)

                    # íŒŒì¼ ë‚´ ìƒì„± ì¸ë±ìŠ¤ë¡œ ì¶”ê°€
                    if table_name not in created_indexes_in_file:
                        created_indexes_in_file[table_name] = []
                    created_indexes_in_file[table_name].append(
                        {
                            "name": index_name,
                            "columns": target_columns,
                            "columns_sorted": target_columns_sorted,
                            "order": i,
                        }
                    )

                    relevant_info[f"{i}_{table_name}.{index_name}"] = {
                        "type": "index",
                        "table_exists": table_exists,
                        "created_in_file": created_in_file,
                        "effective_exists": effective_exists,
                        "index_name": index_name,
                        "target_columns": target_columns,
                        "existing_indexes": existing_indexes,
                        "duplicate_column_indexes": duplicate_column_indexes,
                        "file_duplicate_indexes": file_duplicate_indexes,
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"CREATE INDEX [{i}] - {table_name}.{index_name}, DBì¤‘ë³µ: {len(duplicate_column_indexes)}ê°œ, íŒŒì¼ì¤‘ë³µ: {len(file_duplicate_indexes)}ê°œ"
                    )

                elif ddl_type == "DROP_TABLE":
                    # DROP TABLE: í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (íŒŒì¼ ë‚´ ìƒì„±ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ - DROPì€ ì‹¤ì œ ì¡´ì¬í•´ì•¼ í•¨)
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    relevant_info[f"{i}_{table_name}"] = {
                        "exists": table_exists,
                        "type": "table",
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(f"DROP TABLE [{i}] - {table_name} ì¡´ì¬: {table_exists}")

                elif ddl_type == "DROP_INDEX":
                    # DROP INDEX: í…Œì´ë¸” ì¡´ì¬ + ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ (íŒŒì¼ ë‚´ ìƒì„±ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ)
                    index_name = ddl_statement.get("index_name")

                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    index_exists = False
                    if table_exists:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.statistics 
                            WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
                        """,
                            (table_name, index_name),
                        )
                        index_exists = cursor.fetchone()[0] > 0

                    relevant_info[f"{i}_{table_name}.{index_name}"] = {
                        "type": "index",
                        "table_exists": table_exists,
                        "index_name": index_name,
                        "index_exists": index_exists,
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"DROP INDEX [{i}] - {table_name}.{index_name} ì¡´ì¬: {index_exists}"
                    )

            debug_log(f"ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(relevant_info)}ê°œ í•­ëª©")
            return relevant_info

        except Exception as e:
            logger.error(f"ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            debug_log(f"ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì˜ˆì™¸: {e}")
            return {}

    def validate_dml_columns(self, sql_content: str, cursor, debug_log) -> dict:
        """DML ì¿¼ë¦¬ì˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦ - CREATE êµ¬ë¬¸ ê³ ë ¤"""
        try:
            debug_log("=== DML ì»¬ëŸ¼ ê²€ì¦ ì‹œì‘ ===")
            if not sqlparse:
                debug_log("sqlparse ëª¨ë“ˆì´ ì—†ì–´ DML ì»¬ëŸ¼ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                return {"total_queries": 0, "queries_with_issues": 0, "results": []}

            # í˜„ì¬ SQLì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸” ì¶”ì¶œ
            created_tables = self.extract_created_tables(sql_content)
            debug_log(f"DML ê²€ì¦ì—ì„œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”: {created_tables}")

            # ìŠ¤í‚¤ë§ˆ ìºì‹œ
            schema_cache = {}

            def get_table_columns(table_name: str) -> set:
                """í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)"""
                if table_name in schema_cache:
                    return schema_cache[table_name]

                try:
                    schema, actual_table = parse_table_name(table_name)

                    if schema:
                        cursor.execute(
                            """
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )

                    columns = {row[0].lower() for row in cursor.fetchall()}
                    schema_cache[table_name] = columns
                    debug_log(f"í…Œì´ë¸” '{table_name}' ì»¬ëŸ¼ ì¡°íšŒ: {columns}")
                    return columns
                except Exception as e:
                    debug_log(f"í…Œì´ë¸” '{table_name}' ì»¬ëŸ¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    return set()

            def parse_table_name(full_table_name: str) -> tuple:
                """í…Œì´ë¸”ëª…ì—ì„œ ìŠ¤í‚¤ë§ˆì™€ í…Œì´ë¸”ëª…ì„ ë¶„ë¦¬"""
                if "." in full_table_name:
                    schema, table = full_table_name.split(".", 1)
                    return schema.strip("`"), table.strip("`")
                return None, full_table_name.strip("`")

            def table_exists(table_name: str) -> bool:
                """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)"""
                try:
                    schema, actual_table = parse_table_name(table_name)

                    if schema:
                        # ìŠ¤í‚¤ë§ˆê°€ ëª…ì‹œëœ ê²½ìš°
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        # ìŠ¤í‚¤ë§ˆê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    return cursor.fetchone()[0] > 0
                except:
                    return False

            # SQL ë¬¸ì—ì„œ ì‚¬ìš©ëœ ì»¬ëŸ¼ë“¤ ì¶”ì¶œ
            validation_results = []

            # ì£¼ì„ ì œê±°
            cleaned_sql = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)

            # ê° ì¿¼ë¦¬ë³„ë¡œ ê²€ì¦
            statements = sqlparse.split(cleaned_sql)
            debug_log(f"ì´ {len(statements)}ê°œì˜ êµ¬ë¬¸ìœ¼ë¡œ ë¶„ë¦¬ë¨")

            for i, stmt in enumerate(statements):
                if not stmt.strip():
                    continue

                stmt_lower = stmt.lower()
                issues = []
                debug_log(f"êµ¬ë¬¸ {i+1} ê²€ì¦ ì‹œì‘: {stmt_lower[:50]}...")

                # SELECT, UPDATE, DELETE ì¿¼ë¦¬ì—ì„œ ì»¬ëŸ¼ ì¶”ì¶œ
                if any(
                    keyword in stmt_lower for keyword in ["select", "update", "delete"]
                ):
                    debug_log(f"êµ¬ë¬¸ {i+1}ì€ DML ì¿¼ë¦¬ì…ë‹ˆë‹¤")
                    # FROM ì ˆì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
                    # FROMê³¼ JOINì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)
                    debug_log(f"êµ¬ë¬¸ {i+1} ì›ë³¸: {stmt}")
                    from_pattern = r"from\s+(?:(\w+)\.)?(\w+)(?:\s+(?:as\s+)?(\w+))?(?=\s+(?:where|order|group|limit|join|inner|left|right|full|cross|$|;))"
                    from_tables = re.findall(from_pattern, stmt_lower)
                    debug_log(f"êµ¬ë¬¸ {i+1} FROM íŒ¨í„´ ê²°ê³¼: {from_tables}")

                    join_pattern = r"join\s+(?:(\w+)\.)?(\w+)(?:\s+(?:as\s+)?(\w+))?(?=\s+(?:on|$|;))"
                    join_tables = re.findall(join_pattern, stmt_lower)
                    debug_log(f"êµ¬ë¬¸ {i+1} JOIN íŒ¨í„´ ê²°ê³¼: {join_tables}")

                    # í…Œì´ë¸” ë³„ì¹­ ë§¤í•‘
                    table_aliases = {}
                    all_tables = set()

                    for schema, table, alias in from_tables + join_tables:
                        full_table_name = f"{schema}.{table}" if schema else table
                        all_tables.add(full_table_name)
                        debug_log(
                            f"êµ¬ë¬¸ {i+1} í…Œì´ë¸” ì¶”ê°€: schema={schema}, table={table}, full_name={full_table_name}"
                        )
                        if alias and alias not in [
                            "where",
                            "order",
                            "group",
                            "limit",
                            "join",
                            "inner",
                            "left",
                            "right",
                            "full",
                            "cross",
                            "on",
                        ]:
                            table_aliases[alias] = full_table_name

                    debug_log(f"êµ¬ë¬¸ {i+1}ì—ì„œ ì°¸ì¡°í•˜ëŠ” í…Œì´ë¸”: {all_tables}")

                    # ìƒˆë¡œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸
                    references_new_table = any(
                        table in created_tables for table in all_tables
                    )
                    if references_new_table:
                        debug_log(
                            f"êµ¬ë¬¸ {i+1}: ìƒˆë¡œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ë¯€ë¡œ DML ì»¬ëŸ¼ ê²€ì¦ ìŠ¤í‚µ: {[t for t in all_tables if t in created_tables]}"
                        )
                        continue

                    # í…Œì´ë¸”.ì»¬ëŸ¼ í˜•íƒœì˜ ì»¬ëŸ¼ ì°¸ì¡° ì°¾ê¸° (FROM/JOINì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ í…Œì´ë¸”ë§Œ ê³ ë ¤)
                    column_refs = []
                    for table_or_alias in all_tables | set(table_aliases.keys()):
                        # ê° í…Œì´ë¸”/ë³„ì¹­ì— ëŒ€í•´ ì»¬ëŸ¼ ì°¸ì¡° ì°¾ê¸°
                        pattern = rf"\b{re.escape(table_or_alias)}\.(\w+)"
                        matches = re.findall(pattern, stmt_lower)
                        for column in matches:
                            column_refs.append((table_or_alias, column))

                    debug_log(f"êµ¬ë¬¸ {i+1}ì—ì„œ ë°œê²¬ëœ ì»¬ëŸ¼ ì°¸ì¡°: {column_refs}")

                    # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
                    for table_ref, column_name in column_refs:
                        # ì‹¤ì œ í…Œì´ë¸”ëª… í•´ê²°
                        actual_table = table_aliases.get(table_ref, table_ref)

                        # ìƒˆë¡œ ìƒì„±ë˜ëŠ” í…Œì´ë¸”ì´ë©´ ìŠ¤í‚µ (ì´ì¤‘ ì²´í¬)
                        if actual_table in created_tables:
                            debug_log(
                                f"í…Œì´ë¸” '{actual_table}'ì€ ìƒˆë¡œ ìƒì„±ë˜ë¯€ë¡œ ì»¬ëŸ¼ ê²€ì¦ ìŠ¤í‚µ"
                            )
                            continue

                        if (
                            actual_table in all_tables
                            or actual_table in schema_cache
                            or table_exists(actual_table)
                        ):
                            existing_columns = get_table_columns(actual_table)

                            if (
                                column_name not in existing_columns
                                and column_name != "*"
                            ):
                                issues.append(
                                    {
                                        "type": "MISSING_COLUMN",
                                        "table": actual_table,
                                        "column": column_name,
                                        "message": f"ì»¬ëŸ¼ '{column_name}'ì´ í…Œì´ë¸” '{actual_table}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                                    }
                                )
                else:
                    debug_log(f"êµ¬ë¬¸ {i+1}ì€ DML ì¿¼ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤")

                if issues:
                    validation_results.append(
                        {
                            "sql": (
                                stmt.strip()[:100] + "..."
                                if len(stmt.strip()) > 100
                                else stmt.strip()
                            ),
                            "issues": issues,
                        }
                    )

            debug_log(
                f"=== DML ì»¬ëŸ¼ ê²€ì¦ ì™„ë£Œ: {len(validation_results)}ê°œ ì¿¼ë¦¬ì—ì„œ ì´ìŠˆ ë°œê²¬ ==="
            )
            return {
                "total_queries": len([s for s in statements if s.strip()]),
                "queries_with_issues": len(validation_results),
                "results": validation_results,
            }

        except Exception as e:
            debug_log(f"DML ì»¬ëŸ¼ ê²€ì¦ ì˜ˆì™¸: {e}")
            return {"total_queries": 0, "queries_with_issues": 0, "results": []}

    async def test_database_connection_for_validation(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ê²€ì¦ìš© ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                cursor.close()
                connection.close()

                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "host": "localhost" if tunnel_used else "remote",
                    "port": 3307 if tunnel_used else 3306,
                }

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"MySQL ì˜¤ë¥˜: {str(e)}"}
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}"}

    async def validate_schema_with_debug(
        self,
        ddl_content: str,
        database_secret: str,
        debug_log,
        use_ssh_tunnel: bool = True,
    ) -> Dict[str, Any]:
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        try:
            debug_log("validate_schema ì‹œì‘")
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed_with_debug(ddl_content, debug_log)
            debug_log(f"íŒŒì‹±ëœ DDL ì •ë³´: {ddl_info}")

            if not ddl_info:
                debug_log("DDL íŒŒì‹± ì‹¤íŒ¨")
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()
            debug_log(f"DB ì—°ê²° ì„±ê³µ, í„°ë„ ì‚¬ìš©: {tunnel_used}")

            validation_results = []

            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]
                debug_log(f"ê²€ì¦ ì¤‘: {ddl_type} on {table_name}")

                if ddl_type == "CREATE_TABLE":
                    debug_log("CREATE_TABLE ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_create_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"CREATE_TABLE ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == "ALTER_TABLE":
                    debug_log("ALTER_TABLE ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_alter_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"ALTER_TABLE ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == "CREATE_INDEX":
                    debug_log("CREATE_INDEX ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_create_index_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"CREATE_INDEX ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == "DROP_TABLE":
                    debug_log("DROP_TABLE ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_drop_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"DROP_TABLE ê²€ì¦ ê²°ê³¼: {result}")
                elif ddl_type == "DROP_INDEX":
                    debug_log("DROP_INDEX ê²€ì¦ í˜¸ì¶œ")
                    result = await self.validate_drop_index_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"DROP_INDEX ê²€ì¦ ê²°ê³¼: {result}")
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"],
                    }

                validation_results.append(result)
                debug_log(f"ê²€ì¦ ê²°ê³¼ ì¶”ê°€ë¨: {result}")

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            debug_log(f"validate_schema ì™„ë£Œ, ê²°ê³¼ ê°œìˆ˜: {len(validation_results)}")
            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            debug_log(f"validate_schema ì˜ˆì™¸: {e}")
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def parse_ddl_detailed_with_debug(
        self, ddl_content: str, debug_log
    ) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ (ë””ë²„ê·¸ ë²„ì „)"""
        ddl_statements = []

        debug_log(f"DDL íŒŒì‹± ì‹œì‘: {repr(ddl_content)}")

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?"
        create_table_matches = re.findall(
            create_table_pattern, ddl_content, re.IGNORECASE
        )

        debug_log(f"CREATE TABLE íŒŒì‹± - ê²°ê³¼: {create_table_matches}")

        for table_name in create_table_matches:
            ddl_statements.append({"type": "CREATE_TABLE", "table": table_name.lower()})
            debug_log(f"CREATE TABLE êµ¬ë¬¸ ì¶”ê°€ë¨: {table_name}")

        # ALTER TABLE íŒŒì‹± (ìƒì„¸)
        # ADD COLUMN - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        alter_add_pattern = r"ALTER\s+TABLE\s+`?(?:(\w+)\.)?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)"
        alter_add_matches = re.findall(alter_add_pattern, ddl_content, re.IGNORECASE)
        debug_log(f"ALTER TABLE ADD COLUMN íŒŒì‹± - ê²°ê³¼: {alter_add_matches}")

        for schema, table_name, column_name, column_def in alter_add_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": full_table_name.lower(),
                    "alter_type": "ADD_COLUMN",
                    "column": column_name.lower(),
                    "column_definition": column_def.strip(),
                }
            )
            debug_log(
                f"ALTER TABLE ADD COLUMN êµ¬ë¬¸ ì¶”ê°€ë¨: {full_table_name}.{column_name}"
            )

        # MODIFY COLUMN
        alter_modify_pattern = (
            r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)"
        )
        alter_modify_matches = re.findall(
            alter_modify_pattern, ddl_content, re.IGNORECASE
        )
        debug_log(f"ALTER TABLE MODIFY COLUMN íŒŒì‹± - ê²°ê³¼: {alter_modify_matches}")

        for table_name, column_name, column_def in alter_modify_matches:
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": table_name.lower(),
                    "alter_type": "MODIFY_COLUMN",
                    "column": column_name.lower(),
                    "column_definition": column_def.strip(),
                }
            )
            debug_log(
                f"ALTER TABLE MODIFY COLUMN êµ¬ë¬¸ ì¶”ê°€ë¨: {table_name}.{column_name}"
            )

        # DROP COLUMN
        alter_drop_pattern = (
            r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?"
        )
        alter_drop_matches = re.findall(alter_drop_pattern, ddl_content, re.IGNORECASE)
        debug_log(f"ALTER TABLE DROP COLUMN íŒŒì‹± - ê²°ê³¼: {alter_drop_matches}")

        for table_name, column_name in alter_drop_matches:
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": table_name.lower(),
                    "alter_type": "DROP_COLUMN",
                    "column": column_name.lower(),
                }
            )
            debug_log(
                f"ALTER TABLE DROP COLUMN êµ¬ë¬¸ ì¶”ê°€ë¨: {table_name}.{column_name}"
            )

        # ì¼ë°˜ ALTER TABLE (ìœ„ì˜ íŒ¨í„´ì— ë§¤ì¹˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
        # ì£¼ì„ì„ ì™„ì „íˆ ì œê±°í•œ í›„ ì‹¤ì œ SQL êµ¬ë¬¸ë§Œ ë§¤ì¹˜
        lines = ddl_content.split("\n")
        sql_lines = []
        for line in lines:
            # ì£¼ì„ ì œê±°
            if "--" in line:
                line = line[: line.index("--")]
            line = line.strip()
            if line:
                sql_lines.append(line)

        clean_sql = " ".join(sql_lines)

        # ALTER TABLEë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ êµ¬ë¬¸ë§Œ ë§¤ì¹˜
        alter_table_pattern = r"\bALTER\s+TABLE\s+`?(\w+)`?\s"
        alter_table_matches = re.findall(alter_table_pattern, clean_sql, re.IGNORECASE)

        # ì´ë¯¸ ì²˜ë¦¬ëœ í…Œì´ë¸”ë“¤ ì œì™¸ - ì¼ë°˜ ALTER TABLE íŒŒì‹± ë¹„í™œì„±í™”
        # (ìƒì„¸ íŒŒì‹±ìœ¼ë¡œ ì¶©ë¶„í•˜ë¯€ë¡œ ì¼ë°˜ íŒŒì‹±ì€ ê±´ë„ˆëœ€)
        debug_log("ì¼ë°˜ ALTER TABLE íŒŒì‹± ê±´ë„ˆëœ€ - ìƒì„¸ íŒŒì‹±ìœ¼ë¡œ ì¶©ë¶„")

        # CREATE INDEX íŒŒì‹± - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        create_index_pattern = r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(?:(\w+)\.)?(\w+)`?\s*\((.*?)\)"
        create_index_matches = re.findall(
            create_index_pattern, ddl_content, re.IGNORECASE
        )

        debug_log(f"CREATE INDEX íŒŒì‹± - ê²°ê³¼: {create_index_matches}")

        for index_name, schema, table_name, columns in create_index_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": full_table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": columns.strip(),
                }
            )
            debug_log(
                f"CREATE INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {full_table_name}({columns})"
            )

        # DROP INDEX íŒŒì‹± - ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(?:(\w+)\.)?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        debug_log(f"DROP INDEX íŒŒì‹± - íŒ¨í„´: {drop_index_pattern}")
        debug_log(f"DROP INDEX íŒŒì‹± - ê²°ê³¼: {drop_index_matches}")

        for index_name, schema, table_name in drop_index_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": full_table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )
            debug_log(f"DROP INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {full_table_name}")

        debug_log(f"ì „ì²´ íŒŒì‹± ê²°ê³¼: {len(ddl_statements)}ê°œ êµ¬ë¬¸")
        for i, stmt in enumerate(ddl_statements):
            debug_log(f"  [{i}] {stmt['type']}: {stmt}")

        return ddl_statements

    async def validate_create_index_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        debug_log(
            f"CREATE INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}, columns={columns}"
        )

        issues = []

        # 1. í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)
        schema, actual_table = self.parse_table_name(table_name)
        if schema:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = %s AND table_name = %s
            """,
                (schema, actual_table),
            )
        else:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (actual_table,),
            )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if not table_exists:
            issues.append(
                f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. CREATE INDEXë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            debug_log(f"ì˜¤ë¥˜: í…Œì´ë¸” '{table_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        else:
            # 2. ì¸ë±ìŠ¤ ì´ë¦„ ì¤‘ë³µ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            debug_log(f"ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬ ì—¬ë¶€: {index_exists}")

            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                debug_log(f"ì˜¤ë¥˜: ì¸ë±ìŠ¤ '{index_name}' ì´ë¯¸ ì¡´ì¬")

            # 3. ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}
            debug_log(f"ê¸°ì¡´ ì»¬ëŸ¼: {existing_columns}")

            # ì¸ë±ìŠ¤ ì»¬ëŸ¼ íŒŒì‹±
            index_columns = [
                col.strip().strip("`").lower() for col in columns.split(",")
            ]
            debug_log(f"ì¸ë±ìŠ¤ ëŒ€ìƒ ì»¬ëŸ¼: {index_columns}")

            for col in index_columns:
                if col not in existing_columns:
                    issues.append(
                        f"ì»¬ëŸ¼ '{col}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )
                    debug_log(f"ì˜¤ë¥˜: ì»¬ëŸ¼ '{col}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

            # 4. ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not issues:  # ê¸°ë³¸ ê²€ì¦ì„ í†µê³¼í•œ ê²½ìš°ì—ë§Œ
                cursor.execute(
                    """
                    SELECT index_name, GROUP_CONCAT(column_name ORDER BY seq_in_index) as columns
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    GROUP BY index_name
                """,
                    (table_name,),
                )

                existing_index_columns = {}
                for row in cursor.fetchall():
                    existing_index_columns[row[0]] = row[1].lower()

                target_columns = ",".join(sorted(index_columns))
                debug_log(f"ëŒ€ìƒ ì»¬ëŸ¼ êµ¬ì„±: {target_columns}")
                debug_log(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ì»¬ëŸ¼ êµ¬ì„±: {existing_index_columns}")

                for idx_name, idx_columns in existing_index_columns.items():
                    if idx_columns == target_columns:
                        issues.append(
                            f"ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ '{idx_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                        )
                        debug_log(f"ì˜¤ë¥˜: ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ '{idx_name}' ì¡´ì¬")
                        break

        result = {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "index_name": index_name,
                "columns": index_columns if "index_columns" in locals() else [],
            },
        }

        debug_log(
            f"CREATE INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")

        return result

    async def validate_drop_index_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        debug_log(f"DROP INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}")

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨ ì²˜ë¦¬)
        schema, actual_table = self.parse_table_name(table_name)
        if schema:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = %s AND table_name = %s
            """,
                (schema, actual_table),
            )
        else:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (actual_table,),
            )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” '{table_name}' ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if not table_exists:
            issues.append(
                f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DROP INDEXë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            debug_log(f"ì˜¤ë¥˜: í…Œì´ë¸” '{table_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            debug_log(f"ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬ ì—¬ë¶€: {index_exists}")

            if not index_exists:
                issues.append(
                    f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )
                debug_log(f"ì˜¤ë¥˜: ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            else:
                debug_log(f"ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬í•¨ - DROP INDEX ê°€ëŠ¥")

        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "index_name": index_name,
                "index_exists": index_exists if "index_exists" in locals() else False,
            },
        }

        debug_log(
            f"DROP INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")

        return result

    async def validate_create_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement["table"]

        debug_log(f"CREATE TABLE ê²€ì¦ ì‹œì‘: table={table_name}")

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if table_exists:
            issues.append(
                f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. CREATE TABLEì€ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            debug_log(f"ì˜¤ë¥˜: í…Œì´ë¸” '{table_name}' ì´ë¯¸ ì¡´ì¬")
        else:
            debug_log(f"í…Œì´ë¸” '{table_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ - CREATE TABLE ê°€ëŠ¥")

        result = {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"table_exists": table_exists, "can_create": not table_exists},
        }

        debug_log(
            f"CREATE TABLE ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")

        return result

    async def validate_alter_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement["table"]

        debug_log(f"ALTER TABLE ê²€ì¦ ì‹œì‘: table={table_name}")

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if not table_exists:
            issues.append(
                f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ALTER TABLEì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            debug_log(f"ì˜¤ë¥˜: í…Œì´ë¸” '{table_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

            result = {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "valid": False,
                "issues": issues,
                "details": {"table_exists": table_exists},
            }
            return result

        # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, í˜„ì¬ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, 
                   numeric_precision, numeric_scale, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
            ORDER BY ordinal_position
        """,
            (table_name,),
        )

        existing_columns = {}
        for col_row in cursor.fetchall():
            col_name = col_row[0].lower()
            existing_columns[col_name] = {
                "data_type": col_row[1].upper(),
                "max_length": col_row[2],
                "precision": col_row[3],
                "scale": col_row[4],
                "is_nullable": col_row[5],
                "default_value": col_row[6],
            }

        debug_log(f"ê¸°ì¡´ ì»¬ëŸ¼ ëª©ë¡: {list(existing_columns.keys())}")

        # ALTER íƒ€ì…ë³„ ê²€ì¦
        alter_type = ddl_statement.get("alter_type", "GENERAL")
        debug_log(f"ALTER íƒ€ì…: {alter_type}")

        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement.get("column")
            if column_name and column_name in existing_columns:
                issues.append(
                    f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ADD COLUMNì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                debug_log(f"ì˜¤ë¥˜: ì»¬ëŸ¼ '{column_name}' ì´ë¯¸ ì¡´ì¬")
            else:
                debug_log(f"ì»¬ëŸ¼ '{column_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ADD COLUMN ê°€ëŠ¥")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement.get("column")
            if column_name and column_name not in existing_columns:
                issues.append(
                    f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DROP COLUMNì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                debug_log(f"ì˜¤ë¥˜: ì»¬ëŸ¼ '{column_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            else:
                debug_log(f"ì»¬ëŸ¼ '{column_name}' ì¡´ì¬í•¨ - DROP COLUMN ê°€ëŠ¥")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement.get("column")
            column_definition = ddl_statement.get("column_definition", "")

            if column_name and column_name not in existing_columns:
                issues.append(
                    f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. MODIFY COLUMNì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                debug_log(f"ì˜¤ë¥˜: ì»¬ëŸ¼ '{column_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            else:
                debug_log(f"ì»¬ëŸ¼ '{column_name}' ì¡´ì¬í•¨ - MODIFY COLUMN ê°€ëŠ¥")

                # ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± ê²€ì¦
                if column_name in existing_columns:
                    existing_col = existing_columns[column_name]
                    compatibility_result = self.validate_column_type_compatibility(
                        existing_col, column_definition, debug_log
                    )
                    if not compatibility_result["compatible"]:
                        issues.extend(compatibility_result["issues"])

        result = {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "existing_columns": list(existing_columns.keys()),
                "column_count": len(existing_columns),
                "target_column": ddl_statement.get("column"),
            },
        }

        debug_log(
            f"ALTER TABLE ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")

        return result

    async def validate_constraints(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ì œì•½ì¡°ê±´ ê²€ì¦ - FK, ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)

            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()

            constraint_results = []

            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ê²€ì¦
            if constraints_info.get("foreign_keys"):
                for fk in constraints_info["foreign_keys"]:
                    # ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (fk["referenced_table"],),
                    )

                    ref_table_exists = cursor.fetchone()[0] > 0

                    if ref_table_exists:
                        # ì°¸ì¡° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_schema = DATABASE() 
                            AND table_name = %s AND column_name = %s
                        """,
                            (fk["referenced_table"], fk["referenced_column"]),
                        )

                        ref_column_exists = cursor.fetchone()[0] > 0

                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": ref_column_exists,
                                "issue": (
                                    None
                                    if ref_column_exists
                                    else f"ì°¸ì¡° ì»¬ëŸ¼ '{fk['referenced_table']}.{fk['referenced_column']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                                ),
                            }
                        )
                    else:
                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": False,
                                "issue": f"ì°¸ì¡° í…Œì´ë¸” '{fk['referenced_table']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                            }
                        )

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ"""
        ddl_statements = []

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(
            create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE
        )

        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append(
                {
                    "type": "CREATE_TABLE",
                    "table": table_name.lower(),
                    "columns": columns_info["columns"],
                    "constraints": columns_info["constraints"],
                }
            )

        # ALTER TABLE íŒŒì‹±
        alter_patterns = [
            # ADD COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "ADD_COLUMN",
            ),
            # DROP COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?",
                "DROP_COLUMN",
            ),
            # MODIFY COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "MODIFY_COLUMN",
            ),
            # CHANGE COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)",
                "CHANGE_COLUMN",
            ),
        ]

        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == "CHANGE_COLUMN":
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "old_column": old_column.lower(),
                            "new_column": new_column.lower(),
                            "column_definition": column_def.strip(),
                        }
                    )
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "column": column_name.lower(),
                            "column_definition": (
                                column_def.strip() if column_def else None
                            ),
                        }
                    )

        # CREATE INDEX íŒŒì‹±
        create_index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)"
        )
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name, columns in index_matches:
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": [
                        col.strip().strip("`").lower() for col in columns.split(",")
                    ],
                }
            )

        # DROP TABLE íŒŒì‹±
        drop_table_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?"
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)

        for table_name in drop_table_matches:
            ddl_statements.append({"type": "DROP_TABLE", "table": table_name.lower()})

        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        print(f"[DEBUG] DROP INDEX íŒŒì‹± - íŒ¨í„´: {drop_index_pattern}")
        print(f"[DEBUG] DROP INDEX íŒŒì‹± - ì…ë ¥: {repr(ddl_content)}")
        print(f"[DEBUG] DROP INDEX íŒŒì‹± - ê²°ê³¼: {drop_index_matches}")

        for index_name, table_name in drop_index_matches:
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )
            print(f"[DEBUG] DROP INDEX êµ¬ë¬¸ ì¶”ê°€ë¨: {index_name} on {table_name}")

        print(f"[DEBUG] ì „ì²´ íŒŒì‹± ê²°ê³¼: {len(ddl_statements)}ê°œ êµ¬ë¬¸")
        for i, stmt in enumerate(ddl_statements):
            print(f"[DEBUG]   [{i}] {stmt['type']}: {stmt}")

        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEì˜ ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±"""
        columns = []
        constraints = []

        # ì»¬ëŸ¼ ì •ì˜ì™€ ì œì•½ì¡°ê±´ì„ ë¶„ë¦¬
        lines = [line.strip() for line in columns_def.split(",")]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì œì•½ì¡°ê±´ í™•ì¸
            if re.match(
                r"(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)",
                line,
                re.IGNORECASE,
            ):
                constraints.append(line)
            else:
                # ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±
                column_match = re.match(
                    r"`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?", line, re.IGNORECASE
                )
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ""

                    columns.append(
                        {
                            "name": column_name,
                            "data_type": data_type,
                            "attributes": attributes.strip(),
                        }
                    )

        return {"columns": columns, "constraints": constraints}

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ"""
        constraints = {"foreign_keys": [], "indexes": [], "primary_keys": []}

        # ì™¸ë˜í‚¤ íŒ¨í„´ ë§¤ì¹­
        fk_pattern = (
            r"FOREIGN\s+KEY\s*\(`?(\w+)`?\)\s*REFERENCES\s+`?(\w+)`?\s*\(`?(\w+)`?\)"
        )
        fk_matches = re.findall(fk_pattern, ddl_content, re.IGNORECASE)

        for column, ref_table, ref_column in fk_matches:
            constraints["foreign_keys"].append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return constraints

    async def validate_create_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        columns = ddl_statement["columns"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists, "columns_count": len(columns)},
        }

    async def validate_alter_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        alter_type = ddl_statement["alter_type"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues,
            }

        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        existing_columns = {
            row[0].lower(): {
                "data_type": row[1].upper(),
                "max_length": row[2],
                "precision": row[3],
                "scale": row[4],
                "is_nullable": row[5],
            }
            for row in cursor.fetchall()
        }

        # ALTER ìœ í˜•ë³„ ê²€ì¦
        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement["column"]
            if column_name in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement["column"]
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement["column"]
            new_definition = ddl_statement["column_definition"]

            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        elif alter_type == "CHANGE_COLUMN":
            old_column = ddl_statement["old_column"]
            new_column = ddl_statement["new_column"]
            new_definition = ddl_statement["column_definition"]

            if old_column not in existing_columns:
                issues.append(f"ê¸°ì¡´ ì»¬ëŸ¼ '{old_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ìƒˆ ì»¬ëŸ¼ëª… '{new_column}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"existing_columns": list(existing_columns.keys())},
        }

    async def validate_create_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}

            for column in columns:
                if column not in existing_columns:
                    issues.append(
                        f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "columns": columns},
        }

    async def validate_drop_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦ (ë””ë²„ê·¸ ë²„ì „)"""
        table_name = ddl_statement["table"]

        debug_log(f"DROP TABLE ê²€ì¦ ì‹œì‘: table={table_name}")

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"í…Œì´ë¸” '{table_name}' ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if not table_exists:
            issues.append(
                f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DROP TABLEì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            debug_log(f"ì˜¤ë¥˜: í…Œì´ë¸” '{table_name}' ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        else:
            debug_log(f"í…Œì´ë¸” '{table_name}' ì¡´ì¬í•¨ - DROP TABLE ê°€ëŠ¥")

        result = {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"table_exists": table_exists, "can_drop": table_exists},
        }

        debug_log(
            f"DROP TABLE ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ìµœì¢… ê²°ê³¼: {result}")

        return result

    async def validate_drop_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦ (í˜¸í™˜ì„± ìœ ì§€ìš©)"""
        table_name = ddl_statement["table"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists},
        }

    async def validate_drop_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        print(f"[DEBUG] DROP INDEX ê²€ì¦ ì‹œì‘: table={table_name}, index={index_name}")

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        print(f"[DEBUG] í…Œì´ë¸” '{table_name}' ì¡´ì¬ ì—¬ë¶€: {table_exists}")

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"[DEBUG] í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            print(f"[DEBUG] ì¸ë±ìŠ¤ '{index_name}' ì¡´ì¬ ì—¬ë¶€: {index_exists}")

            if not index_exists:
                issues.append(
                    f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )
                print(f"[DEBUG] ì¸ë±ìŠ¤ '{index_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ì´ìŠˆ ì¶”ê°€")

        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "table_exists": table_exists},
        }

        print(
            f"[DEBUG] DROP INDEX ê²€ì¦ ì™„ë£Œ: issues={len(issues)}, valid={len(issues) == 0}"
        )
        print(f"[DEBUG] ìµœì¢… ê²°ê³¼: {result}")

        return result

    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def generate_html_report(
        self,
        report_path: Path,
        filename: str,
        ddl_content: str,
        sql_type: str,
        status: str,
        summary: str,
        issues: List[str],
        db_connection_info: Optional[Dict],
        schema_validation: Optional[Dict],
        constraint_validation: Optional[Dict],
        database_secret: Optional[str],
        explain_result: Optional[Dict] = None,
        claude_analysis_result: Optional[str] = None,  # Claude ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        dml_column_issues: List[str] = None,  # DML ì»¬ëŸ¼ ì´ìŠˆ ì¶”ê°€
    ):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        # dml_column_issues ì´ˆê¸°í™”
        if dml_column_issues is None:
            dml_column_issues = []

        # ìƒì„¸ ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
        try:
            with open(
                "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(f"=== HTML ìƒì„± í•¨ìˆ˜ ì‹œì‘ ===\n")
                f.write(f"report_path: {report_path}\n")
                f.write(f"filename: {filename}\n")
                f.write(f"sql_type: {sql_type}\n")
                f.write(f"status: {status}\n")
                f.write(f"issues ê°œìˆ˜: {len(issues)}\n")
                f.flush()
        except Exception as debug_e:
            logger.error(f"ë””ë²„ê·¸ ë¡œê·¸ ì‘ì„± ì˜¤ë¥˜: {debug_e}")

        # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
        try:
            with open(
                "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(
                    f"HTML ìƒì„± í•¨ìˆ˜ í˜¸ì¶œë¨ - claude_analysis_result: {claude_analysis_result}\n"
                )
                f.flush()
        except:
            pass
        try:
            # Claude ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœ ì¬í‰ê°€
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("ê²€ì¦ í†µê³¼")
            )

            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜ - Claude ê²€ì¦ ê²°ê³¼ ìš°ì„  ë°˜ì˜
            if (
                claude_success
                and status == "FAIL"
                and not any(
                    "ì˜¤ë¥˜:" in issue or "ì‹¤íŒ¨" in issue or "ì¡´ì¬í•˜ì§€ ì•Š" in issue
                    for issue in issues
                )
            ):
                # Claudeê°€ ì„±ê³µì´ê³  ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ì—†ìœ¼ë©´ PASSë¡œ ë³€ê²½
                status = "PASS"
                summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."

            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"

            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­ì— ë”°ë¼)
            db_info_section = ""

            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜ - Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
            claude_issues = []
            other_issues = []

            for issue in issues:
                if issue.startswith("Claude ê²€ì¦:"):
                    claude_issues.append(issue[12:].strip())  # "Claude ê²€ì¦:" ì œê±°
                else:
                    other_issues.append(issue)

            # ê¸°íƒ€ ê²€ì¦ ë¬¸ì œ ì„¹ì…˜ ì œê±° (ì¤‘ë³µ ë°©ì§€)

            # Claude ê²€ì¦ ê²°ê³¼ ë‚´ìš© ì¤€ë¹„ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ í¬í•¨)
            schema_validation_summary = self.create_schema_validation_summary(
                issues, dml_column_issues
            )

            # Claude ê²€ì¦ê³¼ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ í†µí•©í•œ ë‚´ìš© ìƒì„±
            combined_validation_content = ""

            # Claude AI ê²€ì¦ ê²°ê³¼ ì¶”ê°€ (ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ëŠ” ìˆ¨ê¹€)
            claude_content = (
                claude_analysis_result
                if claude_analysis_result
                else "Claude ê²€ì¦ ê²°ê³¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            combined_validation_content += f"""
<div class="validation-subsection">
    <h4>ğŸ“‹ SQLê²€ì¦ê²°ê³¼</h4>
    <pre class="validation-text">{claude_content}</pre>
</div>
"""

            # ì „ì²´ ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš°
            success_section = ""
            if not issues:
                success_section = """
                <div class="issues-section success" style="display: none;">
                    <h3>âœ… ê²€ì¦ ê²°ê³¼</h3>
                    <p class="no-issues">ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</p>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SQL ê²€ì¦ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
        }}
        .claude-section {{
            margin: 30px 0;
            padding: 25px;
            border-radius: 8px;
            border: 2px solid #667eea;
            background: #f8f9ff;
            box-shadow: 0 2px 10px rgba(102, 126, 234, 0.1);
        }}
        .claude-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            font-size: 1.3em;
        }}
        .claude-result {{
            margin: 20px 0;
            padding: 0;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            box-shadow: 0 1px 5px rgba(0,0,0,0.05);
        }}
        .claude-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 800px;  /* 400pxì—ì„œ 800pxë¡œ ì¦ê°€ */
            overflow-y: auto;
            min-height: 100px;
            resize: vertical;  /* ì‚¬ìš©ìê°€ ìˆ˜ì§ìœ¼ë¡œ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥ */
        }}
        .validation-subsection {{
            margin: 15px 0;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #28a745;
            background: #f8fff9;
        }}
        .validation-subsection h4 {{
            margin: 0 0 10px 0;
            color: #495057;
            font-size: 1.1em;
        }}
        .validation-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 13px;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 300px;
            overflow-y: auto;
        }}
        .success-text {{
            color: #28a745;
            font-weight: 500;
            margin: 0;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} SQL ê²€ì¦ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ SQL íƒ€ì…</h4>
                    <p>{sql_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ SQL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="claude-section">
                <h3>ğŸ” í†µí•© ê²€ì¦ ê²°ê³¼ (ìŠ¤í‚¤ë§ˆ + ì¿¼ë¦¬ì„±ëŠ¥)</h3>
                <div class="claude-result">
                    {combined_validation_content}
                </div>
            </div>
            
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì œê±°
            try:
                with open(report_path, "r", encoding="utf-8") as f:
                    html_content = f.read()

                # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì œê±° - ë” ì •í™•í•œ ë°©ë²•
                lines = html_content.split("\n")
                new_lines = []
                i = 0

                while i < len(lines):
                    line = lines[i]

                    # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ ì‹œì‘ ê°ì§€
                    if '<div class="info-section" style="display: none;">' in line:
                        # ë‹¤ìŒ ë¼ì¸ë“¤ì„ í™•ì¸í•˜ì—¬ ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ì¸ì§€ íŒë‹¨
                        if i + 1 < len(lines) and "ğŸ“Š ê²€ì¦ ê²°ê³¼" in lines[i + 1]:
                            # ê²€ì¦ ê²°ê³¼ ì„¹ì…˜ì´ë¯€ë¡œ </div>ê¹Œì§€ ìŠ¤í‚µ
                            i += 1  # í˜„ì¬ div ë¼ì¸ ìŠ¤í‚µ
                            while i < len(lines):
                                if "</div>" in lines[i]:
                                    i += 1  # </div> ë¼ì¸ë„ ìŠ¤í‚µ
                                    break
                                i += 1
                            continue

                    new_lines.append(line)
                    i += 1

                html_content = "\n".join(new_lines)

                with open(report_path, "w", encoding="utf-8") as f:
                    f.write(html_content)
            except Exception as e:
                pass

            # íŒŒì¼ ìƒì„± í™•ì¸ ë””ë²„ê·¸
            try:
                with open(
                    "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    f.write(f"HTML íŒŒì¼ ìƒì„± ì™„ë£Œ: {report_path}\n")
                    f.write(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {report_path.exists()}\n")
                    f.flush()
            except:
                pass

        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ë¥¼ ë””ë²„ê·¸ íŒŒì¼ì— ê¸°ë¡
            try:
                with open(
                    "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    import traceback

                    f.write(f"HTML ìƒì„± ì˜¤ë¥˜: {e}\n")
                    f.write(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}\n")
                    f.flush()
            except:
                pass

    async def generate_consolidated_html_report(
        self, validation_results: List[Dict], database_secret: str
    ) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì˜ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            # íŒŒì¼ë³„ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"
                status_class = "success" if result["status"] == "PASS" else "error"

                issues_html = ""
                if result["issues"]:
                    issues_html = "<ul class='issues-list'>"
                    for issue in result["issues"]:
                        issues_html += f"<li>{issue}</li>"
                    issues_html += "</ul>"
                else:
                    issues_html = "<p class='no-issues'>ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>"

                # ê°œë³„ íŒŒì¼ ê²€ì¦ì—ì„œë§Œ ìƒì„¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ë¯€ë¡œ ë§í¬ ì—†ì´ íŒŒì¼ëª…ë§Œ í‘œì‹œ
                filename_display = result["filename"]

                file_sections += f"""
                <div class="file-section {status_class}">
                    <h3>{status_icon} {i}. {filename_display}</h3>
                    <div class="file-details">
                        <div class="file-info">
                            <span><strong>DDL íƒ€ì…:</strong> {result['ddl_type']}</span>
                            <span><strong>ìƒíƒœ:</strong> {result['status']}</span>
                            <span><strong>ë¬¸ì œ ìˆ˜:</strong> {len(result['issues'])}ê°œ</span>
                        </div>
                        <div class="sql-code">
{result['ddl_content']}
                        </div>
                        {f'<div class="issues-section">{issues_html}</div>' if result['issues'] else ''}
                    </div>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© SQL ê²€ì¦ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 20px 30px;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-section.success {{
            border-left: 4px solid #28a745;
        }}
        .file-section.error {{
            border-left: 4px solid #dc3545;
        }}
        .file-section h3 {{
            margin: 0;
            padding: 15px 20px;
            background: #f8f9fa;
            border-bottom: 1px solid #e9ecef;
        }}
        .file-section h3 a {{
            color: #495057;
            text-decoration: none;
        }}
        .file-section h3 a:hover {{
            color: #007bff;
            text-decoration: underline;
        }}
        .file-details {{
            padding: 20px;
        }}
        .file-info {{
            display: flex;
            gap: 20px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }}
        .file-info span {{
            background: #e9ecef;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
            font-size: 0.9em;
        }}
        .issues-section {{
            margin-top: 15px;
            padding: 15px;
            background: #fff5f5;
            border: 1px solid #fed7d7;
            border-radius: 6px;
        }}
        .issues-section h4 {{
            margin: 0 0 10px 0;
            color: #c53030;
        }}
        .issues-list {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
            color: #c53030;
        }}
        .no-issues {{
            color: #38a169;
            margin: 0;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
                margin: 20px;
            }}
            .file-section {{
                margin: 20px 15px;
            }}
            .file-info {{
                flex-direction: column;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© SQL ê²€ì¦ë³´ê³ ì„œ</h1>
            <p>ë°ì´í„°ë² ì´ìŠ¤: {database_secret}</p>
            <p>ê²€ì¦ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
        
        <div class="summary-stats">
            <div class="stat-item">
                <div class="stat-number">{total_files}</div>
                <div class="stat-label">ì´ íŒŒì¼ ìˆ˜</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                <div class="stat-label">í†µê³¼</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                <div class="stat-label">ì‹¤íŒ¨</div>
            </div>
            <div class="stat-item">
                <div class="stat-number">{round(passed_files/total_files*100) if total_files > 0 else 0}%</div>
                <div class="stat-label">ì„±ê³µë¥ </div>
            </div>
        </div>
        
        {file_sections}
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>"""

            # íŒŒì¼ ì €ì¥
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_report(
        self,
        keyword: Optional[str] = None,
        report_files: Optional[List[str]] = None,
        date_filter: Optional[str] = None,
        latest_count: Optional[int] = None,
    ) -> str:
        """ê¸°ì¡´ HTML ë³´ê³ ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ë³´ê³ ì„œ íŒŒì¼ ìˆ˜ì§‘
            if report_files:
                # íŠ¹ì • íŒŒì¼ë“¤ ì§€ì •ëœ ê²½ìš°
                html_files = [
                    OUTPUT_DIR / f for f in report_files if (OUTPUT_DIR / f).exists()
                ]
            else:
                # validation_reportë¡œ ì‹œì‘í•˜ëŠ” HTML íŒŒì¼ë§Œ (debug_log ì œì™¸)
                html_files = list(OUTPUT_DIR.glob("validation_report_*.html"))

                # í‚¤ì›Œë“œ í•„í„°ë§
                if keyword:
                    html_files = [f for f in html_files if keyword in f.name]

                # ë‚ ì§œ í•„í„°ë§ (YYYYMMDD í˜•ì‹)
                if date_filter:
                    html_files = [f for f in html_files if date_filter in f.name]

                # ìµœì‹  íŒŒì¼ ê°œìˆ˜ ì œí•œ
                if latest_count:
                    # íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
                    html_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                    html_files = html_files[:latest_count]

            if not html_files:
                return f"ì¡°ê±´ì— ë§ëŠ” HTML ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œ: {keyword}, ë‚ ì§œ: {date_filter}, ê°œìˆ˜: {latest_count})"

            # ê° ë³´ê³ ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ
            report_data = []
            for html_file in html_files:
                try:
                    content = html_file.read_text(encoding="utf-8")

                    # íŒŒì¼ëª…ì—ì„œ ì›ë³¸ SQL íŒŒì¼ëª… ì¶”ì¶œ
                    sql_filename = html_file.name.replace(
                        "validation_report_", ""
                    ).replace(".html", "")
                    if "_2025" in sql_filename:
                        sql_filename = sql_filename.split("_2025")[0] + ".sql"

                    # ê²€ì¦ ê²°ê³¼ ì¶”ì¶œ - HTML êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ì¶”ì¶œ
                    if (
                        'status-badge">PASS' in content
                        or "âœ… SQL ê²€ì¦ë³´ê³ ì„œ" in content
                    ):
                        status = "PASS"
                        status_icon = "âœ…"
                    elif (
                        'status-badge">FAIL' in content
                        or "âŒ SQL ê²€ì¦ë³´ê³ ì„œ" in content
                    ):
                        status = "FAIL"
                        status_icon = "âŒ"
                    else:
                        # ê¸°ë³¸ê°’ìœ¼ë¡œ FAIL ì²˜ë¦¬
                        status = "FAIL"
                        status_icon = "âŒ"

                    # SQL ë‚´ìš© ì¼ë¶€ ì¶”ì¶œ (HTML íŒŒì¼ì—ì„œë§Œ)
                    sql_preview = "SQL ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    if "sql-code" in content:
                        import re

                        sql_match = re.search(
                            r'<div class="sql-code"[^>]*>(.*?)</div>',
                            content,
                            re.DOTALL,
                        )
                        if sql_match:
                            sql_preview = sql_match.group(1).strip()[:100] + "..."

                    # ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                    summary = "ìƒì„¸ ë‚´ìš©ì€ ê°œë³„ ë³´ê³ ì„œ ì°¸ì¡°"
                    if "Claude AI ë¶„ì„" in content:
                        summary = "AI ë¶„ì„ ì™„ë£Œ"

                    report_data.append(
                        {
                            "filename": sql_filename,
                            "html_file": html_file.name,
                            "status": status,
                            "status_icon": status_icon,
                            "sql_preview": sql_preview,
                            "summary": summary,
                        }
                    )

                except Exception as e:
                    logger.error(f"ë³´ê³ ì„œ íŒŒì‹± ì˜¤ë¥˜ {html_file}: {e}")
                    continue

            if not report_data:
                return "ìœ íš¨í•œ ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # í†µí•© ë³´ê³ ì„œ HTML ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # í†µê³„ ê³„ì‚°
            total_reports = len(report_data)
            passed_reports = sum(1 for r in report_data if r["status"] == "PASS")
            failed_reports = total_reports - passed_reports

            # í…Œì´ë¸” í–‰ ìƒì„±
            table_rows = ""
            for i, data in enumerate(report_data, 1):
                table_rows += f"""
                <tr onclick="openReport('{data['html_file']}')" style="cursor: pointer;">
                    <td>{i}</td>
                    <td>{data['status_icon']} {data['filename']}</td>
                    <td><code>{data['sql_preview']}</code></td>
                    <td><span class="status-badge {data['status'].lower()}">{data['status']}</span></td>
                    <td>{data['summary']}</td>
                </tr>
                """

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: 'Segoe UI', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 2.5em; font-weight: 300; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #667eea; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #333; }}
        .stat-label {{ color: #666; margin-top: 5px; }}
        .table-container {{ margin: 30px; overflow-x: auto; }}
        table {{ width: 100%; border-collapse: collapse; background: white; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #f8f9fa; font-weight: 600; }}
        tr:hover {{ background: #f8f9fa; }}
        .status-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .status-badge.pass {{ background: #d4edda; color: #155724; }}
        .status-badge.fail {{ background: #f8d7da; color: #721c24; }}
        code {{ background: #f1f3f4; padding: 2px 4px; border-radius: 3px; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{total_reports}</div>
                <div class="stat-label">ì´ ë³´ê³ ì„œ</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{passed_reports}</div>
                <div class="stat-label">ê²€ì¦ í†µê³¼</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{failed_reports}</div>
                <div class="stat-label">ê²€ì¦ ì‹¤íŒ¨</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{round(passed_reports/total_reports*100) if total_reports > 0 else 0}%</div>
                <div class="stat-label">ì„±ê³µë¥ </div>
            </div>
        </div>
        
        <div class="table-container">
            <h2>ğŸ“‹ ë³´ê³ ì„œ ëª©ë¡ (í´ë¦­í•˜ì—¬ ìƒì„¸ ë³´ê¸°)</h2>
            <table>
                <thead>
                    <tr>
                        <th>#</th>
                        <th>íŒŒì¼ëª…</th>
                        <th>SQL ë¯¸ë¦¬ë³´ê¸°</th>
                        <th>ê²€ì¦ ê²°ê³¼</th>
                        <th>ìš”ì•½</th>
                    </tr>
                </thead>
                <tbody>
                    {table_rows}
                </tbody>
            </table>
        </div>
    </div>
    
    <script>
        function openReport(filename) {{
            window.open(filename, '_blank');
        }}
    </script>
</body>
</html>"""

            report_path.write_text(html_content, encoding="utf-8")

            return f"""ğŸ“Š í†µí•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ“ˆ ìš”ì•½:
â€¢ ì´ ë³´ê³ ì„œ: {total_reports}ê°œ
â€¢ ê²€ì¦ í†µê³¼: {passed_reports}ê°œ ({round(passed_reports/total_reports*100)}%)
â€¢ ê²€ì¦ ì‹¤íŒ¨: {failed_reports}ê°œ ({round(failed_reports/total_reports*100)}%)

ğŸ“„ í†µí•© ë³´ê³ ì„œ: {report_path}

ğŸ’¡ ì‚¬ìš©ë²•: í…Œì´ë¸”ì˜ ê° í–‰ì„ í´ë¦­í•˜ë©´ í•´ë‹¹ ìƒì„¸ ë³´ê³ ì„œê°€ ìƒˆ ì°½ì—ì„œ ì—´ë¦½ë‹ˆë‹¤."""

        except Exception as e:
            return f"í†µí•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def generate_comprehensive_performance_report(
        self,
        database_secret: str,
        db_instance_identifier: str,
        region: Optional[str] = None,
        hours: int = 24,
    ) -> str:
        """ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„±"""
        # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_performance_{db_instance_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        def debug_log(message):
            with open(debug_log_path, "a", encoding="utf-8") as f:
                f.write(f"{datetime.now().strftime('%H:%M:%S')} - {message}\n")
                f.flush()

        debug_log(
            f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì‹œì‘ - ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}"
        )

        try:
            # regionì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ í”„ë¡œíŒŒì¼ì˜ ê¸°ë³¸ ë¦¬ì „ ì‚¬ìš©
            if region is None:
                region = self.default_region

            # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸ ë° ì¸ìŠ¤í„´ìŠ¤ identifier ë³€í™˜
            original_identifier = db_instance_identifier
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_log(f"ë¦¬ì „ ì„¤ì •: {region}, íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")

            # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            debug_log("ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘")
            metrics_result = await self.collect_db_metrics(
                db_instance_identifier, hours, None, region
            )
            if "ì˜¤ë¥˜" in metrics_result:
                debug_log(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {metrics_result}")
                return f"âŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {metrics_result}"

            debug_log("ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")

            # CSV íŒŒì¼ëª… ì¶”ì¶œ
            csv_file = None
            for line in metrics_result.split("\n"):
                if "database_metrics_" in line and ".csv" in line:
                    csv_file = line.split(": ")[-1]
                    break

            if not csv_file:
                debug_log("ë©”íŠ¸ë¦­ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return "âŒ ë©”íŠ¸ë¦­ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

            csv_filename = Path(csv_file).name
            debug_log(f"CSV íŒŒì¼ëª…: {csv_filename}")

            # 2. ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘
            debug_log("ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œì‘")
            slow_queries = await self.collect_slow_queries(
                database_secret, db_instance_identifier
            )
            memory_queries = await self.collect_memory_intensive_queries(
                database_secret, db_instance_identifier
            )
            cpu_queries = await self.collect_cpu_intensive_queries(
                database_secret, db_instance_identifier
            )
            temp_queries = await self.collect_temp_space_intensive_queries(
                database_secret, db_instance_identifier
            )
            debug_log("ì„±ëŠ¥ ì¿¼ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ")

            # 3. ë©”íŠ¸ë¦­ ë¶„ì„
            debug_log("ë©”íŠ¸ë¦­ ë¶„ì„ ì‹œì‘")
            summary = await self.get_metric_summary(csv_filename)
            correlation = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )
            outliers = await self.detect_metric_outliers(csv_filename, 2.0)
            debug_log("ë©”íŠ¸ë¦­ ë¶„ì„ ì™„ë£Œ")

            # 4. ìƒê´€ê´€ê³„ ë¶„ì„ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
            correlation_analysis = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )

            # 5. Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
            debug_log("Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹œì‘")
            logger.info("Claude ê¸°ë°˜ ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹œì‘")
            try:
                claude_recommendations = (
                    await self.generate_performance_recommendations_with_claude(
                        summary,
                        correlation_analysis,
                        outliers,
                        slow_queries,
                        memory_queries,
                        cpu_queries,
                        temp_queries,
                        database_secret,
                    )
                )
                debug_log("Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì™„ë£Œ")
                logger.info(
                    f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì™„ë£Œ: {len(claude_recommendations.get('immediate_improvements', []))}ê°œ ê°œì„ ì‚¬í•­, {len(claude_recommendations.get('action_items', []))}ê°œ ì•¡ì…˜ì•„ì´í…œ"
                )
            except Exception as e:
                debug_log(f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
                logger.error(f"Claude ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê¶Œì¥ì‚¬í•­ ì‚¬ìš©: {e}")
                claude_recommendations = self._get_default_recommendations()

            # 6. HTML ë³´ê³ ì„œ ìƒì„±
            debug_log("HTML ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            report_path = (
                OUTPUT_DIR
                / f"comprehensive_performance_report_{db_instance_identifier}_{timestamp}.html"
            )

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ - {db_instance_identifier}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; text-align: center; }}
        .header h1 {{ font-size: 2.5em; margin-bottom: 10px; }}
        .header .subtitle {{ font-size: 1.2em; opacity: 0.9; }}
        .section {{ background: white; margin-bottom: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); overflow: hidden; }}
        .section-header {{ background: #2c3e50; color: white; padding: 20px; font-size: 1.3em; font-weight: bold; }}
        .section-content {{ padding: 25px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 25px; }}
        .metric-card {{ background: #f8f9fa; border-left: 4px solid #3498db; padding: 20px; border-radius: 5px; }}
        .metric-card.warning {{ border-left-color: #f39c12; }}
        .metric-card.danger {{ border-left-color: #e74c3c; }}
        .metric-card.success {{ border-left-color: #27ae60; }}
        .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 10px; }}
        .metric-value {{ font-size: 1.8em; font-weight: bold; color: #3498db; }}
        .metric-unit {{ font-size: 0.9em; color: #7f8c8d; }}
        .query-box {{ background: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; font-family: 'Courier New', monospace; font-size: 0.9em; overflow-x: auto; }}
        .status-good {{ color: #27ae60; font-weight: bold; }}
        .status-warning {{ color: #f39c12; font-weight: bold; }}
        .status-critical {{ color: #e74c3c; font-weight: bold; }}
        .table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .table th, .table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .table th {{ background: #34495e; color: white; }}
        .table tr:hover {{ background: #f5f5f5; }}
        .chart-container {{ text-align: center; margin: 20px 0; }}
        .recommendation {{ background: #e8f5e9; border-left: 4px solid #4caf50; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .issue {{ background: #ffebee; border-left: 4px solid #f44336; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .info-box {{ background: #e3f2fd; border-left: 4px solid #2196f3; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .toc {{ background: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 30px; }}
        .toc ul {{ list-style: none; }}
        .toc li {{ margin: 5px 0; }}
        .toc a {{ color: #3498db; text-decoration: none; }}
        .toc a:hover {{ text-decoration: underline; }}
        @media (max-width: 768px) {{
            .container {{ padding: 10px; }}
            .header {{ padding: 20px; }}
            .header h1 {{ font-size: 2em; }}
            .metric-grid {{ grid-template-columns: 1fr; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ—„ï¸ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ</h1>
            <div class="subtitle">ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„</div>
            <div style="margin-top: 15px; font-size: 1em;">
                <strong>ì¸ìŠ¤í„´ìŠ¤:</strong> {db_instance_identifier} | 
                <strong>ë¦¬ì „:</strong> {region} | 
                <strong>ë¶„ì„ ê¸°ê°„:</strong> {hours}ì‹œê°„ | 
                <strong>ìƒì„±ì¼ì‹œ:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            </div>
        </div>

        <div class="toc">
            <h3>ğŸ“‹ ëª©ì°¨</h3>
            <ul>
                <li><a href="#executive-summary">1. ìš”ì•½ ì •ë³´</a></li>
                <li><a href="#performance-metrics">2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„</a></li>
                <li><a href="#correlation-analysis">3. ìƒê´€ê´€ê³„ ë¶„ì„</a></li>
                <li><a href="#outlier-analysis">4. ì´ìƒ ì§•í›„ ë¶„ì„</a></li>
                <li><a href="#slow-queries">5. ëŠë¦° ì¿¼ë¦¬ ë¶„ì„</a></li>
                <li><a href="#resource-intensive">6. ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬</a></li>
                <li><a href="#recommendations">7. ìµœì í™” ê¶Œì¥ì‚¬í•­</a></li>
            </ul>
        </div>

        <div class="section" id="executive-summary">
            <div class="section-header">ğŸ“Š 1. ìš”ì•½ ì •ë³´ (Executive Summary)</div>
            <div class="section-content">
                <div class="info-box">
                    <strong>ë¶„ì„ ê°œìš”:</strong> {hours}ì‹œê°„ ë™ì•ˆì˜ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¢…í•© ì§„ë‹¨ ê²°ê³¼ì…ë‹ˆë‹¤.
                </div>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{summary}</pre>
            </div>
        </div>

        <div class="section" id="performance-metrics">
            <div class="section-header">ğŸ“ˆ 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„</div>
            <div class="section-content">
                <h4>ğŸ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ</h4>
                <div class="metric-grid">
                    <div class="metric-card success">
                        <div class="metric-title">ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ</div>
                        <div class="metric-value">ì™„ë£Œ</div>
                        <div class="metric-unit">ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì„±ê³µ</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ“… ë¶„ì„ ê¸°ê°„</div>
                        <div class="metric-value">{hours}</div>
                        <div class="metric-unit">ì‹œê°„</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">ğŸ—„ï¸ ì¸ìŠ¤í„´ìŠ¤</div>
                        <div class="metric-value">{db_instance_identifier}</div>
                        <div class="metric-unit">{region}</div>
                    </div>
                </div>
                
                <h4>ğŸ“Š ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´</h4>
                <div class="info-box">
                    ë©”íŠ¸ë¦­ ë°ì´í„°ëŠ” <strong>{csv_filename}</strong> íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
                </div>
            </div>
        </div>

        <div class="section" id="correlation-analysis">
            <div class="section-header">ğŸ”— 3. ìƒê´€ê´€ê³„ ë¶„ì„</div>
            <div class="section-content">
                <h4>ğŸ“ˆ ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{correlation}</pre>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ ìƒê´€ê´€ê³„ ì¸ì‚¬ì´íŠ¸:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>ë†’ì€ ìƒê´€ê´€ê³„(r > 0.7)ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì€ í•¨ê»˜ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤</li>
                        <li>CPU ì‚¬ìš©ë¥ ê³¼ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ìµœì í™”í•˜ì„¸ìš”</li>
                        <li>ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ê³¼ I/O ë©”íŠ¸ë¦­ì˜ ê´€ê³„ë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ì„¸ìš”</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="outlier-analysis">
            <div class="section-header">ğŸš¨ 4. ì´ìƒ ì§•í›„ ë¶„ì„ (Outlier Detection)</div>
            <div class="section-content">
                <h4>âš ï¸ ë°œê²¬ëœ ì•„ì›ƒë¼ì´ì–´</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{outliers}</pre>
                
                <div class="issue">
                    <strong>ğŸ” ì£¼ì˜ì‚¬í•­:</strong> ì•„ì›ƒë¼ì´ì–´ê°€ ë°œê²¬ëœ ì‹œì ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ì™€ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.
                </div>
            </div>
        </div>

        <div class="section" id="slow-queries">
            <div class="section-header">ğŸŒ 5. Slow ì¿¼ë¦¬ ë¶„ì„ (Slow Query Analysis)</div>
            <div class="section-content">
                <h4>ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼</h4>
                <div class="query-box">{slow_queries}</div>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ Slow ì¿¼ë¦¬ ìµœì í™” ê°€ì´ë“œ:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>ì¸ë±ìŠ¤ ì¶”ê°€ ë˜ëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤ ìµœì í™”</li>
                        <li>WHERE ì ˆ ì¡°ê±´ ìˆœì„œ ìµœì í™”</li>
                        <li>JOIN ì¡°ê±´ ë° ìˆœì„œ ê²€í† </li>
                        <li>ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš(EXPLAIN) ë¶„ì„</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="resource-intensive">
            <div class="section-header">ğŸ’¾ 6. ë¦¬ì†ŒìŠ¤ ì†Œëª¨ê°€ í° ì¿¼ë¦¬ ë¶„ì„</div>
            <div class="section-content">
                <h4>ğŸ§  ë©”ëª¨ë¦¬ ì†Œë¹„ê°€ ë§ì€ ì¿¼ë¦¬</h4>
                <div class="query-box">{memory_queries}</div>
                
                <h4>âš¡ CPU ì†Œë¹„ê°€ ë§ì€ ì¿¼ë¦¬</h4>
                <div class="query-box">{cpu_queries}</div>
                
                <h4>ğŸ’¿ ì„ì‹œ ê³µê°„ì„ ë§ì´ ì†Œë¹„í•˜ëŠ” ì¿¼ë¦¬</h4>
                <div class="query-box">{temp_queries}</div>
                
                <div class="recommendation">
                    <strong>ğŸ’¡ ë¦¬ì†ŒìŠ¤ ìµœì í™” ì „ëµ:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li><strong>ë©”ëª¨ë¦¬:</strong> ì •ë ¬ ë²„í¼ í¬ê¸° ì¡°ì •, ì„ì‹œ í…Œì´ë¸” ì‚¬ìš© ìµœì†Œí™”</li>
                        <li><strong>CPU:</strong> ë³µì¡í•œ ì—°ì‚° ìµœì í™”, í•¨ìˆ˜ ì‚¬ìš© ìµœì†Œí™”</li>
                        <li><strong>ì„ì‹œ ê³µê°„:</strong> GROUP BY, ORDER BY ìµœì í™”, ì¸ë±ìŠ¤ í™œìš©</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="recommendations">
            <div class="section-header">ğŸ¯ 7. ìµœì í™” ê¶Œì¥ì‚¬í•­</div>
            <div class="section-content">
                <div class="info-box" style="margin-bottom: 20px;">
                    <strong>ğŸ¤– AI ê¸°ë°˜ ë¶„ì„:</strong> ì´ ê¶Œì¥ì‚¬í•­ë“¤ì€ Claude AIê°€ ì‹¤ì œ ì„±ëŠ¥ ë©”íŠ¸ë¦­, ìƒê´€ê´€ê³„ ë¶„ì„, ëŠë¦° ì¿¼ë¦¬ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìƒì„±í•œ ë§ì¶¤í˜• ì œì•ˆì…ë‹ˆë‹¤.
                </div>
                {self._generate_recommendations_html(claude_recommendations)}
            </div>
        </div>

        <div class="section">
            <div class="section-header">ğŸ“ ì²¨ë¶€ íŒŒì¼ ë° ì°¸ê³  ìë£Œ</div>
            <div class="section-content">
                <h4>ğŸ“Š ìƒì„±ëœ íŒŒì¼ë“¤</h4>
                <ul style="margin-left: 20px;">
                    <li><strong>ë©”íŠ¸ë¦­ ë°ì´í„°:</strong> {csv_filename}</li>
                    <li><strong>ìƒê´€ê´€ê³„ ë¶„ì„:</strong> í¬í•¨ë¨</li>
                    <li><strong>ì¢…í•© ë³´ê³ ì„œ:</strong> {report_path.name}</li>
                </ul>
                
                <div class="info-box">
                    <strong>ğŸ“ ë¬¸ì˜ ë° ì§€ì›</strong><br>
                    ìƒì„± ë„êµ¬: DB Assistant MCP Server<br>
                    ë¶„ì„ ì—”ì§„: Claude Sonnet 4 + AWS CloudWatch<br>
                    ë³´ê³ ì„œ ë²„ì „: v2.0 
                </div>
                
                <div style="text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em;">
                    <em>ì´ ë³´ê³ ì„œëŠ” {hours}ì‹œê°„ ë™ì•ˆì˜ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
                    ì •í™•í•œ ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 1ì£¼ì¼ ì´ìƒì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</em>
                </div>
            </div>
        </div>
    </div>
</body>
</html>"""

            # HTML íŒŒì¼ ì €ì¥
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"HTML ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_path}")

            return f"""ğŸ—„ï¸ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë³´ê³ ì„œ**
â€¢ ì¸ìŠ¤í„´ìŠ¤: {db_instance_identifier}
â€¢ ë¦¬ì „: {region}
â€¢ ë¶„ì„ ê¸°ê°„: {hours}ì‹œê°„
â€¢ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ **ìƒì„±ëœ íŒŒì¼ë“¤:**
â€¢ ì¢…í•© ë³´ê³ ì„œ: {report_path.name}
â€¢ ë©”íŠ¸ë¦­ ë°ì´í„°: {csv_filename}
â€¢ ìƒê´€ê´€ê³„ ë¶„ì„: í¬í•¨ë¨

ğŸ“ˆ **í¬í•¨ëœ ë¶„ì„:**
âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½ ë° í†µê³„
âœ… ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
âœ… ì´ìƒ ì§•í›„(ì•„ì›ƒë¼ì´ì–´) íƒì§€
âœ… ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘ ë° ë¶„ì„
âœ… ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬ ë¶„ì„ (CPU, ë©”ëª¨ë¦¬, ì„ì‹œê³µê°„)
âœ… ìµœì í™” ê¶Œì¥ì‚¬í•­ ë° ì•¡ì…˜ ì•„ì´í…œ
âœ… ë°˜ì‘í˜• HTML ë””ìì¸

ğŸ’¡ **ì£¼ìš” íŠ¹ì§•:**
â€¢ ëª¨ë°”ì¼ ìµœì í™”ëœ ë°˜ì‘í˜• ë””ìì¸
â€¢ ìƒì„¸í•œ ë©”íŠ¸ë¦­ ë¶„ì„ ë° ì‹œê°í™”
â€¢ ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì í™” ê¶Œì¥ì‚¬í•­
â€¢ ìš°ì„ ìˆœìœ„ë³„ ì•¡ì…˜ ì•„ì´í…œ ì œê³µ

ğŸ” ë³´ê³ ì„œë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.
ğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"""

        except Exception as e:
            debug_log(f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            logger.error(f"ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return (
                f"âŒ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}\nğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"
            )

    async def generate_cluster_performance_report(
        self,
        database_secret: str,
        db_cluster_identifier: str,
        hours: int = 24,
        region: str = "ap-northeast-2",
    ) -> str:
        """
        Aurora í´ëŸ¬ìŠ¤í„° ì „ìš© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        - í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ (ë¶€í•˜ ë¶„ì‚°, ë ˆí”Œë¦¬ì¼€ì´ì…˜ ì§€ì—°, HLL ë“±)
        - ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ ë§í¬ ì œê³µ
        - Writer/Reader ì—­í• ë³„ ë¹„êµ ë¶„ì„
        """
        # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_cluster_{db_cluster_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        def debug_log(message):
            with open(debug_log_path, "a", encoding="utf-8") as f:
                f.write(f"{datetime.now().strftime('%H:%M:%S')} - {message}\n")
                f.flush()

        try:
            debug_log(f"í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {db_cluster_identifier}")

            # 1. database_secretì—ì„œ ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì°¾ê¸°
            debug_log("RDS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
            rds_client = boto3.client("rds", region_name=region)

            # Secretì—ì„œ í˜¸ìŠ¤íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            debug_log("Secret ì •ë³´ ì¡°íšŒ")
            secrets_client = boto3.client(
                "secretsmanager", region_name=region, verify=False
            )
            get_secret_value_response = secrets_client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            secret_info = json.loads(secret)
            host = secret_info.get("host", "")
            debug_log(f"í˜¸ìŠ¤íŠ¸ ì •ë³´: {host}")

            # í˜¸ìŠ¤íŠ¸ ì •ë³´ë¡œ ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
            actual_cluster_id = None
            if host:
                # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì¡°íšŒí•´ì„œ ì—”ë“œí¬ì¸íŠ¸ ë§¤ì¹­
                debug_log("í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ ë° ë§¤ì¹­")
                clusters = rds_client.describe_db_clusters()["DBClusters"]
                for cluster in clusters:
                    if cluster.get("Endpoint", "") in host or host in cluster.get(
                        "Endpoint", ""
                    ):
                        actual_cluster_id = cluster["DBClusterIdentifier"]
                        break

            # ì‹¤ì œ í´ëŸ¬ìŠ¤í„° IDê°€ ì—†ìœ¼ë©´ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ ê°’ ì‚¬ìš©
            if not actual_cluster_id:
                actual_cluster_id = db_cluster_identifier

            debug_log(f"ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ID: {actual_cluster_id}")

            cluster_info = rds_client.describe_db_clusters(
                DBClusterIdentifier=actual_cluster_id
            )["DBClusters"][0]

            cluster_members = cluster_info["DBClusterMembers"]
            writer_instance = next(
                (m for m in cluster_members if m["IsClusterWriter"]), None
            )
            reader_instances = [m for m in cluster_members if not m["IsClusterWriter"]]

            debug_log(f"í´ëŸ¬ìŠ¤í„° êµ¬ì„±: Writer 1ê°œ, Reader {len(reader_instances)}ê°œ")

            # 2. ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
            instance_reports = {}
            cluster_metrics = {}

            # Writer ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬
            if writer_instance:
                writer_id = writer_instance["DBInstanceIdentifier"]
                debug_log(f"Writer ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬: {writer_id}")

                # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
                writer_report = await self.generate_comprehensive_performance_report(
                    database_secret, writer_id, region, hours
                )
                instance_reports[writer_id] = {
                    "role": "Writer",
                    "report": writer_report,
                    "is_writer": True,
                }
                debug_log(
                    f"Writer ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ë³´ê³ ì„œ ìƒì„±: {writer_id},{writer_report}"
                )

                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics_result = await self.collect_db_metrics(writer_id, region, hours)
                # ì‹¤ì œ ìƒì„±ëœ íŒŒì¼ëª… ì¶”ì¶œ
                if "ì €ì¥ ìœ„ì¹˜:" in metrics_result:
                    csv_path = metrics_result.split("ì €ì¥ ìœ„ì¹˜: ")[-1].strip()
                    cluster_metrics[writer_id] = csv_path.split("/")[
                        -1
                    ]  # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                else:
                    cluster_metrics[writer_id] = (
                        f"database_metrics_{writer_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            else:
                debug_log("Writer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # Reader ì¸ìŠ¤í„´ìŠ¤ë“¤ ì²˜ë¦¬
            for reader in reader_instances:
                reader_id = reader["DBInstanceIdentifier"]
                debug_log(f"Reader ì¸ìŠ¤í„´ìŠ¤ ì²˜ë¦¬: {reader_id}")

                # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
                reader_report = await self.generate_comprehensive_performance_report(
                    database_secret, reader_id, region, hours
                )
                instance_reports[reader_id] = {
                    "role": "Reader",
                    "report": reader_report,
                    "is_writer": False,
                }

                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics_result = await self.collect_db_metrics(reader_id, region, hours)
                # ì‹¤ì œ ìƒì„±ëœ íŒŒì¼ëª… ì¶”ì¶œ
                if "ì €ì¥ ìœ„ì¹˜:" in metrics_result:
                    csv_path = metrics_result.split("ì €ì¥ ìœ„ì¹˜: ")[-1].strip()
                    cluster_metrics[reader_id] = csv_path.split("/")[
                        -1
                    ]  # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                else:
                    cluster_metrics[reader_id] = (
                        f"database_metrics_{reader_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            debug_log("í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ ì‹œì‘")
            # 3. í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„

            cluster_analysis = await self._analyze_cluster_metrics(
                cluster_metrics, cluster_info
            )

            # 4. í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ ìƒì„±
            debug_log("í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = (
                f"cluster_performance_report_{actual_cluster_id}_{timestamp}.html"
            )
            report_path = Path.cwd() / "output" / report_filename

            html_content = await self._generate_cluster_html_report(
                cluster_info, instance_reports, cluster_analysis, timestamp
            )

            # ë³´ê³ ì„œ ì €ì¥
            report_path.parent.mkdir(exist_ok=True)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")

            return f"""âœ… Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ

ğŸ—ï¸ í´ëŸ¬ìŠ¤í„° ì •ë³´:
â€¢ í´ëŸ¬ìŠ¤í„° ID: {actual_cluster_id}
â€¢ ì—”ì§„: {cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}
â€¢ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜: {len(cluster_members)}ê°œ (Writer: 1ê°œ, Reader: {len(reader_instances)}ê°œ)
â€¢ ë¶„ì„ ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„

ğŸ“Š ìƒì„±ëœ ë³´ê³ ì„œ:
â€¢ ğŸ¯ í´ëŸ¬ìŠ¤í„° í†µí•© ë³´ê³ ì„œ: {report_path}
â€¢ ğŸ“‹ ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ë³´ê³ ì„œ: {len(instance_reports)}ê°œ

ğŸ” ì£¼ìš” ë¶„ì„ ë‚´ìš©:
â€¢ í´ëŸ¬ìŠ¤í„° ë¶€í•˜ ë¶„ì‚° ìƒíƒœ
â€¢ Writer/Reader ì„±ëŠ¥ ë¹„êµ
â€¢ ë ˆí”Œë¦¬ì¼€ì´ì…˜ ì§€ì—° ë¶„ì„
â€¢ ì¸ìŠ¤í„´ìŠ¤ ê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ
â€¢ í´ëŸ¬ìŠ¤í„° ìµœì í™” ê¶Œì¥ì‚¬í•­

ğŸ“„ í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ ,
   ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë¶„ì„ì€ ë§í¬ë¥¼ í†µí•´ ì ‘ê·¼í•˜ì„¸ìš”.
ğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"""

        except Exception as e:
            debug_log(f"í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"âŒ í´ëŸ¬ìŠ¤í„° ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}\nğŸ“„ ë””ë²„ê·¸ ë¡œê·¸: {debug_log_path}"

    async def _analyze_cluster_metrics(
        self, cluster_metrics: Dict, cluster_info: Dict
    ) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë©”íŠ¸ë¦­ ë¶„ì„"""
        try:
            if not ANALYSIS_AVAILABLE:
                logger.warning("ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                return {"error": "ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            analysis = {
                "load_distribution": {},
                "replication_lag": {},
                "resource_comparison": {},
                "recommendations": [],
            }

            logger.info(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë¶„ì„ ì‹œì‘: {cluster_metrics}")

            # ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ë©”íŠ¸ë¦­ ë¡œë“œ ë° ë¹„êµ - data í´ë”ì—ì„œ ì§ì ‘ ì°¾ê¸°
            metrics_data = {}
            
            # í´ëŸ¬ìŠ¤í„° ë©¤ë²„ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ID ê°€ì ¸ì˜¤ê¸°
            for member in cluster_info["DBClusterMembers"]:
                instance_id = member["DBInstanceIdentifier"]
                
                # data í´ë”ì—ì„œ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì˜ ìµœì‹  CSV íŒŒì¼ ì°¾ê¸°
                data_dir = Path("data")
                csv_files = list(data_dir.glob(f"database_metrics_{instance_id}_*.csv"))
                
                if csv_files:
                    # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ (íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
                    latest_csv = max(csv_files, key=lambda x: x.name.split('_')[-1])
                    logger.info(f"ë©”íŠ¸ë¦­ íŒŒì¼ ë°œê²¬: {instance_id} -> {latest_csv}")
                    
                    try:
                        df = pd.read_csv(latest_csv)
                        metrics_data[instance_id] = df
                        logger.info(f"ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {instance_id} ({len(df)} í–‰)")
                    except Exception as e:
                        logger.warning(f"ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {latest_csv}: {e}")
                else:
                    logger.warning(f"ë©”íŠ¸ë¦­ íŒŒì¼ ì—†ìŒ: {instance_id}")

            logger.info(f"ë¡œë“œëœ ë©”íŠ¸ë¦­ ë°ì´í„°: {len(metrics_data)}ê°œ ì¸ìŠ¤í„´ìŠ¤")

            if len(metrics_data) >= 2:
                # Writer vs Reader ë¹„êµ
                writer_data = None
                reader_data = []

                for instance_id, df in metrics_data.items():
                    # í´ëŸ¬ìŠ¤í„° ë©¤ë²„ ì •ë³´ì—ì„œ ì—­í•  í™•ì¸
                    is_writer = any(
                        m["DBInstanceIdentifier"] == instance_id
                        and m["IsClusterWriter"]
                        for m in cluster_info["DBClusterMembers"]
                    )

                    logger.info(f"ì¸ìŠ¤í„´ìŠ¤ ì—­í•  í™•ì¸: {instance_id} -> {'Writer' if is_writer else 'Reader'}")

                    if is_writer:
                        writer_data = df
                    else:
                        reader_data.append((instance_id, df))

                # ë¶€í•˜ ë¶„ì‚° ë¶„ì„
                if writer_data is not None and reader_data:
                    logger.info("ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì‹œì‘")
                    try:
                        analysis["load_distribution"] = self._analyze_load_distribution(
                            writer_data, reader_data
                        )
                        logger.info("ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì˜¤ë¥˜: {e}")
                        analysis["load_distribution"] = {}
                else:
                    logger.warning(f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ë¶ˆê°€: writer_data={writer_data is not None}, reader_data={len(reader_data)}")

                # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ
                try:
                    logger.info("ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì‹œì‘")
                    analysis["resource_comparison"] = self._compare_resource_usage(
                        metrics_data
                    )
                    logger.info("ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì˜¤ë¥˜: {e}")
                    analysis["resource_comparison"] = {}
            else:
                logger.warning(f"ë¶„ì„ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ë°ì´í„° ë¶€ì¡±: {len(metrics_data)}ê°œ (ìµœì†Œ 2ê°œ í•„ìš”)")

            return analysis

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def _analyze_load_distribution(self, writer_df, reader_data):
        """ë¶€í•˜ ë¶„ì‚° ìƒíƒœ ë¶„ì„"""
        try:
            writer_cpu = (
                writer_df["CPUUtilization"].mean()
                if "CPUUtilization" in writer_df.columns
                else 0
            )
            writer_connections = (
                writer_df["DatabaseConnections"].mean()
                if "DatabaseConnections" in writer_df.columns
                else 0
            )

            reader_stats = []
            for reader_id, reader_df in reader_data:
                reader_cpu = (
                    reader_df["CPUUtilization"].mean()
                    if "CPUUtilization" in reader_df.columns
                    else 0
                )
                reader_connections = (
                    reader_df["DatabaseConnections"].mean()
                    if "DatabaseConnections" in reader_df.columns
                    else 0
                )
                reader_stats.append(
                    {
                        "instance_id": reader_id,
                        "cpu": reader_cpu,
                        "connections": reader_connections,
                    }
                )

            return {
                "writer": {"cpu": writer_cpu, "connections": writer_connections},
                "readers": reader_stats,
                "balance_score": self._calculate_balance_score(
                    writer_cpu, [r["cpu"] for r in reader_stats]
                ),
            }
        except Exception as e:
            logger.error(f"ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}

    def _calculate_balance_score(self, writer_cpu, reader_cpus):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ ê³„ì‚° (0-100)"""
        if not reader_cpus:
            return 0

        total_cpu = writer_cpu + sum(reader_cpus)
        if total_cpu == 0:
            return 100

        # ì´ìƒì ì¸ ë¶„ì‚°: Writerê°€ ì „ì²´ ë¶€í•˜ì˜ 60-70% ë‹´ë‹¹
        writer_ratio = writer_cpu / total_cpu
        ideal_ratio = 0.65

        deviation = abs(writer_ratio - ideal_ratio)
        score = max(0, 100 - (deviation * 200))  # í¸ì°¨ê°€ í´ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ

        return round(score, 1)

    def _compare_resource_usage(self, metrics_data):
        """ì¸ìŠ¤í„´ìŠ¤ ê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ"""
        try:
            comparison = {}

            for instance_id, df in metrics_data.items():
                comparison[instance_id] = {
                    "cpu_avg": (
                        df["CPUUtilization"].mean()
                        if "CPUUtilization" in df.columns
                        else 0
                    ),
                    "memory_usage": (
                        (1 - df["FreeableMemory"].mean() / df["FreeableMemory"].max())
                        * 100
                        if "FreeableMemory" in df.columns
                        else 0
                    ),
                    "connections": (
                        df["DatabaseConnections"].mean()
                        if "DatabaseConnections" in df.columns
                        else 0
                    ),
                    "read_iops": (
                        df["ReadIOPS"].mean() if "ReadIOPS" in df.columns else 0
                    ),
                    "write_iops": (
                        df["WriteIOPS"].mean() if "WriteIOPS" in df.columns else 0
                    ),
                }

            return comparison
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ ì˜¤ë¥˜: {e}")
            return {}

    async def _generate_cluster_html_report(
        self, cluster_info, instance_reports, cluster_analysis, timestamp
    ):
        """í´ëŸ¬ìŠ¤í„° í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""

        cluster_id = cluster_info["DBClusterIdentifier"]
        engine_info = f"{cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}"

        # ì¸ìŠ¤í„´ìŠ¤ ë§í¬ ìƒì„±
        instance_links = []
        session_logger = logging.getLogger(
            "session_" + datetime.now().strftime("%Y%m%d_%H%M%S")
        )
        session_logger.info(f"ì¸ìŠ¤í„´ìŠ¤ ë³´ê³ ì„œ ê°œìˆ˜: {len(instance_reports)}")
        for instance_id, report_info in instance_reports.items():
            session_logger.info(
                f"ì²˜ë¦¬ ì¤‘ì¸ ì¸ìŠ¤í„´ìŠ¤: {instance_id}, ì—­í• : {report_info.get('role', 'Unknown')}"
            )
            role = report_info["role"]
            # ìƒì„¸ ë³´ê³ ì„œ íŒŒì¼ëª… ì¶”ì¶œ
            report_text = report_info["report"]
            session_logger.info(f"ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ì¼ë¶€: {report_text[:200]}...")
            if "comprehensive_performance_report_" in report_text:
                import re

                match = re.search(
                    r"comprehensive_performance_report_[^.]+\.html", report_text
                )
                if match:
                    report_filename = match.group(0)
                    session_logger.info(f"ë§¤ì¹­ëœ íŒŒì¼ëª…: {report_filename}")
                    instance_links.append(
                        f"""
                    <tr>
                        <td><span class="role-badge {'writer' if report_info['is_writer'] else 'reader'}">{role}</span></td>
                        <td>{instance_id}</td>
                        <td><a href="{report_filename}" target="_blank" class="detail-link">ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ë³´ê¸°</a></td>
                    </tr>
                    """
                    )
                else:
                    session_logger.warning(f"íŒŒì¼ëª… ë§¤ì¹­ ì‹¤íŒ¨: {instance_id}")
            else:
                session_logger.warning(
                    f"comprehensive_performance_report_ ë¬¸ìì—´ ì—†ìŒ: {instance_id}"
                )

        session_logger.info(f"ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ ë§í¬ ê°œìˆ˜: {len(instance_links)}")

        # ë¶€í•˜ ë¶„ì‚° ë¶„ì„ ê²°ê³¼
        load_analysis = cluster_analysis.get("load_distribution", {})
        balance_score = load_analysis.get("balance_score", 0)

        # ë¦¬ì†ŒìŠ¤ ë¹„êµ ì°¨íŠ¸ ë°ì´í„°
        resource_comparison = cluster_analysis.get("resource_comparison", {})

        html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ - {cluster_id}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f7fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.2em; }}
        .header .subtitle {{ margin-top: 10px; opacity: 0.9; font-size: 1.1em; }}
        
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; padding: 30px; }}
        .summary-card {{ background: #f8f9fa; border-radius: 8px; padding: 20px; border-left: 4px solid #007bff; }}
        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .summary-card .value {{ font-size: 1.8em; font-weight: bold; color: #007bff; }}
        
        .section {{ margin: 20px 30px; }}
        .section-header {{ background: #e9ecef; padding: 15px; border-radius: 8px; font-weight: bold; font-size: 1.2em; color: #495057; }}
        .section-content {{ padding: 20px 0; }}
        
        .instance-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .instance-table th, .instance-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }}
        .instance-table th {{ background: #f8f9fa; font-weight: bold; }}
        
        .role-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .role-badge.writer {{ background: #d4edda; color: #155724; }}
        .role-badge.reader {{ background: #d1ecf1; color: #0c5460; }}
        
        .detail-link {{ color: #007bff; text-decoration: none; font-weight: bold; }}
        .detail-link:hover {{ text-decoration: underline; }}
        
        .balance-score {{ text-align: center; margin: 20px 0; }}
        .balance-meter {{ width: 200px; height: 200px; margin: 0 auto; position: relative; }}
        .balance-value {{ font-size: 2em; font-weight: bold; color: {self._get_balance_color(balance_score)}; }}
        
        .recommendation {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 15px; margin: 15px 0; }}
        .recommendation h4 {{ margin: 0 0 10px 0; color: #856404; }}
        
        .resource-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }}
        .resource-card {{ background: #f8f9fa; border-radius: 8px; padding: 15px; }}
        .resource-card h4 {{ margin: 0 0 15px 0; color: #495057; }}
        .metric-bar {{ background: #e9ecef; height: 20px; border-radius: 10px; margin: 5px 0; position: relative; }}
        .metric-fill {{ height: 100%; border-radius: 10px; }}
        .metric-label {{ font-size: 0.9em; color: #6c757d; }}
        
        @media (max-width: 768px) {{
            .summary-grid {{ grid-template-columns: 1fr; }}
            .resource-grid {{ grid-template-columns: 1fr; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ—ï¸ Aurora í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ë³´ê³ ì„œ</h1>
            <div class="subtitle">í´ëŸ¬ìŠ¤í„°: {cluster_id} | ì—”ì§„: {engine_info} | ìƒì„±ì¼ì‹œ: {timestamp}</div>
        </div>
        
        <div class="summary-grid">
            <div class="summary-card">
                <h3>ğŸ“Š í´ëŸ¬ìŠ¤í„° êµ¬ì„±</h3>
                <div class="value">{len(cluster_info['DBClusterMembers'])}ê°œ ì¸ìŠ¤í„´ìŠ¤</div>
                <div>Writer: 1ê°œ, Reader: {len([m for m in cluster_info['DBClusterMembers'] if not m['IsClusterWriter']])}ê°œ</div>
            </div>
            <div class="summary-card">
                <h3>âš–ï¸ ë¶€í•˜ ë¶„ì‚° ì ìˆ˜</h3>
                <div class="value" style="color: {self._get_balance_color(balance_score)}">{balance_score}/100</div>
                <div>{self._get_balance_status(balance_score)}</div>
            </div>
            <div class="summary-card">
                <h3>ğŸ”„ í´ëŸ¬ìŠ¤í„° ìƒíƒœ</h3>
                <div class="value" style="color: #28a745">{cluster_info.get('Status', 'N/A').upper()}</div>
                <div>Multi-AZ: {'Yes' if cluster_info.get('MultiAZ') else 'No'}</div>
            </div>
            <div class="summary-card">
                <h3>ğŸ” ë³´ì•ˆ ì„¤ì •</h3>
                <div class="value">{'ğŸ”’' if cluster_info.get('StorageEncrypted') else 'ğŸ”“'}</div>
                <div>ì•”í˜¸í™”: {'í™œì„±í™”' if cluster_info.get('StorageEncrypted') else 'ë¹„í™œì„±í™”'}</div>
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">ğŸ“‹ ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ</div>
            <div class="section-content">
                <table class="instance-table">
                    <thead>
                        <tr>
                            <th>ì—­í• </th>
                            <th>ì¸ìŠ¤í„´ìŠ¤ ID</th>
                            <th>ìƒì„¸ ë¶„ì„</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(instance_links)}
                    </tbody>
                </table>
                <div class="recommendation">
                    <h4>ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ</h4>
                    <p>ê° ì¸ìŠ¤í„´ìŠ¤ì˜ "ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ë³´ê¸°" ë§í¬ë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì˜ ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                </div>
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">âš–ï¸ ë¶€í•˜ ë¶„ì‚° ë¶„ì„</div>
            <div class="section-content">
                {self._generate_load_distribution_html(load_analysis)}
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">ğŸ“Š ì¸ìŠ¤í„´ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¹„êµ</div>
            <div class="section-content">
                <div class="resource-grid">
                    {self._generate_resource_comparison_html(resource_comparison)}
                </div>
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">ğŸ’¡ í´ëŸ¬ìŠ¤í„° ìµœì í™” ê¶Œì¥ì‚¬í•­</div>
            <div class="section-content">
                {self._generate_cluster_recommendations(cluster_analysis, cluster_info)}
            </div>
        </div>
    </div>
</body>
</html>
        """

        return html_content

    def _get_balance_color(self, score):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ ë°˜í™˜"""
        if score >= 80:
            return "#28a745"  # ë…¹ìƒ‰
        elif score >= 60:
            return "#ffc107"  # ë…¸ë€ìƒ‰
        else:
            return "#dc3545"  # ë¹¨ê°„ìƒ‰

    def _get_balance_status(self, score):
        """ë¶€í•˜ ë¶„ì‚° ì ìˆ˜ì— ë”°ë¥¸ ìƒíƒœ ë©”ì‹œì§€"""
        if score >= 80:
            return "ìš°ìˆ˜í•œ ë¶„ì‚°"
        elif score >= 60:
            return "ë³´í†µ ë¶„ì‚°"
        else:
            return "ê°œì„  í•„ìš”"

    def _generate_load_distribution_html(self, load_analysis):
        """ë¶€í•˜ ë¶„ì‚° ë¶„ì„ HTML ìƒì„±"""
        if not load_analysis:
            return """
            <div class="recommendation">
                <h4 style="color: #dc3545;">âš ï¸ ë¶€í•˜ ë¶„ì‚° ë°ì´í„° ë¶„ì„ ë¶ˆê°€</h4>
                <p><strong>ì›ì¸:</strong> í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>
                <p><strong>í™•ì¸ì‚¬í•­:</strong></p>
                <ul>
                    <li>ê° ì¸ìŠ¤í„´ìŠ¤ì˜ CloudWatch ë©”íŠ¸ë¦­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê³  ìˆëŠ”ì§€ í™•ì¸</li>
                    <li>Writerì™€ Reader ì¸ìŠ¤í„´ìŠ¤ê°€ ëª¨ë‘ í™œì„± ìƒíƒœì¸ì§€ í™•ì¸</li>
                    <li>ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ê°„ ë™ì•ˆ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸</li>
                </ul>
                <p><strong>ê¶Œì¥ì¡°ì¹˜:</strong> ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ì—¬ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ì„±ëŠ¥ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”.</p>
            </div>
            """

        writer = load_analysis.get("writer", {})
        readers = load_analysis.get("readers", [])

        html = f"""
        <div style="margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 5px;">
            <h4>Writer ì¸ìŠ¤í„´ìŠ¤</h4>
            <p>CPU: {writer.get('cpu', 0):.1f}% | ì—°ê²° ìˆ˜: {writer.get('connections', 0):.1f}</p>
        </div>
        """

        if readers:
            html += "<h4>Reader ì¸ìŠ¤í„´ìŠ¤ë“¤</h4>"
            for reader in readers:
                html += f"""
                <div style="margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 5px;">
                    <strong>{reader['instance_id']}</strong><br>
                    CPU: {reader['cpu']:.1f}% | ì—°ê²° ìˆ˜: {reader['connections']:.1f}
                </div>
                """

        return html

    def _generate_resource_comparison_html(self, resource_comparison):
        """ë¦¬ì†ŒìŠ¤ ë¹„êµ HTML ìƒì„±"""
        if not resource_comparison:
            return """
            <div class="recommendation">
                <h4 style="color: #dc3545;">âš ï¸ ë¦¬ì†ŒìŠ¤ ë¹„êµ ë°ì´í„° ì—†ìŒ</h4>
                <p><strong>ì›ì¸:</strong> ì¸ìŠ¤í„´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ë¶„ì„ ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>
                <p><strong>í™•ì¸ì‚¬í•­:</strong></p>
                <ul>
                    <li>ê° ì¸ìŠ¤í„´ìŠ¤ì˜ CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìƒíƒœ í™•ì¸</li>
                    <li>ë©”íŠ¸ë¦­ ë°ì´í„° íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸</li>
                    <li>ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬(pandas, numpy)ê°€ ì •ìƒ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸</li>
                </ul>
                <p><strong>ê¶Œì¥ì¡°ì¹˜:</strong> ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ë³´ê³ ì„œì—ì„œ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ìƒì„¸ ë©”íŠ¸ë¦­ì„ í™•ì¸í•˜ì„¸ìš”.</p>
            </div>
            """

        html = ""
        for instance_id, metrics in resource_comparison.items():
            html += f"""
            <div class="resource-card">
                <h4>{instance_id}</h4>
                <div class="metric-label">CPU ì‚¬ìš©ë¥ : {metrics['cpu_avg']:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(metrics['cpu_avg'], 100)}%; background: #007bff;"></div>
                </div>
                
                <div class="metric-label">ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {metrics['memory_usage']:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(metrics['memory_usage'], 100)}%; background: #28a745;"></div>
                </div>
                
                <div class="metric-label">í‰ê·  ì—°ê²° ìˆ˜: {metrics['connections']:.1f}</div>
                <div class="metric-label">Read IOPS: {metrics['read_iops']:.1f}</div>
                <div class="metric-label">Write IOPS: {metrics['write_iops']:.1f}</div>
            </div>
            """

        return html

    def _generate_cluster_recommendations(self, cluster_analysis, cluster_info):
        """í´ëŸ¬ìŠ¤í„° ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ë¶€í•˜ ë¶„ì‚° ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        load_dist = cluster_analysis.get("load_distribution", {})
        balance_score = load_dist.get("balance_score", 0)

        if balance_score < 60:
            recommendations.append(
                {
                    "priority": "ë†’ìŒ",
                    "title": "ë¶€í•˜ ë¶„ì‚° ê°œì„  í•„ìš”",
                    "description": "Writerì™€ Reader ê°„ ë¶€í•˜ ë¶„ì‚°ì´ ë¶ˆê· í˜•í•©ë‹ˆë‹¤. ì½ê¸° ì¿¼ë¦¬ë¥¼ Reader ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì‚°í•˜ì„¸ìš”.",
                    "action": "ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì½ê¸° ì „ìš© ì¿¼ë¦¬ë¥¼ Reader ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¼ìš°íŒ…",
                }
            )

        # ì•”í˜¸í™” ê¶Œì¥ì‚¬í•­
        if not cluster_info.get("StorageEncrypted"):
            recommendations.append(
                {
                    "priority": "ì¤‘ê°„",
                    "title": "ìŠ¤í† ë¦¬ì§€ ì•”í˜¸í™” í™œì„±í™”",
                    "description": "ë°ì´í„° ë³´ì•ˆì„ ìœ„í•´ ìŠ¤í† ë¦¬ì§€ ì•”í˜¸í™”ë¥¼ í™œì„±í™”í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    "action": "ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ì•”í˜¸í™” ì˜µì…˜ í™œì„±í™”",
                }
            )

        # ë°±ì—… ì„¤ì • ê¶Œì¥ì‚¬í•­
        backup_retention = cluster_info.get("BackupRetentionPeriod", 0)
        if backup_retention < 7:
            recommendations.append(
                {
                    "priority": "ì¤‘ê°„",
                    "title": "ë°±ì—… ë³´ì¡´ ê¸°ê°„ ì—°ì¥",
                    "description": f"í˜„ì¬ ë°±ì—… ë³´ì¡´ ê¸°ê°„ì´ {backup_retention}ì¼ì…ë‹ˆë‹¤. ìµœì†Œ 7ì¼ ì´ìƒ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    "action": "ë°±ì—… ë³´ì¡´ ê¸°ê°„ì„ 7-35ì¼ë¡œ ì„¤ì •",
                }
            )

        # HTML ìƒì„±
        if not recommendations:
            return "<div class='recommendation'><h4>âœ… ìš°ìˆ˜í•œ í´ëŸ¬ìŠ¤í„° ì„¤ì •</h4><p>í˜„ì¬ í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì˜ ë”°ë¥´ê³  ìˆìŠµë‹ˆë‹¤.</p></div>"

        html = ""
        for rec in recommendations:
            priority_color = {
                "ë†’ìŒ": "#dc3545",
                "ì¤‘ê°„": "#ffc107",
                "ë‚®ìŒ": "#28a745",
            }.get(rec["priority"], "#6c757d")
            html += f"""
            <div class="recommendation">
                <h4 style="color: {priority_color};">ğŸ¯ {rec['title']} (ìš°ì„ ìˆœìœ„: {rec['priority']})</h4>
                <p><strong>ì„¤ëª…:</strong> {rec['description']}</p>
                <p><strong>ê¶Œì¥ ì¡°ì¹˜:</strong> {rec['action']}</p>
            </div>
            """

        return html

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"

            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # íŒŒì¼ ë³µì‚¬
            import shutil

            shutil.copy2(source, target_path)

            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    # === ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œ ===

    def setup_cloudwatch_client(self, region_name: str = "us-east-1"):
        """CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            self.cloudwatch = boto3.client("cloudwatch", region_name=region_name)
            return True
        except Exception as e:
            logger.error(f"CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def collect_db_metrics(
        self,
        db_instance_identifier: str,
        hours: int = 24,
        metrics: Optional[List[str]] = None,
        region: str = "us-east-1",
    ) -> str:
        """CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandas numpy matplotlib scikit-learnì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."

        try:
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            # í´ëŸ¬ìŠ¤í„° í™•ì¸ ë° ì¸ìŠ¤í„´ìŠ¤ identifier ë³€í™˜
            try:
                rds_client = boto3.client("rds", region_name=region)
                cluster_response = rds_client.describe_db_clusters(
                    DBClusterIdentifier=db_instance_identifier
                )
                if cluster_response["DBClusters"]:
                    cluster = cluster_response["DBClusters"][0]
                    if cluster["DBClusterMembers"]:
                        # ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (ë³´í†µ writer ì¸ìŠ¤í„´ìŠ¤)
                        original_id = db_instance_identifier
                        db_instance_identifier = cluster["DBClusterMembers"][0][
                            "DBInstanceIdentifier"
                        ]
                        logger.info(
                            f"í´ëŸ¬ìŠ¤í„° {original_id}ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ {db_instance_identifier}ë¡œ ë³€í™˜"
                        )
            except rds_client.exceptions.DBClusterNotFoundFault:
                logger.debug(f"{db_instance_identifier}ëŠ” ì¸ìŠ¤í„´ìŠ¤ IDì…ë‹ˆë‹¤")
            except Exception as e:
                logger.debug(f"í´ëŸ¬ìŠ¤í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")

            if not metrics:
                metrics = self.default_metrics

            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            data = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
            for metric in metrics:
                try:
                    response = self.cloudwatch.get_metric_statistics(
                        Namespace="AWS/RDS",
                        MetricName=metric,
                        Dimensions=[
                            {
                                "Name": "DBInstanceIdentifier",
                                "Value": db_instance_identifier,
                            },
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5ë¶„ ê°„ê²©
                        Statistics=["Average"],
                    )

                    # ì‘ë‹µì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    for point in response["Datapoints"]:
                        data.append(
                            {
                                "Timestamp": point["Timestamp"].replace(tzinfo=None),
                                "Metric": metric,
                                "Value": point["Average"],
                            }
                        )
                except Exception as e:
                    logger.error(f"ë©”íŠ¸ë¦­ {metric} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

            if not data:
                return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(data)
            df = df.sort_values("Timestamp")

            # í”¼ë²— í…Œì´ë¸” ìƒì„±
            pivot_df = df.pivot(index="Timestamp", columns="Metric", values="Value")

            # CSV íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = (
                DATA_DIR / f"database_metrics_{db_instance_identifier}_{timestamp}.csv"
            )
            pivot_df.to_csv(csv_file)

            return f"âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ\nğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­: {len(metrics)}ê°œ\nğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ê°œ\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {csv_file}"

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def analyze_metric_correlation(
        self, csv_file: str, target_metric: str = "CPUUtilization", top_n: int = 10
    ) -> str:
        """ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"íƒ€ê²Ÿ ë©”íŠ¸ë¦­ '{target_metric}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ìƒê´€ ë¶„ì„
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(
                target_metric, errors="ignore"
            )
            top_correlations = target_correlations.nlargest(top_n)

            # ê²°ê³¼ ë¬¸ìì—´ ìƒì„±
            result = f"ğŸ“Š {target_metric}ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ {top_n}ê°œ ë©”íŠ¸ë¦­:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"â€¢ {metric}: {correlation:.4f}\n"

            # ì‹œê°í™”
            plt.figure(figsize=(12, 6))
            top_correlations.plot(kind="bar")
            plt.title(f"Top {top_n} Metrics Correlated with {target_metric}")
            plt.xlabel("Metrics")
            plt.ylabel("Correlation Coefficient")
            plt.xticks(rotation=45, ha="right")
            plt.tight_layout()

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = (
                OUTPUT_DIR / f"correlation_analysis_{target_metric}_{timestamp}.png"
            )
            plt.savefig(graph_file, dpi=300, bbox_inches="tight")
            plt.close()

            result += f"\nğŸ“ˆ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def detect_metric_outliers(
        self, csv_file: str, std_threshold: float = 2.0
    ) -> str:
        """ì•„ì›ƒë¼ì´ì–´ íƒì§€"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            result = f"ğŸš¨ ì•„ì›ƒë¼ì´ì–´ íƒì§€ ê²°ê³¼ (ì„ê³„ê°’: Â±{std_threshold}Ïƒ):\n\n"

            outlier_summary = []

            # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ ì•„ì›ƒë¼ì´ì–´ íƒì§€
            for column in df.columns:
                series = df[column]
                mean = series.mean()
                std = series.std()
                lower_bound = mean - std_threshold * std
                upper_bound = mean + std_threshold * std

                outliers = series[(series < lower_bound) | (series > upper_bound)]

                if not outliers.empty:
                    result += f"âš ï¸ {column} ë©”íŠ¸ë¦­ì˜ ì•„ì›ƒë¼ì´ì–´ ({len(outliers)}ê°œ):\n"
                    result += f"   ì •ìƒ ë²”ìœ„: {lower_bound:.2f} ~ {upper_bound:.2f}\n"

                    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ í‘œì‹œ
                    for i, (timestamp, value) in enumerate(outliers.items()):
                        if i >= 5:
                            result += f"   ... ë° {len(outliers) - 5}ê°œ ë”\n"
                            break
                        result += f"   â€¢ {timestamp}: {value:.2f}\n"
                    result += "\n"

                    outlier_summary.append(
                        {
                            "metric": column,
                            "count": len(outliers),
                            "percentage": (len(outliers) / len(series)) * 100,
                        }
                    )
                else:
                    result += f"âœ… {column}: ì•„ì›ƒë¼ì´ì–´ ì—†ìŒ\n"

            # ìš”ì•½ ì •ë³´
            if outlier_summary:
                result += "\nğŸ“‹ ì•„ì›ƒë¼ì´ì–´ ìš”ì•½:\n"
                for summary in outlier_summary:
                    result += f"â€¢ {summary['metric']}: {summary['count']}ê°œ ({summary['percentage']:.1f}%)\n"

            return result

        except Exception as e:
            return f"ì•„ì›ƒë¼ì´ì–´ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def perform_regression_analysis(
        self,
        csv_file: str,
        predictor_metric: str,
        target_metric: str = "CPUUtilization",
    ) -> str:
        """íšŒê·€ ë¶„ì„ ìˆ˜í–‰"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            # í•„ìš”í•œ ë©”íŠ¸ë¦­ í™•ì¸
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­: {list(df.columns)}"

            # ë°ì´í„° ì¤€ë¹„
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN ê°’ ì²˜ë¦¬
            imputer = SimpleImputer(strategy="mean")
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± (2ì°¨)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_poly_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê³„ìˆ˜ ì¶œë ¥
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê²°ê³¼ ({predictor_metric} â†’ {target_metric}):\n\n"
            result += f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥:\n"
            result += f"â€¢ Mean Squared Error: {mse:.4f}\n"
            result += f"â€¢ R-squared Score: {r2:.4f}\n\n"

            result += f"ğŸ”¢ ë‹¤í•­ íšŒê·€ ëª¨ë¸ (2ì°¨):\n"
            result += f"y = {coefficients[1]:.4f}xÂ² + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # í•´ì„ ì¶”ê°€
            result += "í•´ì„:\n"
            if r2 > 0.7:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"
            elif r2 > 0.5:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ì¤‘ê°„ ì˜ˆì¸¡ ì •í™•ë„)\n"
            else:
                result += f"â€¢ ëª¨ë¸ ì„¤ëª…ë ¥: {r2*100:.1f}% (ë‚®ì€ ì˜ˆì¸¡ ì •í™•ë„)\n"

            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            plt.figure(figsize=(12, 8))

            # ì‚°ì ë„
            plt.subplot(2, 1, 1)
            plt.scatter(X_test, y_test, color="blue", alpha=0.6, label="ì‹¤ì œ ë°ì´í„°")

            # ì˜ˆì¸¡ ê³¡ì„ ì„ ìœ„í•œ ì •ë ¬ëœ ë°ì´í„°
            X_plot = np.linspace(X_test.min(), X_test.max(), 100).reshape(-1, 1)
            X_plot_poly = poly_features.transform(X_plot)
            y_plot_pred = model.predict(X_plot_poly)

            plt.plot(X_plot, y_plot_pred, color="red", linewidth=2, label="ì˜ˆì¸¡ ëª¨ë¸")
            plt.title(f"{predictor_metric} vs {target_metric} íšŒê·€ ë¶„ì„")
            plt.xlabel(predictor_metric)
            plt.ylabel(target_metric)
            plt.legend()
            plt.grid(True, alpha=0.3)

            # ì”ì°¨ í”Œë¡¯
            plt.subplot(2, 1, 2)
            residuals = y_test - y_pred
            plt.scatter(y_pred, residuals, color="green", alpha=0.6)
            plt.axhline(y=0, color="red", linestyle="--")
            plt.title("ì”ì°¨ í”Œë¡¯ (Residual Plot)")
            plt.xlabel("ì˜ˆì¸¡ê°’")
            plt.ylabel("ì”ì°¨")
            plt.grid(True, alpha=0.3)

            plt.tight_layout()

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            graph_file = (
                OUTPUT_DIR
                / f"regression_analysis_{predictor_metric}_{target_metric}_{timestamp}.png"
            )
            plt.savefig(graph_file, dpi=300, bbox_inches="tight")
            plt.close()

            result += f"ğŸ“ˆ íšŒê·€ ë¶„ì„ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}"
            return result

        except Exception as e:
            return f"íšŒê·€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def list_data_files(self) -> str:
        """ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ“ ë°ì´í„° íŒŒì¼ ëª©ë¡:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"â€¢ {file.name}\n"
                result += f"  í¬ê¸°: {file_size:,} bytes\n"
                result += f"  ìˆ˜ì •ì¼: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        if not ANALYSIS_AVAILABLE:
            return "âŒ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            # CSV íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

            # ë°ì´í„° ì½ê¸°
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            result = f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ({csv_file}):\n\n"
            result += f"ğŸ“… ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}\n"
            result += f"ğŸ“ˆ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ\n"
            result += f"ğŸ“‹ ë©”íŠ¸ë¦­ ìˆ˜: {len(df.columns)}ê°œ\n\n"

            result += "ğŸ“Š ë©”íŠ¸ë¦­ ëª©ë¡:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}ê°œ ë°ì´í„°)\n"

            # ê¸°ë³¸ í†µê³„
            result += f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def validate_column_type_compatibility(
        self, existing_column: Dict[str, Any], new_definition: str, debug_log
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± ê²€ì¦"""
        debug_log(
            f"íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ ì‹œì‘: ê¸°ì¡´={existing_column['data_type']}, ìƒˆë¡œìš´={new_definition}"
        )

        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        debug_log(f"íŒŒì‹±ëœ ìƒˆ íƒ€ì…: {new_type_info}")

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT", "LONGTEXT", "MEDIUMTEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(f"í˜¸í™˜ì„± ë¬¸ì œ: {existing_type} -> {new_type_info['type']}")

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(f"ê¸¸ì´ ì¶•ì†Œ ë¬¸ì œ: {existing_length} -> {new_length}")

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                debug_log(
                    f"ì •ë°€ë„ ì¶•ì†Œ ë¬¸ì œ: ({existing_precision},{existing_scale}) -> ({new_precision},{new_scale})"
                )

        result = {"compatible": len(issues) == 0, "issues": issues}

        debug_log(
            f"íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ: compatible={result['compatible']}, issues={len(issues)}"
        )
        return result

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def collect_slow_queries(
        self, database_secret: str, db_instance_identifier: str = None
    ) -> str:
        """ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì—°ê²°ì´ ìˆìœ¼ë©´ ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()
            
            # íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒˆë¡œ ì—°ê²°
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

            cursor = self.shared_cursor

            # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = f"_{db_instance_identifier}" if db_instance_identifier else ""
            filename = f"slow_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # ì¤‘ë³µ ì œê±°ìš©

            # 1. performance_schemaì—ì„œ ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œë„
            try:
                cursor.execute(
                    """
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND AVG_TIMER_WAIT >= 1000000000000
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # 2. information_schema.PROCESSLISTì—ì„œ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘
            try:
                cursor.execute(
                    """
                    SELECT INFO 
                    FROM information_schema.PROCESSLIST 
                    WHERE COMMAND = 'Query' 
                        AND TIME >= 1
                        AND INFO IS NOT NULL
                        AND INFO NOT LIKE '%PROCESSLIST%'
                        AND INFO NOT LIKE 'EXPLAIN%'
                    ORDER BY TIME DESC
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"PROCESSLIST ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # ì¿¼ë¦¬ê°€ ìˆì„ ë•Œë§Œ íŒŒì¼ ìƒì„±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- ëŠë¦° ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n")
                    f.write(f"-- ì´ {len(collected_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- ëŠë¦° ì¿¼ë¦¬ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"âœ… ëŠë¦° ì¿¼ë¦¬ {len(collected_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}"
            else:
                return f"âœ… ëŠë¦° ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)"

        except Exception as e:
            return f"âŒ ëŠë¦° ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

    async def collect_memory_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None
    ) -> str:
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì—°ê²°ì´ ìˆìœ¼ë©´ ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()
            
            # íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒˆë¡œ ì—°ê²°
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

            cursor = self.shared_cursor

            # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = f"_{db_instance_identifier}" if db_instance_identifier else ""
            filename = f"memory_intensive_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # ì¤‘ë³µ ì œê±°ìš©

            # 1. performance_schemaì—ì„œ ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œë„
            try:
                cursor.execute(
                    """
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND MAX_MEMORY_USED > 100*1024*1024
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                    ORDER BY MAX_MEMORY_USED DESC 
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema ë©”ëª¨ë¦¬ ì¿¼ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # ì¿¼ë¦¬ê°€ ìˆì„ ë•Œë§Œ íŒŒì¼ ìƒì„±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(
                        f"-- ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n"
                    )
                    f.write(f"-- ì´ {len(collected_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"âœ… ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ {len(collected_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}"
            else:
                return (
                    f"âœ… ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)"
                )

        except Exception as e:
            return f"âŒ ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

    async def collect_cpu_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None
    ) -> str:
        """CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì—°ê²°ì´ ìˆìœ¼ë©´ ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()
            
            # íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒˆë¡œ ì—°ê²°
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

            cursor = self.shared_cursor

            # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = f"_{db_instance_identifier}" if db_instance_identifier else ""
            filename = f"cpu_intensive_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # ì¤‘ë³µ ì œê±°ìš©

            # 1. performance_schemaì—ì„œ CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œë„
            try:
                cursor.execute(
                    """
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND SUM_TIMER_WAIT > 10000000000000
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                    ORDER BY SUM_TIMER_WAIT DESC 
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema CPU ì¿¼ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # 2. information_schemaì—ì„œ í˜„ì¬ CPU ì‚¬ìš© ì¤‘ì¸ ì¿¼ë¦¬ ìˆ˜ì§‘
            try:
                cursor.execute(
                    """
                    SELECT INFO 
                    FROM information_schema.PROCESSLIST 
                    WHERE COMMAND = 'Query' 
                        AND STATE IN ('Sending data', 'Sorting result', 'Creating sort index', 'Copying to tmp table')
                        AND INFO IS NOT NULL
                        AND INFO NOT LIKE '%PROCESSLIST%'
                        AND INFO NOT LIKE 'EXPLAIN%'
                    ORDER BY TIME DESC
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"PROCESSLIST CPU ì¿¼ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # ì¿¼ë¦¬ê°€ ìˆì„ ë•Œë§Œ íŒŒì¼ ìƒì„±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- CPU ì§‘ì•½ì  ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n")
                    f.write(f"-- ì´ {len(collected_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- CPU ì§‘ì•½ì  ì¿¼ë¦¬ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"âœ… CPU ì§‘ì•½ì  ì¿¼ë¦¬ {len(collected_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}"
            else:
                return f"âœ… CPU ì§‘ì•½ì  ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)"

        except Exception as e:
            return f"âŒ CPU ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"

    async def collect_temp_space_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None
    ) -> str:
        """ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ë° SQL íŒŒì¼ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì—°ê²°ì´ ìˆìœ¼ë©´ ì •ë¦¬
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()
            
            # íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒˆë¡œ ì—°ê²°
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"

            cursor = self.shared_cursor

            # í˜„ì¬ ë‚ ì§œì™€ ì¸ìŠ¤í„´ìŠ¤ IDë¡œ íŒŒì¼ëª… ìƒì„±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = f"_{db_instance_identifier}" if db_instance_identifier else ""
            filename = f"temp_space_intensive_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # ì¤‘ë³µ ì œê±°ìš©

            # 1. performance_schemaì—ì„œ ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹œë„
            try:
                cursor.execute(
                    """
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND (SUM_CREATED_TMP_TABLES > 0 OR SUM_CREATED_TMP_DISK_TABLES > 0 OR SUM_SORT_MERGE_PASSES > 0)
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                    ORDER BY (SUM_CREATED_TMP_DISK_TABLES + SUM_SORT_MERGE_PASSES) DESC 
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema ì„ì‹œê³µê°„ ì¿¼ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # 2. information_schemaì—ì„œ í˜„ì¬ ì„ì‹œ í…Œì´ë¸” ì‚¬ìš© ì¤‘ì¸ ì¿¼ë¦¬ ìˆ˜ì§‘
            try:
                cursor.execute(
                    """
                    SELECT INFO 
                    FROM information_schema.PROCESSLIST 
                    WHERE COMMAND = 'Query' 
                        AND STATE IN ('Copying to tmp table', 'Sorting for group', 'Sorting for order', 'Creating sort index')
                        AND INFO IS NOT NULL
                        AND INFO NOT LIKE '%PROCESSLIST%'
                        AND INFO NOT LIKE 'EXPLAIN%'
                    ORDER BY TIME DESC
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¿¼ë¦¬ ì œì™¸
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema ê´€ë ¨ ì¿¼ë¦¬ ì œì™¸
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"PROCESSLIST ì„ì‹œê³µê°„ ì¿¼ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # ì¿¼ë¦¬ê°€ ìˆì„ ë•Œë§Œ íŒŒì¼ ìƒì„±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(
                        f"-- ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ëª¨ìŒ (ìˆ˜ì§‘ì¼ì‹œ: {datetime.now()})\n"
                    )
                    f.write(f"-- ì´ {len(collected_queries)}ê°œì˜ ì¿¼ë¦¬\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"âœ… ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ {len(collected_queries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ: {filename}"
            else:
                return f"âœ… ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)"

        except Exception as e:
            return f"âŒ ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"


# MCP ì„œë²„ ì„¤ì •
server = Server("db-assistant-mcp-server")
db_assistant = DBAssistantMCPServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="list_databases",
            description="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="select_database",
            description="íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ (USE ëª…ë ¹ì–´ ì‹¤í–‰). ë¨¼ì € list_databasesë¡œ ëª©ë¡ì„ í™•ì¸í•œ í›„ ë²ˆí˜¸ë‚˜ ì´ë¦„ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”.",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ì„ íƒí•  ë°ì´í„°ë² ì´ìŠ¤ (ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„)",
                    },
                },
                "required": ["database_secret", "database_selection"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_schema",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="get_table_index",
            description="íŠ¹ì • í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  í…Œì´ë¸” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ìˆ˜ì§‘í•  ë©”íŠ¸ë¦­ ëª©ë¡ (ì„ íƒì‚¬í•­)",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)",
                        "default": "us-east-1",
                    },
                },
                "required": ["db_instance_identifier"],
            },
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ìƒìœ„ Nê°œ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: 10)",
                        "default": 10,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="ë©”íŠ¸ë¦­ ë°ì´í„°ì—ì„œ ì•„ì›ƒë¼ì´ì–´ë¥¼ íƒì§€í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "std_threshold": {
                        "type": "number",
                        "description": "í‘œì¤€í¸ì°¨ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2.0)",
                        "default": 2.0,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="ë©”íŠ¸ë¦­ ê°„ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ë¶„ì„í•  CSV íŒŒì¼ëª…"},
                    "predictor_metric": {
                        "type": "string",
                        "description": "ì˜ˆì¸¡ ë³€ìˆ˜ ë©”íŠ¸ë¦­",
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "íƒ€ê²Ÿ ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                },
                "required": ["csv_file", "predictor_metric"],
            },
        ),
        types.Tool(
            name="list_data_files",
            description="ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV íŒŒì¼ì˜ ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ìš”ì•½í•  CSV íŒŒì¼ëª…"}
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="collect_slow_queries",
            description="ëŠë¦° ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_memory_intensive_queries",
            description="ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_cpu_intensive_queries",
            description="CPU ì§‘ì•½ì  ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_temp_space_intensive_queries",
            description="ì„ì‹œ ê³µê°„ ì§‘ì•½ì  ì¿¼ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” SQLì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="test_individual_query_validation",
            description="ê°œë³„ ì¿¼ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ë””ë²„ê·¸ìš©)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                },
                "required": ["database_secret", "filename"],
            },
        ),
        types.Tool(
            name="generate_consolidated_report",
            description="ê¸°ì¡´ HTML ë³´ê³ ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í†µí•© ë³´ê³ ì„œ ìƒì„±",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "í•„í„°ë§í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    },
                    "report_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "íŠ¹ì • ë³´ê³ ì„œ íŒŒì¼ëª… ëª©ë¡ (ì„ íƒì‚¬í•­)",
                    },
                    "date_filter": {
                        "type": "string",
                        "description": "ë‚ ì§œ í•„í„° (YYYYMMDD í˜•ì‹, ì„ íƒì‚¬í•­)",
                    },
                    "latest_count": {
                        "type": "integer",
                        "description": "ìµœì‹  íŒŒì¼ ê°œìˆ˜ ì œí•œ (ì„ íƒì‚¬í•­)",
                    },
                },
            },
        ),
        types.Tool(
            name="generate_comprehensive_performance_report",
            description="Oracle AWR ìŠ¤íƒ€ì¼ì˜ ì¢…í•© ì„±ëŠ¥ ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± (ë©”íŠ¸ë¦­ ë¶„ì„, ìƒê´€ê´€ê³„, ëŠë¦° ì¿¼ë¦¬, ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  ì¿¼ë¦¬ í¬í•¨)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_instance_identifier"],
            },
        ),
        types.Tool(
            name="generate_cluster_performance_report",
            description="Aurora í´ëŸ¬ìŠ¤í„° ì „ìš© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± (í´ëŸ¬ìŠ¤í„° ë ˆë²¨ ë¶„ì„ + ì¸ìŠ¤í„´ìŠ¤ë³„ ìƒì„¸ ë³´ê³ ì„œ ë§í¬)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "db_cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ìˆ˜ì§‘í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_cluster_identifier"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await db_assistant.list_sql_files()
        elif name == "list_database_secrets":
            result = await db_assistant.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "test_database_connection":
            result = await db_assistant.test_database_connection(
                arguments["database_secret"]
            )
        elif name == "list_databases":
            result = await db_assistant.list_databases(arguments["database_secret"])
        elif name == "select_database":
            result = await db_assistant.select_database(
                arguments["database_secret"], arguments["database_selection"]
            )
        elif name == "get_schema_summary":
            result = await db_assistant.get_schema_summary(arguments["database_secret"])
        elif name == "get_table_schema":
            result = await db_assistant.get_table_schema(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "get_table_index":
            result = await db_assistant.get_table_index(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "get_performance_metrics":
            result = await db_assistant.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "collect_db_metrics":
            result = await db_assistant.collect_db_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", "us-east-1"),
            )
        elif name == "analyze_metric_correlation":
            result = await db_assistant.analyze_metric_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10),
            )
        elif name == "detect_metric_outliers":
            result = await db_assistant.detect_metric_outliers(
                arguments["csv_file"], arguments.get("std_threshold", 2.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_assistant.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization"),
            )
        elif name == "list_data_files":
            result = await db_assistant.list_data_files()
        elif name == "validate_sql_file":
            result = await db_assistant.validate_sql_file(
                arguments["filename"], arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await db_assistant.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "get_metric_summary":
            result = await db_assistant.get_metric_summary(arguments["csv_file"])
        elif name == "collect_slow_queries":
            result = await db_assistant.collect_slow_queries(
                arguments["database_secret"], arguments.get("db_instance_identifier")
            )
        elif name == "collect_memory_intensive_queries":
            result = await db_assistant.collect_memory_intensive_queries(
                arguments["database_secret"], arguments.get("db_instance_identifier")
            )
        elif name == "collect_cpu_intensive_queries":
            result = await db_assistant.collect_cpu_intensive_queries(
                arguments["database_secret"], arguments.get("db_instance_identifier")
            )
        elif name == "collect_temp_space_intensive_queries":
            result = await db_assistant.collect_temp_space_intensive_queries(
                arguments["database_secret"], arguments.get("db_instance_identifier")
            )
        elif name == "test_individual_query_validation":
            result = await db_assistant.test_individual_query_validation(
                arguments["database_secret"], arguments["filename"]
            )
        elif name == "generate_consolidated_report":
            result = await db_assistant.generate_consolidated_report(
                arguments.get("keyword"),
                arguments.get("report_files"),
                arguments.get("date_filter"),
                arguments.get("latest_count"),
            )
        elif name == "generate_comprehensive_performance_report":
            result = await db_assistant.generate_comprehensive_performance_report(
                arguments["database_secret"],
                arguments["db_instance_identifier"],
                arguments.get("region", "ap-northeast-2"),
                arguments.get("hours", 24),
            )
        elif name == "generate_cluster_performance_report":
            result = await db_assistant.generate_cluster_performance_report(
                arguments["database_secret"],
                arguments["db_cluster_identifier"],
                arguments.get("hours", 24),
                arguments.get("region", "ap-northeast-2"),
            )
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="db-assistant-mcp-server",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())

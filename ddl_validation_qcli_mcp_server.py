#!/usr/bin/env python3
"""
DB Assistant Amazon Q CLI MCP ÏÑúÎ≤Ñ
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import sys

import boto3
from botocore.exceptions import ClientError

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

# Î∂ÑÏÑù Í¥ÄÎ†® ÎùºÏù¥Î∏åÎü¨Î¶¨
ANALYSIS_AVAILABLE = False
CHART_AVAILABLE = False
try:
    import pandas as pd
    import numpy as np
    import sqlparse
    import matplotlib

    matplotlib.use("Agg")  # GUI ÏóÜÎäî ÌôòÍ≤ΩÏóêÏÑú ÏÇ¨Ïö©
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.impute import SimpleImputer

    ANALYSIS_AVAILABLE = True
    CHART_AVAILABLE = True
except ImportError:
    sqlparse = None

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


def create_session_log(operation_name: str = "operation"):
    """ÏûëÏóÖÎ≥Ñ ÏÑ∏ÏÖò Î°úÍ∑∏ ÌååÏùº ÏÉùÏÑ± Î∞è Î°úÍ∑∏ Ìï®Ïàò Î∞òÌôò"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"ddl_validation_{operation_name}_{timestamp}.log"
    log_path = Path("logs") / log_filename

    # logs ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
    log_path.parent.mkdir(exist_ok=True)

    def log_message(level: str, message: str):
        """ÏÑ∏ÏÖò Î°úÍ∑∏ ÌååÏùºÏóê Î©îÏãúÏßÄ ÏûëÏÑ±"""
        timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"{timestamp_str} - {operation_name} - {level} - {message}\n"

        with open(log_path, "a", encoding="utf-8") as f:
            f.write(log_entry)

        # ÏΩòÏÜîÏóêÎèÑ Ï∂úÎ†•
        print(f"[{level}] {message}")

    # Ï¥àÍ∏∞ Î°úÍ∑∏ ÏûëÏÑ±
    log_message("INFO", f"ÏÉà ÏûëÏóÖ ÏÑ∏ÏÖò ÏãúÏûë: {operation_name} - Î°úÍ∑∏ ÌååÏùº: {log_path}")

    return log_message, str(log_path)


# ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨ Í∏∞Ï§Ä Í≤ΩÎ°ú ÏÑ§Ï†ï
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"
DATA_DIR = CURRENT_DIR / "data"
LOGS_DIR = CURRENT_DIR / "logs"

# ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)

# Ï†ÑÏó≠ debug_log Ìï®Ïàò
def debug_log(message: str):
    """Ï†ÑÏó≠ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Ìï®Ïàò"""
    debug_log_path = LOGS_DIR / f"debug_{datetime.now().strftime('%Y%m%d')}.log"
    try:
        with open(debug_log_path, "a", encoding="utf-8") as f:
            f.write(f"{datetime.now().strftime('%H:%M:%S')} - {message}\n")
    except Exception:
        pass  # Î°úÍ∑∏ Ïã§Ìå® Ïãú Î¨¥Ïãú


class DBAssistantMCPServer:
    def __init__(self):
        try:
            logger.info("Bedrock ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏãúÏûë")
            self.bedrock_client = boto3.client(
                "bedrock-runtime", region_name="us-west-2", verify=False
            )
            logger.info("Bedrock ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ - Î¶¨Ï†Ñ: us-west-2")

            # Bedrock Ï†ëÍ∑º Í∂åÌïú ÌÖåÏä§Ìä∏
            try:
                # Í∞ÑÎã®Ìïú Î™®Îç∏ Î™©Î°ù Ï°∞ÌöåÎ°ú Í∂åÌïú ÌÖåÏä§Ìä∏
                bedrock_control = boto3.client(
                    "bedrock", region_name="us-west-2", verify=False
                )
                logger.info("Bedrock ÏÑúÎπÑÏä§ Ï†ëÍ∑º Í∂åÌïú ÌôïÏù∏ Ï§ë...")
                # Ïã§Ï†ú Í∂åÌïú ÌÖåÏä§Ìä∏Îäî Î™®Îç∏ Ìò∏Ï∂ú Ïãú ÏàòÌñâ
                logger.info("Bedrock ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï ÏôÑÎ£å")
            except Exception as perm_e:
                logger.warning(
                    f"Bedrock Í∂åÌïú ÏÇ¨Ï†Ñ ÌôïÏù∏ Ïã§Ìå® (Î™®Îç∏ Ìò∏Ï∂ú Ïãú Ïû¨ÏãúÎèÑ): {perm_e}"
                )

        except Exception as e:
            logger.error(f"Bedrock ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise

        self.knowledge_base_id = "0WQUBRHVR8"
        self.selected_database = None
        self.current_plan = None

        # Í≥µÏö© DB Ïó∞Í≤∞ Î≥ÄÏàò (Ïó∞Í≤∞ Ïû¨ÏÇ¨Ïö©ÏùÑ ÏúÑÌï¥)
        self.shared_connection = None
        self.shared_cursor = None
        self.tunnel_used = False

        # ÏÑ±Îä• ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
        self.PERFORMANCE_THRESHOLDS = {
            "max_rows_scan": 10_000_000,  # 1Ï≤úÎßå Ìñâ Ïù¥ÏÉÅ Ïä§Ï∫î Ïãú Ïã§Ìå®
            "table_scan_ratio": 0.1,  # ÌÖåÏù¥Î∏îÏùò 10% Ïù¥ÏÉÅ Ïä§Ï∫î Ïãú Í≤ΩÍ≥†
            "critical_rows_scan": 50_000_000,  # 5Ï≤úÎßå Ìñâ Ïù¥ÏÉÅ Ïä§Ï∫î Ïãú Ïã¨Í∞ÅÌïú Î¨∏Ï†ú
        }

        # Î∂ÑÏÑù Í¥ÄÎ†® Ï¥àÍ∏∞Ìôî
        self.cloudwatch = None
        self.default_metrics = [
            "CPUUtilization",
            "DatabaseConnections",
            "DBLoad",
            "DBLoadCPU",
            "DBLoadNonCPU",
            "FreeableMemory",
            "ReadIOPS",
            "WriteIOPS",
            "ReadLatency",
            "WriteLatency",
            "NetworkReceiveThroughput",
            "NetworkTransmitThroughput",
            "BufferCacheHitRatio",
        ]

        # Í∏∞Î≥∏ Î¶¨Ï†Ñ ÏÑ§Ï†ï
        self.default_region = self.get_default_region()

        # Knowledge Base ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
        self.bedrock_agent_client = boto3.client(
            "bedrock-agent-runtime", region_name="us-east-1", verify=False
        )

    def get_default_region(self) -> str:
        """ÌòÑÏû¨ AWS ÌîÑÎ°úÌååÏùºÏùò Í∏∞Î≥∏ Î¶¨Ï†Ñ Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            session = boto3.Session()
            return session.region_name or "ap-northeast-2"
        except Exception:
            return "ap-northeast-2"

    def parse_table_name(self, full_table_name: str) -> tuple:
        """ÌÖåÏù¥Î∏îÎ™ÖÏóêÏÑú Ïä§ÌÇ§ÎßàÏôÄ ÌÖåÏù¥Î∏îÎ™ÖÏùÑ Î∂ÑÎ¶¨"""
        if "." in full_table_name:
            schema, table = full_table_name.split(".", 1)
            return schema.strip("`"), table.strip("`")
        return None, full_table_name.strip("`")

    def format_file_link(self, file_path: str, display_name: str = None) -> str:
        """ÌååÏùº Í≤ΩÎ°úÎ•º HTML ÎßÅÌÅ¨Î°ú Î≥ÄÌôò"""
        if not display_name:
            display_name = Path(file_path).name
        return f'<a href="file://{file_path}" class="file-link" target="_blank">{display_name}</a>'

    async def query_knowledge_base(self, query: str, sql_type: str) -> str:
        """Knowledge BaseÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            # SQL ÌÉÄÏûÖÏóê Îî∞Î•∏ ÏøºÎ¶¨ Ï°∞Ï†ï
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

            if sql_type in ddl_types:
                # DDLÏù∏ Í≤ΩÏö∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎèÑÎ©îÏù∏ Í¥ÄÎ¶¨ Í∑úÏπô Ï°∞Ìöå
                kb_query = f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎèÑÎ©îÏù∏ Í¥ÄÎ¶¨ Í∑úÏπô {query}"
            elif sql_type in dql_types:
                # DQLÏù∏ Í≤ΩÏö∞ Aurora MySQL ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú Ï°∞Ìöå
                kb_query = f"Aurora MySQL ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú {query}"
            else:
                # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÎèÑÎ©îÏù∏ Í¥ÄÎ¶¨ Í∑úÏπô Ï°∞Ìöå
                kb_query = f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎèÑÎ©îÏù∏ Í¥ÄÎ¶¨ Í∑úÏπô {query}"

            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": kb_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": 3}
                },
            )

            # Í≤ÄÏÉâ Í≤∞Í≥ºÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            knowledge_content = []
            for result in response.get("retrievalResults", []):
                content = result.get("content", {}).get("text", "")
                if content:
                    knowledge_content.append(content)

            if knowledge_content:
                return "\n\n".join(knowledge_content)
            else:
                return "Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

        except Exception as e:
            logger.warning(f"Knowledge Base Ï°∞Ìöå Ïã§Ìå®: {e}")
            return "Knowledge Base Ï°∞Ìöå Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

    def convert_kst_to_utc(self, kst_time_str: str) -> datetime:
        """KST ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥ÏùÑ UTC datetime Í∞ùÏ≤¥Î°ú Î≥ÄÌôò"""
        try:
            kst_dt = datetime.strptime(kst_time_str, "%Y-%m-%d %H:%M:%S")
            utc_dt = kst_dt - timedelta(hours=9)
            return utc_dt
        except ValueError as e:
            raise ValueError(f"ÏãúÍ∞Ñ ÌòïÏãù Ïò§Î•ò. YYYY-MM-DD HH:MM:SS ÌòïÏãùÏúºÎ°ú ÏûÖÎ†•ÌïòÏÑ∏Ïöî: {e}")

    def convert_utc(self, utc_dt: datetime, region_name: str = None) -> datetime:
        """UTC datetime Í∞ùÏ≤¥Î•º ÏßÄÏ†ïÎêú Î¶¨Ï†ÑÏùò Î°úÏª¨ ÏãúÍ∞ÑÏúºÎ°ú Î≥ÄÌôò"""
        if region_name is None:
            region_name = self.default_region
            
        # AWS Î¶¨Ï†ÑÎ≥Ñ ÏãúÍ∞ÑÎåÄ Ïò§ÌîÑÏÖã (UTC Í∏∞Ï§Ä)
        region_timezone_offsets = {
            # ÏïÑÏãúÏïÑ-ÌÉúÌèâÏñë
            'ap-northeast-1': 9,    # ÎèÑÏøÑ (JST)
            'ap-northeast-2': 9,    # ÏÑúÏö∏ (KST)
            'ap-northeast-3': 9,    # Ïò§ÏÇ¨Ïπ¥ (JST)
            'ap-south-1': 5.5,      # Î≠ÑÎ∞îÏù¥ (IST)
            'ap-southeast-1': 8,    # Ïã±Í∞ÄÌè¨Î•¥ (SGT)
            'ap-southeast-2': 10,   # ÏãúÎìúÎãà (AEST) - ÌëúÏ§ÄÏãú Í∏∞Ï§Ä
            'ap-east-1': 8,         # ÌôçÏΩ© (HKT)
            
            # Ïú†ÎüΩ
            'eu-west-1': 0,         # ÏïÑÏùºÎûúÎìú (GMT/UTC)
            'eu-west-2': 0,         # Îü∞Îçò (GMT/UTC)
            'eu-west-3': 1,         # ÌååÎ¶¨ (CET)
            'eu-central-1': 1,      # ÌîÑÎûëÌÅ¨Ìë∏Î•¥Ìä∏ (CET)
            'eu-north-1': 1,        # Ïä§ÌÜ°ÌôÄÎ¶Ñ (CET)
            
            # Î∂ÅÎØ∏
            'us-east-1': -5,        # Î≤ÑÏßÄÎãàÏïÑ (EST)
            'us-east-2': -5,        # Ïò§ÌïòÏù¥Ïò§ (EST)
            'us-west-1': -8,        # Ï∫òÎ¶¨Ìè¨ÎãàÏïÑ (PST)
            'us-west-2': -8,        # Ïò§Î†àÍ≥§ (PST)
            'ca-central-1': -5,     # Ï∫êÎÇòÎã§ Ï§ëÎ∂Ä (EST)
            
            # ÎÇ®ÎØ∏
            'sa-east-1': -3,        # ÏÉÅÌååÏö∏Î£® (BRT)
            
            # Ï§ëÎèô/ÏïÑÌîÑÎ¶¨Ïπ¥
            'me-south-1': 3,        # Î∞îÎ†àÏù∏ (AST)
            'af-south-1': 2,        # ÏºÄÏù¥ÌîÑÌÉÄÏö¥ (SAST)
        }
        
        offset_hours = region_timezone_offsets.get(region_name, 0)
        
        # ÏÜåÏàòÏ†êÏù¥ ÏûàÎäî Í≤ΩÏö∞ (Ïòà: Ïù∏ÎèÑ +5.5ÏãúÍ∞Ñ)
        if isinstance(offset_hours, float):
            hours = int(offset_hours)
            minutes = int((offset_hours - hours) * 60)
            return utc_dt + timedelta(hours=hours, minutes=minutes)
        else:
            return utc_dt + timedelta(hours=offset_hours)

    def set_default_region(self, region_name: str) -> str:
        """Í∏∞Î≥∏ AWS Î¶¨Ï†ÑÏùÑ Î≥ÄÍ≤ΩÌï©ÎãàÎã§"""
        # ÏßÄÏõêÌïòÎäî Î¶¨Ï†Ñ Î™©Î°ù
        supported_regions = {
            'ap-northeast-1', 'ap-northeast-2', 'ap-northeast-3', 'ap-south-1',
            'ap-southeast-1', 'ap-southeast-2', 'ap-east-1',
            'eu-west-1', 'eu-west-2', 'eu-west-3', 'eu-central-1', 'eu-north-1',
            'us-east-1', 'us-east-2', 'us-west-1', 'us-west-2', 'ca-central-1',
            'sa-east-1', 'me-south-1', 'af-south-1'
        }
        
        if region_name not in supported_regions:
            return f"‚ùå ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î¶¨Ï†ÑÏûÖÎãàÎã§: {region_name}\n\n‚úÖ ÏßÄÏõêÌïòÎäî Î¶¨Ï†Ñ:\n" + \
                   "\n".join([f"‚Ä¢ {region}" for region in sorted(supported_regions)])
        
        old_region = self.default_region
        self.default_region = region_name
        
        # ÌôòÍ≤Ω Î≥ÄÏàòÎèÑ ÏóÖÎç∞Ïù¥Ìä∏
        os.environ['AWS_DEFAULT_REGION'] = region_name
        
        return f"‚úÖ Í∏∞Î≥∏ Î¶¨Ï†ÑÏù¥ Î≥ÄÍ≤ΩÎêòÏóàÏäµÎãàÎã§!\n\nÏù¥Ï†Ñ: {old_region}\nÌòÑÏû¨: {self.default_region}\n\nüí° Ïù¥Ï†ú Î™®Îì† AWS ÏÑúÎπÑÏä§ Ìò∏Ï∂úÍ≥º ÏãúÍ∞Ñ Î≥ÄÌôòÏù¥ ÏÉà Î¶¨Ï†Ñ Í∏∞Ï§ÄÏúºÎ°ú ÏûëÎèôÌï©ÎãàÎã§."

    def get_secret(self, secret_name):
        """Secrets ManagerÏóêÏÑú DB Ïó∞Í≤∞ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret Ï°∞Ìöå Ïã§Ìå®: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """ÌÇ§ÏõåÎìúÎ°ú Secret Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Ï≤òÎ¶¨
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ÎßÅ
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH ÌÑ∞ÎÑê ÏÑ§Ï†ï"""
        try:
            import subprocess
            import time

            # Í∏∞Ï°¥ ÌÑ∞ÎÑê Ï¢ÖÎ£å
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH ÌÑ∞ÎÑê ÏãúÏûë
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH ÌÑ∞ÎÑê ÏÑ§Ï†ï Ï§ë: {db_host} -> localhost:3307")

            process = subprocess.run(ssh_command, capture_output=True, text=True)

            # ÌÑ∞ÎÑêÏù¥ ÏÑ§Ï†ïÎê† ÎïåÍπåÏßÄ Ïû†Ïãú ÎåÄÍ∏∞
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH ÌÑ∞ÎÑêÏù¥ ÏÑ§Ï†ïÎêòÏóàÏäµÎãàÎã§.")
                return True
            else:
                logger.error(f"SSH ÌÑ∞ÎÑê ÏÑ§Ï†ï Ïã§Ìå®: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH ÌÑ∞ÎÑê ÏÑ§Ï†ï Ïò§Î•ò: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨"""
        try:
            import subprocess

            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH ÌÑ∞ÎÑêÏù¥ Ï†ïÎ¶¨ÎêòÏóàÏäµÎãàÎã§.")
        except Exception as e:
            logger.error(f"SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

    async def validate_individual_ddl_statements(
        self, sql_content: str, cursor, debug_log, cte_tables: List[str]
    ):
        """Í∞úÎ≥Ñ DDL Íµ¨Î¨∏ Í≤ÄÏ¶ù - CREATE TABLE/INDEX Í∞ÅÍ∞Å Í≤ÄÏ¶ù"""
        debug_log("üî•üî•üî• validate_individual_ddl_statements Ìï®Ïàò ÏãúÏûë üî•üî•üî•")
        result = {"issues": []}

        try:
            # DDL Íµ¨Î¨∏ ÌååÏã±
            ddl_statements = self.parse_ddl_statements(sql_content)
            debug_log(f"ÌååÏã±Îêú DDL Íµ¨Î¨∏ Ïàò: {len(ddl_statements)}")

            # ÌååÏùº ÎÇ¥ÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏î Î™©Î°ù Ï∂îÏ∂ú
            tables_created_in_file = set()
            for ddl in ddl_statements:
                if ddl["type"] == "CREATE_TABLE" and ddl["table"] not in cte_tables:
                    tables_created_in_file.add(ddl["table"])
            debug_log(f"ÌååÏùº ÎÇ¥ ÏÉùÏÑ± ÌÖåÏù¥Î∏î: {list(tables_created_in_file)}")

            for i, ddl in enumerate(ddl_statements):
                debug_log(
                    f"DDL [{i}] Í≤ÄÏ¶ù ÏãúÏûë: {ddl['type']} - {ddl.get('table', ddl.get('index_name', 'unknown'))}"
                )

                if ddl["type"] == "CREATE_TABLE":
                    table_name = ddl["table"]

                    # CTE aliasÎäî Ïä§ÌÇµ
                    if table_name in cte_tables:
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name}ÏùÄ CTE aliasÏù¥ÎØÄÎ°ú Ïä§ÌÇµ"
                        )
                        continue

                    # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)
                    schema, actual_table = self.parse_table_name(table_name)
                    if schema:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    exists = cursor.fetchone()[0] > 0

                    if exists:
                        issue = f"CREATE TABLE Ïã§Ìå®: ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§."
                        result["issues"].append(issue)
                        debug_log(f"CREATE TABLE [{i}] - {table_name} Ïã§Ìå®: Ïù¥ÎØ∏ Ï°¥Ïû¨")
                    else:
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name} ÏÑ±Í≥µ: Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå"
                        )

                elif ddl["type"] == "CREATE_INDEX":
                    table_name = ddl["table"]
                    index_name = ddl["index_name"]
                    columns = ddl["columns"]

                    # CTE alias ÌÖåÏù¥Î∏îÏùÄ Ïä§ÌÇµ
                    if table_name in cte_tables:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}ÏùÄ CTE aliasÏù¥ÎØÄÎ°ú Ïä§ÌÇµ"
                        )
                        continue

                    # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (DB + ÌååÏùº ÎÇ¥ ÏÉùÏÑ±) - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
                    schema, actual_table = self.parse_table_name(table_name)
                    if schema:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    table_exists_in_db = cursor.fetchone()[0] > 0
                    table_created_in_file = actual_table in tables_created_in_file

                    if not table_exists_in_db and not table_created_in_file:
                        issue = f"CREATE INDEX Ïã§Ìå®: ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                        result["issues"].append(issue)
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} Ïã§Ìå®: ÌÖåÏù¥Î∏î ÏóÜÏùå"
                        )
                        continue

                    # ÌÖåÏù¥Î∏îÏù¥ ÌååÏùº ÎÇ¥ÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî Í≤ΩÏö∞, Ïù∏Îç±Ïä§ Ï§ëÎ≥µ Í≤ÄÏÇ¨ Ïä§ÌÇµ
                    if table_created_in_file:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} ÏÑ±Í≥µ: ÌååÏùº ÎÇ¥ ÏÉùÏÑ± ÌÖåÏù¥Î∏î"
                        )
                        continue

                    # Í∏∞Ï°¥ ÌÖåÏù¥Î∏îÏùò Ï§ëÎ≥µ Ïù∏Îç±Ïä§ ÌôïÏù∏
                    cursor.execute(f"SHOW INDEX FROM `{table_name}`")
                    existing_indexes = cursor.fetchall()

                    # ÎèôÏùºÌïú Ïª¨ÎüºÏóê ÎåÄÌïú Ïù∏Îç±Ïä§Í∞Ä Ïù¥ÎØ∏ ÏûàÎäîÏßÄ ÌôïÏù∏
                    duplicate_found = False
                    for existing_idx in existing_indexes:
                        if existing_idx[4] == columns:  # Column_name
                            issue = f"CREATE INDEX Ïã§Ìå®: ÌÖåÏù¥Î∏î '{table_name}'Ïùò Ïª¨Îüº '{columns}'Ïóê Ïù¥ÎØ∏ Ïù∏Îç±Ïä§ '{existing_idx[2]}'Í∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§."
                            result["issues"].append(issue)
                            debug_log(
                                f"CREATE INDEX [{i}] - {table_name}.{index_name} Ïã§Ìå®: Ï§ëÎ≥µ Ïù∏Îç±Ïä§"
                            )
                            duplicate_found = True
                            break

                    if not duplicate_found:
                        debug_log(
                            f"CREATE INDEX [{i}] - {table_name}.{index_name} ÏÑ±Í≥µ: Ï§ëÎ≥µ ÏóÜÏùå"
                        )

            debug_log(
                f"üî•üî•üî• validate_individual_ddl_statements ÏôÑÎ£å: {len(result['issues'])}Í∞ú Ïù¥Ïäà üî•üî•üî•"
            )
            return result

        except Exception as e:
            debug_log(f"Í∞úÎ≥Ñ DDL Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
            result["issues"].append(f"DDL Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}")
            return result

    def extract_successful_created_tables(
        self, sql_content: str, issues: List[str]
    ) -> List[str]:
        """ÏÑ±Í≥µÌïú CREATE TABLEÎßå Ï∂îÏ∂ú (Ïã§Ìå®Ìïú Í≤ÉÏùÄ Ï†úÏô∏)"""
        created_tables = self.extract_created_tables(sql_content)
        successful_tables = []

        for table in created_tables:
            # issuesÏóêÏÑú Ìï¥Îãπ ÌÖåÏù¥Î∏îÏùò CREATE TABLE Ïã§Ìå® Î©îÏãúÏßÄÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            table_failed = any(
                f"ÌÖåÏù¥Î∏î '{table}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§" in issue for issue in issues
            )
            if not table_failed:
                successful_tables.append(table)

        return successful_tables

    def validate_dml_columns_with_context(
        self, sql_content: str, cursor, debug_log, available_tables: List[str]
    ):
        """Ïª®ÌÖçÏä§Ìä∏Î•º Í≥†Î†§Ìïú DML Ïª¨Îüº Í≤ÄÏ¶ù"""
        # Í∏∞Ï°¥ validate_dml_columns Î°úÏßÅÏùÑ ÏÇ¨Ïö©ÌïòÎêò, available_tablesÎ•º Í≥†Î†§
        # Ïù¥ Ìï®ÏàòÎäî Í∏∞Ï°¥ Ìï®ÏàòÎ•º ÌôïÏû•Ìïú Î≤ÑÏ†ÑÏûÖÎãàÎã§
        return self.validate_dml_columns(sql_content, cursor, debug_log)

    def parse_ddl_statements(self, sql_content: str) -> List[Dict[str, Any]]:
        """DDL Íµ¨Î¨∏ÏùÑ ÌååÏã±ÌïòÏó¨ Í∞úÎ≥Ñ Íµ¨Î¨∏ÏúºÎ°ú Î∂ÑÎ¶¨"""
        statements = []

        # CREATE TABLE ÌååÏã±
        create_table_pattern = (
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\("
        )
        for match in re.finditer(create_table_pattern, sql_content, re.IGNORECASE):
            statements.append({"type": "CREATE_TABLE", "table": match.group(1)})

        # CREATE INDEX ÌååÏã±
        create_index_pattern = r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(\s*`?(\w+)`?"
        for match in re.finditer(create_index_pattern, sql_content, re.IGNORECASE):
            statements.append(
                {
                    "type": "CREATE_INDEX",
                    "index_name": match.group(1),
                    "table": match.group(2),
                    "columns": match.group(3),
                }
            )

        return statements

    async def execute_explain_with_cursor(self, sql_content: str, cursor, debug_log):
        """EXPLAIN Ïã§Ìñâ (Ïª§ÏÑú ÏÇ¨Ïö©)"""
        result = {"issues": [], "explain_data": None}

        try:
            if cursor is None:
                debug_log("Ïª§ÏÑúÍ∞Ä NoneÏûÖÎãàÎã§")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
                return result

            # EXPLAIN Ïã§Ìñâ
            explain_query = f"EXPLAIN {sql_content.strip().rstrip(';')}"
            debug_log(f"EXPLAIN ÏøºÎ¶¨: {explain_query}")

            cursor.execute(explain_query)
            explain_data = cursor.fetchall()
            result["explain_data"] = explain_data

            # EXPLAIN Í≤∞Í≥ºÎäî Î¨∏ÏûêÏó¥Î°úÎßå Ï†ÄÏû•

            debug_log("EXPLAIN Ïã§Ìñâ ÏôÑÎ£å")
            return result

        except Exception as e:
            debug_log(f"EXPLAIN Ïã§Ìñâ ÏòàÏô∏: {e}")
            result["issues"].append(f"EXPLAIN Ïã§Ìñâ Ïò§Î•ò: {str(e)}")
            return result

    def check_performance_issues(self, explain_data, query_content, debug_log):
        """EXPLAIN Í≤∞Í≥ºÏóêÏÑú ÏÑ±Îä• Î¨∏Ï†ú Í≤ÄÏÇ¨"""
        debug_log("üîçüîçüîç check_performance_issues Ìï®Ïàò ÏãúÏûë üîçüîçüîç")
        performance_issues = []

        # ÏäπÏù∏Îêú ÎåÄÏö©Îüâ Î∞∞Ïπò ÏøºÎ¶¨ Ï≤¥ÌÅ¨
        batch_approval_patterns = [
            r"ÎåÄÏö©Îüâ\s*Î∞∞Ïπò.*ÏäπÏù∏",
            r"Î∞∞Ïπò.*ÏäπÏù∏.*Î∞õÏùå",
            r"ÏäπÏù∏.*ÎåÄÏö©Îüâ",
            r"approved.*batch",
            r"batch.*approved",
        ]

        is_approved_batch = False
        for pattern in batch_approval_patterns:
            if re.search(pattern, query_content, re.IGNORECASE):
                is_approved_batch = True
                debug_log(f"ÏäπÏù∏Îêú ÎåÄÏö©Îüâ Î∞∞Ïπò ÏøºÎ¶¨Î°ú Ïù∏Ïãù: {pattern}")
                break

        debug_log(f"EXPLAIN Îç∞Ïù¥ÌÑ∞ Ìñâ Ïàò: {len(explain_data)}")
        for idx, row in enumerate(explain_data):
            debug_log(f"EXPLAIN Ìñâ {idx}: {row}")
            if len(row) >= 10:  # EXPLAIN Í≤∞Í≥º Íµ¨Ï°∞ ÌôïÏù∏
                rows_examined = row[9] if row[9] is not None else 0
                debug_log(f"Í≤ÄÏÇ¨Ìï† Ìñâ Ïàò: {rows_examined}")

                if rows_examined >= self.PERFORMANCE_THRESHOLDS["critical_rows_scan"]:
                    if is_approved_batch:
                        issue = f"‚ö†Ô∏è Í≤ΩÍ≥†: ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î Ïä§Ï∫î ({rows_examined:,}Ìñâ) - ÏäπÏù∏Îêú Î∞∞Ïπò ÏûëÏóÖ"
                        performance_issues.append(issue)
                        debug_log(f"ÏäπÏù∏Îêú Î∞∞Ïπò - Í≤ΩÍ≥† Ï∂îÍ∞Ä: {issue}")
                    else:
                        issue = f"‚ùå Ïã§Ìå®: Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú - ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î Ï†ÑÏ≤¥ Ïä§Ï∫î ({rows_examined:,}Ìñâ)"
                        performance_issues.append(issue)
                        debug_log(f"Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú - Ïã§Ìå® Ï∂îÍ∞Ä: {issue}")

                elif rows_examined >= self.PERFORMANCE_THRESHOLDS["max_rows_scan"]:
                    if is_approved_batch:
                        issue = f"‚ö†Ô∏è Í≤ΩÍ≥†: ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î Ïä§Ï∫î ({rows_examined:,}Ìñâ) - ÏäπÏù∏Îêú Î∞∞Ïπò ÏûëÏóÖ"
                        performance_issues.append(issue)
                        debug_log(f"ÏäπÏù∏Îêú Î∞∞Ïπò - Í≤ΩÍ≥† Ï∂îÍ∞Ä: {issue}")
                    else:
                        issue = f"‚ùå Ïã§Ìå®: ÏÑ±Îä• Î¨∏Ï†ú - ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î Ïä§Ï∫î ({rows_examined:,}Ìñâ)"
                        performance_issues.append(issue)
                        debug_log(f"ÏÑ±Îä• Î¨∏Ï†ú - Ïã§Ìå® Ï∂îÍ∞Ä: {issue}")

        debug_log(
            f"üîçüîçüîç check_performance_issues ÏôÑÎ£å - Ïù¥Ïäà: {performance_issues}, ÏäπÏù∏: {is_approved_batch} üîçüîçüîç"
        )
        return performance_issues, is_approved_batch

    async def execute_explain_individual_queries(
        self, sql_content: str, cursor, debug_log
    ):
        """Í∞úÎ≥Ñ ÏøºÎ¶¨Î°ú Î∂ÑÎ¶¨ÌïòÏó¨ EXPLAIN Ïã§Ìñâ - CREATE Íµ¨Î¨∏ Í≥†Î†§"""
        result = {"issues": [], "explain_data": [], "performance_issues": []}

        try:
            if cursor is None:
                debug_log("Ïª§ÏÑúÍ∞Ä NoneÏûÖÎãàÎã§")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
                return result

            # ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏î Ï∂îÏ∂ú
            created_tables = self.extract_created_tables(sql_content)
            debug_log(f"ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏î: {created_tables}")

            # SQL ÌååÏùºÏùÑ Í∞úÎ≥Ñ ÏøºÎ¶¨Î°ú Î∂ÑÎ¶¨
            if sqlparse:
                statements = sqlparse.split(sql_content)
            else:
                # sqlparseÍ∞Ä ÏóÜÏúºÎ©¥ ÏÑ∏ÎØ∏ÏΩúÎ°†ÏúºÎ°ú Î∂ÑÎ¶¨
                statements = [
                    stmt.strip() for stmt in sql_content.split(";") if stmt.strip()
                ]

            debug_log(f"Ï¥ù {len(statements)}Í∞úÏùò Í∞úÎ≥Ñ ÏøºÎ¶¨Î°ú Î∂ÑÎ¶¨")

            for i, stmt in enumerate(statements):
                if not stmt.strip():
                    continue

                # Ï£ºÏÑù Ï†úÍ±∞ (ÎùºÏù∏ Ï£ºÏÑùÍ≥º Î∏îÎ°ù Ï£ºÏÑù Î™®Îëê)
                cleaned_stmt = re.sub(
                    r"--.*$", "", stmt, flags=re.MULTILINE
                )  # ÎùºÏù∏ Ï£ºÏÑù Ï†úÍ±∞
                cleaned_stmt = re.sub(
                    r"/\*.*?\*/", "", cleaned_stmt, flags=re.DOTALL
                )  # Î∏îÎ°ù Ï£ºÏÑù Ï†úÍ±∞
                cleaned_stmt = cleaned_stmt.strip()
                debug_log(
                    f"ÏøºÎ¶¨ {i+1} Ï†ïÎ¶¨ ÌõÑ: {repr(cleaned_stmt[:100])}"
                )  # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Ï∂îÍ∞Ä
                if not cleaned_stmt:
                    continue

                # DDL/DML/Í¥ÄÎ¶¨ Î™ÖÎ†πÏñ¥Îäî EXPLAIN Ïä§ÌÇµ
                ddl_pattern = re.match(
                    r"^\s*(CREATE|ALTER|DROP|RENAME)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )
                dml_pattern = re.match(
                    r"^\s*(INSERT|UPDATE|DELETE)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )
                admin_pattern = re.match(
                    r"^\s*(SHOW|DESCRIBE|DESC|USE|SET|EXPLAIN)",
                    cleaned_stmt,
                    re.IGNORECASE,
                )

                if ddl_pattern:
                    debug_log(
                        f"üî•üî•üî• ÏøºÎ¶¨ {i+1}: DDL Íµ¨Î¨∏Ïù¥ÎØÄÎ°ú EXPLAIN Ïä§ÌÇµ ({ddl_pattern.group(1).upper()}) üî•üî•üî•"
                    )
                    continue
                elif dml_pattern:
                    debug_log(
                        f"üî•üî•üî• ÏøºÎ¶¨ {i+1}: DML Íµ¨Î¨∏Ïù¥ÎØÄÎ°ú EXPLAIN Ïä§ÌÇµ ({dml_pattern.group(1).upper()}) üî•üî•üî•"
                    )
                    continue
                elif admin_pattern:
                    debug_log(
                        f"üî•üî•üî• ÏøºÎ¶¨ {i+1}: Í¥ÄÎ¶¨ Î™ÖÎ†πÏñ¥Ïù¥ÎØÄÎ°ú EXPLAIN Ïä§ÌÇµ ({admin_pattern.group(1).upper()}) üî•üî•üî•"
                    )
                    continue

                # ÏøºÎ¶¨ÏóêÏÑú Ï∞∏Ï°∞ÌïòÎäî ÌÖåÏù¥Î∏î Ï∂îÏ∂ú
                query_tables = self.extract_table_names(cleaned_stmt)

                # ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞ÌïòÎäîÏßÄ ÌôïÏù∏
                references_new_table = any(
                    table in created_tables for table in query_tables
                )

                if references_new_table:
                    debug_log(
                        f"ÏøºÎ¶¨ {i+1}: ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞ÌïòÎØÄÎ°ú EXPLAIN Ïä§ÌÇµ - ÌÖåÏù¥Î∏î: {[t for t in query_tables if t in created_tables]}"
                    )
                    continue

                try:
                    # Í∞Å ÏøºÎ¶¨Ïóê ÎåÄÌï¥ EXPLAIN Ïã§Ìñâ
                    explain_query = f"EXPLAIN {cleaned_stmt}"
                    debug_log(f"Í∞úÎ≥Ñ ÏøºÎ¶¨ {i+1} EXPLAIN: {cleaned_stmt[:100]}...")
                    debug_log(
                        f"üö®üö®üö® ÏÑ±Îä• Í≤ÄÏÇ¨ ÏΩîÎìú Î≤ÑÏ†Ñ ÌôïÏù∏ - ÏûÑÍ≥ÑÍ∞í: {self.PERFORMANCE_THRESHOLDS} üö®üö®üö®"
                    )

                    cursor.execute(explain_query)
                    explain_data = cursor.fetchall()

                    # ÏÑ±Îä• Î¨∏Ï†ú Í≤ÄÏÇ¨
                    debug_log(
                        f"üîç ÏÑ±Îä• Í≤ÄÏÇ¨ ÏãúÏûë - ÏøºÎ¶¨ {i+1}, EXPLAIN Ìñâ Ïàò: {len(explain_data)}"
                    )
                    perf_issues, is_approved = self.check_performance_issues(
                        explain_data, cleaned_stmt, debug_log
                    )
                    debug_log(
                        f"üîç ÏÑ±Îä• Í≤ÄÏÇ¨ ÏôÑÎ£å - Ïù¥Ïäà: {perf_issues}, ÏäπÏù∏Îê®: {is_approved}"
                    )

                    if perf_issues:
                        result["performance_issues"].extend(perf_issues)
                        debug_log(f"‚ö†Ô∏è ÏÑ±Îä• Ïù¥Ïäà Ï∂îÍ∞ÄÎê®: {perf_issues}")
                        # ÏäπÏù∏ÎêòÏßÄ ÏïäÏùÄ ÎåÄÏö©Îüâ Ïä§Ï∫îÏùÄ Ïò§Î•òÎ°ú Ï≤òÎ¶¨
                        if not is_approved and any(
                            "‚ùå Ïã§Ìå®" in issue for issue in perf_issues
                        ):
                            result["issues"].extend(perf_issues)
                            debug_log(f"‚ùå ÏÑ±Îä• Ïù¥ÏäàÎ•º Ïò§Î•òÎ°ú Ï≤òÎ¶¨: {perf_issues}")

                    result["explain_data"].append(
                        {
                            "query_index": i + 1,
                            "query": (
                                cleaned_stmt[:200] + "..."
                                if len(cleaned_stmt) > 200
                                else cleaned_stmt
                            ),
                            "explain_result": explain_data,
                        }
                    )
                    debug_log(f"Í∞úÎ≥Ñ ÏøºÎ¶¨ {i+1} EXPLAIN ÏÑ±Í≥µ")

                except Exception as e:
                    error_msg = f"ÏøºÎ¶¨ {i+1} EXPLAIN Ïò§Î•ò: {str(e)}"
                    debug_log(error_msg)
                    result["issues"].append(error_msg)

                    # Ïª¨Îüº Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå Ïò§Î•òÏù∏ Í≤ΩÏö∞ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                    if "Unknown column" in str(e):
                        result["issues"].append(
                            f"ÏøºÎ¶¨ {i+1}ÏóêÏÑú Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Ïª¨ÎüºÏùÑ Ï∞∏Ï°∞Ìï©ÎãàÎã§: {cleaned_stmt[:100]}..."
                        )

            debug_log(
                f"Í∞úÎ≥Ñ ÏøºÎ¶¨ EXPLAIN ÏôÑÎ£å: {len(result['explain_data'])}Í∞ú ÏÑ±Í≥µ, {len(result['issues'])}Í∞ú Ïò§Î•ò"
            )
            return result

        except Exception as e:
            debug_log(f"Í∞úÎ≥Ñ ÏøºÎ¶¨ EXPLAIN Ï†ÑÏ≤¥ ÏòàÏô∏: {e}")
            result["issues"].append(f"Í∞úÎ≥Ñ ÏøºÎ¶¨ EXPLAIN Ïã§Ìñâ Ïò§Î•ò: {str(e)}")
            return result

    async def test_individual_query_validation(
        self, database_secret: str, filename: str
    ) -> str:
        """Í∞úÎ≥Ñ ÏøºÎ¶¨ Í≤ÄÏ¶ù ÌÖåÏä§Ìä∏ Ìï®Ïàò"""
        try:
            # SQL ÌååÏùº ÏùΩÍ∏∞
            sql_file_path = os.path.join("sql", filename)
            if not os.path.exists(sql_file_path):
                return f"‚ùå SQL ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                sql_content = f.read()

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            if not connection:
                return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"

            cursor = connection.cursor()

            # Í∞úÎ≥Ñ ÏøºÎ¶¨ Í≤ÄÏ¶ù Ïã§Ìñâ
            result = await self.execute_explain_individual_queries(
                sql_content, cursor, print
            )

            # Ïó∞Í≤∞ Ï†ïÎ¶¨
            cursor.close()
            connection.close()
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return f"‚úÖ Í∞úÎ≥Ñ ÏøºÎ¶¨ Í≤ÄÏ¶ù ÏôÑÎ£å\nÏÑ±Í≥µ: {len(result['explain_data'])}Í∞ú\nÏò§Î•ò: {len(result['issues'])}Í∞ú\nÏÉÅÏÑ∏: {result}"

        except Exception as e:
            return f"‚ùå Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {str(e)}"

    async def validate_schema_with_cursor(self, ddl_content: str, cursor):
        """Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù (Ïª§ÏÑú ÏÇ¨Ïö©) - Îã§Ï§ë CREATE Íµ¨Î¨∏ Í≥†Î†§"""
        result = {"valid": True, "issues": []}

        try:
            if cursor is None:
                result["valid"] = False
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
                return result

            # ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÎì§ Ï∂îÏ∂ú
            created_tables = self.extract_created_tables(ddl_content)

            # DDL ÌÉÄÏûÖÏóê Îî∞Î•∏ Í≤ÄÏ¶ù
            ddl_type = self.extract_ddl_type(ddl_content)

            if ddl_type == "CREATE_TABLE":
                # Í∞Å CREATE TABLE Íµ¨Î¨∏Ïóê ÎåÄÌï¥ Í≤ÄÏ¶ù
                for table_name in created_tables:
                    cursor.execute("SHOW TABLES LIKE %s", (table_name,))
                    if cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§."
                        )

            elif ddl_type == "ALTER_TABLE":
                # ÌÖåÏù¥Î∏î Î≥ÄÍ≤Ω Í≤ÄÏ¶ù
                table_name = self.extract_table_name_from_alter(ddl_content)
                if table_name:
                    cursor.execute("SHOW TABLES LIKE %s", (table_name,))
                    if not cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                        )

            return result

        except Exception as e:
            result["valid"] = False
            result["issues"].append(f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}")
            return result

    async def validate_constraints_with_cursor(self, ddl_content: str, cursor):
        """Ï†úÏïΩÏ°∞Í±¥ Í≤ÄÏ¶ù (Ïª§ÏÑú ÏÇ¨Ïö©)"""
        result = {"valid": True, "issues": []}

        try:
            if cursor is None:
                result["valid"] = False
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
                return result

            # Ïô∏ÎûòÌÇ§ Í≤ÄÏ¶ù
            foreign_keys = self.extract_foreign_keys(ddl_content)
            for fk in foreign_keys:
                ref_table = fk.get("referenced_table")
                if ref_table:
                    cursor.execute("SHOW TABLES LIKE %s", (ref_table,))
                    if not cursor.fetchone():
                        result["valid"] = False
                        result["issues"].append(
                            f"Ï∞∏Ï°∞ ÌÖåÏù¥Î∏î '{ref_table}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                        )

            return result

        except Exception as e:
            result["valid"] = False
            result["issues"].append(f"Ï†úÏïΩÏ°∞Í±¥ Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}")
            return result

    async def validate_table_existence(self, sql_content: str, connection, debug_log):
        """ÌÖåÏù¥Î∏î Ï°¥Ïû¨ÏÑ± Í≤ÄÏ¶ù"""
        result = {"issues": [], "tables_checked": []}

        try:
            if connection is None:
                debug_log("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïù¥ NoneÏûÖÎãàÎã§")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïù¥ ÏóÜÏäµÎãàÎã§.")
                return result

            cursor = connection.cursor()
            if cursor is None:
                debug_log("Ïª§ÏÑú ÏÉùÏÑ± Ïã§Ìå®")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑú ÏÉùÏÑ± Ïã§Ìå®")
                return result

            # SQLÏóêÏÑú ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú
            tables = self.extract_table_names(sql_content)
            debug_log(f"Ï∂îÏ∂úÎêú ÌÖåÏù¥Î∏îÎ™Ö: {tables}")

            for table in tables:
                result["tables_checked"].append(table)

                try:
                    # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
                    cursor.execute("SHOW TABLES LIKE %s", (table,))
                    exists = cursor.fetchone()

                    if not exists:
                        issue = f"ÌÖåÏù¥Î∏î '{table}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                        result["issues"].append(issue)
                        debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ÏÑ± Í≤ÄÏ¶ù Ïã§Ìå®: {table}")
                    else:
                        debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ÏÑ± Í≤ÄÏ¶ù ÌÜµÍ≥º: {table}")
                except Exception as table_check_error:
                    debug_log(f"ÌÖåÏù¥Î∏î {table} Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò: {table_check_error}")
                    result["issues"].append(
                        f"ÌÖåÏù¥Î∏î '{table}' Í≤ÄÏ¶ù Ïò§Î•ò: {str(table_check_error)}"
                    )

            cursor.close()

        except Exception as e:
            debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ÏÑ± Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
            result["issues"].append(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ÏÑ± Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}")

        return result

    def extract_table_name_from_alter(self, ddl_content: str) -> str:
        """ALTER TABLE Íµ¨Î¨∏ÏóêÏÑú ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú"""
        # Ï£ºÏÑù Ï†úÍ±∞
        sql_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # ALTER TABLE Ìå®ÌÑ¥
        alter_pattern = r"ALTER\s+TABLE\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        match = re.search(alter_pattern, sql_clean, re.IGNORECASE)

        if match:
            return match.group(1)
        return None

    def extract_created_tables(self, sql_content: str) -> List[str]:
        """ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú"""
        tables = set()

        # Ï£ºÏÑù Ï†úÍ±∞
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE TABLE Ìå®ÌÑ¥ - Îçî Ï†ïÌôïÌïú Îß§Ïπ≠
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)

        # Ïú†Ìö®Ìïú ÌÖåÏù¥Î∏îÎ™ÖÎßå ÌïÑÌÑ∞ÎßÅ (SQL ÌÇ§ÏõåÎìú Ï†úÏô∏)
        sql_keywords = {
            "and",
            "or",
            "not",
            "in",
            "on",
            "as",
            "is",
            "if",
            "by",
            "to",
            "from",
            "where",
            "select",
            "insert",
            "update",
            "delete",
        }
        for table in create_matches:
            if table.lower() not in sql_keywords and len(table) > 1:
                tables.add(table)

        return list(tables)

    def extract_created_indexes(self, sql_content: str) -> List[str]:
        """ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî Ïù∏Îç±Ïä§Î™Ö Ï∂îÏ∂ú"""
        indexes = set()

        # Ï£ºÏÑù Ï†úÍ±∞
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # CREATE INDEX Ìå®ÌÑ¥
        index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+ON"
        )
        index_matches = re.findall(index_pattern, sql_clean, re.IGNORECASE)
        indexes.update(index_matches)

        return list(indexes)

    def extract_cte_tables(self, sql_content: str) -> List[str]:
        """WITHÏ†àÏùò CTE(Common Table Expression) ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú"""
        cte_tables = set()

        # Ï£ºÏÑù Ï†úÍ±∞
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITH RECURSIVE Ìå®ÌÑ¥ (Í∞ÄÏû• ÏùºÎ∞òÏ†Å)
        recursive_with_pattern = (
            r"WITH\s+(?:RECURSIVE\s+)?([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        )
        recursive_matches = re.findall(recursive_with_pattern, sql_clean, re.IGNORECASE)
        cte_tables.update(recursive_matches)

        # Ï∂îÍ∞Ä CTE ÌÖåÏù¥Î∏îÎì§ (ÏâºÌëú ÌõÑ)
        additional_cte_pattern = r",\s*([a-zA-Z_][a-zA-Z0-9_]*)\s+AS\s*\("
        additional_matches = re.findall(
            additional_cte_pattern, sql_clean, re.IGNORECASE
        )
        cte_tables.update(additional_matches)

        return list(cte_tables)

    def extract_foreign_keys(self, ddl_content: str) -> List[Dict[str, str]]:
        """DDLÏóêÏÑú Ïô∏ÎûòÌÇ§ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        foreign_keys = []

        # Ï£ºÏÑù Ï†úÍ±∞
        ddl_clean = re.sub(r"--.*$", "", ddl_content, flags=re.MULTILINE)
        ddl_clean = re.sub(r"/\*.*?\*/", "", ddl_clean, flags=re.DOTALL)

        # FOREIGN KEY Ìå®ÌÑ¥ Îß§Ïπ≠
        fk_pattern = r"FOREIGN\s+KEY\s*\(\s*([^)]+)\s*\)\s*REFERENCES\s+([^\s(]+)\s*\(\s*([^)]+)\s*\)"
        matches = re.finditer(fk_pattern, ddl_clean, re.IGNORECASE)

        for match in matches:
            column = match.group(1).strip().strip("`")
            ref_table = match.group(2).strip().strip("`")
            ref_column = match.group(3).strip().strip("`")

            foreign_keys.append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return foreign_keys

    def extract_table_names(self, sql_content: str) -> List[str]:
        """SQLÏóêÏÑú ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú (WITHÏ†à CTE ÌÖåÏù¥Î∏î Ï†úÏô∏)"""
        tables = set()

        # Ï£ºÏÑù Ï†úÍ±∞
        sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
        sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)

        # WITHÏ†àÏùò CTE ÌÖåÏù¥Î∏îÎì§ Ï∂îÏ∂ú
        cte_tables = set(self.extract_cte_tables(sql_content))

        # MySQL ÌÇ§ÏõåÎìúÎì§ (ÌÖåÏù¥Î∏îÎ™ÖÏù¥ ÏïÑÎãå Í≤ÉÎì§)
        mysql_keywords = {
            "CURRENT_TIMESTAMP",
            "NOW",
            "NULL",
            "TRUE",
            "FALSE",
            "DEFAULT",
            "AUTO_INCREMENT",
            "PRIMARY",
            "KEY",
            "UNIQUE",
            "INDEX",
            "FOREIGN",
            "REFERENCES",
            "ON",
            "DELETE",
            "UPDATE",
            "CASCADE",
            "SET",
            "RESTRICT",
            "NO",
            "ACTION",
            "CHECK",
            "CONSTRAINT",
            "ENUM",
            "VARCHAR",
            "INT",
            "DECIMAL",
            "DATETIME",
            "TIMESTAMP",
            "TEXT",
            "BOOLEAN",
            "TINYINT",
            "SMALLINT",
            "MEDIUMINT",
            "BIGINT",
            "FLOAT",
            "DOUBLE",
            "CHAR",
            "BINARY",
            "VARBINARY",
            "BLOB",
            "TINYBLOB",
            "MEDIUMBLOB",
            "LONGBLOB",
            "TINYTEXT",
            "MEDIUMTEXT",
            "LONGTEXT",
            "DATE",
            "TIME",
            "YEAR",
        }

        # CREATE TABLE Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        create_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s*\("
        create_matches = re.findall(create_pattern, sql_clean, re.IGNORECASE)
        for schema, table in create_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # ALTER TABLE Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        alter_pattern = r"ALTER\s+TABLE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?\s+"
        alter_matches = re.findall(alter_pattern, sql_clean, re.IGNORECASE)
        for schema, table in alter_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # DROP TABLE Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        drop_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?"
        drop_matches = re.findall(drop_pattern, sql_clean, re.IGNORECASE)
        for schema, table in drop_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # FROM Ìå®ÌÑ¥ (SELECT, DELETE) - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        from_pattern = r"\bFROM\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|WHERE|ORDER|GROUP|LIMIT|JOIN|INNER|LEFT|RIGHT|FULL|CROSS)"
        from_matches = re.findall(from_pattern, sql_clean, re.IGNORECASE)
        for schema, table in from_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # JOIN Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        join_pattern = r"\b(?:INNER\s+|LEFT\s+|RIGHT\s+|FULL\s+|CROSS\s+)?JOIN\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s+(?:AS\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\s|$|,|;|\)|ON)"
        join_matches = re.findall(join_pattern, sql_clean, re.IGNORECASE)
        for schema, table in join_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # UPDATE Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        update_pattern = r"\bUPDATE\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|SET)"
        update_matches = re.findall(update_pattern, sql_clean, re.IGNORECASE)
        for schema, table in update_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        # INSERT INTO Ìå®ÌÑ¥ - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        insert_pattern = r"\bINSERT\s+INTO\s+`?(?:([a-zA-Z_][a-zA-Z0-9_]*)\.)??`?([a-zA-Z_][a-zA-Z0-9_]*)`?(?:\s|$|,|;|\)|\()"
        insert_matches = re.findall(insert_pattern, sql_clean, re.IGNORECASE)
        for schema, table in insert_matches:
            full_table_name = f"{schema}.{table}" if schema else table
            if table not in cte_tables and table.upper() not in mysql_keywords:
                tables.add(full_table_name)

        return list(tables)

    async def execute_explain(self, sql_content: str, connection, debug_log):
        """EXPLAIN Ïã§Ìñâ Î∞è Î∂ÑÏÑù"""
        result = {"issues": [], "explain_data": None}

        try:
            if connection is None:
                debug_log("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïù¥ NoneÏûÖÎãàÎã§")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïù¥ ÏóÜÏäµÎãàÎã§.")
                return result

            cursor = connection.cursor(dictionary=True)
            if cursor is None:
                debug_log("Ïª§ÏÑú ÏÉùÏÑ± Ïã§Ìå®")
                result["issues"].append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑú ÏÉùÏÑ± Ïã§Ìå®")
                return result

            # Ï£ºÏÑù Ï†úÍ±∞ÌïòÍ≥† Ïã§Ï†ú SQLÎßå Ï∂îÏ∂ú
            sql_clean = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)
            sql_clean = re.sub(r"/\*.*?\*/", "", sql_clean, flags=re.DOTALL)
            sql_clean = sql_clean.strip()

            if sql_clean.endswith(";"):
                sql_clean = sql_clean[:-1]

            explain_sql = f"EXPLAIN {sql_clean}"
            debug_log(f"EXPLAIN Ïã§Ìñâ: {explain_sql}")

            cursor.execute(explain_sql)
            explain_result = cursor.fetchall()
            result["explain_data"] = explain_result

            debug_log(f"EXPLAIN Í≤∞Í≥º: {explain_result}")

            # EXPLAIN Í≤∞Í≥ºÎäî Î¨∏ÏûêÏó¥Î°úÎßå Ï†ÄÏû•
            cursor.close()

        except Exception as e:
            debug_log(f"EXPLAIN Ïã§Ìñâ Ïò§Î•ò: {e}")
            result["issues"].append(f"EXPLAIN Ïã§Ìñâ Ïò§Î•ò: {str(e)}")

        return result

    def get_db_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
        db_instance_identifier: str = None,
    ):
        """Í≥µÌÜµ DB Ïó∞Í≤∞ Ìï®Ïàò"""
        if mysql is None:
            raise Exception(
                "mysql-connector-pythonÏù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. pip install mysql-connector-pythonÏùÑ Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî."
            )

        # SecretÏóêÏÑú DB Ïó∞Í≤∞ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
        session = boto3.session.Session()
        client = session.client(
            service_name="secretsmanager",
            region_name="ap-northeast-2",
            verify=False,
        )
        get_secret_value_response = client.get_secret_value(SecretId=database_secret)
        secret = get_secret_value_response["SecretString"]
        db_config = json.loads(secret)

        connection_config = None
        tunnel_used = False

        # ÏÑ†ÌÉùÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
        database_name = selected_database or db_config.get(
            "dbname", db_config.get("database")
        )
        # database_nameÏù¥ NoneÏù¥ ÏïÑÎãå Í≤ΩÏö∞ÏóêÎßå Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
        if database_name is not None:
            database_name = str(database_name)

        # db_instance_identifierÍ∞Ä Ï†úÍ≥µÎêòÎ©¥ Ìï¥Îãπ Ïù∏Ïä§ÌÑ¥Ïä§ ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÇ¨Ïö©
        host = db_config.get("host")
        if db_instance_identifier:
            # ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏóîÎìúÌè¨Ïù∏Ìä∏Î•º Ïù∏Ïä§ÌÑ¥Ïä§ ÏóîÎìúÌè¨Ïù∏Ìä∏Î°ú Î≥ÄÍ≤Ω
            if ".cluster-" in host:
                # aurora-cluster.cluster-xxx.region.rds.amazonaws.com -> instance-id.xxx.region.rds.amazonaws.com
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    # cluster- Î∂ÄÎ∂ÑÏùÑ Ï†úÍ±∞ÌïòÍ≥† Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ°ú ÍµêÏ≤¥
                    host_parts[1] = host_parts[1].replace("cluster-", "")
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"
            else:
                # Îã®Ïùº Ïù∏Ïä§ÌÑ¥Ïä§Ïù∏ Í≤ΩÏö∞ Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ°ú ÍµêÏ≤¥
                host_parts = host.split(".")
                if len(host_parts) >= 4:
                    host = f"{db_instance_identifier}.{'.'.join(host_parts[1:])}"

        if use_ssh_tunnel:
            if self.setup_ssh_tunnel(host):
                connection_config = {
                    "host": "localhost",
                    "port": 3307,
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "database": database_name,
                    "connection_timeout": 10,
                }
                tunnel_used = True

        if not connection_config:
            connection_config = {
                "host": host,
                "port": db_config.get("port", 3306),
                "user": db_config.get("username"),
                "password": db_config.get("password"),
                "database": database_name,
                "connection_timeout": 10,
            }

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def setup_shared_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
        db_instance_identifier: str = None,
    ):
        """Í≥µÏö© DB Ïó∞Í≤∞ ÏÑ§Ï†ï (Ìïú Î≤àÎßå Ìò∏Ï∂ú)"""
        try:
            if self.shared_connection and self.shared_connection.is_connected():
                logger.info("Ïù¥ÎØ∏ ÌôúÏÑ±ÌôîÎêú Í≥µÏö© Ïó∞Í≤∞Ïù¥ ÏûàÏäµÎãàÎã§.")
                return True

            self.shared_connection, self.tunnel_used = self.get_db_connection(
                database_secret,
                selected_database,
                use_ssh_tunnel,
                db_instance_identifier,
            )

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_cursor = self.shared_connection.cursor()
                # Ïó∞Í≤∞Îêú Ìò∏Ïä§Ìä∏ Ï†ïÎ≥¥ Î°úÍπÖ
                host_info = (
                    f"Ïù∏Ïä§ÌÑ¥Ïä§: {db_instance_identifier}"
                    if db_instance_identifier
                    else "ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏóîÎìúÌè¨Ïù∏Ìä∏"
                )
                logger.info(
                    f"Í≥µÏö© DB Ïó∞Í≤∞ ÏÑ§Ï†ï ÏôÑÎ£å - {host_info} (ÌÑ∞ÎÑê: {self.tunnel_used})"
                )
                return True
            else:
                logger.error("Í≥µÏö© DB Ïó∞Í≤∞ Ïã§Ìå®")
                return False

        except Exception as e:
            logger.error(f"Í≥µÏö© DB Ïó∞Í≤∞ ÏÑ§Ï†ï Ïò§Î•ò: {e}")
            return False

    def cleanup_shared_connection(self):
        """Í≥µÏö© DB Ïó∞Í≤∞ Ï†ïÎ¶¨"""
        try:
            if self.shared_cursor:
                self.shared_cursor.close()
                self.shared_cursor = None
                logger.info("Í≥µÏö© Ïª§ÏÑú Îã´Í∏∞ ÏôÑÎ£å")

            if self.shared_connection and self.shared_connection.is_connected():
                self.shared_connection.close()
                self.shared_connection = None
                logger.info("Í≥µÏö© DB Ïó∞Í≤∞ Îã´Í∏∞ ÏôÑÎ£å")

            if self.tunnel_used:
                self.cleanup_ssh_tunnel()
                self.tunnel_used = False
                logger.info("SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨ ÏôÑÎ£å")

        except Exception as e:
            logger.error(f"Í≥µÏö© Ïó∞Í≤∞ Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò: {e}")

    def get_shared_cursor(self):
        """Í≥µÏö© Ïª§ÏÑú Î∞òÌôò"""
        if self.shared_cursor is None:
            logger.error(
                "Í≥µÏö© Ïª§ÏÑúÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. setup_shared_connection()ÏùÑ Î®ºÏ†Ä Ìò∏Ï∂úÌïòÏÑ∏Ïöî."
            )
            return None
        return self.shared_cursor

    async def list_sql_files(self) -> str:
        """SQL ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ÎîîÎ†âÌÜ†Î¶¨Ïóê SQL ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL ÌååÏùº Î™©Î°ù:\n{file_list}"
        except Exception as e:
            return f"SQL ÌååÏùº Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Î™©Î°ù Ï°∞Ìöå"""
        try:
            secrets = self.get_secrets_by_keyword(keyword)
            if not secrets:
                return (
                    f"'{keyword}' ÌÇ§ÏõåÎìúÎ°ú Ï∞æÏùÄ ÏãúÌÅ¨Î¶øÏù¥ ÏóÜÏäµÎãàÎã§."
                    if keyword
                    else "ÏãúÌÅ¨Î¶øÏù¥ ÏóÜÏäµÎãàÎã§."
                )

            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Î™©Î°ù:\n{secret_list}"
        except Exception as e:
            return f"ÏãúÌÅ¨Î¶ø Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> str:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                # SHOW DATABASES Ïã§Ìñâ
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # ÌòÑÏû¨ DBÏùò ÌÖåÏù¥Î∏î Î™©Î°ù
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = f"""‚úÖ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ!

**Ïó∞Í≤∞ Ï†ïÎ≥¥:**
- ÏÑúÎ≤Ñ Î≤ÑÏ†Ñ: {db_info}
- ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§: {current_db}
- Ïó∞Í≤∞ Î∞©Ïãù: {'SSH Tunnel' if tunnel_used else 'Direct'}

**Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù:**"""
                for db in databases:
                    if db not in [
                        "information_schema",
                        "performance_schema",
                        "mysql",
                        "sys",
                    ]:
                        result += f"\n   - {db}"

                if tables:
                    result += f"\n\n**ÌòÑÏû¨ DB ÌÖåÏù¥Î∏î Î™©Î°ù:**"
                    for table in tables:
                        result += f"\n   - {table}"

                # SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"‚ùå MySQL Ïò§Î•ò: {str(e)}"
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return f"‚ùå Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ Ïò§Î•ò: {str(e)}"

    async def list_databases(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> str:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù Ï°∞Ìöå"""
        try:
            if mysql is None:
                raise Exception("mysql-connector-pythonÏù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")

            # SecretÏóêÏÑú DB Ïó∞Í≤∞ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)

            connection_config = None
            tunnel_used = False

            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get("host")):
                    connection_config = {
                        "host": "localhost",
                        "port": 3307,
                        "user": db_config.get("username"),
                        "password": db_config.get("password"),
                        "connection_timeout": 10,
                    }
                    tunnel_used = True

            if not connection_config:
                connection_config = {
                    "host": db_config.get("host"),
                    "port": db_config.get("port", 3306),
                    "user": db_config.get("username"),
                    "password": db_config.get("password"),
                    "connection_timeout": 10,
                }

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏóÜÏù¥ Ïó∞Í≤∞
            connection = mysql.connector.connect(**connection_config)
            cursor = connection.cursor()

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù Ï°∞Ìöå
            cursor.execute("SHOW DATABASES")
            databases = [
                db[0]
                for db in cursor.fetchall()
                if db[0]
                not in ["information_schema", "performance_schema", "mysql", "sys"]
            ]

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            result = "üìã ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù:\n\n"
            for i, db in enumerate(databases, 1):
                result += f"{i}. {db}\n"
            result += f"\nÏ¥ù {len(databases)}Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Í∞Ä ÏûàÏäµÎãàÎã§."
            result += "\n\nüí° ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º ÏÑ†ÌÉùÌïòÎ†§Î©¥ Î≤àÌò∏ÎÇò Ïù¥Î¶ÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî."

            return result

        except Exception as e:
            return f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def select_database(
        self, database_secret: str, database_selection: str, use_ssh_tunnel: bool = True
    ) -> str:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ†ÌÉù (USE Î™ÖÎ†πÏñ¥ Ïã§Ìñâ)"""
        try:
            # Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ùÏùÑ Í∞ÄÏ†∏ÏôÄÏÑú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
            db_list_result = await self.list_databases(database_secret, use_ssh_tunnel)

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ùÏóêÏÑú Ïã§Ï†ú DB Ïù¥Î¶ÑÎì§ Ï∂îÏ∂ú
            lines = db_list_result.split("\n")
            databases = []
            for line in lines:
                if line.strip() and line[0].isdigit() and ". " in line:
                    db_name = line.split(". ", 1)[1]
                    databases.append(db_name)

            selected_db = None

            # Î≤àÌò∏Î°ú ÏÑ†ÌÉùÌïú Í≤ΩÏö∞
            if database_selection.isdigit():
                index = int(database_selection) - 1
                if 0 <= index < len(databases):
                    selected_db = databases[index]
                else:
                    return f"‚ùå ÏûòÎ™ªÎêú Î≤àÌò∏ÏûÖÎãàÎã§. 1-{len(databases)} Î≤îÏúÑÏóêÏÑú ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.\n\n{db_list_result}"
            else:
                # Ïù¥Î¶ÑÏúºÎ°ú ÏÑ†ÌÉùÌïú Í≤ΩÏö∞
                if database_selection in databases:
                    selected_db = database_selection
                else:
                    return f"‚ùå '{database_selection}' Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\n\n{db_list_result}"

            # ÏÑ†ÌÉùÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î°ú USE Î™ÖÎ†πÏñ¥ Ïã§Ìñâ
            connection, tunnel_used = self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )

            if connection.is_connected():
                cursor = connection.cursor()

                # USE Î™ÖÎ†πÏñ¥ Ïã§Ìñâ
                cursor.execute(f"USE `{selected_db}`")

                # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌôïÏù∏
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                cursor.close()
                connection.close()

                # ÏÑ†ÌÉùÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû•
                self.selected_database = selected_db

                result = f"‚úÖ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ '{selected_db}' ÏÑ†ÌÉù ÏôÑÎ£å!\n\n"
                result += f"üîó ÌòÑÏû¨ ÌôúÏÑ± Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§: {current_db}\n"
                result += f"üí° Ïù¥Ï†ú Ïù¥ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÎåÄÌï¥ Ïä§ÌÇ§Îßà Î∂ÑÏÑùÏù¥ÎÇò SQL Í≤ÄÏ¶ùÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§."

                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                return f"‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"

        except Exception as e:
            logger.error(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ†ÌÉù Ïò§Î•ò: {e}")
            return f"‚ùå Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """ÌòÑÏû¨ Ïä§ÌÇ§Îßà ÏöîÏïΩ Ï†ïÎ≥¥ Î∞òÌôò"""
        try:
            # ÏßÅÏ†ë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ìï¥ÏÑú Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï°∞Ìöå
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌôïÏù∏
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            # ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ ÏàòÏßë
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            summary = f"""üìä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïä§ÌÇ§Îßà ÏöîÏïΩ (DB: {current_db})

üìã **ÌÖåÏù¥Î∏î Î™©Î°ù** ({len(tables_info)}Í∞ú):"""

            for table_info in tables_info:
                table_name = table_info[0]
                table_type = table_info[1]
                engine = table_info[2]
                rows = table_info[3] or 0
                comment = table_info[6] or ""

                # Ïª¨Îüº Ïàò Ï°∞Ìöå
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                column_count = cursor.fetchone()[0]

                # Ïù∏Îç±Ïä§ Ïàò Ï°∞Ìöå
                cursor.execute(
                    """
                    SELECT COUNT(DISTINCT index_name) FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                """,
                    (table_name,),
                )
                index_count = cursor.fetchone()[0]

                summary += f"""
  üîπ **{table_name}** ({engine})
     - Ïª¨Îüº: {column_count}Í∞ú, Ïù∏Îç±Ïä§: {index_count}Í∞ú
     - ÏòàÏÉÅ Ìñâ Ïàò: {rows:,}"""

                if comment:
                    summary += f"\n     - ÏÑ§Î™Ö: {comment}"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return summary

        except Exception as e:
            return f"‚ùå Ïä§ÌÇ§Îßà ÏöîÏïΩ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"

    async def get_table_schema(self, database_secret: str, table_name: str) -> str:
        """ÌäπÏ†ï ÌÖåÏù¥Î∏îÏùò ÏÉÅÏÑ∏ Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"‚ùå ÌÖåÏù¥Î∏î '{table_name}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            # Ïª¨Îüº Ï†ïÎ≥¥ Ï°∞Ìöå
            cursor.execute(
                """
                SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT, 
                       COLUMN_COMMENT, COLUMN_KEY, EXTRA
                FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY ORDINAL_POSITION
            """,
                (table_name,),
            )

            columns = cursor.fetchall()

            result = f"üìã **ÌÖåÏù¥Î∏î '{table_name}' Ïä§ÌÇ§Îßà Ï†ïÎ≥¥**\n\n"
            result += f"üìä **Ïª¨Îüº Î™©Î°ù** ({len(columns)}Í∞ú):\n"

            for col in columns:
                col_name, data_type, is_nullable, default_val, comment, key, extra = col

                result += f"\nüîπ **{col_name}**\n"
                result += f"   - ÌÉÄÏûÖ: {data_type}\n"
                result += (
                    f"   - NULL ÌóàÏö©: {'Ïòà' if is_nullable == 'YES' else 'ÏïÑÎãàÏò§'}\n"
                )

                if default_val is not None:
                    result += f"   - Í∏∞Î≥∏Í∞í: {default_val}\n"

                if key:
                    key_type = {"PRI": "Í∏∞Î≥∏ÌÇ§", "UNI": "Í≥†Ïú†ÌÇ§", "MUL": "Ïù∏Îç±Ïä§"}.get(
                        key, key
                    )
                    result += f"   - ÌÇ§ ÌÉÄÏûÖ: {key_type}\n"

                if extra:
                    result += f"   - Ï∂îÍ∞Ä ÏÜçÏÑ±: {extra}\n"

                if comment:
                    result += f"   - ÏÑ§Î™Ö: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"‚ùå ÌÖåÏù¥Î∏î Ïä§ÌÇ§Îßà Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def get_table_index(self, database_secret: str, table_name: str) -> str:
        """ÌäπÏ†ï ÌÖåÏù¥Î∏îÏùò Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            if cursor.fetchone()[0] == 0:
                return f"‚ùå ÌÖåÏù¥Î∏î '{table_name}'ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            # Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå
            cursor.execute(
                """
                SELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, NON_UNIQUE, 
                       INDEX_TYPE, CARDINALITY, NULLABLE, INDEX_COMMENT
                FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s
                ORDER BY INDEX_NAME, SEQ_IN_INDEX
            """,
                (table_name,),
            )

            indexes = cursor.fetchall()

            if not indexes:
                result = (
                    f"üìã **ÌÖåÏù¥Î∏î '{table_name}' Ïù∏Îç±Ïä§ Ï†ïÎ≥¥**\n\n‚ùå Ïù∏Îç±Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§."
                )
            else:
                result = f"üìã **ÌÖåÏù¥Î∏î '{table_name}' Ïù∏Îç±Ïä§ Ï†ïÎ≥¥**\n\n"

                # Ïù∏Îç±Ïä§Î≥ÑÎ°ú Í∑∏Î£πÌôî
                index_groups = {}
                for idx in indexes:
                    idx_name = idx[0]
                    if idx_name not in index_groups:
                        index_groups[idx_name] = []
                    index_groups[idx_name].append(idx)

                result += f"üìä **Ïù∏Îç±Ïä§ Î™©Î°ù** ({len(index_groups)}Í∞ú):\n"

                for idx_name, idx_cols in index_groups.items():
                    first_col = idx_cols[0]
                    is_unique = "Í≥†Ïú†" if first_col[3] == 0 else "ÏùºÎ∞ò"
                    idx_type = first_col[4]
                    comment = first_col[7] or ""

                    result += f"\nüîπ **{idx_name}** ({is_unique} Ïù∏Îç±Ïä§)\n"
                    result += f"   - ÌÉÄÏûÖ: {idx_type}\n"

                    # Ïª¨Îüº Î™©Î°ù
                    columns = [f"{col[1]}" for col in idx_cols]
                    result += f"   - Ïª¨Îüº: {', '.join(columns)}\n"

                    if comment:
                        result += f"   - ÏÑ§Î™Ö: {comment}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"‚ùå ÌÖåÏù¥Î∏î Ïù∏Îç±Ïä§ Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå"""
        try:
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database
            )
            cursor = connection.cursor()

            result = f"üìä **Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î©îÌä∏Î¶≠**\n\n"

            if metric_type in ["all", "query"]:
                # ÏøºÎ¶¨ ÏÑ±Îä• ÌÜµÍ≥Ñ
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "üîç **ÎäêÎ¶∞ ÏøºÎ¶¨ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - Ïã§ÌñâÌöüÏàò: {count:,}, ÌèâÍ∑†ÏãúÍ∞Ñ: {avg_time:.3f}Ï¥à, ÏµúÎåÄÏãúÍ∞Ñ: {max_time:.3f}Ï¥à\n\n"

            if metric_type in ["all", "connection"]:
                # Ïó∞Í≤∞ ÌÜµÍ≥Ñ
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"üîó **Ïó∞Í≤∞ ÌÜµÍ≥Ñ:**\n"
                    result += f"- Ï¥ù Ïó∞Í≤∞: {conn_stats[0]}Í∞ú\n"
                    result += f"- ÌôúÏÑ± Ïó∞Í≤∞: {conn_stats[1]}Í∞ú\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"‚ùå ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    # === DDL Í≤ÄÏ¶ù Í¥ÄÎ†® Î©îÏÑúÎìú ===

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """ÌäπÏ†ï SQL ÌååÏùº Í≤ÄÏ¶ù"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            result = await self.validate_ddl(ddl_content, database_secret, filename)
            return result
        except Exception as e:
            return f"SQL ÌååÏùº Í≤ÄÏ¶ù Ïã§Ìå®: {str(e)}"

    async def validate_ddl(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """DDL/DML Í≤ÄÏ¶ù Ïã§Ìñâ (Ïó∞Í≤∞ Ïû¨ÏÇ¨Ïö© Ìå®ÌÑ¥ Ï†ÅÏö©)"""
        try:
            # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ ÌååÏùº ÏÉùÏÑ±
            debug_log_path = (
                LOGS_DIR
                / f"debug_log_{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            )

            debug_log(f"validate_ddl ÏãúÏûë - ÌååÏùº: {filename}")
            debug_log(f"SQL ÎÇ¥Ïö©: {ddl_content.strip()}")

            issues = []
            db_connection_info = None
            schema_validation = None
            claude_analysis_result = None  # Claude Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•Ïö©
            constraint_validation = None
            explain_result = None

            # Î≥ÄÏàò Ï¥àÍ∏∞Ìôî
            dml_column_issues = []

            # 1. Í∏∞Î≥∏ Î¨∏Î≤ï Í≤ÄÏ¶ù - Í∞úÏÑ†Îêú ÏÑ∏ÎØ∏ÏΩúÎ°† Í≤ÄÏ¶ù
            semicolon_valid = self.validate_semicolon_usage(ddl_content)
            if not semicolon_valid:
                issues.append("ÏÑ∏ÎØ∏ÏΩúÎ°†Ïù¥ ÎàÑÎùΩÎêòÏóàÏäµÎãàÎã§.")
                debug_log("ÏÑ∏ÎØ∏ÏΩúÎ°† Í≤ÄÏ¶ù Ïã§Ìå®")
            else:
                debug_log("ÏÑ∏ÎØ∏ÏΩúÎ°† Í≤ÄÏ¶ù ÌÜµÍ≥º")

            # 2. SQL ÌÉÄÏûÖ ÌôïÏù∏
            sql_type = self.extract_ddl_type(ddl_content, debug_log)
            debug_log(f"SQL ÌÉÄÏûÖ: {sql_type}")

            # 3. SQL ÌÉÄÏûÖÏóê Îî∞Î•∏ Í≤ÄÏ¶ù Î∂ÑÍ∏∞
            ddl_types = [
                "CREATE_TABLE",
                "ALTER_TABLE",
                "CREATE_INDEX",
                "DROP_TABLE",
                "DROP_INDEX",
            ]
            dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT", "MIXED_SELECT"]
            skip_types = ["SHOW", "SET", "USE"]  # Ïä§ÌÇµÌï† SQL ÌÉÄÏûÖ

            if database_secret:
                try:
                    debug_log("Í≥µÏö© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ§Ï†ï ÏãúÏûë")

                    # Í≥µÏö© Ïó∞Í≤∞Ïù¥ ÏóÜÏúºÎ©¥ ÏÑ§Ï†ï
                    if (
                        not self.shared_connection
                        or not self.shared_connection.is_connected()
                    ):
                        if not self.setup_shared_connection(
                            database_secret, self.selected_database
                        ):
                            debug_log("Í≥µÏö© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®")
                            issues.append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.")
                        else:
                            debug_log("Í≥µÏö© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ")

                    # Í≥µÏö© Ïª§ÏÑú ÏÇ¨Ïö©
                    cursor = self.get_shared_cursor()
                    if cursor:
                        debug_log("Í≥µÏö© Ïª§ÏÑú ÏÇ¨Ïö© ÏãúÏûë")

                        # WITHÏ†à ÌôïÏù∏
                        cte_tables = self.extract_cte_tables(ddl_content)
                        has_with_clause = len(cte_tables) > 0
                        debug_log(f"WITHÏ†à CTE ÌÖåÏù¥Î∏î: {cte_tables}")
                        debug_log(f"WITHÏ†à Ï°¥Ïû¨ Ïó¨Î∂Ä: {has_with_clause}")

                        # SQL ÌÉÄÏûÖÎ≥Ñ Í≤ÄÏ¶ù Î∂ÑÍ∏∞
                        if sql_type in skip_types:
                            debug_log(
                                f"SQL ÌÉÄÏûÖ Ïä§ÌÇµ: {sql_type} (SHOW/SET/USE Íµ¨Î¨∏ÏùÄ Í≤ÄÏ¶ùÌïòÏßÄ ÏïäÏùå)"
                            )

                        # DDL Í≤ÄÏ¶ù
                        elif sql_type in ddl_types:
                            debug_log(f"DDL Í≤ÄÏ¶ù ÏàòÌñâ: {sql_type}")
                            debug_log("=== ÏÉàÎ°úÏö¥ Í∞úÎ≥Ñ DDL Í≤ÄÏ¶ù Î°úÏßÅ ÏãúÏûë ===")

                            # Í∞úÎ≥Ñ DDL Íµ¨Î¨∏ Í≤ÄÏ¶ù (WITHÏ†àÍ≥º Í¥ÄÍ≥ÑÏóÜÏù¥ Ïã§Ìñâ)
                            ddl_validation = (
                                await self.validate_individual_ddl_statements(
                                    ddl_content, cursor, debug_log, cte_tables
                                )
                            )
                            debug_log(
                                f"Í∞úÎ≥Ñ DDL Í≤ÄÏ¶ù ÏôÑÎ£å: {len(ddl_validation['issues'])}Í∞ú Ïù¥Ïäà"
                            )
                            if ddl_validation["issues"]:
                                issues.extend(ddl_validation["issues"])

                        # DQL(DML) Í≤ÄÏ¶ù - MIXED_SELECT Ìè¨Ìï®
                        elif sql_type in dql_types:
                            debug_log(f"DQL Í≤ÄÏ¶ù ÏàòÌñâ: {sql_type}")

                            # MIXED_SELECTÏù∏ Í≤ΩÏö∞ DDLÍ≥º DML Î™®Îëê Í≤ÄÏ¶ù
                            if sql_type == "MIXED_SELECT":
                                debug_log("=== ÌòºÌï© SQL ÌååÏùº Í≤ÄÏ¶ù ÏãúÏûë ===")

                                # 1. DDL Íµ¨Î¨∏ Í≤ÄÏ¶ù
                                debug_log("ÌòºÌï© ÌååÏùº ÎÇ¥ DDL Íµ¨Î¨∏ Í≤ÄÏ¶ù ÏãúÏûë")
                                ddl_validation = (
                                    await self.validate_individual_ddl_statements(
                                        ddl_content, cursor, debug_log, cte_tables
                                    )
                                )
                                debug_log(
                                    f"ÌòºÌï© ÌååÏùº DDL Í≤ÄÏ¶ù ÏôÑÎ£å: {len(ddl_validation['issues'])}Í∞ú Ïù¥Ïäà"
                                )
                                if ddl_validation["issues"]:
                                    issues.extend(ddl_validation["issues"])

                            # DML Í≤ÄÏ¶ù (CTE aliasÎäî Ïä§ÌÇµÌïòÎêò, Ïã§Ï†ú ÌÖåÏù¥Î∏îÏùÄ Í≤ÄÏ¶ù)
                            debug_log("Í∞úÎ≥Ñ ÏøºÎ¶¨ EXPLAIN Ìï®Ïàò Ìò∏Ï∂ú ÏãúÏûë")
                            explain_result = (
                                await self.execute_explain_individual_queries(
                                    ddl_content, cursor, debug_log
                                )
                            )
                            debug_log(
                                f"Í∞úÎ≥Ñ ÏøºÎ¶¨ EXPLAIN Ìï®Ïàò Ìò∏Ï∂ú ÏôÑÎ£å: {explain_result}"
                            )
                            if explain_result["issues"]:
                                issues.extend(explain_result["issues"])

                            # ÏÑ±Îä• Ïù¥Ïäà Ï≤òÎ¶¨
                            if (
                                "performance_issues" in explain_result
                                and explain_result["performance_issues"]
                            ):
                                debug_log(
                                    f"ÏÑ±Îä• Ïù¥Ïäà Î∞úÍ≤¨: {explain_result['performance_issues']}"
                                )
                                # ÏÑ±Îä• Ïù¥ÏäàÍ∞Ä ÏûàÏúºÎ©¥ Ï†ÑÏ≤¥ Í≤ÄÏ¶ù Ïã§Ìå®Î°ú Ï≤òÎ¶¨
                                for perf_issue in explain_result["performance_issues"]:
                                    if "‚ùå Ïã§Ìå®" in perf_issue:
                                        issues.append(perf_issue)

                        else:
                            debug_log(f"Ïïå Ïàò ÏóÜÎäî SQL ÌÉÄÏûÖ: {sql_type}")

                    else:
                        debug_log("Í≥µÏö© Ïª§ÏÑúÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§")
                        issues.append("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª§ÏÑúÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.")

                except Exception as e:
                    debug_log(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
                    issues.append(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}")

            # 3.5. DML ÏøºÎ¶¨Ïùò Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä Í≤ÄÏ¶ù
            dml_column_issues = []
            if database_secret:  # sql_type Ï°∞Í±¥ Ï†úÍ±∞ÌïòÏó¨ Î™®Îì† Í≤ΩÏö∞Ïóê Ïã§Ìñâ
                try:
                    debug_log("DML Ïª¨Îüº Í≤ÄÏ¶ù ÏãúÏûë")
                    cursor = self.get_shared_cursor()
                    if cursor:
                        # ÏÑ±Í≥µÌïú CREATE TABLEÎßå Ï∂îÏ∂ú (Ïã§Ìå®Ìïú Í≤ÉÏùÄ Ï†úÏô∏)
                        successful_created_tables = (
                            self.extract_successful_created_tables(ddl_content, issues)
                        )
                        debug_log(f"ÏÑ±Í≥µÌïú CREATE TABLE: {successful_created_tables}")

                        # CTE alias Ï∂îÏ∂ú
                        cte_tables = self.extract_cte_tables(ddl_content)
                        debug_log(f"CTE alias ÌÖåÏù¥Î∏î: {cte_tables}")

                        # DML Ïª¨Îüº Í≤ÄÏ¶ù (ÏÑ±Í≥µÌïú ÌÖåÏù¥Î∏îÍ≥º CTE aliasÎäî Ïä§ÌÇµ)
                        available_tables = successful_created_tables + cte_tables
                        dml_validation_result = self.validate_dml_columns_with_context(
                            ddl_content, cursor, debug_log, available_tables
                        )
                        debug_log(f"DML Í≤ÄÏ¶ù Í≤∞Í≥º: {dml_validation_result}")
                        if dml_validation_result["queries_with_issues"] > 0:
                            for query_result in dml_validation_result["results"]:
                                for issue in query_result["issues"]:
                                    dml_column_issues.append(
                                        {
                                            "type": "COLUMN_NOT_EXISTS",
                                            "severity": "ERROR",
                                            "message": issue["message"],
                                            "table": issue["table"],
                                            "column": issue["column"],
                                            "query_type": sql_type,
                                        }
                                    )
                                    debug_log(f"DML Ïª¨Îüº Ïò§Î•ò Î∞úÍ≤¨: {issue['message']}")

                        debug_log(
                            f"DML Ïª¨Îüº Í≤ÄÏ¶ù ÏôÑÎ£å: {len(dml_column_issues)}Í∞ú Ïù¥Ïäà Î∞úÍ≤¨"
                        )
                    else:
                        debug_log("DML Ïª¨Îüº Í≤ÄÏ¶ù Í±¥ÎÑàÎúÄ: Í≥µÏö© Ïª§ÏÑú ÏóÜÏùå")
                except Exception as e:
                    debug_log(f"DML Ïª¨Îüº Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
                    import traceback

                    debug_log(f"DML Ïª¨Îüº Í≤ÄÏ¶ù Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§: {traceback.format_exc()}")

            # 4. ClaudeÎ•º ÌÜµÌïú Í≤ÄÏ¶ù
            try:
                debug_log("Claude Í≤ÄÏ¶ù ÏãúÏûë")
                # Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú
                relevant_schema_info = None
                explain_info_str = None

                if database_secret:
                    try:
                        relevant_schema_info = await self.extract_relevant_schema_info(
                            ddl_content, database_secret, debug_log
                        )
                        debug_log(f"Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏôÑÎ£å: {relevant_schema_info}")

                        # EXPLAIN Í≤∞Í≥ºÎ•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôòÌïòÏó¨ Î≥ÑÎèÑ Ï≤òÎ¶¨
                        if explain_result:
                            explain_info_str = (
                                f"EXPLAIN Î∂ÑÏÑù Í≤∞Í≥º: {str(explain_result)}"
                            )
                            debug_log(
                                f"EXPLAIN Ï†ïÎ≥¥ Î¨∏ÏûêÏó¥ Î≥ÄÌôò ÏôÑÎ£å: {explain_info_str}"
                            )

                    except Exception as e:
                        debug_log(f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå®: {e}")

                # Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥º ÏöîÏïΩ ÏÉùÏÑ±
                schema_validation_summary = self.create_schema_validation_summary(
                    issues, dml_column_issues
                )
                debug_log(f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù ÏöîÏïΩ ÏÉùÏÑ±: {schema_validation_summary}")

                claude_result = await self.validate_with_claude(
                    ddl_content,
                    database_secret,
                    relevant_schema_info,
                    explain_info_str,
                    sql_type,
                    schema_validation_summary,
                )
                debug_log(f"Claude Í≤ÄÏ¶ù Í≤∞Í≥º: {claude_result}")
                debug_log(f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï†ÑÎã¨Îê®: {relevant_schema_info is not None}")

                # Claude Í≤∞Í≥ºÎ•º Ìï≠ÏÉÅ Ï†ÄÏû• (ÏÑ±Í≥µ/Ïã§Ìå® ÏÉÅÍ¥ÄÏóÜÏù¥)
                claude_analysis_result = claude_result

                # Claude ÏùëÎãµ Î∂ÑÏÑù - Îçî ÏóÑÍ≤©Ìïú Í≤ÄÏ¶ù
                if "Ïò§Î•ò:" in claude_result or "Ï°¥Ïû¨ÌïòÏßÄ Ïïä" in claude_result:
                    issues.append(f"Claude Í≤ÄÏ¶ù: {claude_result}")
                    debug_log("Claude Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•ò Î∞úÍ≤¨")
                elif claude_result.startswith("Í≤ÄÏ¶ù ÌÜµÍ≥º"):
                    debug_log("Claude Í≤ÄÏ¶ù ÌÜµÍ≥º")
                else:
                    debug_log("Claude Í≤ÄÏ¶ù ÏôÑÎ£å")

            except Exception as e:
                logger.error(f"Claude Í≤ÄÏ¶ù Ïò§Î•ò: {e}")
                issues.append(f"Claude Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
                debug_log(f"Claude Í≤ÄÏ¶ù ÏòàÏô∏: {e}")
                # ÏòàÏô∏ Î∞úÏÉù ÏãúÏóêÎèÑ Claude Í≤∞Í≥º ÏÑ§Ï†ï
                claude_analysis_result = f"Claude Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

            # DML Ïª¨Îüº Ïù¥ÏäàÎ•º Í∏∞Ï°¥ Ïù¥Ïäà Î™©Î°ùÏóê Ï∂îÍ∞Ä
            if dml_column_issues:
                for dml_issue in dml_column_issues:
                    issues.append(dml_issue["message"])
                debug_log(
                    f"DML Ïª¨Îüº Ïù¥Ïäà {len(dml_column_issues)}Í∞úÎ•º ÏµúÏ¢Ö Ïù¥Ïäà Î™©Î°ùÏóê Ï∂îÍ∞Ä"
                )

            # Í≤ÄÏ¶ù ÏôÑÎ£å
            debug_log(f"ÏµúÏ¢Ö Ïù¥Ïäà Í∞úÏàò: {len(issues)}")
            debug_log(f"Ïù¥Ïäà Î™©Î°ù: {issues}")

            # Claude Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º Í∏∞Î∞òÏúºÎ°ú ÏµúÏ¢Ö ÏÉÅÌÉú Í≤∞Ï†ï
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("Í≤ÄÏ¶ù ÌÜµÍ≥º")
            )

            # Í≤∞Í≥º ÏÉùÏÑ± - Claude Í≤ÄÏ¶ùÏù¥ ÏÑ±Í≥µÏù¥Î©¥ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú PASS Ï≤òÎ¶¨
            if claude_success and not any(
                "Ïò§Î•ò:" in issue or "Ïã§Ìå®" in issue or "Ï°¥Ïû¨ÌïòÏßÄ Ïïä" in issue
                for issue in issues
            ):
                summary = "‚úÖ Î™®Îì† Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌñàÏäµÎãàÎã§."
                status = "PASS"
                debug_log("Claude Í≤ÄÏ¶ù ÏÑ±Í≥µÏúºÎ°ú ÏµúÏ¢Ö ÏÉÅÌÉúÎ•º PASSÎ°ú ÏÑ§Ï†ï")
            elif not issues:
                summary = "‚úÖ Î™®Îì† Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌñàÏäµÎãàÎã§."
                status = "PASS"
            else:
                # ÏÑ±Îä• Î¨∏Ï†úÏôÄ Í∏∞ÌÉÄ Î¨∏Ï†ú Î∂ÑÎ•ò
                performance_issues = [
                    issue for issue in issues if "Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú" in str(issue)
                ]
                claude_issues = [
                    issue for issue in issues if "Claude Í≤ÄÏ¶ù:" in str(issue)
                ]
                other_issues = [
                    issue
                    for issue in issues
                    if issue not in performance_issues and issue not in claude_issues
                ]

                # Î¨∏Ï†ú ÏöîÏïΩ ÏÉùÏÑ±
                problem_parts = []
                if performance_issues:
                    unique_performance = len(
                        set(str(issue) for issue in performance_issues)
                    )
                    if unique_performance == 1:
                        problem_parts.append("ÏÑ±Îä• Î¨∏Ï†ú")
                    else:
                        problem_parts.append(f"ÏÑ±Îä• Î¨∏Ï†ú {unique_performance}Í±¥")

                if claude_issues:
                    problem_parts.append("AI Î∂ÑÏÑù Î¨∏Ï†ú")

                if other_issues:
                    problem_parts.append(f"Í∏∞ÌÉÄ Î¨∏Ï†ú {len(other_issues)}Í±¥")

                if (
                    len(problem_parts) == 1
                    and "ÏÑ±Îä• Î¨∏Ï†ú" in problem_parts[0]
                    and not other_issues
                    and not claude_issues
                ):
                    summary = "‚ùå Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú Î∞úÍ≤¨"
                else:
                    summary = f"‚ùå Î∞úÍ≤¨Îêú Î¨∏Ï†ú: {', '.join(problem_parts)}"

                status = "FAIL"

            debug_log(f"ÏµúÏ¢Ö ÏÉÅÌÉú: {status}, ÏöîÏïΩ: {summary}")

            # Î≥¥Í≥†ÏÑú ÏÉùÏÑ± (HTML ÌòïÏãù)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"

            # HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
            debug_log("HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏãúÏûë")
            debug_log(f"dml_column_issues Í∞í: {dml_column_issues}")
            debug_log(f"report_path Í∞í: {report_path}")
            try:
                await self.generate_html_report(
                    report_path,
                    filename,
                    ddl_content,
                    sql_type,
                    status,
                    summary,
                    issues,
                    db_connection_info,
                    schema_validation,
                    constraint_validation,
                    database_secret,
                    explain_result,
                    claude_analysis_result,  # Claude Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
                    dml_column_issues,  # DML Ïª¨Îüº Ïù¥Ïäà Ï∂îÍ∞Ä
                )
                debug_log("HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏôÑÎ£å")
            except Exception as html_error:
                debug_log(f"HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {html_error}")
                import traceback

                debug_log(f"HTML Ïò§Î•ò ÏÉÅÏÑ∏: {traceback.format_exc()}")

            # Í≥µÏö© Ïó∞Í≤∞ Ï†ïÎ¶¨ (Claude Í≤ÄÏ¶ù ÏôÑÎ£å ÌõÑ)
            debug_log("Í≥µÏö© Ïó∞Í≤∞ Ï†ïÎ¶¨ ÏãúÏûë")
            self.cleanup_shared_connection()
            debug_log("Í≥µÏö© Ïó∞Í≤∞ Ï†ïÎ¶¨ ÏôÑÎ£å")

            return f"{summary}\n\nüìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑúÍ∞Ä Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§: {report_path}\nüîç ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: {debug_log_path}"

        except Exception as e:
            # ÏòàÏô∏ Î∞úÏÉù ÏãúÏóêÎèÑ Ïó∞Í≤∞ Ï†ïÎ¶¨
            try:
                self.cleanup_shared_connection()
            except:
                pass
            return f"SQL Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    def validate_semicolon_usage(self, ddl_content: str) -> bool:
        """Í∞úÏÑ†Îêú ÏÑ∏ÎØ∏ÏΩúÎ°† Í≤ÄÏ¶ù - ÎèÖÎ¶ΩÏ†ÅÏù∏ Î¨∏Ïû•ÏùÄ ÏÑ∏ÎØ∏ÏΩúÎ°† ÏóÜÏñ¥ÎèÑ ÌóàÏö©"""
        content = ddl_content.strip()

        # Îπà ÎÇ¥Ïö©ÏùÄ ÌÜµÍ≥º
        if not content:
            return True

        # Ï£ºÏÑù Ï†úÍ±∞ÌïòÍ≥† Ïã§Ï†ú SQL Íµ¨Î¨∏Îßå Ï∂îÏ∂ú
        lines = content.split("\n")
        sql_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith("--") and not line.startswith("/*"):
                sql_lines.append(line)

        if not sql_lines:
            return True

        # Ïã§Ï†ú SQL Íµ¨Î¨∏ Í≤∞Ìï©
        actual_sql = " ".join(sql_lines).strip()

        # Ïó¨Îü¨ Î¨∏Ïû•Ïù¥ ÏûàÎäî Í≤ΩÏö∞ (ÏÑ∏ÎØ∏ÏΩúÎ°†ÏúºÎ°ú Íµ¨Î∂Ñ)
        statements = [stmt.strip() for stmt in actual_sql.split(";") if stmt.strip()]

        # ÎßàÏßÄÎßâ Î¨∏Ïû•Ïù¥ ÎèÖÎ¶ΩÏ†ÅÏù∏ Îã®Ïùº Íµ¨Î¨∏Ïù∏ÏßÄ ÌôïÏù∏
        if len(statements) == 1:
            # Îã®Ïùº Íµ¨Î¨∏Ïù∏ Í≤ΩÏö∞ ÏÑ∏ÎØ∏ÏΩúÎ°† ÏóÜÏñ¥ÎèÑ ÌóàÏö©
            single_stmt = statements[0].upper().strip()

            # SET, USE, SHOW Îì± ÎèÖÎ¶ΩÏ†ÅÏù∏ Íµ¨Î¨∏Îì§ÏùÄ ÏÑ∏ÎØ∏ÏΩúÎ°† ÏóÜÏñ¥ÎèÑ ÌóàÏö©
            independent_keywords = [
                "SET SESSION",
                "SET GLOBAL",
                "SET @",
                "SET @@",
                "USE ",
                "SHOW ",
                "DESCRIBE ",
                "DESC ",
                "EXPLAIN ",
                "SELECT ",
                "INSERT ",
                "UPDATE ",
                "DELETE ",
                "CREATE TABLE",
                "CREATE INDEX",
                "ALTER TABLE",
                "DROP TABLE",
            ]

            for keyword in independent_keywords:
                if single_stmt.startswith(keyword):
                    return True

        # Ïó¨Îü¨ Î¨∏Ïû•Ïù¥ ÏûàÎäî Í≤ΩÏö∞ ÎßàÏßÄÎßâÏùÑ Ï†úÏô∏ÌïòÍ≥†Îäî Î™®Îëê ÏÑ∏ÎØ∏ÏΩúÎ°†Ïù¥ ÏûàÏñ¥Ïïº Ìï®
        return content.endswith(";")

    def extract_ddl_type(self, ddl_content: str, debug_log=None) -> str:
        """ÌòºÌï© SQL ÌååÏùº ÌÉÄÏûÖ Ï∂îÏ∂ú - SELECT ÏøºÎ¶¨Í∞Ä ÎßéÏúºÎ©¥ MIXED_SELECTÎ°ú Î∂ÑÎ•ò"""
        import re

        # Ï£ºÏÑùÍ≥º Îπà Ï§ÑÏùÑ Ï†úÍ±∞ÌïòÍ≥† Ïã§Ï†ú Íµ¨Î¨∏Îßå Ï∂îÏ∂ú
        # Î®ºÏ†Ä /* */ Ïä§ÌÉÄÏùº Ï£ºÏÑùÏùÑ Ï†ÑÏ≤¥Ï†ÅÏúºÎ°ú Ï†úÍ±∞
        ddl_content = re.sub(r"/\*.*?\*/", "", ddl_content, flags=re.DOTALL)

        lines = ddl_content.strip().split("\n")
        ddl_lines = []

        for line in lines:
            line = line.strip()
            # Ï£ºÏÑù ÎùºÏù∏Ïù¥ÎÇò Îπà ÎùºÏù∏ Í±¥ÎÑàÎõ∞Í∏∞
            if line and not line.startswith("--") and not line.startswith("#"):
                ddl_lines.append(line)

        if not ddl_lines:
            return "UNKNOWN"

        # Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Íµ¨Î¨∏ ÌÉÄÏûÖÎ≥Ñ Í∞úÏàò Í≥ÑÏÇ∞
        full_content = " ".join(ddl_lines).upper()

        # Í∞úÎ≥Ñ Íµ¨Î¨∏Îì§ÏùÑ Î∂ÑÏÑù
        statements = []
        for line in ddl_lines:
            line_upper = line.upper().strip()
            if line_upper and not line_upper.startswith("/*"):
                statements.append(line_upper)

        # Íµ¨Î¨∏ ÌÉÄÏûÖÎ≥Ñ Í∞úÏàò Í≥ÑÏÇ∞
        type_counts = {
            "SELECT": 0,
            "INSERT": 0,
            "UPDATE": 0,
            "DELETE": 0,
            "CREATE_TABLE": 0,
            "ALTER_TABLE": 0,
            "CREATE_INDEX": 0,
            "DROP_TABLE": 0,
            "DROP_INDEX": 0,
            "RENAME": 0,
        }

        # Í∞Å Íµ¨Î¨∏ Î∂ÑÏÑù - ÏÑ∏ÎØ∏ÏΩúÎ°†ÏúºÎ°ú Î∂ÑÎ¶¨Îêú Ïã§Ï†ú Íµ¨Î¨∏ Îã®ÏúÑÎ°ú Í≥ÑÏÇ∞
        sql_statements = [
            stmt.strip() for stmt in ddl_content.split(";") if stmt.strip()
        ]

        for stmt in sql_statements:
            stmt_upper = stmt.upper().strip()

            # /* */ Ïä§ÌÉÄÏùº Ï£ºÏÑù Ï†úÍ±∞
            stmt_upper = re.sub(r"/\*.*?\*/", "", stmt_upper, flags=re.DOTALL)

            # -- Ïä§ÌÉÄÏùº Ï£ºÏÑù Ï†úÍ±∞
            stmt_lines = [
                line.strip()
                for line in stmt_upper.split("\n")
                if line.strip() and not line.strip().startswith("--")
            ]
            if not stmt_lines:
                continue

            stmt_clean = " ".join(stmt_lines).strip()

            if stmt_clean.startswith("SELECT"):
                type_counts["SELECT"] += 1
            elif stmt_clean.startswith("INSERT"):
                type_counts["INSERT"] += 1
            elif stmt_clean.startswith("UPDATE"):
                type_counts["UPDATE"] += 1
            elif stmt_clean.startswith("DELETE"):
                type_counts["DELETE"] += 1
            elif stmt_clean.startswith("CREATE TABLE"):
                type_counts["CREATE_TABLE"] += 1
            elif stmt_clean.startswith("ALTER TABLE") or re.search(
                r"\bALTER\s+TABLE\b", stmt_clean
            ):
                type_counts["ALTER_TABLE"] += 1
            elif stmt_clean.startswith("CREATE INDEX"):
                type_counts["CREATE_INDEX"] += 1
            elif stmt_clean.startswith("DROP TABLE"):
                type_counts["DROP_TABLE"] += 1
            elif stmt_clean.startswith("DROP INDEX"):
                type_counts["DROP_INDEX"] += 1
            elif stmt_clean.startswith("RENAME TABLE") or re.search(
                r"\bRENAME\s+TABLE\b", stmt_clean
            ):
                type_counts["RENAME"] += 1

        # Ï¥ù Íµ¨Î¨∏ Ïàò
        total_statements = sum(type_counts.values())

        # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Ï∂îÍ∞Ä
        if debug_log:
            debug_log(f"DEBUG - Íµ¨Î¨∏ Í∞úÏàò: {type_counts}")
            debug_log(f"DEBUG - Ï¥ù Íµ¨Î¨∏: {total_statements}")

        # SELECT ÏøºÎ¶¨Í∞Ä 50% Ïù¥ÏÉÅÏù¥Î©¥ MIXED_SELECTÎ°ú Î∂ÑÎ•ò
        if (
            type_counts["SELECT"] > 0
            and type_counts["SELECT"] >= total_statements * 0.5
        ):
            if debug_log:
                debug_log("DEBUG - MIXED_SELECTÎ°ú Î∂ÑÎ•òÎê®")
            return "MIXED_SELECT"

        # Í∏∞Ï°¥ Ïö∞ÏÑ†ÏàúÏúÑ Î°úÏßÅ (DDL Ïö∞ÏÑ†)
        if type_counts["CREATE_TABLE"] > 0:
            return "CREATE_TABLE"
        elif type_counts["ALTER_TABLE"] > 0 or type_counts["RENAME"] > 0:
            return "ALTER_TABLE"
        elif type_counts["CREATE_INDEX"] > 0:
            return "CREATE_INDEX"
        elif type_counts["DROP_TABLE"] > 0:
            return "DROP_TABLE"
        elif type_counts["DROP_INDEX"] > 0:
            return "DROP_INDEX"
        elif type_counts["SELECT"] > 0:
            return "SELECT"
        elif type_counts["INSERT"] > 0:
            return "INSERT"
        elif type_counts["UPDATE"] > 0:
            return "UPDATE"
        elif type_counts["DELETE"] > 0:
            return "DELETE"

        # Í∏∞ÌÉÄ Íµ¨Î¨∏ Ï≤òÎ¶¨
        if any(stmt.startswith("SHOW ") for stmt in statements):
            return "SHOW"
        elif any(stmt.startswith("SET ") for stmt in statements):
            return "SET"
        elif any(stmt.startswith("USE ") for stmt in statements):
            return "USE"
        else:
            return "UNKNOWN"

    def detect_ddl_type(self, ddl_content: str) -> str:
        """DDL ÌÉÄÏûÖ Í∞êÏßÄ"""
        ddl_upper = ddl_content.upper().strip()

        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("DROP TABLE"):
            return "DROP_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP INDEX"):
            return "DROP_INDEX"
        elif ddl_upper.startswith("INSERT"):
            return "INSERT"
        elif ddl_upper.startswith("UPDATE"):
            return "UPDATE"
        elif ddl_upper.startswith("DELETE"):
            return "DELETE"
        elif ddl_upper.startswith("SELECT"):
            return "SELECT"
        else:
            return "UNKNOWN"

    def create_schema_validation_summary(
        self, issues: list, dml_column_issues: list
    ) -> str:
        """Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º ÏöîÏïΩÌïòÏó¨ ClaudeÏóêÍ≤å Ï†ÑÎã¨Ìï† ÌòïÌÉúÎ°ú ÏÉùÏÑ±"""
        if not issues and not dml_column_issues:
            return "Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù: Î™®Îì† Í≤ÄÏ¶ù ÌÜµÍ≥º"

        summary_parts = []
        if issues:
            summary_parts.append(f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Î¨∏Ï†úÏ†ê ({len(issues)}Í∞ú):")
            for i, issue in enumerate(issues, 1):  # Î™®Îì† Î¨∏Ï†ú ÌëúÏãú
                summary_parts.append(f"  {i}. {issue}")

        if dml_column_issues:
            summary_parts.append(f"Ïª¨Îüº Í≤ÄÏ¶ù Î¨∏Ï†úÏ†ê ({len(dml_column_issues)}Í∞ú):")
            for i, issue in enumerate(dml_column_issues, 1):  # Î™®Îì† Î¨∏Ï†ú ÌëúÏãú
                summary_parts.append(f"  {i}. {issue}")

        return "\n".join(summary_parts)

    async def validate_with_claude(
        self,
        ddl_content: str,
        database_secret: str = None,
        schema_info: dict = None,
        explain_info: str = None,
        sql_type: str = None,
        schema_validation_summary: str = None,
    ) -> str:
        """
        Claude cross-region ÌîÑÎ°úÌååÏùºÏùÑ ÌôúÏö©Ìïú DDL Í≤ÄÏ¶ù (Ïã§Ï†ú Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï®)
        """
        # Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Í∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÍ≥† database_secretÏù¥ ÏûàÏúºÎ©¥ Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú
        if schema_info is None and database_secret:
            try:
                schema_info = await self.extract_current_schema_info(database_secret)
            except Exception as e:
                logger.warning(f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
                schema_info = {}

        # Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± (ÏàúÏÑú Í≥†Î†§)
        schema_context = ""
        if schema_info:
            schema_details = []

            # ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨ÌïòÏó¨ Ï≤òÎ¶¨
            sorted_items = sorted(
                schema_info.items(), key=lambda x: x[1].get("order", 0)
            )

            for key, info in sorted_items:
                order = info.get("order", 0)

                if info["type"] == "table":
                    table_name = info.get("table_name", key)

                    if "columns" in info:
                        # ALTER TABLE ÏºÄÏù¥Ïä§
                        columns_info = [
                            f"{col['name']}({col['data_type']})"
                            for col in info["columns"]
                        ]
                        schema_details.append(f"[{order}] ALTER TABLE '{table_name}':")
                        schema_details.append(f"  - DBÏóê Ï°¥Ïû¨: {info['exists']}")
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - ÌååÏùº ÎÇ¥ ÏÉùÏÑ±Îê®: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - Ïú†Ìö® Ï°¥Ïû¨: {info.get('effective_exists', info['exists'])}"
                        )

                        if info["exists"] and columns_info:
                            schema_details.append(
                                f"  - Í∏∞Ï°¥ Ïª¨Îüº: {', '.join(columns_info)}"
                            )

                        if info.get("alter_type") and info.get("target_column"):
                            schema_details.append(
                                f"  - ALTER ÏûëÏóÖ: {info['alter_type']} {info['target_column']}"
                            )
                    else:
                        # CREATE/DROP TABLE ÏºÄÏù¥Ïä§
                        action = (
                            "CREATE"
                            if "CREATE" in key or info.get("exists") == False
                            else "DROP"
                        )
                        schema_details.append(
                            f"[{order}] {action} TABLE '{table_name}': DBÏ°¥Ïû¨={info['exists']}"
                        )

                elif info["type"] == "index":
                    table_name = info.get("table_name", key.split(".")[0])
                    index_name = info["index_name"]

                    if "duplicate_column_indexes" in info:
                        # CREATE INDEX ÏºÄÏù¥Ïä§
                        schema_details.append(
                            f"[{order}] CREATE INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - ÌÖåÏù¥Î∏î DBÏ°¥Ïû¨: {info['table_exists']}"
                        )
                        if info.get("created_in_file"):
                            schema_details.append(
                                f"  - ÌÖåÏù¥Î∏î ÌååÏùºÎÇ¥ÏÉùÏÑ±: {info['created_in_file']}"
                            )
                        schema_details.append(
                            f"  - ÌÖåÏù¥Î∏î Ïú†Ìö®Ï°¥Ïû¨: {info.get('effective_exists', info['table_exists'])}"
                        )
                        schema_details.append(
                            f"  - ÏÉùÏÑ±Ìï† Ïª¨Îüº: {', '.join(info['target_columns'])}"
                        )

                        # DBÏùò Ï§ëÎ≥µ Ïù∏Îç±Ïä§
                        if info["duplicate_column_indexes"]:
                            schema_details.append("  - DBÏùò ÎèôÏùº Ïª¨Îüº Íµ¨ÏÑ± Ïù∏Îç±Ïä§:")
                            for dup_idx in info["duplicate_column_indexes"]:
                                schema_details.append(
                                    f"    * {dup_idx['name']} ({dup_idx['columns']}) - {'UNIQUE' if dup_idx['unique'] else 'NON-UNIQUE'}"
                                )

                        # ÌååÏùº ÎÇ¥ Ï§ëÎ≥µ Ïù∏Îç±Ïä§
                        if info.get("file_duplicate_indexes"):
                            schema_details.append("  - ÌååÏùº ÎÇ¥ ÎèôÏùº Ïª¨Îüº Íµ¨ÏÑ± Ïù∏Îç±Ïä§:")
                            for dup_idx in info["file_duplicate_indexes"]:
                                schema_details.append(
                                    f"    * [{dup_idx['order']}] {dup_idx['name']} ({','.join(dup_idx['columns'])})"
                                )

                        if not info["duplicate_column_indexes"] and not info.get(
                            "file_duplicate_indexes"
                        ):
                            schema_details.append("  - Ï§ëÎ≥µ Ïª¨Îüº Íµ¨ÏÑ± Ïù∏Îç±Ïä§ ÏóÜÏùå")
                    else:
                        # DROP INDEX ÏºÄÏù¥Ïä§
                        schema_details.append(
                            f"[{order}] DROP INDEX '{index_name}' on '{table_name}':"
                        )
                        schema_details.append(
                            f"  - ÌÖåÏù¥Î∏î Ï°¥Ïû¨: {info['table_exists']}"
                        )
                        schema_details.append(
                            f"  - Ïù∏Îç±Ïä§ Ï°¥Ïû¨: {info['index_exists']}"
                        )

            if schema_details:
                schema_context = f"""
Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ (Ïã§Ìñâ ÏàúÏÑúÎ≥Ñ):
{chr(10).join(schema_details)}

ÏúÑ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú DDLÏùò Ï†ÅÏ†àÏÑ±ÏùÑ ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî.
ÌäπÌûà Îã§Ïùå ÏÇ¨Ìï≠ÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî:
1. ÌååÏùº ÎÇ¥ÏóêÏÑú Î®ºÏ†Ä ÏÉùÏÑ±Îêú ÌÖåÏù¥Î∏îÏùÄ Ïù¥ÌõÑ ALTER/INDEX ÏûëÏóÖÏóêÏÑú Ï°¥Ïû¨ÌïòÎäî Í≤ÉÏúºÎ°ú Í∞ÑÏ£º
2. ÎèôÏùºÌïú Ïª¨Îüº Íµ¨ÏÑ±Ïùò Ïù∏Îç±Ïä§ Ï§ëÎ≥µ Ïó¨Î∂Ä
3. Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÌÖåÏù¥Î∏î/Ïù∏Îç±Ïä§Ïóê ÎåÄÌïú DROP ÏãúÎèÑ
4. Ïã§Ìñâ ÏàúÏÑúÏÉÅ ÎÖºÎ¶¨Ï†Å Ïò§Î•ò
"""
            else:
                schema_context = """
Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.
Í∏∞Î≥∏Ï†ÅÏù∏ Î¨∏Î≤ï Í≤ÄÏ¶ùÎßå ÏàòÌñâÌï©ÎãàÎã§.
"""
        else:
            schema_context = """
Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.
Í∏∞Î≥∏Ï†ÅÏù∏ Î¨∏Î≤ï Í≤ÄÏ¶ùÎßå ÏàòÌñâÌï©ÎãàÎã§.
"""

        # EXPLAIN Ï†ïÎ≥¥ Ïª®ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
        explain_context = ""
        if explain_info:
            explain_context = f"""
EXPLAIN Î∂ÑÏÑù Í≤∞Í≥º:
{explain_info}

ÏúÑ EXPLAIN Í≤∞Í≥ºÎ•º Ï∞∏Í≥†ÌïòÏó¨ ÏÑ±Îä•ÏÉÅ Î¨∏Ï†úÍ∞Ä ÏûàÎäîÏßÄÎèÑ Ìï®Íªò Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.
Ï∞∏Í≥†: DDL Íµ¨Î¨∏(CREATE, ALTER, DROP Îì±)Ïóê ÎåÄÌï¥ÏÑúÎäî EXPLAINÏùÑ Ïã§ÌñâÌïòÏßÄ ÏïäÏúºÎ©∞, 
SELECT, UPDATE, DELETE, INSERT Îì±Ïùò DML Íµ¨Î¨∏Ïóê ÎåÄÌï¥ÏÑúÎßå EXPLAIN Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§.
"""

        # Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥º Ïª®ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
        schema_validation_context = ""
        schema_has_errors = False
        if schema_validation_summary:
            # Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            schema_has_errors = (
                "Ïò§Î•ò" in schema_validation_summary
                or "Ïã§Ìå®" in schema_validation_summary
                or "Ï°¥Ïû¨ÌïòÏßÄ Ïïä" in schema_validation_summary
                or "Ïù¥ÎØ∏ Ï°¥Ïû¨" in schema_validation_summary
            )

            schema_validation_context = f"""
Í∏∞Ï°¥ Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥º:
{schema_validation_summary}

ÏúÑ Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º Ï∞∏Í≥†ÌïòÏó¨ Ï¢ÖÌï©Ï†ÅÏù∏ ÌåêÎã®ÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.
Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Î¨∏Ï†úÍ∞Ä Î∞úÍ≤¨Îêú Í≤ΩÏö∞, Ïã§Ìå®Î°ú Í≤∞Î°†ÏùÑ ÎÇ¥Î¶¨Í≥† Ïôú Î¨∏Ï†úÍ∞Ä ÎÇòÏôîÎäîÏßÄÎèÑ ÏÑ§Î™ÖÌïòÎ©¥ÏÑú Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî.
"""

        # Knowledge BaseÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥ Ï°∞Ìöå
        knowledge_context = ""
        try:
            knowledge_info = await self.query_knowledge_base(ddl_content, sql_type)
            if knowledge_info and knowledge_info != "Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.":
                knowledge_context = f"""
Knowledge Base Ï∞∏Í≥† Ï†ïÎ≥¥:
{knowledge_info}

ÏúÑ Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨ Í≤ÄÏ¶ùÏùÑ ÏàòÌñâÌï¥Ï£ºÏÑ∏Ïöî.
"""
        except Exception as e:
            logger.warning(f"Knowledge Base Ï°∞Ìöå Ï§ë Ïò§Î•ò: {e}")

        # DDLÍ≥º DQLÏóê Îî∞Î•∏ ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨Î∂Ñ
        ddl_types = [
            "CREATE_TABLE",
            "ALTER_TABLE",
            "CREATE_INDEX",
            "DROP_TABLE",
            "DROP_INDEX",
        ]
        dql_types = ["SELECT", "UPDATE", "DELETE", "INSERT"]

        if sql_type in ddl_types:
            # DDL Í≤ÄÏ¶ù ÌîÑÎ°¨ÌîÑÌä∏
            prompt = f"""
        Îã§Ïùå DDL Î¨∏ÏùÑ Aurora MySQL Î¨∏Î≤ïÏúºÎ°ú Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî:

        {ddl_content}

        {schema_context}

        {schema_validation_context}

        {knowledge_context}

        **Í≤ÄÏ¶ù Í∏∞Ï§Ä:**
        Aurora MySQL 8.0ÏóêÏÑú Î¨∏Î≤ïÏ†ÅÏúºÎ°ú Ïò¨Î∞îÎ•¥Í≥† Ïã§Ìñâ Í∞ÄÎä•ÌïúÏßÄÎßå ÌôïÏù∏ÌïòÏÑ∏Ïöî.

        **Ï§ëÏöî: Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïã§Ìå® Ï≤òÎ¶¨**
        {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§. ÌÖåÏù¥Î∏îÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Ïù∏Îç±Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Í∏∞ÌÉÄ Ïä§ÌÇ§Îßà Í¥ÄÎ†® Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ Î∞òÎìúÏãú Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî." if schema_has_errors else ""}

        **ÏùëÎãµ Í∑úÏπô:**
        1. {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨Îêú Í≤ΩÏö∞ Î∞òÎìúÏãú 'Ïò§Î•ò:'Î°ú ÏãúÏûëÌïòÏó¨ Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî" if schema_has_errors else "DDLÏù¥ Aurora MySQLÏóêÏÑú Ïã§Ìñâ Í∞ÄÎä•ÌïòÎ©¥ Î∞òÎìúÏãú 'Í≤ÄÏ¶ù ÌÜµÍ≥º'Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî"}
        2. ÏÑ±Îä• Í∞úÏÑ†Ïù¥ÎÇò Î™®Î≤î ÏÇ¨Î°ÄÎäî "Í≤ÄÏ¶ù ÌÜµÍ≥º (Í∂åÏû•ÏÇ¨Ìï≠: ...)"Î°ú ÌëúÏãúÌïòÏÑ∏Ïöî  
        3. Ïã§ÌñâÏùÑ ÎßâÎäî Ïã¨Í∞ÅÌïú Î¨∏Î≤ï Ïò§Î•òÎßå "Ïò§Î•ò:"Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî

        **ÏòàÏãú:**
        - Ïã§Ìñâ Í∞ÄÎä•Ìïú Í≤ΩÏö∞: "Í≤ÄÏ¶ù ÌÜµÍ≥º"
        - Í∞úÏÑ†Ï†ê ÏûàÎäî Í≤ΩÏö∞: "Í≤ÄÏ¶ù ÌÜµÍ≥º (Í∂åÏû•ÏÇ¨Ìï≠: NULL ÏÜçÏÑ±ÏùÑ Î™ÖÏãúÌïòÎ©¥ Îçî Î™ÖÌôïÌï©ÎãàÎã§)"
        - Ïã§Ìñâ Î∂àÍ∞ÄÎä•Ìïú Í≤ΩÏö∞: "Ïò§Î•ò: Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞Ìï©ÎãàÎã§"

        Î∞òÎìúÏãú ÏúÑ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî.
        """
        elif sql_type in dql_types:
            # DQL Í≤ÄÏ¶ù ÌîÑÎ°¨ÌîÑÌä∏
            prompt = f"""
        Îã§Ïùå DQL(DML) ÏøºÎ¶¨Î•º Aurora MySQLÏóêÏÑú Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî:

        {ddl_content}

        {explain_context}

        {schema_validation_context}

        {knowledge_context}

        **Í≤ÄÏ¶ù Í∏∞Ï§Ä:**
        1. Aurora MySQL 8.0ÏóêÏÑú Î¨∏Î≤ïÏ†ÅÏúºÎ°ú Ïò¨Î∞îÎ•∏ÏßÄ ÌôïÏù∏
        2. ÏÑ±Îä•ÏÉÅ Î¨∏Ï†úÍ∞Ä ÏûàÎäîÏßÄ Î∂ÑÏÑù
        3. Ïù∏Îç±Ïä§ ÏÇ¨Ïö© Ìö®Ïú®ÏÑ± Í≤ÄÌÜ†
        4. **Ï§ëÏöî: Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ Í≤ÄÏ¶ù Ïã§Ìå®Î°ú Ï≤òÎ¶¨**

        **Ï§ëÏöî: Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïã§Ìå® Ï≤òÎ¶¨**
        {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§. ÌÖåÏù¥Î∏îÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Ïù∏Îç±Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Í∏∞ÌÉÄ Ïä§ÌÇ§Îßà Í¥ÄÎ†® Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ Î∞òÎìúÏãú Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî." if schema_has_errors else ""}

        **ÏÑ±Îä• Î¨∏Ï†ú Ïã§Ìå® Í∏∞Ï§Ä:**
        Îã§ÏùåÍ≥º Í∞ôÏùÄ Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†úÍ∞Ä Î∞úÍ≤¨ÎêòÎ©¥ Î∞òÎìúÏãú "Ïò§Î•ò:"Î°ú ÏãúÏûëÌïòÏó¨ Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî:
        - 1Ï≤úÎßå Ìñâ Ïù¥ÏÉÅÏùò ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏îÏóê ÎåÄÌïú Ï†ÑÏ≤¥ ÌÖåÏù¥Î∏î Ïä§Ï∫î (Full Table Scan)
        - WHERE Ï†à ÏóÜÎäî ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î UPDATE/DELETE
        - Ïù∏Îç±Ïä§ ÏóÜÎäî ÎåÄÏö©Îüâ ÌÖåÏù¥Î∏î JOIN
        - Ïπ¥ÎîîÎÑêÎ¶¨Ìã∞Í∞Ä Îß§Ïö∞ ÎÜíÏùÄ GROUP BYÎÇò ORDER BY ÏûëÏóÖ
        - ÏûÑÏãú ÌÖåÏù¥Î∏îÏùÑ ÏÇ¨Ïö©ÌïòÎäî Î≥µÏû°Ìïú ÏÑúÎ∏åÏøºÎ¶¨

        **ÏùëÎãµ Í∑úÏπô:**
        1. {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨Îêú Í≤ΩÏö∞ Î∞òÎìúÏãú 'Ïò§Î•ò:'Î°ú ÏãúÏûëÌïòÏó¨ Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî" if schema_has_errors else "ÏøºÎ¶¨Í∞Ä Ïã§Ìñâ Í∞ÄÎä•ÌïòÍ≥† Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†úÍ∞Ä ÏóÜÏúºÎ©¥ 'Í≤ÄÏ¶ù ÌÜµÍ≥º'Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî"}
        2. Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ "Ïò§Î•ò: Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú - ..."Î°ú ÏãúÏûëÌïòÏó¨ Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî
        3. Í≤ΩÎØ∏Ìïú ÏÑ±Îä• Í∞úÏÑ†Ï†êÎßå ÏûàÏúºÎ©¥ "Í≤ÄÏ¶ù ÌÜµÍ≥º (ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠: ...)"Î°ú ÌëúÏãúÌïòÏÑ∏Ïöî
        4. Ïã§Ìñâ Î∂àÍ∞ÄÎä•Ìïú Í≤ΩÏö∞Îäî "Ïò§Î•ò:"Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî

        **ÏòàÏãú:**
        - Ïã§Ìñâ Í∞ÄÎä•ÌïòÍ≥† ÏÑ±Îä• Î¨∏Ï†ú ÏóÜÏùå: "Í≤ÄÏ¶ù ÌÜµÍ≥º"
        - Í≤ΩÎØ∏Ìïú ÏÑ±Îä• Í∞úÏÑ†Ï†ê: "Í≤ÄÏ¶ù ÌÜµÍ≥º (ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠: Ïù∏Îç±Ïä§ Ï∂îÍ∞ÄÎ•º Í≥†Î†§ÌïòÏÑ∏Ïöî)"
        - Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú: "Ïò§Î•ò: Ïã¨Í∞ÅÌïú ÏÑ±Îä• Î¨∏Ï†ú - 1Ï≤úÎßå Ìñâ Ïù¥ÏÉÅ ÌÖåÏù¥Î∏îÏùò Ï†ÑÏ≤¥ Ïä§Ï∫îÏúºÎ°ú Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑú ÏÇ¨Ïö© Î∂àÍ∞Ä"
        - Ïã§Ìñâ Î∂àÍ∞ÄÎä•Ìïú Í≤ΩÏö∞: "Ïò§Î•ò: Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞Ìï©ÎãàÎã§"

        Î∞òÎìúÏãú ÏúÑ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî.
        """
        else:
            # Í∏∞Î≥∏ ÌîÑÎ°¨ÌîÑÌä∏
            prompt = f"""
        Îã§Ïùå SQL Î¨∏ÏùÑ Aurora MySQL Î¨∏Î≤ïÏúºÎ°ú Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî: 

        {ddl_content}

        {schema_context}

        {explain_context}

        {schema_validation_context}

        {knowledge_context}

        **Í≤ÄÏ¶ù Í∏∞Ï§Ä:**
        Aurora MySQL 8.0ÏóêÏÑú Î¨∏Î≤ïÏ†ÅÏúºÎ°ú Ïò¨Î∞îÎ•¥Í≥† Ïã§Ìñâ Í∞ÄÎä•ÌïúÏßÄÎßå ÌôïÏù∏ÌïòÏÑ∏Ïöî. 

        **Ï§ëÏöî: Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïã§Ìå® Ï≤òÎ¶¨**
        {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§. ÌÖåÏù¥Î∏îÏù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Ïù∏Îç±Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÍ±∞ÎÇò, Í∏∞ÌÉÄ Ïä§ÌÇ§Îßà Í¥ÄÎ†® Î¨∏Ï†úÍ∞Ä ÏûàÏúºÎ©¥ Î∞òÎìúÏãú Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî." if schema_has_errors else ""}

        **ÏùëÎãµ Í∑úÏπô:**
        1. {"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÍ≤¨Îêú Í≤ΩÏö∞ Î∞òÎìúÏãú 'Ïò§Î•ò:'Î°ú ÏãúÏûëÌïòÏó¨ Ïã§Ìå®Î°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî" if schema_has_errors else "SQLÏù¥ Aurora MySQLÏóêÏÑú Ïã§Ìñâ Í∞ÄÎä•ÌïòÎ©¥ Î∞òÎìúÏãú 'Í≤ÄÏ¶ù ÌÜµÍ≥º'Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî"}
        2. ÏÑ±Îä• Í∞úÏÑ†Ïù¥ÎÇò Î™®Î≤î ÏÇ¨Î°ÄÎäî "Í≤ÄÏ¶ù ÌÜµÍ≥º (Í∂åÏû•ÏÇ¨Ìï≠: ...)"Î°ú ÌëúÏãúÌïòÏÑ∏Ïöî  
        3. Ïã§ÌñâÏùÑ ÎßâÎäî Ïã¨Í∞ÅÌïú Î¨∏Î≤ï Ïò§Î•òÎßå "Ïò§Î•ò:"Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî
        4. Í∂åÏû•ÏÇ¨Ìï≠ÏùÑ Ï†úÏïàÌï†Îïå, Aurora MySQL 8.0 Ïù¥ ÏïÑÎãå Í∏∞Îä•Ïù¥ÎÇò ÌôïÏù∏ÎêòÏßÄ ÏïäÏùÄ ÎÇ¥Ïö©ÏùÄ Í∂åÏû•ÌïòÏßÄ ÎßàÏÑ∏Ïöî. ÏòàÎ•º Îì§Ïñ¥ ÏøºÎ¶¨Ï∫êÏãú Í∏∞Îä•ÏùÄ 8.0Î∂ÄÌÑ∞ ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÏäµÎãàÎã§. Íµ¨ mysqlÍ∏∞Îä•ÏùÑ Í∂åÏû•ÌïòÍ±∞ÎÇò Í±∞Î°†ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

        Î∞òÎìúÏãú ÏúÑ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî.
        """

        claude_input = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,  # ÌÜ†ÌÅ∞ ÏàòÎ•º 4Î∞∞Î°ú Ï¶ùÍ∞Ä
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            }
        )

        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile Ìò∏Ï∂ú
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            response_body = json.loads(response.get("body").read())
            return response_body.get("content", [{}])[0].get("text", "")
        except Exception as e:
            logger.warning(
                f"Claude Sonnet 4 Ìò∏Ï∂ú Ïã§Ìå® ‚Üí Claude 3.7 Sonnet cross-region profileÎ°ú fallback: {e}"
            )
            # Claude 3.7 Sonnet inference profile Ìò∏Ï∂ú (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                return response_body.get("content", [{}])[0].get("text", "")
            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet Ìò∏Ï∂ú Ïò§Î•ò: {e}")
                return f"Claude Ìò∏Ï∂ú Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    async def generate_performance_recommendations_with_claude(
        self,
        metrics_summary: str,
        correlation_analysis: str,
        outliers_analysis: str,
        slow_queries: str,
        memory_queries: str,
        cpu_queries: str,
        temp_queries: str,
        database_secret: str = None,
    ) -> Dict[str, Any]:
        """
        ClaudeÎ•º ÌôúÏö©ÌïòÏó¨ ÏÑ±Îä• Î©îÌä∏Î¶≠Í≥º ÏøºÎ¶¨ Î∂ÑÏÑùÏùÑ Í∏∞Î∞òÏúºÎ°ú ÎèôÏ†Å Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
        """
        try:
            # Knowledge BaseÏóêÏÑú ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú Ï°∞Ìöå
            knowledge_context = ""
            try:
                knowledge_info = await self.query_knowledge_base(
                    "database performance optimization recommendations", "PERFORMANCE"
                )
                if knowledge_info and knowledge_info != "Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.":
                    knowledge_context = f"""
Knowledge Base ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú:
{knowledge_info}

ÏúÑ Í∞ÄÏù¥ÎìúÎ•º Ï∞∏Í≥†ÌïòÏó¨ Í∂åÏû•ÏÇ¨Ìï≠ÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
"""
            except Exception as e:
                logger.warning(f"Knowledge Base Ï°∞Ìöå Ï§ë Ïò§Î•ò: {e}")

            prompt = f"""
Îã§Ïùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ìñâ Í∞ÄÎä•Ìïú ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠Í≥º Ïï°ÏÖò ÏïÑÏù¥ÌÖúÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:

**Î©îÌä∏Î¶≠ ÏöîÏïΩ:**
{metrics_summary}

**ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù:**
{correlation_analysis}

**Ïù¥ÏÉÅ ÏßïÌõÑ Î∂ÑÏÑù:**
{outliers_analysis}

**ÎäêÎ¶∞ ÏøºÎ¶¨ Î∂ÑÏÑù:**
{slow_queries}

**Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨:**
{memory_queries}

**CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨:**
{cpu_queries}

**ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨:**
{temp_queries}

{knowledge_context}

**ÏöîÍµ¨ÏÇ¨Ìï≠:**
1. ÏúÑ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï¢ÖÌï©ÌïòÏó¨ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Ïóê Í∏∞Î∞òÌïú Íµ¨Ï≤¥Ï†ÅÏù∏ Í∂åÏû•ÏÇ¨Ìï≠ÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî
2. Ïö∞ÏÑ†ÏàúÏúÑÎ≥ÑÎ°ú Î∂ÑÎ•òÌïòÏó¨ Ï¶âÏãú Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∞úÏÑ†ÏÇ¨Ìï≠ÏùÑ Ï†úÏïàÌïòÏÑ∏Ïöî
3. Í∞Å Í∂åÏû•ÏÇ¨Ìï≠Ïóê ÎåÄÌï¥ ÏòàÏÉÅ Ìö®Í≥ºÏôÄ Íµ¨ÌòÑ ÎÇúÏù¥ÎèÑÎ•º Ìè¨Ìï®ÌïòÏÑ∏Ïöî
4. Ïï°ÏÖò ÏïÑÏù¥ÌÖúÏùÄ Îã¥ÎãπÏûê, ÏòàÏÉÅ ÏÜåÏöîÏãúÍ∞Ñ, Ïö∞ÏÑ†ÏàúÏúÑÎ•º Ìè¨Ìï®ÌïòÏó¨ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî

**ÏùëÎãµ ÌòïÏãù (JSON):**
{{
    "immediate_improvements": [
        {{
            "category": "Î™®ÎãàÌÑ∞ÎßÅ/ÏÑ±Îä•/Ïö©ÎüâÍ≥ÑÌöç",
            "title": "Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ†ÏÇ¨Ìï≠ Ï†úÎ™©",
            "description": "ÏÉÅÏÑ∏ ÏÑ§Î™Ö",
            "items": ["Íµ¨Ï≤¥Ï†ÅÏù∏ Ïã§Ìñâ Ìï≠Î™©1", "Íµ¨Ï≤¥Ï†ÅÏù∏ Ïã§Ìñâ Ìï≠Î™©2"],
            "expected_impact": "ÏòàÏÉÅ Ìö®Í≥º",
            "difficulty": "ÎÇÆÏùå/Ï§ëÍ∞Ñ/ÎÜíÏùå"
        }}
    ],
    "action_items": [
        {{
            "priority": "ÎÜíÏùå/Ï§ëÍ∞Ñ/ÎÇÆÏùå",
            "item": "Íµ¨Ï≤¥Ï†ÅÏù∏ Ïï°ÏÖò ÏïÑÏù¥ÌÖú",
            "estimated_time": "ÏòàÏÉÅ ÏÜåÏöîÏãúÍ∞Ñ",
            "assignee": "Îã¥ÎãπÏûê Ïó≠Ìï†",
            "rationale": "Ïù¥ Ïï°ÏÖòÏù¥ ÌïÑÏöîÌïú Ïù¥Ïú†"
        }}
    ]
}}

Î∞òÎìúÏãú ÏúÑ JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî. Î∂ÑÏÑù Í≤∞Í≥ºÏóêÏÑú Ïã§Ï†ú Î∞úÍ≤¨Îêú Î¨∏Ï†úÏ†êÏùÑ Í∏∞Î∞òÏúºÎ°ú Íµ¨Ï≤¥Ï†ÅÏù∏ Í∂åÏû•ÏÇ¨Ìï≠ÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî.
"""

            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4096,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 Ìò∏Ï∂ú
            try:
                logger.info(f"Claude Sonnet 4 Ìò∏Ï∂ú ÏãúÏûë - Î™®Îç∏ID: {sonnet_4_model_id}")
                logger.debug(f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: {len(claude_input)} bytes")

                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                logger.info("Claude Sonnet 4 ÏùëÎãµ ÏàòÏã† ÏôÑÎ£å")

                response_body = json.loads(response.get("body").read())
                logger.debug(f"ÏùëÎãµ Î≥∏Î¨∏ ÌååÏã± ÏôÑÎ£å: {list(response_body.keys())}")

                claude_response = response_body.get("content", [{}])[0].get("text", "")
                logger.info(
                    f"Claude ÏùëÎãµ ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(claude_response)} characters"
                )
                logger.debug(f"Claude ÏùëÎãµ ÎØ∏Î¶¨Î≥¥Í∏∞: {claude_response[:200]}...")

                # JSON ÌååÏã± ÏãúÎèÑ - Î®ºÏ†Ä ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ù ÌôïÏù∏
                try:
                    # ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú ÏãúÎèÑ
                    import re

                    markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                    markdown_match = re.search(
                        markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                    )

                    if markdown_match:
                        json_content = markdown_match.group(1).strip()
                        logger.info(
                            f"ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú, Í∏∏Ïù¥: {len(json_content)}"
                        )
                        parsed_result = json.loads(json_content)
                    else:
                        # ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏù¥ ÏóÜÏúºÎ©¥ ÏßÅÏ†ë ÌååÏã± ÏãúÎèÑ
                        logger.info("ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ù ÏóÜÏùå, ÏßÅÏ†ë JSON ÌååÏã± ÏãúÎèÑ")
                        parsed_result = json.loads(claude_response)

                    logger.info("Claude ÏùëÎãµ JSON ÌååÏã± ÏÑ±Í≥µ")
                    logger.debug(f"ÌååÏã±Îêú Í≤∞Í≥º ÌÇ§: {list(parsed_result.keys())}")

                    # ÌïÑÏöîÌïú ÌÇ§Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
                    if isinstance(parsed_result, dict) and (
                        "immediate_improvements" in parsed_result
                        or "action_items" in parsed_result
                    ):
                        improvements_count = len(
                            parsed_result.get("immediate_improvements", [])
                        )
                        actions_count = len(parsed_result.get("action_items", []))
                        logger.info(
                            f"Ïú†Ìö®Ìïú Í∂åÏû•ÏÇ¨Ìï≠ ÌååÏã± ÏôÑÎ£å: {improvements_count}Í∞ú Í∞úÏÑ†ÏÇ¨Ìï≠, {actions_count}Í∞ú Ïï°ÏÖòÏïÑÏù¥ÌÖú"
                        )
                        return parsed_result
                    else:
                        logger.warning("ÌååÏã±Îêú JSONÏóê ÌïÑÏöîÌïú ÌÇ§Í∞Ä ÏóÜÏùå")
                        return self._get_default_recommendations()

                except json.JSONDecodeError as json_err:
                    logger.error(f"Claude Sonnet 4 ÏùëÎãµ JSON ÌååÏã± Ïã§Ìå®: {json_err}")
                    logger.info("ÌÖçÏä§Ìä∏ ÌååÏã± ÏãúÎèÑ")
                    # ÌÖçÏä§Ìä∏ÏóêÏÑú JSON Ï∂îÏ∂ú ÏãúÎèÑ
                    parsed_result = self._parse_claude_text_response(claude_response)
                    if parsed_result:
                        logger.info("ÌÖçÏä§Ìä∏ ÌååÏã± ÏÑ±Í≥µ")
                        return parsed_result
                    logger.error(f"ÌååÏã± Ïã§Ìå®Ìïú ÏùëÎãµ ÎÇ¥Ïö©: {claude_response}")
                    return self._get_default_recommendations()

            except Exception as e:
                logger.error(
                    f"Claude Sonnet 4 Ìò∏Ï∂ú Ïã§Ìå® - ÏóêÎü¨ ÌÉÄÏûÖ: {type(e).__name__}"
                )
                logger.error(f"Claude Sonnet 4 Ìò∏Ï∂ú Ïã§Ìå® - ÏóêÎü¨ Î©îÏãúÏßÄ: {str(e)}")
                if hasattr(e, "response"):
                    logger.error(
                        f"AWS ÏùëÎãµ ÏΩîÎìú: {e.response.get('Error', {}).get('Code', 'Unknown')}"
                    )
                    logger.error(
                        f"AWS ÏùëÎãµ Î©îÏãúÏßÄ: {e.response.get('Error', {}).get('Message', 'Unknown')}"
                    )

                # Claude 3.7 Sonnet Ìò∏Ï∂ú (fallback)
                try:
                    logger.info(
                        f"Claude 3.7 Sonnet fallback ÏãúÏûë - Î™®Îç∏ID: {sonnet_3_7_model_id}"
                    )

                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    logger.info("Claude 3.7 Sonnet ÏùëÎãµ ÏàòÏã† ÏôÑÎ£å")

                    response_body = json.loads(response.get("body").read())
                    claude_response = response_body.get("content", [{}])[0].get(
                        "text", ""
                    )
                    logger.info(
                        f"Claude 3.7 ÏùëÎãµ ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(claude_response)} characters"
                    )

                    try:
                        # ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú ÏãúÎèÑ
                        import re

                        markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
                        markdown_match = re.search(
                            markdown_pattern, claude_response, re.DOTALL | re.IGNORECASE
                        )

                        if markdown_match:
                            json_content = markdown_match.group(1).strip()
                            logger.info(
                                f"Claude 3.7 ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú, Í∏∏Ïù¥: {len(json_content)}"
                            )
                            parsed_result = json.loads(json_content)
                        else:
                            # ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏù¥ ÏóÜÏúºÎ©¥ ÏßÅÏ†ë ÌååÏã± ÏãúÎèÑ
                            logger.info(
                                "Claude 3.7 ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ù ÏóÜÏùå, ÏßÅÏ†ë JSON ÌååÏã± ÏãúÎèÑ"
                            )
                            parsed_result = json.loads(claude_response)

                        logger.info("Claude 3.7 ÏùëÎãµ JSON ÌååÏã± ÏÑ±Í≥µ")

                        # ÌïÑÏöîÌïú ÌÇ§Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
                        if isinstance(parsed_result, dict) and (
                            "immediate_improvements" in parsed_result
                            or "action_items" in parsed_result
                        ):
                            improvements_count = len(
                                parsed_result.get("immediate_improvements", [])
                            )
                            actions_count = len(parsed_result.get("action_items", []))
                            logger.info(
                                f"Claude 3.7 Ïú†Ìö®Ìïú Í∂åÏû•ÏÇ¨Ìï≠ ÌååÏã± ÏôÑÎ£å: {improvements_count}Í∞ú Í∞úÏÑ†ÏÇ¨Ìï≠, {actions_count}Í∞ú Ïï°ÏÖòÏïÑÏù¥ÌÖú"
                            )
                            return parsed_result
                        else:
                            logger.warning("Claude 3.7 ÌååÏã±Îêú JSONÏóê ÌïÑÏöîÌïú ÌÇ§Í∞Ä ÏóÜÏùå")
                            return self._get_default_recommendations()

                    except json.JSONDecodeError as json_err:
                        logger.error(f"Claude 3.7 ÏùëÎãµ JSON ÌååÏã± Ïã§Ìå®: {json_err}")
                        logger.info("Claude 3.7 ÌÖçÏä§Ìä∏ ÌååÏã± ÏãúÎèÑ")
                        # ÌÖçÏä§Ìä∏ÏóêÏÑú JSON Ï∂îÏ∂ú ÏãúÎèÑ
                        parsed_result = self._parse_claude_text_response(
                            claude_response
                        )
                        if parsed_result:
                            logger.info("Claude 3.7 ÌÖçÏä§Ìä∏ ÌååÏã± ÏÑ±Í≥µ")
                            return parsed_result
                        logger.error(f"ÌååÏã± Ïã§Ìå®Ìïú ÏùëÎãµ ÎÇ¥Ïö©: {claude_response}")
                        return self._get_default_recommendations()

                except Exception as fallback_e:
                    logger.error(
                        f"Claude 3.7 Sonnet fallback Ïã§Ìå® - ÏóêÎü¨ ÌÉÄÏûÖ: {type(fallback_e).__name__}"
                    )
                    logger.error(
                        f"Claude 3.7 Sonnet fallback Ïã§Ìå® - ÏóêÎü¨ Î©îÏãúÏßÄ: {str(fallback_e)}"
                    )
                    if hasattr(fallback_e, "response"):
                        logger.error(
                            f"AWS fallback ÏùëÎãµ ÏΩîÎìú: {fallback_e.response.get('Error', {}).get('Code', 'Unknown')}"
                        )
                        logger.error(
                            f"AWS fallback ÏùëÎãµ Î©îÏãúÏßÄ: {fallback_e.response.get('Error', {}).get('Message', 'Unknown')}"
                        )
                    return self._get_default_recommendations()

        except Exception as e:
            logger.error(
                f"ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë Ï†ÑÏ≤¥ Ïò§Î•ò - ÏóêÎü¨ ÌÉÄÏûÖ: {type(e).__name__}"
            )
            logger.error(f"ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë Ï†ÑÏ≤¥ Ïò§Î•ò - ÏóêÎü¨ Î©îÏãúÏßÄ: {str(e)}")
            logger.error(
                f"ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë Ï†ÑÏ≤¥ Ïò§Î•ò - Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§:", exc_info=True
            )
            return self._get_default_recommendations()

    def _parse_claude_text_response(
        self, text_response: str
    ) -> Optional[Dict[str, Any]]:
        """Claude ÌÖçÏä§Ìä∏ ÏùëÎãµÏóêÏÑú JSON Ï∂îÏ∂ú Î∞è ÌååÏã±"""
        try:
            import re

            logger.info(f"Claude ÏùëÎãµ ÌååÏã± ÏãúÏûë, ÏùëÎãµ Í∏∏Ïù¥: {len(text_response)}")
            logger.debug(f"ÏùëÎãµ ÏãúÏûë Î∂ÄÎ∂Ñ: {text_response[:200]}")

            # Î®ºÏ†Ä ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú
            markdown_pattern = r"```(?:json)?\s*(.*?)\s*```"
            markdown_match = re.search(
                markdown_pattern, text_response, re.DOTALL | re.IGNORECASE
            )

            if markdown_match:
                json_content = markdown_match.group(1).strip()
                logger.info(
                    f"ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏóêÏÑú JSON Ï∂îÏ∂ú ÏÑ±Í≥µ, Í∏∏Ïù¥: {len(json_content)}"
                )
                logger.debug(f"Ï∂îÏ∂úÎêú JSON ÏãúÏûë Î∂ÄÎ∂Ñ: {json_content[:200]}")
                try:
                    parsed = json.loads(json_content)
                    if isinstance(parsed, dict) and (
                        "immediate_improvements" in parsed or "action_items" in parsed
                    ):
                        logger.info(
                            f"ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ù JSON ÌååÏã± ÏÑ±Í≥µ - Í∞úÏÑ†ÏÇ¨Ìï≠: {len(parsed.get('immediate_improvements', []))}Í∞ú, Ïï°ÏÖòÏïÑÏù¥ÌÖú: {len(parsed.get('action_items', []))}Í∞ú"
                        )
                        return parsed
                    else:
                        logger.warning("ÌååÏã±Îêú JSONÏóê ÌïÑÏöîÌïú ÌÇ§Í∞Ä ÏóÜÏùå")
                except json.JSONDecodeError as e:
                    logger.error(f"ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ù JSON ÌååÏã± Ïã§Ìå®: {e}")
                    logger.debug(
                        f"ÌååÏã± Ïã§Ìå®Ìïú JSON ÎÇ¥Ïö© (Ï≤òÏùå 500Ïûê): {json_content[:500]}"
                    )
            else:
                logger.warning("ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ùÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")

            # ÎßàÌÅ¨Îã§Ïö¥ Î∏îÎ°ùÏù¥ ÏóÜÍ±∞ÎÇò ÌååÏã± Ïã§Ìå®Ïãú Îã§Î•∏ Ìå®ÌÑ¥Îì§ ÏãúÎèÑ
            logger.info("ÎåÄÏ≤¥ JSON Ìå®ÌÑ¥ Îß§Ïπ≠ ÏãúÎèÑ")
            json_patterns = [
                r'(\{[^{}]*"immediate_improvements"[^{}]*\})',
                r'(\{.*?"immediate_improvements".*?\})',
                r'(\{.*?"action_items".*?\})',
                r"(\{.*?\})",
            ]

            for i, pattern in enumerate(json_patterns):
                logger.debug(f"Ìå®ÌÑ¥ {i+1} ÏãúÎèÑ: {pattern}")
                matches = re.findall(pattern, text_response, re.DOTALL | re.IGNORECASE)
                logger.debug(f"Ìå®ÌÑ¥ {i+1}ÏóêÏÑú {len(matches)}Í∞ú Îß§Ïπò Î∞úÍ≤¨")
                for j, match in enumerate(matches):
                    try:
                        parsed = json.loads(match)
                        if isinstance(parsed, dict) and (
                            "immediate_improvements" in parsed
                            or "action_items" in parsed
                        ):
                            logger.info(f"JSON Ìå®ÌÑ¥ Îß§Ïπ≠ ÏÑ±Í≥µ: Ìå®ÌÑ¥ {i+1}, Îß§Ïπò {j+1}")
                            return parsed
                    except json.JSONDecodeError as e:
                        logger.debug(f"Ìå®ÌÑ¥ {i+1}, Îß§Ïπò {j+1} JSON ÌååÏã± Ïã§Ìå®: {e}")
                        continue

            # Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏ÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú
            logger.info("Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏ ÌååÏã± ÏãúÎèÑ")
            return self._extract_from_structured_text(text_response)

        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ ÌååÏã± Ï§ë Ïò§Î•ò: {e}")
            return None

    def _extract_from_structured_text(self, text: str) -> Optional[Dict[str, Any]]:
        """Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏ÏóêÏÑú Í∂åÏû•ÏÇ¨Ìï≠ Ï∂îÏ∂ú"""
        try:
            import re

            result = {"immediate_improvements": [], "action_items": []}

            # Í∞úÏÑ†ÏÇ¨Ìï≠ ÏÑπÏÖò Ï∞æÍ∏∞
            improvements_pattern = r'"immediate_improvements":\s*\[(.*?)\]'
            improvements_match = re.search(improvements_pattern, text, re.DOTALL)

            if improvements_match:
                improvements_text = improvements_match.group(1)
                # Í∞Å Í∞úÏÑ†ÏÇ¨Ìï≠ ÌååÏã±
                item_pattern = r"\{([^{}]+)\}"
                for item_match in re.finditer(item_pattern, improvements_text):
                    item_text = item_match.group(1)
                    improvement = self._parse_improvement_item(item_text)
                    if improvement:
                        result["immediate_improvements"].append(improvement)

            # Ïï°ÏÖò ÏïÑÏù¥ÌÖú ÏÑπÏÖò Ï∞æÍ∏∞
            actions_pattern = r'"action_items":\s*\[(.*?)\]'
            actions_match = re.search(actions_pattern, text, re.DOTALL)

            if actions_match:
                actions_text = actions_match.group(1)
                # Í∞Å Ïï°ÏÖò ÏïÑÏù¥ÌÖú ÌååÏã±
                for item_match in re.finditer(item_pattern, actions_text):
                    item_text = item_match.group(1)
                    action = self._parse_action_item(item_text)
                    if action:
                        result["action_items"].append(action)

            if result["immediate_improvements"] or result["action_items"]:
                return result

            return None

        except Exception as e:
            logger.error(f"Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ï§ë Ïò§Î•ò: {e}")
            return None

    def _parse_improvement_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """Í∞úÏÑ†ÏÇ¨Ìï≠ ÏïÑÏù¥ÌÖú ÌååÏã±"""
        try:
            import re

            improvement = {}

            # Í∞Å ÌïÑÎìú Ï∂îÏ∂ú
            fields = {
                "category": r'"category":\s*"([^"]+)"',
                "title": r'"title":\s*"([^"]+)"',
                "description": r'"description":\s*"([^"]+)"',
                "expected_impact": r'"expected_impact":\s*"([^"]+)"',
                "difficulty": r'"difficulty":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    improvement[field] = match.group(1)

            # items Î∞∞Ïó¥ Ï∂îÏ∂ú
            items_pattern = r'"items":\s*\[(.*?)\]'
            items_match = re.search(items_pattern, item_text, re.DOTALL)
            if items_match:
                items_text = items_match.group(1)
                items = re.findall(r'"([^"]+)"', items_text)
                improvement["items"] = items

            return improvement if improvement else None

        except Exception as e:
            logger.error(f"Í∞úÏÑ†ÏÇ¨Ìï≠ ÌååÏã± Ï§ë Ïò§Î•ò: {e}")
            return None

    def _parse_action_item(self, item_text: str) -> Optional[Dict[str, Any]]:
        """Ïï°ÏÖò ÏïÑÏù¥ÌÖú ÌååÏã±"""
        try:
            import re

            action = {}

            # Í∞Å ÌïÑÎìú Ï∂îÏ∂ú
            fields = {
                "priority": r'"priority":\s*"([^"]+)"',
                "item": r'"item":\s*"([^"]+)"',
                "estimated_time": r'"estimated_time":\s*"([^"]+)"',
                "assignee": r'"assignee":\s*"([^"]+)"',
                "rationale": r'"rationale":\s*"([^"]+)"',
            }

            for field, pattern in fields.items():
                match = re.search(pattern, item_text)
                if match:
                    action[field] = match.group(1)

            return action if action else None

        except Exception as e:
            logger.error(f"Ïï°ÏÖò ÏïÑÏù¥ÌÖú ÌååÏã± Ï§ë Ïò§Î•ò: {e}")
            return None

    def _generate_recommendations_html(self, recommendations: Dict[str, Any]) -> str:
        """Claude Í∂åÏû•ÏÇ¨Ìï≠ÏùÑ HTMLÎ°ú Î≥ÄÌôò"""
        try:
            html_parts = []

            # Ï¶âÏãú Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∞úÏÑ†ÏÇ¨Ìï≠
            html_parts.append("<h4>üöÄ Ï¶âÏãú Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∞úÏÑ†ÏÇ¨Ìï≠</h4>")

            improvements = recommendations.get("immediate_improvements", [])
            if not improvements:
                html_parts.append(
                    '<div class="info-box">ÌòÑÏû¨ Î∂ÑÏÑùÎêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑúÎäî Ï¶âÏãú Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∞úÏÑ†ÏÇ¨Ìï≠Ïù¥ Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.</div>'
                )
            else:
                for improvement in improvements:
                    category = improvement.get("category", "Í∏∞ÌÉÄ")
                    title = improvement.get("title", "Í∞úÏÑ†ÏÇ¨Ìï≠")
                    description = improvement.get("description", "")
                    items = improvement.get("items", [])
                    expected_impact = improvement.get("expected_impact", "")
                    difficulty = improvement.get("difficulty", "Ï§ëÍ∞Ñ")

                    html_parts.append(
                        f"""
                    <div class="recommendation">
                        <strong>{category}: {title}</strong>
                        <p style="margin: 10px 0; color: #666;">{description}</p>
                        <ul style="margin-top: 10px; margin-left: 20px;">
                    """
                    )

                    for item in items:
                        html_parts.append(f"<li>{item}</li>")

                    html_parts.append(
                        f"""
                        </ul>
                        <div style="margin-top: 10px; font-size: 0.9em;">
                            <span style="color: #27ae60;"><strong>ÏòàÏÉÅ Ìö®Í≥º:</strong> {expected_impact}</span> | 
                            <span style="color: #3498db;"><strong>Íµ¨ÌòÑ ÎÇúÏù¥ÎèÑ:</strong> {difficulty}</span>
                        </div>
                    </div>
                    """
                    )

            # Ïï°ÏÖò ÏïÑÏù¥ÌÖú ÌÖåÏù¥Î∏î
            html_parts.append("<h4>üìã Ïï°ÏÖò ÏïÑÏù¥ÌÖú</h4>")

            actions = recommendations.get("action_items", [])
            if not actions:
                html_parts.append(
                    '<div class="info-box">ÌòÑÏû¨ Î∂ÑÏÑùÎêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑúÎäî ÌäπÎ≥ÑÌïú Ïï°ÏÖò ÏïÑÏù¥ÌÖúÏù¥ ÌïÑÏöîÌïòÏßÄ ÏïäÏäµÎãàÎã§.</div>'
                )
            else:
                html_parts.append(
                    """
                    <table class="table">
                        <thead>
                            <tr>
                                <th>Ïö∞ÏÑ†ÏàúÏúÑ</th>
                                <th>Ìï≠Î™©</th>
                                <th>ÏòàÏÉÅ ÏÜåÏöîÏãúÍ∞Ñ</th>
                                <th>Îã¥ÎãπÏûê</th>
                                <th>Í∑ºÍ±∞</th>
                            </tr>
                        </thead>
                        <tbody>
                """
                )

                for action in actions:
                    priority = action.get("priority", "Ï§ëÍ∞Ñ")
                    item = action.get("item", "Ïï°ÏÖò ÏïÑÏù¥ÌÖú")
                    estimated_time = action.get("estimated_time", "ÎØ∏Ï†ï")
                    assignee = action.get("assignee", "Îã¥ÎãπÏûê")
                    rationale = action.get("rationale", "")

                    # Ïö∞ÏÑ†ÏàúÏúÑÏóê Îî∞Î•∏ Ïä§ÌÉÄÏùº ÌÅ¥ÎûòÏä§
                    priority_class = {
                        "ÎÜíÏùå": "status-critical",
                        "Ï§ëÍ∞Ñ": "status-warning",
                        "ÎÇÆÏùå": "status-good",
                    }.get(priority, "status-warning")

                    html_parts.append(
                        f"""
                            <tr>
                                <td><span class="{priority_class}">{priority}</span></td>
                                <td>{item}</td>
                                <td>{estimated_time}</td>
                                <td>{assignee}</td>
                                <td style="font-size: 0.9em; color: #666;">{rationale}</td>
                            </tr>
                    """
                    )

                html_parts.append(
                    """
                        </tbody>
                    </table>
                """
                )

            return "".join(html_parts)

        except Exception as e:
            logger.error(f"HTML Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {e}")
            return f'<div class="issue">Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}</div>'

    def _get_default_recommendations(self) -> Dict[str, Any]:
        """Claude Ìò∏Ï∂ú Ïã§Ìå® Ïãú Í∏∞Î≥∏ Í∂åÏû•ÏÇ¨Ìï≠ Î∞òÌôò"""
        return {
            "immediate_improvements": [
                {
                    "category": "Î™®ÎãàÌÑ∞ÎßÅ",
                    "title": "Í∏∞Î≥∏ Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî",
                    "description": "Í∏∞Î≥∏Ï†ÅÏù∏ ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ Ï≤¥Í≥Ñ Íµ¨Ï∂ï",
                    "items": [
                        "CloudWatch ÏïåÎûå ÏÑ§Ï†ï",
                        "Performance Insights ÌôúÏÑ±Ìôî",
                        "Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ Ï†ïÍ∏∞ Î∂ÑÏÑù",
                    ],
                    "expected_impact": "ÏÑ±Îä• Î¨∏Ï†ú Ï°∞Í∏∞ Î∞úÍ≤¨",
                    "difficulty": "ÎÇÆÏùå",
                }
            ],
            "action_items": [
                {
                    "priority": "ÎÜíÏùå",
                    "item": "CloudWatch ÏïåÎûå ÏÑ§Ï†ï",
                    "estimated_time": "1Ïùº",
                    "assignee": "DBA",
                    "rationale": "ÏÑ±Îä• Î¨∏Ï†ú Ï°∞Í∏∞ Í∞êÏßÄÎ•º ÏúÑÌï¥ ÌïÑÏöî",
                }
            ],
        }

    async def extract_current_schema_info(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïùò Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        try:
            logger.info(f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏãúÏûë: database_secret={database_secret}")
            connection, tunnel_used = self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌôïÏù∏
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            logger.info(f"ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§: {current_db}")

            schema_info = {"tables": [], "columns": {}, "indexes": {}}

            # ÌÖåÏù¥Î∏î Î™©Î°ù Ï°∞Ìöå
            cursor.execute(
                """
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """
            )
            tables = [row[0] for row in cursor.fetchall()]
            schema_info["tables"] = tables
            logger.info(f"Î∞úÍ≤¨Îêú ÌÖåÏù¥Î∏î: {tables}")

            # Í∞Å ÌÖåÏù¥Î∏îÏùò Ïª¨Îüº Ï†ïÎ≥¥ Ï°∞Ìöå
            for table in tables:
                cursor.execute(
                    """
                    SELECT column_name, data_type, character_maximum_length, 
                           numeric_precision, numeric_scale, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table,),
                )

                columns = []
                for col_row in cursor.fetchall():
                    col_info = {
                        "name": col_row[0],
                        "data_type": col_row[1],
                        "max_length": col_row[2],
                        "precision": col_row[3],
                        "scale": col_row[4],
                        "is_nullable": col_row[5],
                        "default_value": col_row[6],
                    }
                    columns.append(col_info)

                schema_info["columns"][table] = columns

                # Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ Ï°∞Ìöå
                cursor.execute(
                    """
                    SELECT index_name, column_name, non_unique, seq_in_index
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table,),
                )

                indexes = {}
                for idx_row in cursor.fetchall():
                    idx_name = idx_row[0]
                    col_name = idx_row[1]
                    is_unique = idx_row[2] == 0

                    if idx_name not in indexes:
                        indexes[idx_name] = {"columns": [], "unique": is_unique}
                    indexes[idx_name]["columns"].append(col_name)

                schema_info["indexes"][table] = indexes

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            logger.info(
                f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏôÑÎ£å: {len(schema_info['tables'])}Í∞ú ÌÖåÏù¥Î∏î, {len(schema_info['columns'])}Í∞ú ÌÖåÏù¥Î∏îÏùò Ïª¨Îüº Ï†ïÎ≥¥"
            )
            return schema_info

        except Exception as e:
            logger.error(f"Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            if "tunnel_used" in locals() and tunnel_used:
                self.cleanup_ssh_tunnel()
            return {}

    async def extract_relevant_schema_info(
        self,
        ddl_content: str,
        database_secret: str,
        debug_log,
        use_ssh_tunnel: bool = True,
    ) -> Dict[str, Any]:
        """DDL Ïú†ÌòïÏóê Îî∞Îùº Í¥ÄÎ†®Îêú Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Îßå Ï∂îÏ∂ú (Í≥µÏö© Ïª§ÏÑú ÏÇ¨Ïö©)"""
        try:
            # DDL Ïú†Ìòï ÌååÏã±
            ddl_info = self.parse_ddl_detailed_with_debug(ddl_content, debug_log)
            if not ddl_info:
                return {}

            # Í≥µÏö© Ïª§ÏÑú ÏÇ¨Ïö©
            cursor = self.get_shared_cursor()
            if cursor is None:
                debug_log("Í≥µÏö© Ïª§ÏÑúÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏùå - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ëÎã®")
                return {}

            relevant_info = {}

            # ÌååÏùº ÎÇ¥ÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÎì§ÏùÑ Ï∂îÏ†Å
            created_tables_in_file = set()
            created_indexes_in_file = {}  # table_name -> [index_info]

            debug_log(f"Ï¥ù {len(ddl_info)}Í∞úÏùò DDL Íµ¨Î¨∏ Ï≤òÎ¶¨ ÏãúÏûë")

            for i, ddl_statement in enumerate(ddl_info):
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                debug_log(f"Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú: {ddl_type} for {table_name}")

                if ddl_type == "CREATE_TABLE":
                    # CREATE TABLE: ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
                    try:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (table_name,),
                        )
                        result = cursor.fetchone()
                        table_exists = result[0] > 0 if result else False

                        relevant_info[f"{i}_{table_name}"] = {
                            "exists": table_exists,
                            "type": "table",
                            "order": i,
                            "table_name": table_name,
                        }

                        # ÌååÏùº ÎÇ¥ÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÎ°ú Ï∂îÍ∞Ä
                        created_tables_in_file.add(table_name)
                        debug_log(
                            f"CREATE TABLE [{i}] - {table_name} Ï°¥Ïû¨: {table_exists}, ÌååÏùº ÎÇ¥ ÏÉùÏÑ± Ï∂îÍ∞Ä"
                        )
                    except Exception as e:
                        debug_log(f"CREATE TABLE Ï†ïÎ≥¥ Ï°∞Ìöå Ïò§Î•ò: {e}")
                        table_exists = False

                elif ddl_type == "ALTER_TABLE":
                    # ALTER TABLE: ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä + Ïª¨Îüº Ï†ïÎ≥¥ (ÌååÏùº ÎÇ¥ ÏÉùÏÑ± Í≥†Î†§)
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    # ÌååÏùº ÎÇ¥ÏóêÏÑú Ïù¥ÎØ∏ ÏÉùÏÑ±Îêú ÌÖåÏù¥Î∏îÏù∏ÏßÄ ÌôïÏù∏
                    created_in_file = table_name in created_tables_in_file
                    effective_exists = table_exists or created_in_file

                    columns_info = []
                    if table_exists:  # Ïã§Ï†ú DBÏóê Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞Îßå Ïª¨Îüº Ï†ïÎ≥¥ Ï°∞Ìöå
                        cursor.execute(
                            """
                            SELECT column_name, data_type, character_maximum_length, 
                                   numeric_precision, numeric_scale, is_nullable
                            FROM information_schema.columns 
                            WHERE table_schema = DATABASE() AND table_name = %s
                            ORDER BY ordinal_position
                        """,
                            (table_name,),
                        )

                        for col_row in cursor.fetchall():
                            columns_info.append(
                                {
                                    "name": col_row[0],
                                    "data_type": col_row[1],
                                    "max_length": col_row[2],
                                    "precision": col_row[3],
                                    "scale": col_row[4],
                                    "is_nullable": col_row[5],
                                }
                            )

                    relevant_info[f"{i}_{table_name}"] = {
                        "exists": table_exists,
                        "created_in_file": created_in_file,
                        "effective_exists": effective_exists,
                        "type": "table",
                        "columns": columns_info,
                        "alter_type": ddl_statement.get("alter_type", "GENERAL"),
                        "target_column": ddl_statement.get("column"),
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"ALTER TABLE [{i}] - {table_name} DBÏ°¥Ïû¨: {table_exists}, ÌååÏùºÎÇ¥ÏÉùÏÑ±: {created_in_file}, Ïú†Ìö®Ï°¥Ïû¨: {effective_exists}"
                    )

                elif ddl_type == "CREATE_INDEX":
                    # CREATE INDEX: ÌÖåÏù¥Î∏î Ï°¥Ïû¨ + Ïù∏Îç±Ïä§ Ï†ïÎ≥¥ + ÎèôÏùº Ïª¨Îüº Íµ¨ÏÑ± Ïù∏Îç±Ïä§ ÌôïÏù∏ (ÌååÏùº ÎÇ¥ ÏÉùÏÑ± Í≥†Î†§)
                    index_name = ddl_statement.get("index_name")
                    columns_str = ddl_statement.get("columns", "")
                    target_columns = [
                        col.strip().strip("`").lower() for col in columns_str.split(",")
                    ]

                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    # ÌååÏùº ÎÇ¥ÏóêÏÑú Ïù¥ÎØ∏ ÏÉùÏÑ±Îêú ÌÖåÏù¥Î∏îÏù∏ÏßÄ ÌôïÏù∏
                    created_in_file = table_name in created_tables_in_file
                    effective_exists = table_exists or created_in_file

                    existing_indexes = []
                    duplicate_column_indexes = []
                    file_duplicate_indexes = []

                    # DBÏóê Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî Ïù∏Îç±Ïä§ ÌôïÏù∏
                    if table_exists:
                        cursor.execute(
                            """
                            SELECT index_name, GROUP_CONCAT(column_name ORDER BY seq_in_index) as columns,
                                   non_unique
                            FROM information_schema.statistics 
                            WHERE table_schema = DATABASE() AND table_name = %s
                            GROUP BY index_name, non_unique
                        """,
                            (table_name,),
                        )

                        target_columns_sorted = ",".join(sorted(target_columns))

                        for idx_row in cursor.fetchall():
                            idx_name = idx_row[0]
                            idx_columns = idx_row[1].lower()
                            is_unique = idx_row[2] == 0

                            existing_indexes.append(
                                {
                                    "name": idx_name,
                                    "columns": idx_columns,
                                    "unique": is_unique,
                                }
                            )

                            # ÎèôÏùºÌïú Ïª¨Îüº Íµ¨ÏÑ± ÌôïÏù∏
                            if idx_columns == target_columns_sorted:
                                duplicate_column_indexes.append(
                                    {
                                        "name": idx_name,
                                        "columns": idx_columns,
                                        "unique": is_unique,
                                    }
                                )

                    # ÌååÏùº ÎÇ¥ÏóêÏÑú Ïù¥ÎØ∏ ÏÉùÏÑ±Îêú Ïù∏Îç±Ïä§ÏôÄ Ï§ëÎ≥µ ÌôïÏù∏
                    target_columns_sorted = ",".join(sorted(target_columns))
                    if table_name in created_indexes_in_file:
                        for file_idx in created_indexes_in_file[table_name]:
                            if file_idx["columns_sorted"] == target_columns_sorted:
                                file_duplicate_indexes.append(file_idx)

                    # ÌååÏùº ÎÇ¥ ÏÉùÏÑ± Ïù∏Îç±Ïä§Î°ú Ï∂îÍ∞Ä
                    if table_name not in created_indexes_in_file:
                        created_indexes_in_file[table_name] = []
                    created_indexes_in_file[table_name].append(
                        {
                            "name": index_name,
                            "columns": target_columns,
                            "columns_sorted": target_columns_sorted,
                            "order": i,
                        }
                    )

                    relevant_info[f"{i}_{table_name}.{index_name}"] = {
                        "type": "index",
                        "table_exists": table_exists,
                        "created_in_file": created_in_file,
                        "effective_exists": effective_exists,
                        "index_name": index_name,
                        "target_columns": target_columns,
                        "existing_indexes": existing_indexes,
                        "duplicate_column_indexes": duplicate_column_indexes,
                        "file_duplicate_indexes": file_duplicate_indexes,
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"CREATE INDEX [{i}] - {table_name}.{index_name}, DBÏ§ëÎ≥µ: {len(duplicate_column_indexes)}Í∞ú, ÌååÏùºÏ§ëÎ≥µ: {len(file_duplicate_indexes)}Í∞ú"
                    )

                elif ddl_type == "DROP_TABLE":
                    # DROP TABLE: ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (ÌååÏùº ÎÇ¥ ÏÉùÏÑ±ÏùÄ Í≥†Î†§ÌïòÏßÄ ÏïäÏùå - DROPÏùÄ Ïã§Ï†ú Ï°¥Ïû¨Ìï¥Ïïº Ìï®)
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    relevant_info[f"{i}_{table_name}"] = {
                        "exists": table_exists,
                        "type": "table",
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(f"DROP TABLE [{i}] - {table_name} Ï°¥Ïû¨: {table_exists}")

                elif ddl_type == "DROP_INDEX":
                    # DROP INDEX: ÌÖåÏù¥Î∏î Ï°¥Ïû¨ + Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä (ÌååÏùº ÎÇ¥ ÏÉùÏÑ±ÏùÄ Í≥†Î†§ÌïòÏßÄ ÏïäÏùå)
                    index_name = ddl_statement.get("index_name")

                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (table_name,),
                    )
                    table_exists = cursor.fetchone()[0] > 0

                    index_exists = False
                    if table_exists:
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.statistics 
                            WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
                        """,
                            (table_name, index_name),
                        )
                        index_exists = cursor.fetchone()[0] > 0

                    relevant_info[f"{i}_{table_name}.{index_name}"] = {
                        "type": "index",
                        "table_exists": table_exists,
                        "index_name": index_name,
                        "index_exists": index_exists,
                        "order": i,
                        "table_name": table_name,
                    }
                    debug_log(
                        f"DROP INDEX [{i}] - {table_name}.{index_name} Ï°¥Ïû¨: {index_exists}"
                    )

            debug_log(f"Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏôÑÎ£å: {len(relevant_info)}Í∞ú Ìï≠Î™©")
            return relevant_info

        except Exception as e:
            logger.error(f"Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            debug_log(f"Í¥ÄÎ†® Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏòàÏô∏: {e}")
            return {}

    def validate_dml_columns(self, sql_content: str, cursor, debug_log) -> dict:
        """DML ÏøºÎ¶¨Ïùò Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä Í≤ÄÏ¶ù - CREATE Íµ¨Î¨∏ Í≥†Î†§"""
        try:
            debug_log("=== DML Ïª¨Îüº Í≤ÄÏ¶ù ÏãúÏûë ===")
            if not sqlparse:
                debug_log("sqlparse Î™®ÎìàÏù¥ ÏóÜÏñ¥ DML Ïª¨Îüº Í≤ÄÏ¶ùÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§")
                return {"total_queries": 0, "queries_with_issues": 0, "results": []}

            # ÌòÑÏû¨ SQLÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏î Ï∂îÏ∂ú
            created_tables = self.extract_created_tables(sql_content)
            debug_log(f"DML Í≤ÄÏ¶ùÏóêÏÑú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏î: {created_tables}")

            # Ïä§ÌÇ§Îßà Ï∫êÏãú
            schema_cache = {}

            def get_table_columns(table_name: str) -> set:
                """ÌÖåÏù¥Î∏îÏùò Ïª¨Îüº Î™©Î°ù Ï°∞Ìöå (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)"""
                if table_name in schema_cache:
                    return schema_cache[table_name]

                try:
                    schema, actual_table = parse_table_name(table_name)

                    if schema:
                        cursor.execute(
                            """
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        cursor.execute(
                            """
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )

                    columns = {row[0].lower() for row in cursor.fetchall()}
                    schema_cache[table_name] = columns
                    debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ïª¨Îüº Ï°∞Ìöå: {columns}")
                    return columns
                except Exception as e:
                    debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ïª¨Îüº Ï°∞Ìöå Ïã§Ìå®: {e}")
                    return set()

            def table_exists(table_name: str) -> bool:
                """ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)"""
                try:
                    schema, actual_table = parse_table_name(table_name)

                    if schema:
                        # Ïä§ÌÇ§ÎßàÍ∞Ä Î™ÖÏãúÎêú Í≤ΩÏö∞
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                        """,
                            (schema, actual_table),
                        )
                    else:
                        # Ïä§ÌÇ§ÎßàÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Í≤ÄÏÉâ
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.tables 
                            WHERE table_schema = DATABASE() AND table_name = %s
                        """,
                            (actual_table,),
                        )
                    return cursor.fetchone()[0] > 0
                except:
                    return False

            # SQL Î¨∏ÏóêÏÑú ÏÇ¨Ïö©Îêú Ïª¨ÎüºÎì§ Ï∂îÏ∂ú
            validation_results = []

            # Ï£ºÏÑù Ï†úÍ±∞
            cleaned_sql = re.sub(r"--.*$", "", sql_content, flags=re.MULTILINE)

            # Í∞Å ÏøºÎ¶¨Î≥ÑÎ°ú Í≤ÄÏ¶ù
            statements = sqlparse.split(cleaned_sql)
            debug_log(f"Ï¥ù {len(statements)}Í∞úÏùò Íµ¨Î¨∏ÏúºÎ°ú Î∂ÑÎ¶¨Îê®")

            for i, stmt in enumerate(statements):
                if not stmt.strip():
                    continue

                stmt_lower = stmt.lower()
                issues = []
                debug_log(f"Íµ¨Î¨∏ {i+1} Í≤ÄÏ¶ù ÏãúÏûë: {stmt_lower[:50]}...")

                # SELECT, UPDATE, DELETE ÏøºÎ¶¨ÏóêÏÑú Ïª¨Îüº Ï∂îÏ∂ú
                if any(
                    keyword in stmt_lower for keyword in ["select", "update", "delete"]
                ):
                    debug_log(f"Íµ¨Î¨∏ {i+1}ÏùÄ DML ÏøºÎ¶¨ÏûÖÎãàÎã§")
                    # FROM Ï†àÏóêÏÑú ÌÖåÏù¥Î∏î Ï∂îÏ∂ú
                    # FROMÍ≥º JOINÏóêÏÑú ÌÖåÏù¥Î∏îÎ™Ö Ï∂îÏ∂ú (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)
                    debug_log(f"Íµ¨Î¨∏ {i+1} ÏõêÎ≥∏: {stmt}")
                    from_pattern = r"from\s+(?:(\w+)\.)?(\w+)(?:\s+(?:as\s+)?(\w+))?(?=\s+(?:where|order|group|limit|join|inner|left|right|full|cross|$|;))"
                    from_tables = re.findall(from_pattern, stmt_lower)
                    debug_log(f"Íµ¨Î¨∏ {i+1} FROM Ìå®ÌÑ¥ Í≤∞Í≥º: {from_tables}")

                    join_pattern = r"join\s+(?:(\w+)\.)?(\w+)(?:\s+(?:as\s+)?(\w+))?(?=\s+(?:on|$|;))"
                    join_tables = re.findall(join_pattern, stmt_lower)
                    debug_log(f"Íµ¨Î¨∏ {i+1} JOIN Ìå®ÌÑ¥ Í≤∞Í≥º: {join_tables}")

                    # ÌÖåÏù¥Î∏î Î≥ÑÏπ≠ Îß§Ìïë
                    table_aliases = {}
                    all_tables = set()

                    for schema, table, alias in from_tables + join_tables:
                        full_table_name = f"{schema}.{table}" if schema else table
                        all_tables.add(full_table_name)
                        debug_log(
                            f"Íµ¨Î¨∏ {i+1} ÌÖåÏù¥Î∏î Ï∂îÍ∞Ä: schema={schema}, table={table}, full_name={full_table_name}"
                        )
                        if alias and alias not in [
                            "where",
                            "order",
                            "group",
                            "limit",
                            "join",
                            "inner",
                            "left",
                            "right",
                            "full",
                            "cross",
                            "on",
                        ]:
                            table_aliases[alias] = full_table_name

                    debug_log(f"Íµ¨Î¨∏ {i+1}ÏóêÏÑú Ï∞∏Ï°∞ÌïòÎäî ÌÖåÏù¥Î∏î: {all_tables}")

                    # ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞ÌïòÎäîÏßÄ ÌôïÏù∏
                    references_new_table = any(
                        table in created_tables for table in all_tables
                    )
                    if references_new_table:
                        debug_log(
                            f"Íµ¨Î¨∏ {i+1}: ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÏùÑ Ï∞∏Ï°∞ÌïòÎØÄÎ°ú DML Ïª¨Îüº Í≤ÄÏ¶ù Ïä§ÌÇµ: {[t for t in all_tables if t in created_tables]}"
                        )
                        continue

                    # ÌÖåÏù¥Î∏î.Ïª¨Îüº ÌòïÌÉúÏùò Ïª¨Îüº Ï∞∏Ï°∞ Ï∞æÍ∏∞ (FROM/JOINÏóêÏÑú Ïù¥ÎØ∏ Ï∂îÏ∂úÎêú ÌÖåÏù¥Î∏îÎßå Í≥†Î†§)
                    column_refs = []
                    for table_or_alias in all_tables | set(table_aliases.keys()):
                        # Í∞Å ÌÖåÏù¥Î∏î/Î≥ÑÏπ≠Ïóê ÎåÄÌï¥ Ïª¨Îüº Ï∞∏Ï°∞ Ï∞æÍ∏∞
                        pattern = rf"\b{re.escape(table_or_alias)}\.(\w+)"
                        matches = re.findall(pattern, stmt_lower)
                        for column in matches:
                            column_refs.append((table_or_alias, column))

                    debug_log(f"Íµ¨Î¨∏ {i+1}ÏóêÏÑú Î∞úÍ≤¨Îêú Ïª¨Îüº Ï∞∏Ï°∞: {column_refs}")

                    # Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä Í≤ÄÏ¶ù
                    for table_ref, column_name in column_refs:
                        # Ïã§Ï†ú ÌÖåÏù¥Î∏îÎ™Ö Ìï¥Í≤∞
                        actual_table = table_aliases.get(table_ref, table_ref)

                        # ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎäî ÌÖåÏù¥Î∏îÏù¥Î©¥ Ïä§ÌÇµ (Ïù¥Ï§ë Ï≤¥ÌÅ¨)
                        if actual_table in created_tables:
                            debug_log(
                                f"ÌÖåÏù¥Î∏î '{actual_table}'ÏùÄ ÏÉàÎ°ú ÏÉùÏÑ±ÎêòÎØÄÎ°ú Ïª¨Îüº Í≤ÄÏ¶ù Ïä§ÌÇµ"
                            )
                            continue

                        if (
                            actual_table in all_tables
                            or actual_table in schema_cache
                            or table_exists(actual_table)
                        ):
                            existing_columns = get_table_columns(actual_table)

                            if (
                                column_name not in existing_columns
                                and column_name != "*"
                            ):
                                issues.append(
                                    {
                                        "type": "MISSING_COLUMN",
                                        "table": actual_table,
                                        "column": column_name,
                                        "message": f"Ïª¨Îüº '{column_name}'Ïù¥ ÌÖåÏù¥Î∏î '{actual_table}'Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§",
                                    }
                                )
                else:
                    debug_log(f"Íµ¨Î¨∏ {i+1}ÏùÄ DML ÏøºÎ¶¨Í∞Ä ÏïÑÎãôÎãàÎã§")

                if issues:
                    validation_results.append(
                        {
                            "sql": (
                                stmt.strip()[:100] + "..."
                                if len(stmt.strip()) > 100
                                else stmt.strip()
                            ),
                            "issues": issues,
                        }
                    )

            debug_log(
                f"=== DML Ïª¨Îüº Í≤ÄÏ¶ù ÏôÑÎ£å: {len(validation_results)}Í∞ú ÏøºÎ¶¨ÏóêÏÑú Ïù¥Ïäà Î∞úÍ≤¨ ==="
            )
            return {
                "total_queries": len([s for s in statements if s.strip()]),
                "queries_with_issues": len(validation_results),
                "results": validation_results,
            }

        except Exception as e:
            debug_log(f"DML Ïª¨Îüº Í≤ÄÏ¶ù ÏòàÏô∏: {e}")
            return {"total_queries": 0, "queries_with_issues": 0, "results": []}

    async def test_database_connection_for_validation(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """Í≤ÄÏ¶ùÏö© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db = cursor.fetchone()[0]

                cursor.close()
                connection.close()

                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "host": "localhost" if tunnel_used else "remote",
                    "port": 3307 if tunnel_used else 3306,
                }

                # SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {"success": False, "error": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."}

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"MySQL Ïò§Î•ò: {str(e)}"}
        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ Ïò§Î•ò: {str(e)}"}

    async def validate_schema_with_debug(
        self,
        ddl_content: str,
        database_secret: str,
        debug_log,
        use_ssh_tunnel: bool = True,
    ) -> Dict[str, Any]:
        """DDL Íµ¨Î¨∏ Ïú†ÌòïÏóê Îî∞Î•∏ Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        try:
            debug_log("validate_schema ÏãúÏûë")
            # DDL Íµ¨Î¨∏ Ïú†Ìòï Î∞è ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÌååÏã±
            ddl_info = self.parse_ddl_detailed_with_debug(ddl_content, debug_log)
            debug_log(f"ÌååÏã±Îêú DDL Ï†ïÎ≥¥: {ddl_info}")

            if not ddl_info:
                debug_log("DDL ÌååÏã± Ïã§Ìå®")
                return {
                    "success": False,
                    "error": "DDLÏóêÏÑú Íµ¨Î¨∏ Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§.",
                }

            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()
            debug_log(f"DB Ïó∞Í≤∞ ÏÑ±Í≥µ, ÌÑ∞ÎÑê ÏÇ¨Ïö©: {tunnel_used}")

            validation_results = []

            # DDL Íµ¨Î¨∏ Ïú†ÌòïÎ≥Ñ Í≤ÄÏ¶ù
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]
                debug_log(f"Í≤ÄÏ¶ù Ï§ë: {ddl_type} on {table_name}")

                if ddl_type == "CREATE_TABLE":
                    debug_log("CREATE_TABLE Í≤ÄÏ¶ù Ìò∏Ï∂ú")
                    result = await self.validate_create_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"CREATE_TABLE Í≤ÄÏ¶ù Í≤∞Í≥º: {result}")
                elif ddl_type == "ALTER_TABLE":
                    debug_log("ALTER_TABLE Í≤ÄÏ¶ù Ìò∏Ï∂ú")
                    result = await self.validate_alter_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"ALTER_TABLE Í≤ÄÏ¶ù Í≤∞Í≥º: {result}")
                elif ddl_type == "CREATE_INDEX":
                    debug_log("CREATE_INDEX Í≤ÄÏ¶ù Ìò∏Ï∂ú")
                    result = await self.validate_create_index_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"CREATE_INDEX Í≤ÄÏ¶ù Í≤∞Í≥º: {result}")
                elif ddl_type == "DROP_TABLE":
                    debug_log("DROP_TABLE Í≤ÄÏ¶ù Ìò∏Ï∂ú")
                    result = await self.validate_drop_table_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"DROP_TABLE Í≤ÄÏ¶ù Í≤∞Í≥º: {result}")
                elif ddl_type == "DROP_INDEX":
                    debug_log("DROP_INDEX Í≤ÄÏ¶ù Ìò∏Ï∂ú")
                    result = await self.validate_drop_index_with_debug(
                        cursor, ddl_statement, debug_log
                    )
                    debug_log(f"DROP_INDEX Í≤ÄÏ¶ù Í≤∞Í≥º: {result}")
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî DDL Íµ¨Î¨∏ Ïú†Ìòï: {ddl_type}"],
                    }

                validation_results.append(result)
                debug_log(f"Í≤ÄÏ¶ù Í≤∞Í≥º Ï∂îÍ∞ÄÎê®: {result}")

            cursor.close()
            connection.close()

            # SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            debug_log(f"validate_schema ÏôÑÎ£å, Í≤∞Í≥º Í∞úÏàò: {len(validation_results)}")
            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            debug_log(f"validate_schema ÏòàÏô∏: {e}")
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}"}

    def parse_ddl_detailed_with_debug(
        self, ddl_content: str, debug_log
    ) -> List[Dict[str, Any]]:
        """DDL Íµ¨Î¨∏ÏùÑ ÏÉÅÏÑ∏ÌïòÍ≤å ÌååÏã±ÌïòÏó¨ Íµ¨Î¨∏ Ïú†ÌòïÎ≥Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        ddl_statements = []

        debug_log(f"DDL ÌååÏã± ÏãúÏûë: {repr(ddl_content)}")

        # CREATE TABLE ÌååÏã±
        create_table_pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?"
        create_table_matches = re.findall(
            create_table_pattern, ddl_content, re.IGNORECASE
        )

        debug_log(f"CREATE TABLE ÌååÏã± - Í≤∞Í≥º: {create_table_matches}")

        for table_name in create_table_matches:
            ddl_statements.append({"type": "CREATE_TABLE", "table": table_name.lower()})
            debug_log(f"CREATE TABLE Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {table_name}")

        # ALTER TABLE ÌååÏã± (ÏÉÅÏÑ∏)
        # ADD COLUMN - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        alter_add_pattern = r"ALTER\s+TABLE\s+`?(?:(\w+)\.)?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)"
        alter_add_matches = re.findall(alter_add_pattern, ddl_content, re.IGNORECASE)
        debug_log(f"ALTER TABLE ADD COLUMN ÌååÏã± - Í≤∞Í≥º: {alter_add_matches}")

        for schema, table_name, column_name, column_def in alter_add_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": full_table_name.lower(),
                    "alter_type": "ADD_COLUMN",
                    "column": column_name.lower(),
                    "column_definition": column_def.strip(),
                }
            )
            debug_log(
                f"ALTER TABLE ADD COLUMN Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {full_table_name}.{column_name}"
            )

        # MODIFY COLUMN
        alter_modify_pattern = (
            r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)"
        )
        alter_modify_matches = re.findall(
            alter_modify_pattern, ddl_content, re.IGNORECASE
        )
        debug_log(f"ALTER TABLE MODIFY COLUMN ÌååÏã± - Í≤∞Í≥º: {alter_modify_matches}")

        for table_name, column_name, column_def in alter_modify_matches:
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": table_name.lower(),
                    "alter_type": "MODIFY_COLUMN",
                    "column": column_name.lower(),
                    "column_definition": column_def.strip(),
                }
            )
            debug_log(
                f"ALTER TABLE MODIFY COLUMN Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {table_name}.{column_name}"
            )

        # DROP COLUMN
        alter_drop_pattern = (
            r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?"
        )
        alter_drop_matches = re.findall(alter_drop_pattern, ddl_content, re.IGNORECASE)
        debug_log(f"ALTER TABLE DROP COLUMN ÌååÏã± - Í≤∞Í≥º: {alter_drop_matches}")

        for table_name, column_name in alter_drop_matches:
            ddl_statements.append(
                {
                    "type": "ALTER_TABLE",
                    "table": table_name.lower(),
                    "alter_type": "DROP_COLUMN",
                    "column": column_name.lower(),
                }
            )
            debug_log(
                f"ALTER TABLE DROP COLUMN Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {table_name}.{column_name}"
            )

        # ÏùºÎ∞ò ALTER TABLE (ÏúÑÏùò Ìå®ÌÑ¥Ïóê Îß§ÏπòÎêòÏßÄ ÏïäÎäî Í≤ΩÏö∞)
        # Ï£ºÏÑùÏùÑ ÏôÑÏ†ÑÌûà Ï†úÍ±∞Ìïú ÌõÑ Ïã§Ï†ú SQL Íµ¨Î¨∏Îßå Îß§Ïπò
        lines = ddl_content.split("\n")
        sql_lines = []
        for line in lines:
            # Ï£ºÏÑù Ï†úÍ±∞
            if "--" in line:
                line = line[: line.index("--")]
            line = line.strip()
            if line:
                sql_lines.append(line)

        clean_sql = " ".join(sql_lines)

        # ALTER TABLEÎ°ú ÏãúÏûëÌïòÎäî Ïã§Ï†ú Íµ¨Î¨∏Îßå Îß§Ïπò
        alter_table_pattern = r"\bALTER\s+TABLE\s+`?(\w+)`?\s"
        alter_table_matches = re.findall(alter_table_pattern, clean_sql, re.IGNORECASE)

        # Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú ÌÖåÏù¥Î∏îÎì§ Ï†úÏô∏ - ÏùºÎ∞ò ALTER TABLE ÌååÏã± ÎπÑÌôúÏÑ±Ìôî
        # (ÏÉÅÏÑ∏ ÌååÏã±ÏúºÎ°ú Ï∂©Î∂ÑÌïòÎØÄÎ°ú ÏùºÎ∞ò ÌååÏã±ÏùÄ Í±¥ÎÑàÎúÄ)
        debug_log("ÏùºÎ∞ò ALTER TABLE ÌååÏã± Í±¥ÎÑàÎúÄ - ÏÉÅÏÑ∏ ÌååÏã±ÏúºÎ°ú Ï∂©Î∂Ñ")

        # CREATE INDEX ÌååÏã± - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        create_index_pattern = r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(?:(\w+)\.)?(\w+)`?\s*\((.*?)\)"
        create_index_matches = re.findall(
            create_index_pattern, ddl_content, re.IGNORECASE
        )

        debug_log(f"CREATE INDEX ÌååÏã± - Í≤∞Í≥º: {create_index_matches}")

        for index_name, schema, table_name, columns in create_index_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": full_table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": columns.strip(),
                }
            )
            debug_log(
                f"CREATE INDEX Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {index_name} on {full_table_name}({columns})"
            )

        # DROP INDEX ÌååÏã± - Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(?:(\w+)\.)?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        debug_log(f"DROP INDEX ÌååÏã± - Ìå®ÌÑ¥: {drop_index_pattern}")
        debug_log(f"DROP INDEX ÌååÏã± - Í≤∞Í≥º: {drop_index_matches}")

        for index_name, schema, table_name in drop_index_matches:
            full_table_name = f"{schema}.{table_name}" if schema else table_name
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": full_table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )
            debug_log(f"DROP INDEX Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {index_name} on {full_table_name}")

        debug_log(f"Ï†ÑÏ≤¥ ÌååÏã± Í≤∞Í≥º: {len(ddl_statements)}Í∞ú Íµ¨Î¨∏")
        for i, stmt in enumerate(ddl_statements):
            debug_log(f"  [{i}] {stmt['type']}: {stmt}")

        return ddl_statements

    async def validate_create_index_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """CREATE INDEX Íµ¨Î¨∏ Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        debug_log(
            f"CREATE INDEX Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}, index={index_name}, columns={columns}"
        )

        issues = []

        # 1. ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)
        schema, actual_table = self.parse_table_name(table_name)
        if schema:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = %s AND table_name = %s
            """,
                (schema, actual_table),
            )
        else:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (actual_table,),
            )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if not table_exists:
            issues.append(
                f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. CREATE INDEXÎ•º Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            debug_log(f"Ïò§Î•ò: ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
        else:
            # 2. Ïù∏Îç±Ïä§ Ïù¥Î¶Ñ Ï§ëÎ≥µ ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            debug_log(f"Ïù∏Îç±Ïä§ '{index_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {index_exists}")

            if index_exists:
                issues.append(f"Ïù∏Îç±Ïä§ '{index_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§.")
                debug_log(f"Ïò§Î•ò: Ïù∏Îç±Ïä§ '{index_name}' Ïù¥ÎØ∏ Ï°¥Ïû¨")

            # 3. Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}
            debug_log(f"Í∏∞Ï°¥ Ïª¨Îüº: {existing_columns}")

            # Ïù∏Îç±Ïä§ Ïª¨Îüº ÌååÏã±
            index_columns = [
                col.strip().strip("`").lower() for col in columns.split(",")
            ]
            debug_log(f"Ïù∏Îç±Ïä§ ÎåÄÏÉÅ Ïª¨Îüº: {index_columns}")

            for col in index_columns:
                if col not in existing_columns:
                    issues.append(
                        f"Ïª¨Îüº '{col}'Ïù¥ ÌÖåÏù¥Î∏î '{table_name}'Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                    )
                    debug_log(f"Ïò§Î•ò: Ïª¨Îüº '{col}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")

            # 4. ÎèôÏùºÌïú Ïª¨Îüº Íµ¨ÏÑ±Ïùò Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            if not issues:  # Í∏∞Î≥∏ Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌïú Í≤ΩÏö∞ÏóêÎßå
                cursor.execute(
                    """
                    SELECT index_name, GROUP_CONCAT(column_name ORDER BY seq_in_index) as columns
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    GROUP BY index_name
                """,
                    (table_name,),
                )

                existing_index_columns = {}
                for row in cursor.fetchall():
                    existing_index_columns[row[0]] = row[1].lower()

                target_columns = ",".join(sorted(index_columns))
                debug_log(f"ÎåÄÏÉÅ Ïª¨Îüº Íµ¨ÏÑ±: {target_columns}")
                debug_log(f"Í∏∞Ï°¥ Ïù∏Îç±Ïä§ Ïª¨Îüº Íµ¨ÏÑ±: {existing_index_columns}")

                for idx_name, idx_columns in existing_index_columns.items():
                    if idx_columns == target_columns:
                        issues.append(
                            f"ÎèôÏùºÌïú Ïª¨Îüº Íµ¨ÏÑ±Ïùò Ïù∏Îç±Ïä§ '{idx_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§."
                        )
                        debug_log(f"Ïò§Î•ò: ÎèôÏùºÌïú Ïª¨Îüº Íµ¨ÏÑ±Ïùò Ïù∏Îç±Ïä§ '{idx_name}' Ï°¥Ïû¨")
                        break

        result = {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "index_name": index_name,
                "columns": index_columns if "index_columns" in locals() else [],
            },
        }

        debug_log(
            f"CREATE INDEX Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    async def validate_drop_index_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """DROP INDEX Íµ¨Î¨∏ Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        debug_log(f"DROP INDEX Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}, index={index_name}")

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ìè¨Ìï® Ï≤òÎ¶¨)
        schema, actual_table = self.parse_table_name(table_name)
        if schema:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = %s AND table_name = %s
            """,
                (schema, actual_table),
            )
        else:
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (actual_table,),
            )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if not table_exists:
            issues.append(
                f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. DROP INDEXÎ•º Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            debug_log(f"Ïò§Î•ò: ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
        else:
            # Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            debug_log(f"Ïù∏Îç±Ïä§ '{index_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {index_exists}")

            if not index_exists:
                issues.append(
                    f"Ïù∏Îç±Ïä§ '{index_name}'Ïù¥ ÌÖåÏù¥Î∏î '{table_name}'Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                )
                debug_log(f"Ïò§Î•ò: Ïù∏Îç±Ïä§ '{index_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
            else:
                debug_log(f"Ïù∏Îç±Ïä§ '{index_name}' Ï°¥Ïû¨Ìï® - DROP INDEX Í∞ÄÎä•")

        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "index_name": index_name,
                "index_exists": index_exists if "index_exists" in locals() else False,
            },
        }

        debug_log(
            f"DROP INDEX Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    async def validate_create_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """CREATE TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        table_name = ddl_statement["table"]

        debug_log(f"CREATE TABLE Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}")

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if table_exists:
            issues.append(
                f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§. CREATE TABLEÏùÄ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            debug_log(f"Ïò§Î•ò: ÌÖåÏù¥Î∏î '{table_name}' Ïù¥ÎØ∏ Ï°¥Ïû¨")
        else:
            debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå - CREATE TABLE Í∞ÄÎä•")

        result = {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"table_exists": table_exists, "can_create": not table_exists},
        }

        debug_log(
            f"CREATE TABLE Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    async def validate_alter_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """ALTER TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        table_name = ddl_statement["table"]

        debug_log(f"ALTER TABLE Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}")

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if not table_exists:
            issues.append(
                f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. ALTER TABLEÏùÑ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            debug_log(f"Ïò§Î•ò: ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")

            result = {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "valid": False,
                "issues": issues,
                "details": {"table_exists": table_exists},
            }
            return result

        # ÌÖåÏù¥Î∏îÏù¥ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞, ÌòÑÏû¨ Ïª¨Îüº Ï†ïÎ≥¥ Ï°∞Ìöå
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, 
                   numeric_precision, numeric_scale, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
            ORDER BY ordinal_position
        """,
            (table_name,),
        )

        existing_columns = {}
        for col_row in cursor.fetchall():
            col_name = col_row[0].lower()
            existing_columns[col_name] = {
                "data_type": col_row[1].upper(),
                "max_length": col_row[2],
                "precision": col_row[3],
                "scale": col_row[4],
                "is_nullable": col_row[5],
                "default_value": col_row[6],
            }

        debug_log(f"Í∏∞Ï°¥ Ïª¨Îüº Î™©Î°ù: {list(existing_columns.keys())}")

        # ALTER ÌÉÄÏûÖÎ≥Ñ Í≤ÄÏ¶ù
        alter_type = ddl_statement.get("alter_type", "GENERAL")
        debug_log(f"ALTER ÌÉÄÏûÖ: {alter_type}")

        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement.get("column")
            if column_name and column_name in existing_columns:
                issues.append(
                    f"Ïª¨Îüº '{column_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§. ADD COLUMNÏùÑ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
                )
                debug_log(f"Ïò§Î•ò: Ïª¨Îüº '{column_name}' Ïù¥ÎØ∏ Ï°¥Ïû¨")
            else:
                debug_log(f"Ïª¨Îüº '{column_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå - ADD COLUMN Í∞ÄÎä•")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement.get("column")
            if column_name and column_name not in existing_columns:
                issues.append(
                    f"Ïª¨Îüº '{column_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. DROP COLUMNÏùÑ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
                )
                debug_log(f"Ïò§Î•ò: Ïª¨Îüº '{column_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
            else:
                debug_log(f"Ïª¨Îüº '{column_name}' Ï°¥Ïû¨Ìï® - DROP COLUMN Í∞ÄÎä•")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement.get("column")
            column_definition = ddl_statement.get("column_definition", "")

            if column_name and column_name not in existing_columns:
                issues.append(
                    f"Ïª¨Îüº '{column_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. MODIFY COLUMNÏùÑ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
                )
                debug_log(f"Ïò§Î•ò: Ïª¨Îüº '{column_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
            else:
                debug_log(f"Ïª¨Îüº '{column_name}' Ï°¥Ïû¨Ìï® - MODIFY COLUMN Í∞ÄÎä•")

                # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù
                if column_name in existing_columns:
                    existing_col = existing_columns[column_name]
                    compatibility_result = self.validate_column_type_compatibility(
                        existing_col, column_definition, debug_log
                    )
                    if not compatibility_result["compatible"]:
                        issues.extend(compatibility_result["issues"])

        result = {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {
                "table_exists": table_exists,
                "existing_columns": list(existing_columns.keys()),
                "column_count": len(existing_columns),
                "target_column": ddl_statement.get("column"),
            },
        }

        debug_log(
            f"ALTER TABLE Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    async def validate_constraints(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """Ï†úÏïΩÏ°∞Í±¥ Í≤ÄÏ¶ù - FK, Ïù∏Îç±Ïä§, Ï†úÏïΩÏ°∞Í±¥ ÌôïÏù∏"""
        try:
            # DDLÏóêÏÑú Ï†úÏïΩÏ°∞Í±¥ Ï†ïÎ≥¥ Ï∂îÏ∂ú
            constraints_info = self.parse_ddl_constraints(ddl_content)

            connection, tunnel_used = await self.get_db_connection(
                database_secret, self.selected_database, use_ssh_tunnel
            )
            cursor = connection.cursor()

            constraint_results = []

            # Ïô∏ÎûòÌÇ§ Ï†úÏïΩÏ°∞Í±¥ Í≤ÄÏ¶ù
            if constraints_info.get("foreign_keys"):
                for fk in constraints_info["foreign_keys"]:
                    # Ï∞∏Ï°∞ ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (fk["referenced_table"],),
                    )

                    ref_table_exists = cursor.fetchone()[0] > 0

                    if ref_table_exists:
                        # Ï∞∏Ï°∞ Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_schema = DATABASE() 
                            AND table_name = %s AND column_name = %s
                        """,
                            (fk["referenced_table"], fk["referenced_column"]),
                        )

                        ref_column_exists = cursor.fetchone()[0] > 0

                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": ref_column_exists,
                                "issue": (
                                    None
                                    if ref_column_exists
                                    else f"Ï∞∏Ï°∞ Ïª¨Îüº '{fk['referenced_table']}.{fk['referenced_column']}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                                ),
                            }
                        )
                    else:
                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": False,
                                "issue": f"Ï∞∏Ï°∞ ÌÖåÏù¥Î∏î '{fk['referenced_table']}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.",
                            }
                        )

            cursor.close()
            connection.close()

            # SSH ÌÑ∞ÎÑê Ï†ïÎ¶¨
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"Ï†úÏïΩÏ°∞Í±¥ Í≤ÄÏ¶ù Ïò§Î•ò: {str(e)}"}

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL Íµ¨Î¨∏ÏùÑ ÏÉÅÏÑ∏ÌïòÍ≤å ÌååÏã±ÌïòÏó¨ Íµ¨Î¨∏ Ïú†ÌòïÎ≥Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        ddl_statements = []

        # CREATE TABLE ÌååÏã±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(
            create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE
        )

        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append(
                {
                    "type": "CREATE_TABLE",
                    "table": table_name.lower(),
                    "columns": columns_info["columns"],
                    "constraints": columns_info["constraints"],
                }
            )

        # ALTER TABLE ÌååÏã±
        alter_patterns = [
            # ADD COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "ADD_COLUMN",
            ),
            # DROP COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?",
                "DROP_COLUMN",
            ),
            # MODIFY COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "MODIFY_COLUMN",
            ),
            # CHANGE COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)",
                "CHANGE_COLUMN",
            ),
        ]

        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == "CHANGE_COLUMN":
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "old_column": old_column.lower(),
                            "new_column": new_column.lower(),
                            "column_definition": column_def.strip(),
                        }
                    )
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "column": column_name.lower(),
                            "column_definition": (
                                column_def.strip() if column_def else None
                            ),
                        }
                    )

        # CREATE INDEX ÌååÏã±
        create_index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)"
        )
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name, columns in index_matches:
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": [
                        col.strip().strip("`").lower() for col in columns.split(",")
                    ],
                }
            )

        # DROP TABLE ÌååÏã±
        drop_table_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?"
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)

        for table_name in drop_table_matches:
            ddl_statements.append({"type": "DROP_TABLE", "table": table_name.lower()})

        # DROP INDEX ÌååÏã±
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        print(f"[DEBUG] DROP INDEX ÌååÏã± - Ìå®ÌÑ¥: {drop_index_pattern}")
        print(f"[DEBUG] DROP INDEX ÌååÏã± - ÏûÖÎ†•: {repr(ddl_content)}")
        print(f"[DEBUG] DROP INDEX ÌååÏã± - Í≤∞Í≥º: {drop_index_matches}")

        for index_name, table_name in drop_index_matches:
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )
            print(f"[DEBUG] DROP INDEX Íµ¨Î¨∏ Ï∂îÍ∞ÄÎê®: {index_name} on {table_name}")

        print(f"[DEBUG] Ï†ÑÏ≤¥ ÌååÏã± Í≤∞Í≥º: {len(ddl_statements)}Í∞ú Íµ¨Î¨∏")
        for i, stmt in enumerate(ddl_statements):
            print(f"[DEBUG]   [{i}] {stmt['type']}: {stmt}")

        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEÏùò Ïª¨Îüº Ï†ïÏùò ÌååÏã±"""
        columns = []
        constraints = []

        # Ïª¨Îüº Ï†ïÏùòÏôÄ Ï†úÏïΩÏ°∞Í±¥ÏùÑ Î∂ÑÎ¶¨
        lines = [line.strip() for line in columns_def.split(",")]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Ï†úÏïΩÏ°∞Í±¥ ÌôïÏù∏
            if re.match(
                r"(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)",
                line,
                re.IGNORECASE,
            ):
                constraints.append(line)
            else:
                # Ïª¨Îüº Ï†ïÏùò ÌååÏã±
                column_match = re.match(
                    r"`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?", line, re.IGNORECASE
                )
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ""

                    columns.append(
                        {
                            "name": column_name,
                            "data_type": data_type,
                            "attributes": attributes.strip(),
                        }
                    )

        return {"columns": columns, "constraints": constraints}

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLÏóêÏÑú Ï†úÏïΩÏ°∞Í±¥ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        constraints = {"foreign_keys": [], "indexes": [], "primary_keys": []}

        # Ïô∏ÎûòÌÇ§ Ìå®ÌÑ¥ Îß§Ïπ≠
        fk_pattern = (
            r"FOREIGN\s+KEY\s*\(`?(\w+)`?\)\s*REFERENCES\s+`?(\w+)`?\s*\(`?(\w+)`?\)"
        )
        fk_matches = re.findall(fk_pattern, ddl_content, re.IGNORECASE)

        for column, ref_table, ref_column in fk_matches:
            constraints["foreign_keys"].append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return constraints

    async def validate_create_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù"""
        table_name = ddl_statement["table"]
        columns = ddl_statement["columns"]

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if table_exists:
            issues.append(f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§.")

        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists, "columns_count": len(columns)},
        }

    async def validate_alter_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ALTER TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù"""
        table_name = ddl_statement["table"]
        alter_type = ddl_statement["alter_type"]

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues,
            }

        # ÌòÑÏû¨ ÌÖåÏù¥Î∏îÏùò Ïª¨Îüº Ï†ïÎ≥¥ Ï°∞Ìöå
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        existing_columns = {
            row[0].lower(): {
                "data_type": row[1].upper(),
                "max_length": row[2],
                "precision": row[3],
                "scale": row[4],
                "is_nullable": row[5],
            }
            for row in cursor.fetchall()
        }

        # ALTER Ïú†ÌòïÎ≥Ñ Í≤ÄÏ¶ù
        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement["column"]
            if column_name in existing_columns:
                issues.append(f"Ïª¨Îüº '{column_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§.")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement["column"]
            if column_name not in existing_columns:
                issues.append(f"Ïª¨Îüº '{column_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement["column"]
            new_definition = ddl_statement["column_definition"]

            if column_name not in existing_columns:
                issues.append(f"Ïª¨Îüº '{column_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            else:
                # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÍ≤Ω Í∞ÄÎä•ÏÑ± Í≤ÄÏ¶ù
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        elif alter_type == "CHANGE_COLUMN":
            old_column = ddl_statement["old_column"]
            new_column = ddl_statement["new_column"]
            new_definition = ddl_statement["column_definition"]

            if old_column not in existing_columns:
                issues.append(f"Í∏∞Ï°¥ Ïª¨Îüº '{old_column}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ÏÉà Ïª¨ÎüºÎ™Ö '{new_column}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§.")
            else:
                # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÍ≤Ω Í∞ÄÎä•ÏÑ± Í≤ÄÏ¶ù
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"existing_columns": list(existing_columns.keys())},
        }

    async def validate_create_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE INDEX Íµ¨Î¨∏ Í≤ÄÏ¶ù"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
        else:
            # Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if index_exists:
                issues.append(f"Ïù∏Îç±Ïä§ '{index_name}'Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§.")

            # Ïª¨Îüº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}

            for column in columns:
                if column not in existing_columns:
                    issues.append(
                        f"Ïª¨Îüº '{column}'Ïù¥ ÌÖåÏù¥Î∏î '{table_name}'Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                    )

        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "columns": columns},
        }

    async def validate_drop_table_with_debug(
        self, cursor, ddl_statement: Dict[str, Any], debug_log
    ) -> Dict[str, Any]:
        """DROP TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù (ÎîîÎ≤ÑÍ∑∏ Î≤ÑÏ†Ñ)"""
        table_name = ddl_statement["table"]

        debug_log(f"DROP TABLE Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}")

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if not table_exists:
            issues.append(
                f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. DROP TABLEÏùÑ Ïã§ÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            debug_log(f"Ïò§Î•ò: ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå")
        else:
            debug_log(f"ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨Ìï® - DROP TABLE Í∞ÄÎä•")

        result = {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"table_exists": table_exists, "can_drop": table_exists},
        }

        debug_log(
            f"DROP TABLE Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        debug_log(f"ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    async def validate_drop_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP TABLE Íµ¨Î¨∏ Í≤ÄÏ¶ù (Ìò∏ÌôòÏÑ± Ïú†ÏßÄÏö©)"""
        table_name = ddl_statement["table"]

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")

        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists},
        }

    async def validate_drop_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP INDEX Íµ¨Î¨∏ Í≤ÄÏ¶ù"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        print(f"[DEBUG] DROP INDEX Í≤ÄÏ¶ù ÏãúÏûë: table={table_name}, index={index_name}")

        issues = []

        # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        print(f"[DEBUG] ÌÖåÏù¥Î∏î '{table_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {table_exists}")

        if not table_exists:
            issues.append(f"ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            print(f"[DEBUG] ÌÖåÏù¥Î∏î '{table_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå - Ïù¥Ïäà Ï∂îÍ∞Ä")
        else:
            # Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0
            print(f"[DEBUG] Ïù∏Îç±Ïä§ '{index_name}' Ï°¥Ïû¨ Ïó¨Î∂Ä: {index_exists}")

            if not index_exists:
                issues.append(
                    f"Ïù∏Îç±Ïä§ '{index_name}'Ïù¥ ÌÖåÏù¥Î∏î '{table_name}'Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§."
                )
                print(f"[DEBUG] Ïù∏Îç±Ïä§ '{index_name}'Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùå - Ïù¥Ïäà Ï∂îÍ∞Ä")

        result = {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "table_exists": table_exists},
        }

        print(
            f"[DEBUG] DROP INDEX Í≤ÄÏ¶ù ÏôÑÎ£å: issues={len(issues)}, valid={len(issues) == 0}"
        )
        print(f"[DEBUG] ÏµúÏ¢Ö Í≤∞Í≥º: {result}")

        return result

    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """Ïª¨Îüº Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÍ≤Ω Í∞ÄÎä•ÏÑ± Í≤ÄÏ¶ù"""
        issues = []

        # ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌååÏã±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # Ìò∏ÌôòÎêòÏßÄ ÏïäÎäî ÌÉÄÏûÖ Î≥ÄÍ≤Ω Í≤ÄÏÇ¨
        incompatible_changes = [
            # Î¨∏ÏûêÏó¥ -> Ïà´Ïûê
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # Ïà´Ïûê -> Î¨∏ÏûêÏó¥ (ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏïàÏ†ÑÌïòÏßÄÎßå Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Í∞ÄÎä•)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌÉÄÏûÖ Î≥ÄÍ≤Ω
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏùÑ {existing_type}ÏóêÏÑú {new_type_info['type']}Î°ú Î≥ÄÍ≤ΩÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )

        # Í∏∏Ïù¥ Ï∂ïÏÜå Í≤ÄÏÇ¨
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"Ïª¨Îüº Í∏∏Ïù¥Î•º {existing_length}ÏóêÏÑú {new_length}Î°ú Ï∂ïÏÜåÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )

        # Ï†ïÎ∞ÄÎèÑ Ï∂ïÏÜå Í≤ÄÏÇ¨ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL Ï†ïÎ∞ÄÎèÑÎ•º ({existing_precision},{existing_scale})ÏóêÏÑú ({new_precision},{new_scale})Î°ú Ï∂ïÏÜåÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î¨∏ÏûêÏó¥ÏùÑ ÌååÏã±ÌïòÏó¨ ÌÉÄÏûÖÍ≥º Í∏∏Ïù¥ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) Îì±ÏùÑ ÌååÏã±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) ÌòïÌÉú
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) ÌòïÌÉú
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def generate_html_report(
        self,
        report_path: Path,
        filename: str,
        ddl_content: str,
        sql_type: str,
        status: str,
        summary: str,
        issues: List[str],
        db_connection_info: Optional[Dict],
        schema_validation: Optional[Dict],
        constraint_validation: Optional[Dict],
        database_secret: Optional[str],
        explain_result: Optional[Dict] = None,
        claude_analysis_result: Optional[str] = None,  # Claude Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
        dml_column_issues: List[str] = None,  # DML Ïª¨Îüº Ïù¥Ïäà Ï∂îÍ∞Ä
    ):
        """HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        # dml_column_issues Ï¥àÍ∏∞Ìôî
        if dml_column_issues is None:
            dml_column_issues = []

        # ÏÉÅÏÑ∏ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Ï∂îÍ∞Ä
        try:
            with open(
                "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(f"=== HTML ÏÉùÏÑ± Ìï®Ïàò ÏãúÏûë ===\n")
                f.write(f"report_path: {report_path}\n")
                f.write(f"filename: {filename}\n")
                f.write(f"sql_type: {sql_type}\n")
                f.write(f"status: {status}\n")
                f.write(f"issues Í∞úÏàò: {len(issues)}\n")
                f.flush()
        except Exception as debug_e:
            logger.error(f"ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ ÏûëÏÑ± Ïò§Î•ò: {debug_e}")

        # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ Ï∂îÍ∞Ä
        try:
            with open(
                "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                "a",
                encoding="utf-8",
            ) as f:
                f.write(
                    f"HTML ÏÉùÏÑ± Ìï®Ïàò Ìò∏Ï∂úÎê® - claude_analysis_result: {claude_analysis_result}\n"
                )
                f.flush()
        except:
            pass
        try:
            # Claude Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º Í∏∞Î∞òÏúºÎ°ú ÏÉÅÌÉú Ïû¨ÌèâÍ∞Ä
            claude_success = (
                claude_analysis_result
                and claude_analysis_result.startswith("Í≤ÄÏ¶ù ÌÜµÍ≥º")
            )

            # ÏÉÅÌÉúÏóê Îî∞Î•∏ ÏÉâÏÉÅ Î∞è ÏïÑÏù¥ÏΩò - Claude Í≤ÄÏ¶ù Í≤∞Í≥º Ïö∞ÏÑ† Î∞òÏòÅ
            if (
                claude_success
                and status == "FAIL"
                and not any(
                    "Ïò§Î•ò:" in issue or "Ïã§Ìå®" in issue or "Ï°¥Ïû¨ÌïòÏßÄ Ïïä" in issue
                    for issue in issues
                )
            ):
                # ClaudeÍ∞Ä ÏÑ±Í≥µÏù¥Í≥† Ïã¨Í∞ÅÌïú Ïò§Î•òÍ∞Ä ÏóÜÏúºÎ©¥ PASSÎ°ú Î≥ÄÍ≤Ω
                status = "PASS"
                summary = "‚úÖ Î™®Îì† Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌñàÏäµÎãàÎã§."

            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "‚úÖ" if status == "PASS" else "‚ùå"

            # DB Ïó∞Í≤∞ Ï†ïÎ≥¥ ÏÑπÏÖò Ï†úÍ±∞ (ÏöîÏ≤≠ÏÇ¨Ìï≠Ïóê Îî∞Îùº)
            db_info_section = ""

            # Î∞úÍ≤¨Îêú Î¨∏Ï†ú ÏÑπÏÖò - Claude Í≤ÄÏ¶ùÍ≥º Í∏∞ÌÉÄ Í≤ÄÏ¶ù Î∂ÑÎ¶¨
            claude_issues = []
            other_issues = []

            for issue in issues:
                if issue.startswith("Claude Í≤ÄÏ¶ù:"):
                    claude_issues.append(issue[12:].strip())  # "Claude Í≤ÄÏ¶ù:" Ï†úÍ±∞
                else:
                    other_issues.append(issue)

            # Í∏∞ÌÉÄ Í≤ÄÏ¶ù Î¨∏Ï†ú ÏÑπÏÖò Ï†úÍ±∞ (Ï§ëÎ≥µ Î∞©ÏßÄ)

            # Claude Í≤ÄÏ¶ù Í≤∞Í≥º ÎÇ¥Ïö© Ï§ÄÎπÑ (Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥º Ìè¨Ìï®)
            schema_validation_summary = self.create_schema_validation_summary(
                issues, dml_column_issues
            )

            # Claude Í≤ÄÏ¶ùÍ≥º Ïä§ÌÇ§Îßà Í≤ÄÏ¶ùÏùÑ ÌÜµÌï©Ìïú ÎÇ¥Ïö© ÏÉùÏÑ±
            combined_validation_content = ""

            # Claude AI Í≤ÄÏ¶ù Í≤∞Í≥º Ï∂îÍ∞Ä (Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Í≤∞Í≥ºÎäî Ïà®ÍπÄ)
            claude_content = (
                claude_analysis_result
                if claude_analysis_result
                else "Claude Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
            )
            combined_validation_content += f"""
<div class="validation-subsection">
    <h4>üìã SQLÍ≤ÄÏ¶ùÍ≤∞Í≥º</h4>
    <pre class="validation-text">{claude_content}</pre>
</div>
"""

            # Ï†ÑÏ≤¥ Î¨∏Ï†úÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞
            success_section = ""
            if not issues:
                success_section = """
                <div class="issues-section success" style="display: none;">
                    <h3>‚úÖ Í≤ÄÏ¶ù Í≤∞Í≥º</h3>
                    <p class="no-issues">Î™®Îì† Í≤ÄÏ¶ùÏùÑ ÌÜµÍ≥ºÌñàÏäµÎãàÎã§.</p>
                </div>
                """

            # HTML Î≥¥Í≥†ÏÑú ÎÇ¥Ïö©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
        }}
        .claude-section {{
            margin: 30px 0;
            padding: 25px;
            border-radius: 8px;
            border: 2px solid #667eea;
            background: #f8f9ff;
            box-shadow: 0 2px 10px rgba(102, 126, 234, 0.1);
        }}
        .claude-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            font-size: 1.3em;
        }}
        .claude-result {{
            margin: 20px 0;
            padding: 0;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            box-shadow: 0 1px 5px rgba(0,0,0,0.05);
        }}
        .claude-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 800px;  /* 400pxÏóêÏÑú 800pxÎ°ú Ï¶ùÍ∞Ä */
            overflow-y: auto;
            min-height: 100px;
            resize: vertical;  /* ÏÇ¨Ïö©ÏûêÍ∞Ä ÏàòÏßÅÏúºÎ°ú ÌÅ¨Í∏∞ Ï°∞Ï†à Í∞ÄÎä• */
        }}
        .validation-subsection {{
            margin: 15px 0;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #28a745;
            background: #f8fff9;
        }}
        .validation-subsection h4 {{
            margin: 0 0 10px 0;
            color: #495057;
            font-size: 1.1em;
        }}
        .validation-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 13px;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 300px;
            overflow-y: auto;
        }}
        .success-text {{
            color: #28a745;
            font-weight: 500;
            margin: 0;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>üìÑ ÌååÏùºÎ™Ö</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>üïí Í≤ÄÏ¶ù ÏùºÏãú</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>üîß SQL ÌÉÄÏûÖ</h4>
                    <p>{sql_type}</p>
                </div>
                <div class="summary-item">
                    <h4>üóÑÔ∏è Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>üìù ÏõêÎ≥∏ SQL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="claude-section">
                <h3>üîç ÌÜµÌï© Í≤ÄÏ¶ù Í≤∞Í≥º (Ïä§ÌÇ§Îßà + ÏøºÎ¶¨ÏÑ±Îä•)</h3>
                <div class="claude-result">
                    {combined_validation_content}
                </div>
            </div>
            
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            # Í≤ÄÏ¶ù Í≤∞Í≥º ÏÑπÏÖò Ï†úÍ±∞
            try:
                with open(report_path, "r", encoding="utf-8") as f:
                    html_content = f.read()

                # Í≤ÄÏ¶ù Í≤∞Í≥º ÏÑπÏÖò Ï†úÍ±∞ - Îçî Ï†ïÌôïÌïú Î∞©Î≤ï
                lines = html_content.split("\n")
                new_lines = []
                i = 0

                while i < len(lines):
                    line = lines[i]

                    # Í≤ÄÏ¶ù Í≤∞Í≥º ÏÑπÏÖò ÏãúÏûë Í∞êÏßÄ
                    if '<div class="info-section" style="display: none;">' in line:
                        # Îã§Ïùå ÎùºÏù∏Îì§ÏùÑ ÌôïÏù∏ÌïòÏó¨ Í≤ÄÏ¶ù Í≤∞Í≥º ÏÑπÏÖòÏù∏ÏßÄ ÌåêÎã®
                        if i + 1 < len(lines) and "üìä Í≤ÄÏ¶ù Í≤∞Í≥º" in lines[i + 1]:
                            # Í≤ÄÏ¶ù Í≤∞Í≥º ÏÑπÏÖòÏù¥ÎØÄÎ°ú </div>ÍπåÏßÄ Ïä§ÌÇµ
                            i += 1  # ÌòÑÏû¨ div ÎùºÏù∏ Ïä§ÌÇµ
                            while i < len(lines):
                                if "</div>" in lines[i]:
                                    i += 1  # </div> ÎùºÏù∏ÎèÑ Ïä§ÌÇµ
                                    break
                                i += 1
                            continue

                    new_lines.append(line)
                    i += 1

                html_content = "\n".join(new_lines)

                with open(report_path, "w", encoding="utf-8") as f:
                    f.write(html_content)
            except Exception as e:
                pass

            # ÌååÏùº ÏÉùÏÑ± ÌôïÏù∏ ÎîîÎ≤ÑÍ∑∏
            try:
                with open(
                    "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    f.write(f"HTML ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å: {report_path}\n")
                    f.write(f"ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä: {report_path.exists()}\n")
                    f.flush()
            except:
                pass

        except Exception as e:
            logger.error(f"HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïò§Î•ò: {e}")
            # ÏÉÅÏÑ∏ Ïò§Î•ò Ï†ïÎ≥¥Î•º ÎîîÎ≤ÑÍ∑∏ ÌååÏùºÏóê Í∏∞Î°ù
            try:
                with open(
                    "/Users/heungh/Documents/SA/05.Project/01.Infra-Assistant/01.DB-Assistant/output/html_debug.txt",
                    "a",
                    encoding="utf-8",
                ) as f:
                    import traceback

                    f.write(f"HTML ÏÉùÏÑ± Ïò§Î•ò: {e}\n")
                    f.write(f"ÏÉÅÏÑ∏ Ïò§Î•ò: {traceback.format_exc()}\n")
                    f.flush()
            except:
                pass

    async def generate_consolidated_html_report(
        self, validation_results: List[Dict], database_secret: str
    ) -> str:
        """Ïó¨Îü¨ SQL ÌååÏùºÏùò ÌÜµÌï© HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            # ÌååÏùºÎ≥Ñ Í≤∞Í≥º ÏÑπÏÖò ÏÉùÏÑ±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_icon = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
                status_class = "success" if result["status"] == "PASS" else "error"

                issues_html = ""
                if result["issues"]:
                    issues_html = "<ul class='issues-list'>"
                    for issue in result["issues"]:
                        issues_html += f"<li>{issue}</li>"
                    issues_html += "</ul>"
                else:
                    issues_html = "<p class='no-issues'>Î¨∏Ï†úÍ∞Ä Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.</p>"

                # Í∞úÎ≥Ñ ÌååÏùº Í≤ÄÏ¶ùÏóêÏÑúÎßå ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑúÍ∞Ä ÏÉùÏÑ±ÎêòÎØÄÎ°ú ÎßÅÌÅ¨ ÏóÜÏù¥ ÌååÏùºÎ™ÖÎßå ÌëúÏãú
                filename_display = result["filename"]

                file_sections += f"""
                <div class="file-section {status_class}">
                    <h3>{status_icon} {i}. {filename_display}</h3>
                    <div class="file-details">
                        <div class="file-info">
                            <span><strong>DDL ÌÉÄÏûÖ:</strong> {result['ddl_type']}</span>
                            <span><strong>ÏÉÅÌÉú:</strong> {result['status']}</span>
                            <span><strong>Î¨∏Ï†ú Ïàò:</strong> {len(result['issues'])}Í∞ú</span>
                        </div>
                        <div class="sql-code">
{result['ddl_content']}
                        </div>
                        {f'<div class="issues-section">{issues_html}</div>' if result['issues'] else ''}
                    </div>
                </div>
                """

            # HTML Î≥¥Í≥†ÏÑú ÎÇ¥Ïö©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÌÜµÌï© SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 20px 30px;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-section.success {{
            border-left: 4px solid #28a745;
        }}
        .file-section.error {{
            border-left: 4px solid #dc3545;
        }}
        .file-section h3 {{
            margin: 0;
            padding: 15px 20px;
            background: #f8f9fa;
            border-bottom: 1px solid #e9ecef;
        }}
        .file-section h3 a {{
            color: #495057;
            text-decoration: none;
        }}
        .file-section h3 a:hover {{
            color: #007bff;
            text-decoration: underline;
        }}
        .file-details {{
            padding: 20px;
        }}
        .file-info {{
            display: flex;
            gap: 20px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }}
        .file-info span {{
            background: #e9ecef;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
            font-size: 0.9em;
        }}
        .issues-section {{
            margin-top: 15px;
            padding: 15px;
            background: #fff5f5;
            border: 1px solid #fed7d7;
            border-radius: 6px;
        }}
        .issues-section h4 {{
            margin: 0 0 10px 0;
            color: #c53030;
        }}
        .issues-list {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
            color: #c53030;
        }}
        .no-issues {{
            color: #38a169;
            margin: 0;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
                margin: 20px;
            }}
            .file-section {{
                margin: 20px 15px;
            }}
            .file-info {{
                flex-direction: column;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üìä ÌÜµÌï© SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú</h1>
            <p>Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§: {database_secret}</p>
            <p>Í≤ÄÏ¶ù ÏùºÏãú: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
        
        <div class="summary-stats">
            <div class="stat-item">
                <div class="stat-number">{total_files}</div>
                <div class="stat-label">Ï¥ù ÌååÏùº Ïàò</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                <div class="stat-label">ÌÜµÍ≥º</div>
            </div>
            <div class="stat-item">
                <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                <div class="stat-label">Ïã§Ìå®</div>
            </div>
            <div class="stat-item">
                <div class="stat-number">{round(passed_files/total_files*100) if total_files > 0 else 0}%</div>
                <div class="stat-label">ÏÑ±Í≥µÎ•†</div>
            </div>
        </div>
        
        {file_sections}
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>"""

            # ÌååÏùº Ï†ÄÏû•
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"ÌÜµÌï© HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return f"Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"

    async def generate_consolidated_report(
        self,
        keyword: Optional[str] = None,
        report_files: Optional[List[str]] = None,
        date_filter: Optional[str] = None,
        latest_count: Optional[int] = None,
    ) -> str:
        """Í∏∞Ï°¥ HTML Î≥¥Í≥†ÏÑúÎì§ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        try:
            # Î≥¥Í≥†ÏÑú ÌååÏùº ÏàòÏßë
            if report_files:
                # ÌäπÏ†ï ÌååÏùºÎì§ ÏßÄÏ†ïÎêú Í≤ΩÏö∞
                html_files = [
                    OUTPUT_DIR / f for f in report_files if (OUTPUT_DIR / f).exists()
                ]
            else:
                # validation_reportÎ°ú ÏãúÏûëÌïòÎäî HTML ÌååÏùºÎßå (debug_log Ï†úÏô∏)
                html_files = list(OUTPUT_DIR.glob("validation_report_*.html"))

                # ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ÎßÅ
                if keyword:
                    html_files = [f for f in html_files if keyword in f.name]

                # ÎÇ†Ïßú ÌïÑÌÑ∞ÎßÅ (YYYYMMDD ÌòïÏãù)
                if date_filter:
                    html_files = [f for f in html_files if date_filter in f.name]

                # ÏµúÏã† ÌååÏùº Í∞úÏàò Ï†úÌïú
                if latest_count:
                    # ÌååÏùºÎ™ÖÏùò ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑÎ°ú Ï†ïÎ†¨ (ÏµúÏã†Ïàú)
                    html_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                    html_files = html_files[:latest_count]

            if not html_files:
                return f"Ï°∞Í±¥Ïóê ÎßûÎäî HTML Î≥¥Í≥†ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. (ÌÇ§ÏõåÎìú: {keyword}, ÎÇ†Ïßú: {date_filter}, Í∞úÏàò: {latest_count})"

            # Í∞Å Î≥¥Í≥†ÏÑúÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú
            report_data = []
            for html_file in html_files:
                try:
                    content = html_file.read_text(encoding="utf-8")

                    # ÌååÏùºÎ™ÖÏóêÏÑú ÏõêÎ≥∏ SQL ÌååÏùºÎ™Ö Ï∂îÏ∂ú
                    sql_filename = html_file.name.replace(
                        "validation_report_", ""
                    ).replace(".html", "")
                    if "_2025" in sql_filename:
                        sql_filename = sql_filename.split("_2025")[0] + ".sql"

                    # Í≤ÄÏ¶ù Í≤∞Í≥º Ï∂îÏ∂ú - HTML Íµ¨Ï°∞ Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôïÌûà Ï∂îÏ∂ú
                    if (
                        'status-badge">PASS' in content
                        or "‚úÖ SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú" in content
                    ):
                        status = "PASS"
                        status_icon = "‚úÖ"
                    elif (
                        'status-badge">FAIL' in content
                        or "‚ùå SQL Í≤ÄÏ¶ùÎ≥¥Í≥†ÏÑú" in content
                    ):
                        status = "FAIL"
                        status_icon = "‚ùå"
                    else:
                        # Í∏∞Î≥∏Í∞íÏúºÎ°ú FAIL Ï≤òÎ¶¨
                        status = "FAIL"
                        status_icon = "‚ùå"

                    # SQL ÎÇ¥Ïö© ÏùºÎ∂Ä Ï∂îÏ∂ú (HTML ÌååÏùºÏóêÏÑúÎßå)
                    sql_preview = "SQL ÎÇ¥Ïö©ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"
                    if "sql-code" in content:
                        import re

                        sql_match = re.search(
                            r'<div class="sql-code"[^>]*>(.*?)</div>',
                            content,
                            re.DOTALL,
                        )
                        if sql_match:
                            sql_preview = sql_match.group(1).strip()[:100] + "..."

                    # ÏöîÏïΩ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    summary = "ÏÉÅÏÑ∏ ÎÇ¥Ïö©ÏùÄ Í∞úÎ≥Ñ Î≥¥Í≥†ÏÑú Ï∞∏Ï°∞"
                    if "Claude AI Î∂ÑÏÑù" in content:
                        summary = "AI Î∂ÑÏÑù ÏôÑÎ£å"

                    report_data.append(
                        {
                            "filename": sql_filename,
                            "html_file": html_file.name,
                            "status": status,
                            "status_icon": status_icon,
                            "sql_preview": sql_preview,
                            "summary": summary,
                        }
                    )

                except Exception as e:
                    logger.error(f"Î≥¥Í≥†ÏÑú ÌååÏã± Ïò§Î•ò {html_file}: {e}")
                    continue

            if not report_data:
                return "Ïú†Ìö®Ìïú Î≥¥Í≥†ÏÑú Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            # ÌÜµÌï© Î≥¥Í≥†ÏÑú HTML ÏÉùÏÑ±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            total_reports = len(report_data)
            passed_reports = sum(1 for r in report_data if r["status"] == "PASS")
            failed_reports = total_reports - passed_reports

            # ÌÖåÏù¥Î∏î Ìñâ ÏÉùÏÑ±
            table_rows = ""
            for i, data in enumerate(report_data, 1):
                table_rows += f"""
                <tr onclick="openReport('{data['html_file']}')" style="cursor: pointer;">
                    <td>{i}</td>
                    <td>{data['status_icon']} {data['filename']}</td>
                    <td><code>{data['sql_preview']}</code></td>
                    <td><span class="status-badge {data['status'].lower()}">{data['status']}</span></td>
                    <td>{data['summary']}</td>
                </tr>
                """

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÌÜµÌï© Í≤ÄÏ¶ù Î≥¥Í≥†ÏÑú</title>
    <style>
        body {{ font-family: 'Segoe UI', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1400px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 2.5em; font-weight: 300; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #667eea; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #333; }}
        .stat-label {{ color: #666; margin-top: 5px; }}
        .table-container {{ margin: 30px; overflow-x: auto; }}
        table {{ width: 100%; border-collapse: collapse; background: white; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #f8f9fa; font-weight: 600; }}
        tr:hover {{ background: #f8f9fa; }}
        .status-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .status-badge.pass {{ background: #d4edda; color: #155724; }}
        .status-badge.fail {{ background: #f8d7da; color: #721c24; }}
        code {{ background: #f1f3f4; padding: 2px 4px; border-radius: 3px; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üìä ÌÜµÌï© Í≤ÄÏ¶ù Î≥¥Í≥†ÏÑú</h1>
            <p>ÏÉùÏÑ±ÏùºÏãú: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{total_reports}</div>
                <div class="stat-label">Ï¥ù Î≥¥Í≥†ÏÑú</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{passed_reports}</div>
                <div class="stat-label">Í≤ÄÏ¶ù ÌÜµÍ≥º</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{failed_reports}</div>
                <div class="stat-label">Í≤ÄÏ¶ù Ïã§Ìå®</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{round(passed_reports/total_reports*100) if total_reports > 0 else 0}%</div>
                <div class="stat-label">ÏÑ±Í≥µÎ•†</div>
            </div>
        </div>
        
        <div class="table-container">
            <h2>üìã Î≥¥Í≥†ÏÑú Î™©Î°ù (ÌÅ¥Î¶≠ÌïòÏó¨ ÏÉÅÏÑ∏ Î≥¥Í∏∞)</h2>
            <table>
                <thead>
                    <tr>
                        <th>#</th>
                        <th>ÌååÏùºÎ™Ö</th>
                        <th>SQL ÎØ∏Î¶¨Î≥¥Í∏∞</th>
                        <th>Í≤ÄÏ¶ù Í≤∞Í≥º</th>
                        <th>ÏöîÏïΩ</th>
                    </tr>
                </thead>
                <tbody>
                    {table_rows}
                </tbody>
            </table>
        </div>
    </div>
    
    <script>
        function openReport(filename) {{
            window.open(filename, '_blank');
        }}
    </script>
</body>
</html>"""

            report_path.write_text(html_content, encoding="utf-8")

            return f"""üìä ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏôÑÎ£å

üìà ÏöîÏïΩ:
‚Ä¢ Ï¥ù Î≥¥Í≥†ÏÑú: {total_reports}Í∞ú
‚Ä¢ Í≤ÄÏ¶ù ÌÜµÍ≥º: {passed_reports}Í∞ú ({round(passed_reports/total_reports*100)}%)
‚Ä¢ Í≤ÄÏ¶ù Ïã§Ìå®: {failed_reports}Í∞ú ({round(failed_reports/total_reports*100)}%)

üìÑ ÌÜµÌï© Î≥¥Í≥†ÏÑú: {report_path}

üí° ÏÇ¨Ïö©Î≤ï: ÌÖåÏù¥Î∏îÏùò Í∞Å ÌñâÏùÑ ÌÅ¥Î¶≠ÌïòÎ©¥ Ìï¥Îãπ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑúÍ∞Ä ÏÉà Ï∞ΩÏóêÏÑú Ïó¥Î¶ΩÎãàÎã§."""

        except Exception as e:
            return f"ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"

    def extract_key_metrics_from_csv(self, csv_filename: str) -> dict:
        """CSV ÌååÏùºÏóêÏÑú ÏßÅÏ†ë ÌïµÏã¨ Î©îÌä∏Î¶≠ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôò"""
        import csv
        import statistics
        
        metrics = {}
        
        try:
            # CSV ÌååÏùº ÏùΩÍ∏∞
            csv_path = DATA_DIR / csv_filename
            
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                data = list(reader)
            
            if not data:
                raise ValueError("CSV ÌååÏùºÏù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§")
            
            # Í∞Å Î©îÌä∏Î¶≠Î≥ÑÎ°ú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            def calculate_stats(column_name):
                values = []
                for row in data:
                    if column_name in row and row[column_name]:
                        try:
                            values.append(float(row[column_name]))
                        except ValueError:
                            continue
                
                if values:
                    return {
                        'mean': statistics.mean(values),
                        'min': min(values),
                        'max': max(values)
                    }
                return {'mean': 0.0, 'min': 0.0, 'max': 0.0}
            
            # CPU ÏÇ¨Ïö©Î•†
            cpu_stats = calculate_stats('CPUUtilization')
            metrics['cpu_mean'] = cpu_stats['mean']
            metrics['cpu_min'] = cpu_stats['min']
            metrics['cpu_max'] = cpu_stats['max']
            
            # DB Load
            dbload_stats = calculate_stats('DBLoad')
            metrics['dbload_mean'] = dbload_stats['mean']
            metrics['dbload_min'] = dbload_stats['min']
            metrics['dbload_max'] = dbload_stats['max']
            
            # Ïó∞Í≤∞ Ïàò
            conn_stats = calculate_stats('DatabaseConnections')
            metrics['connections_mean'] = conn_stats['mean']
            metrics['connections_min'] = conn_stats['min']
            metrics['connections_max'] = conn_stats['max']
            
            # Read IOPS
            read_stats = calculate_stats('ReadIOPS')
            metrics['read_iops_mean'] = read_stats['mean']
            metrics['read_iops_min'] = read_stats['min']
            metrics['read_iops_max'] = read_stats['max']
            
            # Write IOPS
            write_stats = calculate_stats('WriteIOPS')
            metrics['write_iops_mean'] = write_stats['mean']
            metrics['write_iops_min'] = write_stats['min']
            metrics['write_iops_max'] = write_stats['max']
            
            # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† Í≥ÑÏÇ∞
            memory_stats = calculate_stats('FreeableMemory')
            if memory_stats['mean'] > 0:
                total_memory = 8 * 1024 * 1024 * 1024  # 8GB
                metrics['memory_usage_mean'] = ((total_memory - memory_stats['mean']) / total_memory * 100)
                metrics['memory_usage_min'] = ((total_memory - memory_stats['max']) / total_memory * 100)
                metrics['memory_usage_max'] = ((total_memory - memory_stats['min']) / total_memory * 100)
            else:
                metrics['memory_usage_mean'] = 0.0
                metrics['memory_usage_min'] = 0.0
                metrics['memory_usage_max'] = 0.0
            
            print(f"DEBUG: Extracted metrics from CSV: {metrics}")
            
        except Exception as e:
            print(f"Error reading CSV file {csv_filename}: {e}")
            # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
            for key in ['cpu_mean', 'cpu_min', 'cpu_max', 'dbload_mean', 'dbload_min', 'dbload_max',
                       'connections_mean', 'connections_min', 'connections_max',
                       'read_iops_mean', 'read_iops_min', 'read_iops_max',
                       'write_iops_mean', 'write_iops_min', 'write_iops_max',
                       'memory_usage_mean', 'memory_usage_min', 'memory_usage_max']:
                metrics[key] = 0.0
        
        return metrics

    def extract_key_metrics(self, summary_text: str) -> dict:
        """ÌïµÏã¨ Î©îÌä∏Î¶≠Îßå Ï∂îÏ∂úÌïòÏó¨ ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôò"""
        import re
        
        # ÌÜµÍ≥Ñ ÌÖåÏù¥Î∏îÏóêÏÑú ÌïµÏã¨ Î©îÌä∏Î¶≠ Ï∂îÏ∂ú
        metrics = {}
        
        # Ïã§Ï†ú pandas describe() Ï∂úÎ†•ÏóêÏÑú ÏßÅÏ†ë Í∞í Ï∂îÏ∂ú
        lines = summary_text.split('\n')
        
        # mean, min, max ÎùºÏù∏ÏùÑ Ï∞æÏïÑÏÑú Í∞íÎì§ÏùÑ Ï∂îÏ∂ú
        for line in lines:
            # mean ÎùºÏù∏ Ï≤òÎ¶¨
            if line.strip().startswith('mean') and 'CPUUtilization' in summary_text:
                # Í≥µÎ∞±ÏúºÎ°ú Î∂ÑÌï†ÌïòÏó¨ Ïà´Ïûê Í∞íÎì§Îßå Ï∂îÏ∂ú
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics['cpu_mean'] = float(parts[2])  # CPUUtilization
                        metrics['dbload_mean'] = float(parts[3])  # DBLoad
                        metrics['connections_mean'] = float(parts[6])  # DatabaseConnections
                        freeable_memory_mean = float(parts[7])  # FreeableMemory (scientific notation)
                        metrics['read_iops_mean'] = float(parts[10])  # ReadIOPS
                        metrics['write_iops_mean'] = float(parts[12])  # WriteIOPS
                        
                        # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† Í≥ÑÏÇ∞ (8GB Í∞ÄÏ†ï)
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics['memory_usage_mean'] = ((total_memory - freeable_memory_mean) / total_memory * 100)
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing mean line: {e}")
                        
            # min ÎùºÏù∏ Ï≤òÎ¶¨
            elif line.strip().startswith('min') and 'CPUUtilization' in summary_text:
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics['cpu_min'] = float(parts[2])
                        metrics['dbload_min'] = float(parts[3])
                        metrics['connections_min'] = float(parts[6])
                        freeable_memory_max = float(parts[7])  # min freeable = max usage
                        metrics['read_iops_min'] = float(parts[10])
                        metrics['write_iops_min'] = float(parts[12])
                        
                        # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† ÏµúÎåÄÍ∞í Í≥ÑÏÇ∞
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics['memory_usage_max'] = ((total_memory - freeable_memory_max) / total_memory * 100)
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing min line: {e}")
                        
            # max ÎùºÏù∏ Ï≤òÎ¶¨
            elif line.strip().startswith('max') and 'CPUUtilization' in summary_text:
                parts = line.split()
                if len(parts) >= 13:
                    try:
                        metrics['cpu_max'] = float(parts[2])
                        metrics['dbload_max'] = float(parts[3])
                        metrics['connections_max'] = float(parts[6])
                        freeable_memory_min = float(parts[7])  # max freeable = min usage
                        metrics['read_iops_max'] = float(parts[10])
                        metrics['write_iops_max'] = float(parts[12])
                        
                        # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† ÏµúÏÜåÍ∞í Í≥ÑÏÇ∞
                        total_memory = 8 * 1024 * 1024 * 1024
                        metrics['memory_usage_min'] = ((total_memory - freeable_memory_min) / total_memory * 100)
                    except (ValueError, IndexError) as e:
                        print(f"Error parsing max line: {e}")
        
        return metrics

    def format_metrics_as_html(self, metrics: dict) -> str:
        """Î©îÌä∏Î¶≠ ÎîïÏÖîÎÑàÎ¶¨Î•º HTMLÎ°ú Ìè¨Îß∑"""
        # HTML ÌòïÌÉúÎ°ú Ìè¨Îß∑
        html = f"""
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-title">üñ•Ô∏è CPU ÏÇ¨Ïö©Î•† (%)</div>
                <div class="metric-value">ÌèâÍ∑†: {metrics.get('cpu_mean', 0):.1f}%</div>
                <div class="metric-unit">ÏµúÎåÄ: {metrics.get('cpu_max', 0):.1f}% | ÏµúÏÜå: {metrics.get('cpu_min', 0):.1f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-title">üíæ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† (%)</div>
                <div class="metric-value">ÌèâÍ∑†: {metrics.get('memory_usage_mean', 0):.1f}%</div>
                <div class="metric-unit">ÏµúÎåÄ: {metrics.get('memory_usage_max', 0):.1f}% | ÏµúÏÜå: {metrics.get('memory_usage_min', 0):.1f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-title">‚úçÔ∏è Write IOPS</div>
                <div class="metric-value">ÌèâÍ∑†: {metrics.get('write_iops_mean', 0):.2f}</div>
                <div class="metric-unit">ÏµúÎåÄ: {metrics.get('write_iops_max', 0):.2f} | ÏµúÏÜå: {metrics.get('write_iops_min', 0):.2f}</div>
            </div>
            <div class="metric-card">
                <div class="metric-title">üìñ Read IOPS</div>
                <div class="metric-value">ÌèâÍ∑†: {metrics.get('read_iops_mean', 0):.3f}</div>
                <div class="metric-unit">ÏµúÎåÄ: {metrics.get('read_iops_max', 0):.3f} | ÏµúÏÜå: {metrics.get('read_iops_min', 0):.3f}</div>
            </div>
            <div class="metric-card">
                <div class="metric-title">üîó Ïó∞Í≤∞ Ïàò</div>
                <div class="metric-value">ÌèâÍ∑†: {metrics.get('connections_mean', 0):.1f}Í∞ú</div>
                <div class="metric-unit">ÏµúÎåÄ: {metrics.get('connections_max', 0):.0f}Í∞ú | ÏµúÏÜå: {metrics.get('connections_min', 0):.0f}Í∞ú</div>
            </div>
        </div>
        """
        
        return html

    async def generate_comprehensive_performance_report(
        self,
        database_secret: str,
        db_instance_identifier: str,
        region: Optional[str] = None,
        hours: int = 24,
    ) -> str:
        """Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ ÌååÏùº ÏÉùÏÑ±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_performance_{db_instance_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        debug_log(
            f"Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏãúÏûë - Ïù∏Ïä§ÌÑ¥Ïä§: {db_instance_identifier}"
        )

        try:
            # regionÏù¥ Ï†úÍ≥µÎêòÏßÄ ÏïäÏúºÎ©¥ ÌòÑÏû¨ ÌîÑÎ°úÌååÏùºÏùò Í∏∞Î≥∏ Î¶¨Ï†Ñ ÏÇ¨Ïö©
            if region is None:
                region = self.default_region

            # ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏóîÎìúÌè¨Ïù∏Ìä∏ ÌôïÏù∏ Î∞è Ïù∏Ïä§ÌÑ¥Ïä§ identifier Î≥ÄÌôò
            original_identifier = db_instance_identifier
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_log(f"Î¶¨Ï†Ñ ÏÑ§Ï†ï: {region}, ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ: {timestamp}")

            # 1. Î©îÌä∏Î¶≠ ÏàòÏßë
            debug_log("Î©îÌä∏Î¶≠ ÏàòÏßë ÏãúÏûë")
            metrics_result = await self.collect_db_metrics(
                db_instance_identifier, hours, None, region
            )
            if "Ïò§Î•ò" in metrics_result:
                debug_log(f"Î©îÌä∏Î¶≠ ÏàòÏßë Ïã§Ìå®: {metrics_result}")
                return f"‚ùå Î©îÌä∏Î¶≠ ÏàòÏßë Ïã§Ìå®: {metrics_result}"

            debug_log("Î©îÌä∏Î¶≠ ÏàòÏßë ÏôÑÎ£å")

            # CSV ÌååÏùºÎ™Ö Ï∂îÏ∂ú
            csv_file = None
            for line in metrics_result.split("\n"):
                if "database_metrics_" in line and ".csv" in line:
                    csv_file = line.split(": ")[-1]
                    break

            if not csv_file:
                debug_log("Î©îÌä∏Î¶≠ CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
                return "‚ùå Î©îÌä∏Î¶≠ CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"

            csv_filename = Path(csv_file).name
            debug_log(f"CSV ÌååÏùºÎ™Ö: {csv_filename}")

            # 2. ÏÑ±Îä• ÏøºÎ¶¨ ÏàòÏßë Î∞è ÌååÏùº Ï∂îÏ†Å
            debug_log("ÏÑ±Îä• ÏøºÎ¶¨ ÏàòÏßë ÏãúÏûë")
            generated_files = []  # ÏÉùÏÑ±Îêú ÌååÏùºÎì§ Ï∂îÏ†Å
            
            slow_queries = await self.collect_slow_queries(database_secret)
            memory_queries = await self.collect_memory_intensive_queries(
                database_secret, db_instance_identifier, None, None
            )
            cpu_queries = await self.collect_cpu_intensive_queries(
                database_secret, db_instance_identifier, None, None
            )
            temp_queries = await self.collect_temp_space_intensive_queries(
                database_secret, db_instance_identifier, None, None
            )
            
            # SQL ÌååÏùºÎì§Ïù¥ ÏÉùÏÑ±ÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† Ï∂îÏ†Å
            sql_files = list(Path("sql").glob("*.sql")) if Path("sql").exists() else []
            for sql_file in sql_files:
                if any(keyword in sql_file.name.lower() for keyword in ['slow', 'cpu', 'memory', 'temp']):
                    generated_files.append({"name": sql_file.name, "type": "SQL ÏøºÎ¶¨", "path": f"../sql/{sql_file.name}"})
            
            debug_log("ÏÑ±Îä• ÏøºÎ¶¨ ÏàòÏßë ÏôÑÎ£å")

            # 3. Î©îÌä∏Î¶≠ Î∂ÑÏÑù Î∞è ÌååÏùº Ï∂îÏ†Å
            debug_log("Î©îÌä∏Î¶≠ Î∂ÑÏÑù ÏãúÏûë")
            summary = await self.get_metric_summary(csv_filename)
            
            # ÌïµÏã¨ Î©îÌä∏Î¶≠ Ï∂îÏ∂ú - CSV ÌååÏùºÏóêÏÑú ÏßÅÏ†ë Í≥ÑÏÇ∞
            key_metrics_dict = self.extract_key_metrics_from_csv(csv_filename)
            key_metrics_html = self.format_metrics_as_html(key_metrics_dict)
            
            correlation = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )
            outliers = await self.detect_metric_outliers(csv_filename, 2.0, skip_html_report=True)
            
            # ÏïÑÏõÉÎùºÏù¥Ïñ¥ Í≤∞Í≥ºÏóêÏÑú ÎßÅÌÅ¨ Î∂ÄÎ∂Ñ Ï†úÍ±∞ (Ï¢ÖÌï©Î≥¥Í≥†ÏÑúÏö©)
            if "üìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú:" in outliers:
                outliers_lines = outliers.split('\n')
                filtered_lines = []
                skip_next = False
                for line in outliers_lines:
                    if "üìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú:" in line:
                        break  # Ïù¥ ÎùºÏù∏Î∂ÄÌÑ∞Îäî Î™®Îëê Ï†úÏô∏
                    filtered_lines.append(line)
                outliers = '\n'.join(filtered_lines).strip()
            
            # ÏûÑÍ≥ÑÍ∞í Î™®Îã¨ HTML ÏÉùÏÑ±
            metric_thresholds = self.load_metric_thresholds()
            threshold_modal_html = self.generate_threshold_html(metric_thresholds)
            
            # Î∂ÑÏÑù Í≤∞Í≥º ÌååÏùºÎì§ Ï∂îÏ†Å
            data_files = list(Path("data").glob("*.csv")) if Path("data").exists() else []
            for data_file in data_files:
                if db_instance_identifier in data_file.name:
                    generated_files.append({"name": data_file.name, "type": "Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞", "path": f"../data/{data_file.name}"})
            
            debug_log("Î©îÌä∏Î¶≠ Î∂ÑÏÑù ÏôÑÎ£å")

            # 4. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù (Í∏∞Ï°¥ Ìï®Ïàò ÏÇ¨Ïö©)
            correlation_analysis = await self.analyze_metric_correlation(
                csv_filename, "CPUUtilization", 10
            )

            # 5. Claude Í∏∞Î∞ò ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
            debug_log("Claude Í∏∞Î∞ò ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± ÏãúÏûë")
            logger.info("Claude Í∏∞Î∞ò ÏÑ±Îä• Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± ÏãúÏûë")
            try:
                claude_recommendations = (
                    await self.generate_performance_recommendations_with_claude(
                        summary,
                        correlation_analysis,
                        outliers,
                        slow_queries,
                        memory_queries,
                        cpu_queries,
                        temp_queries,
                        database_secret,
                    )
                )
                debug_log("Claude Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± ÏôÑÎ£å")
                logger.info(
                    f"Claude Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± ÏôÑÎ£å: {len(claude_recommendations.get('immediate_improvements', []))}Í∞ú Í∞úÏÑ†ÏÇ¨Ìï≠, {len(claude_recommendations.get('action_items', []))}Í∞ú Ïï°ÏÖòÏïÑÏù¥ÌÖú"
                )
            except Exception as e:
                debug_log(f"Claude Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ïã§Ìå®: {e}")
                logger.error(f"Claude Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ïã§Ìå®, Í∏∞Î≥∏ Í∂åÏû•ÏÇ¨Ìï≠ ÏÇ¨Ïö©: {e}")
                claude_recommendations = self._get_default_recommendations()

            # 6. HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
            debug_log("HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏãúÏûë")
            report_path = (
                OUTPUT_DIR
                / f"comprehensive_performance_report_{db_instance_identifier}_{timestamp}.html"
            )

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú - {db_instance_identifier}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; text-align: center; }}
        .header h1 {{ font-size: 2.5em; margin-bottom: 10px; }}
        .header .subtitle {{ font-size: 1.2em; opacity: 0.9; }}
        .section {{ background: white; margin-bottom: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); overflow: hidden; }}
        .section-header {{ background: #2c3e50; color: white; padding: 20px; font-size: 1.3em; font-weight: bold; }}
        .section-content {{ padding: 25px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 25px; }}
        .metric-card {{ background: #f8f9fa; border-left: 4px solid #3498db; padding: 20px; border-radius: 5px; }}
        .metric-card.warning {{ border-left-color: #f39c12; }}
        .metric-card.danger {{ border-left-color: #e74c3c; }}
        .metric-card.success {{ border-left-color: #27ae60; }}
        .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 10px; }}
        .metric-value {{ font-size: 1.8em; font-weight: bold; color: #3498db; }}
        .metric-unit {{ font-size: 0.9em; color: #7f8c8d; }}
        .query-box {{ background: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; font-family: 'Courier New', monospace; font-size: 0.9em; overflow-x: auto; }}
        .status-good {{ color: #27ae60; font-weight: bold; }}
        .status-warning {{ color: #f39c12; font-weight: bold; }}
        .status-critical {{ color: #e74c3c; font-weight: bold; }}
        .table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .table th, .table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .table th {{ background: #34495e; color: white; }}
        .table tr:hover {{ background: #f5f5f5; }}
        .recommendation {{ background: #e8f5e9; border-left: 4px solid #4caf50; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .issue {{ background: #ffebee; border-left: 4px solid #f44336; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .info-box {{ background: #e3f2fd; border-left: 4px solid #2196f3; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .file-link {{ color: #007bff; text-decoration: none; font-weight: bold; }}
        .file-link:hover {{ text-decoration: underline; color: #0056b3; }}
        .toc {{ background: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 30px; }}
        .toc ul {{ list-style: none; }}
        .toc li {{ margin: 5px 0; }}
        .toc a {{ color: #3498db; text-decoration: none; }}
        .toc a:hover {{ text-decoration: underline; }}
        .btn {{ background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 14px; }}
        .btn:hover {{ background: #0056b3; }}
        .modal {{ display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: rgba(0,0,0,0.4); }}
        .modal-content {{ background-color: #fefefe; margin: 5% auto; padding: 20px; border: none; border-radius: 10px; width: 80%; max-width: 800px; box-shadow: 0 4px 20px rgba(0,0,0,0.3); }}
        .close {{ color: #aaa; float: right; font-size: 28px; font-weight: bold; cursor: pointer; }}
        .close:hover, .close:focus {{ color: black; text-decoration: none; }}
        .threshold-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .threshold-table th, .threshold-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .threshold-table th {{ background: #f8f9fa; font-weight: bold; }}
        .threshold-table tr:hover {{ background: #f5f5f5; }}
        @media (max-width: 768px) {{
            .container {{ padding: 10px; }}
            .header {{ padding: 20px; }}
            .header h1 {{ font-size: 2em; }}
            .metric-grid {{ grid-template-columns: 1fr; }}
            .modal-content {{ width: 95%; margin: 10% auto; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üóÑÔ∏è Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú</h1>
            <div class="subtitle">Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î∂ÑÏÑù</div>
            <div style="margin-top: 15px; font-size: 1em;">
                <strong>Ïù∏Ïä§ÌÑ¥Ïä§:</strong> {db_instance_identifier} | 
                <strong>Î¶¨Ï†Ñ:</strong> {region} | 
                <strong>Î∂ÑÏÑù Í∏∞Í∞Ñ:</strong> {hours}ÏãúÍ∞Ñ | 
                <strong>ÏÉùÏÑ±ÏùºÏãú:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            </div>
        </div>

        <div class="toc">
            <h3>üìã Î™©Ï∞®</h3>
            <ul>
                <li><a href="#executive-summary">1. ÏöîÏïΩ Ï†ïÎ≥¥</a></li>
                <li><a href="#performance-metrics">2. ÏÑ±Îä• Î©îÌä∏Î¶≠ Î∂ÑÏÑù</a></li>
                <li><a href="#correlation-analysis">3. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù</a></li>
                <li><a href="#outlier-analysis">4. Ïù¥ÏÉÅ ÏßïÌõÑ Î∂ÑÏÑù</a></li>
                <li><a href="#slow-queries">5. ÎäêÎ¶∞ ÏøºÎ¶¨ Î∂ÑÏÑù</a></li>
                <li><a href="#resource-intensive">6. Î¶¨ÏÜåÏä§ ÏßëÏïΩÏ†Å ÏøºÎ¶¨</a></li>
                <li><a href="#recommendations">7. ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠</a></li>
            </ul>
        </div>

        <div class="section" id="executive-summary">
            <div class="section-header">üìä 1. ÏöîÏïΩ Ï†ïÎ≥¥ (Executive Summary)</div>
            <div class="section-content">
                <div class="info-box">
                    <strong>Î∂ÑÏÑù Í∞úÏöî:</strong> {hours}ÏãúÍ∞Ñ ÎèôÏïàÏùò ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Ìïú Ï¢ÖÌï© ÏßÑÎã® Í≤∞Í≥ºÏûÖÎãàÎã§.
                </div>
                
                <h4>üìä Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÎ≥¥</h4>
                <div class="metric-grid">
                    <div class="metric-card">
                        <div class="metric-title">üóÑÔ∏è Ïù∏Ïä§ÌÑ¥Ïä§ ID</div>
                        <div class="metric-value">{db_instance_identifier}</div>
                        <div class="metric-unit">{region}</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">üìÖ Î∂ÑÏÑù Í∏∞Í∞Ñ</div>
                        <div class="metric-value">{hours}</div>
                        <div class="metric-unit">ÏãúÍ∞Ñ</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">üìà Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏</div>
                        <div class="metric-value">288</div>
                        <div class="metric-unit">Í∞ú</div>
                    </div>
                </div>
                
                <h4>üìà ÌïµÏã¨ ÏÑ±Îä• ÌÜµÍ≥Ñ</h4>
                {key_metrics_html}
            </div>
        </div>

        <div class="section" id="performance-metrics">
            <div class="section-header">üìà 2. ÏÑ±Îä• Î©îÌä∏Î¶≠ Î∂ÑÏÑù</div>
            <div class="section-content">
                <h4>üéØ ÌïµÏã¨ ÏÑ±Îä• ÏßÄÌëú</h4>
                <div class="metric-grid">
                    <div class="metric-card success">
                        <div class="metric-title">üìä Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏÉÅÌÉú</div>
                        <div class="metric-value">ÏôÑÎ£å</div>
                        <div class="metric-unit">Î©îÌä∏Î¶≠ ÏàòÏßë ÏÑ±Í≥µ</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">üìÖ Î∂ÑÏÑù Í∏∞Í∞Ñ</div>
                        <div class="metric-value">{hours}</div>
                        <div class="metric-unit">ÏãúÍ∞Ñ</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-title">üóÑÔ∏è Ïù∏Ïä§ÌÑ¥Ïä§</div>
                        <div class="metric-value">{db_instance_identifier}</div>
                        <div class="metric-unit">{region}</div>
                    </div>
                </div>
                
                <h4>üìä ÏÉÅÏÑ∏ Î©îÌä∏Î¶≠ Ï†ïÎ≥¥</h4>
                <div class="info-box">
                    Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞Îäî <strong><a href="file://{DATA_DIR / csv_filename}" target="_blank">{csv_filename}</a></strong> ÌååÏùºÏóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.
                </div>
            </div>
        </div>

        <div class="section" id="correlation-analysis">
            <div class="section-header">üîó 3. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù</div>
            <div class="section-content">
                <h4>üìà Î©îÌä∏Î¶≠ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{correlation}</pre>
                
                <div class="recommendation">
                    <strong>üí° ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Ïù∏ÏÇ¨Ïù¥Ìä∏:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ(r > 0.7)Î•º Î≥¥Ïù¥Îäî Î©îÌä∏Î¶≠Îì§ÏùÄ Ìï®Íªò Î™®ÎãàÌÑ∞ÎßÅÌï¥Ïïº Ìï©ÎãàÎã§</li>
                        <li>CPU ÏÇ¨Ïö©Î•†Í≥º Í∞ïÌïú ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Î≥¥Ïù¥Îäî Î©îÌä∏Î¶≠Îì§ÏùÑ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÏµúÏ†ÅÌôîÌïòÏÑ∏Ïöî</li>
                        <li>ÎÑ§Ìä∏ÏõåÌÅ¨ Ìä∏ÎûòÌîΩÍ≥º I/O Î©îÌä∏Î¶≠Ïùò Í¥ÄÍ≥ÑÎ•º Ï£ºÏùò ÍπäÍ≤å Í¥ÄÏ∞∞ÌïòÏÑ∏Ïöî</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="outlier-analysis">
            <div class="section-header">üö® 4. Ïù¥ÏÉÅ ÏßïÌõÑ Î∂ÑÏÑù (Outlier Detection)</div>
            <div class="section-content">
                <h4>‚ö†Ô∏è Î∞úÍ≤¨Îêú ÏïÑÏõÉÎùºÏù¥Ïñ¥</h4>
                <pre style="background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;">{outliers}</pre>
                
                <div style="margin: 15px 0;">
                    <button class="btn" onclick="document.getElementById('thresholdModal').style.display='block'">
                        üìä ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï Î≥¥Í∏∞
                    </button>
                </div>
                
                <div class="issue">
                    <strong>üîç Ï£ºÏùòÏÇ¨Ìï≠:</strong> ÏïÑÏõÉÎùºÏù¥Ïñ¥Í∞Ä Î∞úÍ≤¨Îêú ÏãúÏ†êÏùò Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î°úÍ∑∏ÏôÄ ÏãúÏä§ÌÖú Ïù¥Î≤§Ìä∏Î•º Ìï®Íªò Î∂ÑÏÑùÌïòÏó¨ Í∑ºÎ≥∏ ÏõêÏù∏ÏùÑ ÌååÏïÖÌïòÏÑ∏Ïöî.
                </div>
            </div>
        </div>

        <div class="section" id="slow-queries">
            <div class="section-header">üêå 5. Slow ÏøºÎ¶¨ Î∂ÑÏÑù (Slow Query Analysis)</div>
            <div class="section-content">
                <h4>üìä ÏàòÏßë Í≤∞Í≥º</h4>
                <div class="query-box">{slow_queries}</div>
                
                <div class="recommendation">
                    <strong>üí° Slow ÏøºÎ¶¨ ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>Ïù∏Îç±Ïä§ Ï∂îÍ∞Ä ÎòêÎäî Í∏∞Ï°¥ Ïù∏Îç±Ïä§ ÏµúÏ†ÅÌôî</li>
                        <li>WHERE Ï†à Ï°∞Í±¥ ÏàúÏÑú ÏµúÏ†ÅÌôî</li>
                        <li>JOIN Ï°∞Í±¥ Î∞è ÏàúÏÑú Í≤ÄÌÜ†</li>
                        <li>ÏøºÎ¶¨ Ïã§Ìñâ Í≥ÑÌöç(EXPLAIN) Î∂ÑÏÑù</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="resource-intensive">
            <div class="section-header">üíæ 6. Î¶¨ÏÜåÏä§ ÏÜåÎ™®Í∞Ä ÌÅ∞ ÏøºÎ¶¨ Î∂ÑÏÑù</div>
            <div class="section-content">
                <h4>üß† Î©îÎ™®Î¶¨ ÏÜåÎπÑÍ∞Ä ÎßéÏùÄ ÏøºÎ¶¨</h4>
                <div class="query-box">{memory_queries}</div>
                
                <h4>‚ö° CPU ÏÜåÎπÑÍ∞Ä ÎßéÏùÄ ÏøºÎ¶¨</h4>
                <div class="query-box">{cpu_queries}</div>
                
                <h4>üíø ÏûÑÏãú Í≥µÍ∞ÑÏùÑ ÎßéÏù¥ ÏÜåÎπÑÌïòÎäî ÏøºÎ¶¨</h4>
                <div class="query-box">{temp_queries}</div>
                
                <div class="recommendation">
                    <strong>üí° Î¶¨ÏÜåÏä§ ÏµúÏ†ÅÌôî Ï†ÑÎûµ:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li><strong>Î©îÎ™®Î¶¨:</strong> Ï†ïÎ†¨ Î≤ÑÌçº ÌÅ¨Í∏∞ Ï°∞Ï†ï, ÏûÑÏãú ÌÖåÏù¥Î∏î ÏÇ¨Ïö© ÏµúÏÜåÌôî</li>
                        <li><strong>CPU:</strong> Î≥µÏû°Ìïú Ïó∞ÏÇ∞ ÏµúÏ†ÅÌôî, Ìï®Ïàò ÏÇ¨Ïö© ÏµúÏÜåÌôî</li>
                        <li><strong>ÏûÑÏãú Í≥µÍ∞Ñ:</strong> GROUP BY, ORDER BY ÏµúÏ†ÅÌôî, Ïù∏Îç±Ïä§ ÌôúÏö©</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section" id="recommendations">
            <div class="section-header">üéØ 7. ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠</div>
            <div class="section-content">
                <div class="info-box" style="margin-bottom: 20px;">
                    <strong>ü§ñ AI Í∏∞Î∞ò Î∂ÑÏÑù:</strong> Ïù¥ Í∂åÏû•ÏÇ¨Ìï≠Îì§ÏùÄ Claude AIÍ∞Ä Ïã§Ï†ú ÏÑ±Îä• Î©îÌä∏Î¶≠, ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù, ÎäêÎ¶∞ ÏøºÎ¶¨ Îç∞Ïù¥ÌÑ∞Î•º Ï¢ÖÌï© Î∂ÑÏÑùÌïòÏó¨ ÏÉùÏÑ±Ìïú ÎßûÏ∂§Ìòï Ï†úÏïàÏûÖÎãàÎã§.
                </div>
                {self._generate_recommendations_html(claude_recommendations)}
            </div>
        </div>

        <div class="section">
            <div class="section-header">üìä {db_instance_identifier} Ïù∏Ïä§ÌÑ¥Ïä§ Î≥¥Í≥†ÏÑú Ï†ïÎ≥¥</div>
            <div class="section-content">
                <div class="info-box">
                    <strong>üìã Î≥¥Í≥†ÏÑú ÏÉÅÏÑ∏ Ï†ïÎ≥¥:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li><strong>Ïù∏Ïä§ÌÑ¥Ïä§ ID:</strong> {db_instance_identifier}</li>
                        <li><strong>Î∂ÑÏÑù Í∏∞Í∞Ñ:</strong> {hours}ÏãúÍ∞Ñ</li>
                        <li><strong>Î¶¨Ï†Ñ:</strong> {region}</li>
                        <li><strong>ÏÉùÏÑ± ÏãúÍ∞Ñ:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</li>
                        <li><strong>Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞:</strong> <a href="file://{DATA_DIR / csv_filename}" target="_blank">{csv_filename}</a></li>
                    </ul>
                </div>
                
                <div style="text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em;">
                    <em>Ïù¥ Î≥¥Í≥†ÏÑúÎäî {db_instance_identifier} Ïù∏Ïä§ÌÑ¥Ïä§Ïùò {hours}ÏãúÍ∞Ñ ÎèôÏïàÏùò ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.<br>
                    Ï†ïÌôïÌïú ÏÑ±Îä• Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ÏÑúÎäî ÏµúÏÜå 1Ï£ºÏùº Ïù¥ÏÉÅÏùò Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.</em>
                </div>
            </div>
        </div>
    </div>
    
    <!-- ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï Î™®Îã¨ -->
    {threshold_modal_html}
    
    <script>
        // Î™®Îã¨ Ï†úÏñ¥ JavaScript
        var modal = document.getElementById('thresholdModal');
        var span = document.getElementsByClassName('close')[0];
        
        span.onclick = function() {{
            modal.style.display = 'none';
        }}
        
        window.onclick = function(event) {{
            if (event.target == modal) {{
                modal.style.display = 'none';
            }}
        }}
    </script>
</body>
</html>"""

            # HTML ÌååÏùº Ï†ÄÏû•
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"HTML Î≥¥Í≥†ÏÑú Ï†ÄÏû• ÏôÑÎ£å: {report_path}")

            return f"""üóÑÔ∏è Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏôÑÎ£å

üìä **Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î≥¥Í≥†ÏÑú**
‚Ä¢ Ïù∏Ïä§ÌÑ¥Ïä§: {db_instance_identifier}
‚Ä¢ Î¶¨Ï†Ñ: {region}
‚Ä¢ Î∂ÑÏÑù Í∏∞Í∞Ñ: {hours}ÏãúÍ∞Ñ
‚Ä¢ ÏÉùÏÑ± ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

üìÅ **ÏÉùÏÑ±Îêú ÌååÏùºÎì§:**
‚Ä¢ Ï¢ÖÌï© Î≥¥Í≥†ÏÑú: {report_path.name}
‚Ä¢ Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞: <a href="file://{DATA_DIR / csv_filename}" target="_blank">{csv_filename}</a>
‚Ä¢ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù: Ìè¨Ìï®Îê®

üìà **Ìè¨Ìï®Îêú Î∂ÑÏÑù:**
‚úÖ ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏöîÏïΩ Î∞è ÌÜµÍ≥Ñ
‚úÖ Î©îÌä∏Î¶≠ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù
‚úÖ Ïù¥ÏÉÅ ÏßïÌõÑ(ÏïÑÏõÉÎùºÏù¥Ïñ¥) ÌÉêÏßÄ
‚úÖ ÎäêÎ¶∞ ÏøºÎ¶¨ ÏàòÏßë Î∞è Î∂ÑÏÑù
‚úÖ Î¶¨ÏÜåÏä§ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ Î∂ÑÏÑù (CPU, Î©îÎ™®Î¶¨, ÏûÑÏãúÍ≥µÍ∞Ñ)
‚úÖ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ Î∞è Ïï°ÏÖò ÏïÑÏù¥ÌÖú
‚úÖ Î∞òÏùëÌòï HTML ÎîîÏûêÏù∏

üí° **Ï£ºÏöî ÌäπÏßï:**
‚Ä¢ Î™®Î∞îÏùº ÏµúÏ†ÅÌôîÎêú Î∞òÏùëÌòï ÎîîÏûêÏù∏
‚Ä¢ ÏÉÅÏÑ∏Ìïú Î©îÌä∏Î¶≠ Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî
‚Ä¢ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠
‚Ä¢ Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Ïï°ÏÖò ÏïÑÏù¥ÌÖú Ï†úÍ≥µ

üîç Î≥¥Í≥†ÏÑúÎ•º Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú Ïó¥Ïñ¥ ÏÉÅÏÑ∏ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.
üìÑ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: {debug_log_path}"""

        except Exception as e:
            debug_log(f"Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïò§Î•ò: {e}")
            logger.error(f"Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return (
                f"‚ùå Ï¢ÖÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}\nüìÑ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: {debug_log_path}"
            )

    async def generate_cluster_performance_report(
        self,
        database_secret: str,
        db_cluster_identifier: str,
        hours: int = 24,
        region: str = "ap-northeast-2",
    ) -> str:
        """
        Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ÑÏö© ÏÑ±Îä• Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        - ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î∂ÑÏÑù (Î∂ÄÌïò Î∂ÑÏÇ∞, Î†àÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏßÄÏó∞, HLL Îì±)
        - Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú ÎßÅÌÅ¨ Ï†úÍ≥µ
        - Writer/Reader Ïó≠Ìï†Î≥Ñ ÎπÑÍµê Î∂ÑÏÑù
        """
        # ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ ÌååÏùº ÏÉùÏÑ±
        debug_log_path = (
            LOGS_DIR
            / f"debug_log_cluster_{db_cluster_identifier}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )

        try:
            debug_log(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ±Îä• Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏãúÏûë: {db_cluster_identifier}")

            # 1. database_secretÏóêÏÑú Ïã§Ï†ú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ïÎ≥¥ Ï∞æÍ∏∞
            debug_log("RDS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî")
            rds_client = boto3.client("rds", region_name=region)

            # SecretÏóêÏÑú Ìò∏Ïä§Ìä∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            debug_log("Secret Ï†ïÎ≥¥ Ï°∞Ìöå")
            secrets_client = boto3.client(
                "secretsmanager", region_name=region, verify=False
            )
            get_secret_value_response = secrets_client.get_secret_value(
                SecretId=database_secret
            )
            secret = get_secret_value_response["SecretString"]
            secret_info = json.loads(secret)
            host = secret_info.get("host", "")
            debug_log(f"Ìò∏Ïä§Ìä∏ Ï†ïÎ≥¥: {host}")

            # Ìò∏Ïä§Ìä∏ Ï†ïÎ≥¥Î°ú Ïã§Ï†ú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï∞æÍ∏∞
            actual_cluster_id = None
            if host:
                # Î™®Îì† ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï°∞ÌöåÌï¥ÏÑú ÏóîÎìúÌè¨Ïù∏Ìä∏ Îß§Ïπ≠
                debug_log("ÌÅ¥Îü¨Ïä§ÌÑ∞ Î™©Î°ù Ï°∞Ìöå Î∞è Îß§Ïπ≠")
                clusters = rds_client.describe_db_clusters()["DBClusters"]
                for cluster in clusters:
                    if cluster.get("Endpoint", "") in host or host in cluster.get(
                        "Endpoint", ""
                    ):
                        actual_cluster_id = cluster["DBClusterIdentifier"]
                        break

            # Ïã§Ï†ú ÌÅ¥Îü¨Ïä§ÌÑ∞ IDÍ∞Ä ÏóÜÏúºÎ©¥ ÌååÎùºÎØ∏ÌÑ∞Î°ú Î∞õÏùÄ Í∞í ÏÇ¨Ïö©
            if not actual_cluster_id:
                actual_cluster_id = db_cluster_identifier

            debug_log(f"Ïã§Ï†ú ÌÅ¥Îü¨Ïä§ÌÑ∞ ID: {actual_cluster_id}")

            cluster_info = rds_client.describe_db_clusters(
                DBClusterIdentifier=actual_cluster_id
            )["DBClusters"][0]

            cluster_members = cluster_info["DBClusterMembers"]
            writer_instance = next(
                (m for m in cluster_members if m["IsClusterWriter"]), None
            )
            reader_instances = [m for m in cluster_members if not m["IsClusterWriter"]]

            debug_log(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Íµ¨ÏÑ±: Writer 1Í∞ú, Reader {len(reader_instances)}Í∞ú")

            # 2. Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ Î©îÌä∏Î¶≠ ÏàòÏßë Î∞è ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
            instance_reports = {}
            cluster_metrics = {}

            # Writer Ïù∏Ïä§ÌÑ¥Ïä§ Ï≤òÎ¶¨
            if writer_instance:
                writer_id = writer_instance["DBInstanceIdentifier"]
                debug_log(f"Writer Ïù∏Ïä§ÌÑ¥Ïä§ Ï≤òÎ¶¨: {writer_id}")

                # ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
                writer_report = await self.generate_comprehensive_performance_report(
                    database_secret, writer_id, region, hours
                )
                instance_reports[writer_id] = {
                    "role": "Writer",
                    "report": writer_report,
                    "is_writer": True,
                }
                debug_log(
                    f"Writer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉÅÏÑ∏Î≥¥Í≥†ÏÑú ÏÉùÏÑ±: {writer_id},{writer_report}"
                )

                # Î©îÌä∏Î¶≠ ÏàòÏßë
                metrics_result = await self.collect_db_metrics(writer_id, region, hours)
                # Ïã§Ï†ú ÏÉùÏÑ±Îêú ÌååÏùºÎ™Ö Ï∂îÏ∂ú
                if "Ï†ÄÏû• ÏúÑÏπò:" in metrics_result:
                    csv_path = metrics_result.split("Ï†ÄÏû• ÏúÑÏπò: ")[-1].strip()
                    cluster_metrics[writer_id] = csv_path.split("/")[
                        -1
                    ]  # ÌååÏùºÎ™ÖÎßå Ï∂îÏ∂ú
                else:
                    cluster_metrics[writer_id] = (
                        f"database_metrics_{writer_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            else:
                debug_log("Writer Ïù∏Ïä§ÌÑ¥Ïä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå")

            # Reader Ïù∏Ïä§ÌÑ¥Ïä§Îì§ Ï≤òÎ¶¨
            for reader in reader_instances:
                reader_id = reader["DBInstanceIdentifier"]
                debug_log(f"Reader Ïù∏Ïä§ÌÑ¥Ïä§ Ï≤òÎ¶¨: {reader_id}")

                # ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
                reader_report = await self.generate_comprehensive_performance_report(
                    database_secret, reader_id, region, hours
                )
                instance_reports[reader_id] = {
                    "role": "Reader",
                    "report": reader_report,
                    "is_writer": False,
                }

                # Î©îÌä∏Î¶≠ ÏàòÏßë
                metrics_result = await self.collect_db_metrics(reader_id, region, hours)
                # Ïã§Ï†ú ÏÉùÏÑ±Îêú ÌååÏùºÎ™Ö Ï∂îÏ∂ú
                if "Ï†ÄÏû• ÏúÑÏπò:" in metrics_result:
                    csv_path = metrics_result.split("Ï†ÄÏû• ÏúÑÏπò: ")[-1].strip()
                    cluster_metrics[reader_id] = csv_path.split("/")[
                        -1
                    ]  # ÌååÏùºÎ™ÖÎßå Ï∂îÏ∂ú
                else:
                    cluster_metrics[reader_id] = (
                        f"database_metrics_{reader_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    )
            debug_log("ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î∂ÑÏÑù ÏãúÏûë")
            # 3. ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î©îÌä∏Î¶≠ ÏàòÏßë
            cluster_level_metrics = await self._collect_cluster_level_metrics(
                actual_cluster_id, region, hours
            )

            # 4. ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïù¥Î≤§Ìä∏ ÏàòÏßë (ÏµúÍ∑º 7Ïùº)
            cluster_events = await self._collect_cluster_events(
                actual_cluster_id, region, 7 * 24  # 7Ïùº
            )

            # 5. ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î∂ÑÏÑù
            cluster_analysis = await self._analyze_cluster_metrics(
                cluster_metrics, cluster_info, cluster_level_metrics, cluster_events
            )

            # 4. ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
            debug_log("ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏãúÏûë")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = (
                f"cluster_performance_report_{actual_cluster_id}_{timestamp}.html"
            )
            report_path = Path.cwd() / "output" / report_filename

            html_content = await self._generate_cluster_html_report(
                cluster_info,
                instance_reports,
                cluster_analysis,
                timestamp,
                cluster_level_metrics,
                cluster_events,
            )

            # Î≥¥Í≥†ÏÑú Ï†ÄÏû•
            report_path.parent.mkdir(exist_ok=True)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            debug_log(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏôÑÎ£å: {report_path}")

            return f"""‚úÖ Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ±Îä• Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏôÑÎ£å

üèóÔ∏è ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ïÎ≥¥:
‚Ä¢ ÌÅ¥Îü¨Ïä§ÌÑ∞ ID: {actual_cluster_id}
‚Ä¢ ÏóîÏßÑ: {cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}
‚Ä¢ Ïù∏Ïä§ÌÑ¥Ïä§ Ïàò: {len(cluster_members)}Í∞ú (Writer: 1Í∞ú, Reader: {len(reader_instances)}Í∞ú)
‚Ä¢ Î∂ÑÏÑù Í∏∞Í∞Ñ: ÏµúÍ∑º {hours}ÏãúÍ∞Ñ

üìä ÏÉùÏÑ±Îêú Î≥¥Í≥†ÏÑú:
‚Ä¢ üéØ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÜµÌï© Î≥¥Í≥†ÏÑú: file://{report_path}
‚Ä¢ üìã Í∞úÎ≥Ñ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú: {len(instance_reports)}Í∞ú

üîç Ï£ºÏöî Î∂ÑÏÑù ÎÇ¥Ïö©:
‚Ä¢ ÌÅ¥Îü¨Ïä§ÌÑ∞ Î∂ÄÌïò Î∂ÑÏÇ∞ ÏÉÅÌÉú
‚Ä¢ Writer/Reader ÏÑ±Îä• ÎπÑÍµê
‚Ä¢ Î†àÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏßÄÏó∞ Î∂ÑÏÑù
‚Ä¢ Ïù∏Ïä§ÌÑ¥Ïä§ Í∞Ñ Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê
‚Ä¢ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠

üìÑ ÌÅ¥Îü¨Ïä§ÌÑ∞ Î≥¥Í≥†ÏÑúÎ•º Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú Ïó¥Ïñ¥ Ï†ÑÏ≤¥ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÍ≥†,
   Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÏÉÅÏÑ∏ Î∂ÑÏÑùÏùÄ ÎßÅÌÅ¨Î•º ÌÜµÌï¥ Ï†ëÍ∑ºÌïòÏÑ∏Ïöî.
üìÑ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: {debug_log_path}"""

        except Exception as e:
            debug_log(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ±Îä• Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïò§Î•ò: {e}")
            return f"‚ùå ÌÅ¥Îü¨Ïä§ÌÑ∞ Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}\nüìÑ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: {debug_log_path}"

    async def _collect_cluster_level_metrics(
        self, cluster_id: str, region: str, hours: int
    ) -> Dict:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î©îÌä∏Î¶≠ ÏàòÏßë"""
        try:
            cloudwatch = boto3.client("cloudwatch", region_name=region)
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î©îÌä∏Î¶≠ Ï†ïÏùò
            cluster_metrics = [
                "CPUUtilization",
                "DatabaseConnections",
                "FreeableMemory",
                "ReadIOPS",
                "WriteIOPS",
                "ReadLatency",
                "WriteLatency",
                "NetworkReceiveThroughput",
                "NetworkTransmitThroughput",
                "AuroraReplicaLag",
                "AuroraReplicaLagMaximum",
                "AuroraReplicaLagMinimum",
                "VolumeBytesUsed",
                "VolumeReadIOPs",
                "VolumeWriteIOPs",
            ]

            metrics_data = {}

            for metric_name in cluster_metrics:
                try:
                    response = cloudwatch.get_metric_statistics(
                        Namespace="AWS/RDS",
                        MetricName=metric_name,
                        Dimensions=[
                            {"Name": "DBClusterIdentifier", "Value": cluster_id}
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5Î∂Ñ Í∞ÑÍ≤©
                        Statistics=["Average", "Maximum", "Minimum"],
                    )

                    if response["Datapoints"]:
                        metrics_data[metric_name] = response["Datapoints"]

                except Exception as e:
                    logger.warning(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ ÏàòÏßë Ïã§Ìå® {metric_name}: {e}")

            return metrics_data

        except Exception as e:
            logger.error(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ ÏàòÏßë Ïò§Î•ò: {e}")
            return {}

    async def _collect_cluster_events(
        self, cluster_id: str, region: str, hours: int
    ) -> List[Dict]:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïù¥Î≤§Ìä∏ ÏàòÏßë (ÏµúÍ∑º NÏãúÍ∞Ñ)"""
        try:
            rds_client = boto3.client("rds", region_name=region)
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïù¥Î≤§Ìä∏ Ï°∞Ìöå
            response = rds_client.describe_events(
                SourceIdentifier=cluster_id,
                SourceType="db-cluster",
                StartTime=start_time,
                EndTime=end_time,
            )

            events = []
            for event in response.get("Events", []):
                events.append(
                    {
                        "date": event["Date"].strftime("%Y-%m-%d %H:%M:%S"),
                        "message": event["Message"],
                        "source_id": event.get("SourceId", ""),
                        "event_categories": event.get("EventCategories", []),
                        "severity": self._categorize_event_severity(event["Message"]),
                    }
                )

            # Ïù∏Ïä§ÌÑ¥Ïä§ Î†àÎ≤® Ïù¥Î≤§Ìä∏ÎèÑ ÏàòÏßë
            cluster_info = rds_client.describe_db_clusters(
                DBClusterIdentifier=cluster_id
            )["DBClusters"][0]

            for member in cluster_info["DBClusterMembers"]:
                instance_id = member["DBInstanceIdentifier"]
                try:
                    instance_response = rds_client.describe_events(
                        SourceIdentifier=instance_id,
                        SourceType="db-instance",
                        StartTime=start_time,
                        EndTime=end_time,
                    )

                    for event in instance_response.get("Events", []):
                        events.append(
                            {
                                "date": event["Date"].strftime("%Y-%m-%d %H:%M:%S"),
                                "message": f"[{instance_id}] {event['Message']}",
                                "source_id": instance_id,
                                "event_categories": event.get("EventCategories", []),
                                "severity": self._categorize_event_severity(
                                    event["Message"]
                                ),
                            }
                        )

                except Exception as e:
                    logger.warning(f"Ïù∏Ïä§ÌÑ¥Ïä§ Ïù¥Î≤§Ìä∏ ÏàòÏßë Ïã§Ìå® {instance_id}: {e}")

            # ÎÇ†ÏßúÏàú Ï†ïÎ†¨
            events.sort(key=lambda x: x["date"], reverse=True)
            return events

        except Exception as e:
            logger.error(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïù¥Î≤§Ìä∏ ÏàòÏßë Ïò§Î•ò: {e}")
            return []

    def _categorize_event_severity(self, message: str) -> str:
        """Ïù¥Î≤§Ìä∏ Î©îÏãúÏßÄ Í∏∞Î∞ò Ïã¨Í∞ÅÎèÑ Î∂ÑÎ•ò"""
        message_lower = message.lower()

        if any(
            keyword in message_lower
            for keyword in [
                "failed",
                "error",
                "critical",
                "fatal",
                "crash",
                "corruption",
            ]
        ):
            return "HIGH"
        elif any(
            keyword in message_lower
            for keyword in ["warning", "slow", "timeout", "retry", "restart", "reboot"]
        ):
            return "MEDIUM"
        else:
            return "LOW"

    async def _analyze_cluster_metrics(
        self,
        cluster_metrics: Dict,
        cluster_info: Dict,
        cluster_level_metrics: Dict = None,
        cluster_events: List = None,
    ) -> Dict:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î©îÌä∏Î¶≠ Î∂ÑÏÑù"""
        try:
            if not ANALYSIS_AVAILABLE:
                logger.warning("Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏóÜÏùå - Í∏∞Î≥∏ Î∂ÑÏÑùÎßå ÏàòÌñâ")
                return {"error": "Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§"}

            analysis = {
                "load_distribution": {},
                "replication_lag": {},
                "resource_comparison": {},
                "recommendations": [],
            }

            logger.info(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ Î∂ÑÏÑù ÏãúÏûë: {cluster_metrics}")

            # Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Ïùò Î©îÌä∏Î¶≠ Î°úÎìú Î∞è ÎπÑÍµê - data Ìè¥ÎçîÏóêÏÑú ÏßÅÏ†ë Ï∞æÍ∏∞
            metrics_data = {}

            # ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©§Î≤ÑÏóêÏÑú Ïù∏Ïä§ÌÑ¥Ïä§ ID Í∞ÄÏ†∏Ïò§Í∏∞
            for member in cluster_info["DBClusterMembers"]:
                instance_id = member["DBInstanceIdentifier"]

                # data Ìè¥ÎçîÏóêÏÑú Ìï¥Îãπ Ïù∏Ïä§ÌÑ¥Ïä§Ïùò ÏµúÏã† CSV ÌååÏùº Ï∞æÍ∏∞
                data_dir = Path("data")
                csv_files = list(data_dir.glob(f"database_metrics_{instance_id}_*.csv"))

                if csv_files:
                    # Í∞ÄÏû• ÏµúÏã† ÌååÏùº ÏÑ†ÌÉù (ÌååÏùºÎ™ÖÏùò ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Í∏∞Ï§Ä)
                    latest_csv = max(csv_files, key=lambda x: x.name.split("_")[-1])
                    logger.info(f"Î©îÌä∏Î¶≠ ÌååÏùº Î∞úÍ≤¨: {instance_id} -> {latest_csv}")

                    try:
                        df = pd.read_csv(latest_csv)
                        metrics_data[instance_id] = df
                        logger.info(
                            f"Î©îÌä∏Î¶≠ ÌååÏùº Î°úÎìú ÏÑ±Í≥µ: {instance_id} ({len(df)} Ìñâ)"
                        )
                    except Exception as e:
                        logger.warning(f"Î©îÌä∏Î¶≠ ÌååÏùº Î°úÎìú Ïã§Ìå® {latest_csv}: {e}")
                else:
                    logger.warning(f"Î©îÌä∏Î¶≠ ÌååÏùº ÏóÜÏùå: {instance_id}")

            logger.info(f"Î°úÎìúÎêú Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞: {len(metrics_data)}Í∞ú Ïù∏Ïä§ÌÑ¥Ïä§")

            if len(metrics_data) >= 2:
                # Writer vs Reader ÎπÑÍµê
                writer_data = None
                reader_data = []

                for instance_id, df in metrics_data.items():
                    # ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©§Î≤Ñ Ï†ïÎ≥¥ÏóêÏÑú Ïó≠Ìï† ÌôïÏù∏
                    is_writer = any(
                        m["DBInstanceIdentifier"] == instance_id
                        and m["IsClusterWriter"]
                        for m in cluster_info["DBClusterMembers"]
                    )

                    logger.info(
                        f"Ïù∏Ïä§ÌÑ¥Ïä§ Ïó≠Ìï† ÌôïÏù∏: {instance_id} -> {'Writer' if is_writer else 'Reader'}"
                    )

                    if is_writer:
                        writer_data = df
                    else:
                        reader_data.append((instance_id, df))

                # Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù
                if writer_data is not None and reader_data:
                    logger.info("Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù ÏãúÏûë")
                    try:
                        analysis["load_distribution"] = self._analyze_load_distribution(
                            writer_data, reader_data
                        )
                        logger.info("Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù ÏôÑÎ£å")
                    except Exception as e:
                        logger.error(f"Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù Ïò§Î•ò: {e}")
                        analysis["load_distribution"] = {}
                else:
                    logger.warning(
                        f"Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù Î∂àÍ∞Ä: writer_data={writer_data is not None}, reader_data={len(reader_data)}"
                    )

                # Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê
                try:
                    logger.info("Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê ÏãúÏûë")
                    analysis["resource_comparison"] = self._compare_resource_usage(
                        metrics_data
                    )
                    logger.info("Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê ÏôÑÎ£å")
                except Exception as e:
                    logger.error(f"Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê Ïò§Î•ò: {e}")
                    analysis["resource_comparison"] = {}
            else:
                logger.warning(
                    f"Î∂ÑÏÑùÏùÑ ÏúÑÌïú Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±: {len(metrics_data)}Í∞ú (ÏµúÏÜå 2Í∞ú ÌïÑÏöî)"
                )

            return analysis

        except Exception as e:
            logger.error(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return {"error": str(e)}

    def _analyze_load_distribution(self, writer_df, reader_data):
        """Î∂ÄÌïò Î∂ÑÏÇ∞ ÏÉÅÌÉú Î∂ÑÏÑù"""
        try:
            writer_cpu = (
                writer_df["CPUUtilization"].mean()
                if "CPUUtilization" in writer_df.columns
                else 0
            )
            writer_connections = (
                writer_df["DatabaseConnections"].mean()
                if "DatabaseConnections" in writer_df.columns
                else 0
            )

            reader_stats = []
            for reader_id, reader_df in reader_data:
                reader_cpu = (
                    reader_df["CPUUtilization"].mean()
                    if "CPUUtilization" in reader_df.columns
                    else 0
                )
                reader_connections = (
                    reader_df["DatabaseConnections"].mean()
                    if "DatabaseConnections" in reader_df.columns
                    else 0
                )
                reader_stats.append(
                    {
                        "instance_id": reader_id,
                        "cpu": reader_cpu,
                        "connections": reader_connections,
                    }
                )

            return {
                "writer": {"cpu": writer_cpu, "connections": writer_connections},
                "readers": reader_stats,
                "balance_score": self._calculate_balance_score(
                    writer_cpu, [r["cpu"] for r in reader_stats]
                ),
            }
        except Exception as e:
            logger.error(f"Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return {}

    def _calculate_balance_score(self, writer_cpu, reader_cpus):
        """Î∂ÄÌïò Î∂ÑÏÇ∞ Ï†êÏàò Í≥ÑÏÇ∞ (0-100)"""
        if not reader_cpus:
            return 0

        total_cpu = writer_cpu + sum(reader_cpus)
        if total_cpu == 0:
            return 100

        # Ïù¥ÏÉÅÏ†ÅÏù∏ Î∂ÑÏÇ∞: WriterÍ∞Ä Ï†ÑÏ≤¥ Î∂ÄÌïòÏùò 60-70% Îã¥Îãπ
        writer_ratio = writer_cpu / total_cpu
        ideal_ratio = 0.65

        deviation = abs(writer_ratio - ideal_ratio)
        score = max(0, 100 - (deviation * 200))  # Ìé∏Ï∞®Í∞Ä ÌÅ¥ÏàòÎ°ù Ï†êÏàò Í∞êÏÜå

        return round(score, 1)

    def _compare_resource_usage(self, metrics_data):
        """Ïù∏Ïä§ÌÑ¥Ïä§ Í∞Ñ Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê"""
        try:
            comparison = {}

            for instance_id, df in metrics_data.items():
                # Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•† Í≥ÑÏÇ∞ (FreeableMemory Í∏∞Î∞ò)
                memory_usage_percent = 0
                if "FreeableMemory" in df.columns and not df["FreeableMemory"].empty:
                    freeable_memory = df["FreeableMemory"].mean()
                    # Í∞ÄÏ†ï: Ï¥ù Î©îÎ™®Î¶¨ 16GB (16 * 1024 * 1024 * 1024 bytes)
                    total_memory = 16 * 1024 * 1024 * 1024
                    memory_usage_percent = ((total_memory - freeable_memory) / total_memory) * 100

                comparison[instance_id] = {
                    "cpu_avg": (
                        df["CPUUtilization"].mean()
                        if "CPUUtilization" in df.columns
                        else 0
                    ),
                    "memory_usage_percent": memory_usage_percent,
                    "connections_avg": (
                        df["DatabaseConnections"].mean()
                        if "DatabaseConnections" in df.columns
                        else 0
                    ),
                    "read_iops": (
                        df["ReadIOPS"].mean() if "ReadIOPS" in df.columns else 0
                    ),
                    "write_iops": (
                        df["WriteIOPS"].mean() if "WriteIOPS" in df.columns else 0
                    ),
                }

            return comparison
        except Exception as e:
            logger.error(f"Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê Ïò§Î•ò: {e}")
            return {}

    async def _generate_cluster_html_report(
        self,
        cluster_info,
        instance_reports,
        cluster_analysis,
        timestamp,
        cluster_level_metrics=None,
        cluster_events=None,
    ):
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌÜµÌï© HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± - Í∏∞Ï°¥ ÌòïÌÉúÎ°ú Îã®ÏàúÌôî"""

        cluster_id = cluster_info["DBClusterIdentifier"]
        engine_info = f"{cluster_info.get('Engine', 'N/A')} {cluster_info.get('EngineVersion', 'N/A')}"

        # Ïù∏Ïä§ÌÑ¥Ïä§ ÎßÅÌÅ¨ ÏÉùÏÑ±
        instance_links = []
        for instance_id, report_info in instance_reports.items():
            role = report_info["role"]
            report_text = report_info["report"]
            if "comprehensive_performance_report_" in report_text:
                import re

                match = re.search(
                    r"comprehensive_performance_report_[^.]+\.html", report_text
                )
                if match:
                    report_filename = match.group(0)
                    instance_links.append(
                        f"""
                    <tr>
                        <td><span class="role-badge {'writer' if report_info['is_writer'] else 'reader'}">{role}</span></td>
                        <td>{instance_id}</td>
                        <td><a href="{report_filename}" target="_blank" class="detail-link">üìä ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú Î≥¥Í∏∞</a></td>
                    </tr>
                    """
                    )

        # Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù HTML ÏÉùÏÑ± (Ï†úÍ±∞)
        load_analysis_html = ""

        # Î¶¨ÏÜåÏä§ ÎπÑÍµê HTML ÏÉùÏÑ±
        resource_comparison_html = self._generate_resource_comparison_html(cluster_analysis.get('resource_comparison', {}))

        return f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ±Îä• Î≥¥Í≥†ÏÑú - {cluster_id}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f7fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.2em; }}
        .header .subtitle {{ margin-top: 10px; opacity: 0.9; font-size: 1.1em; }}
        
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; padding: 30px; }}
        .summary-card {{ background: #f8f9fa; border-radius: 8px; padding: 20px; border-left: 4px solid #007bff; }}
        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .summary-card .value {{ font-size: 1.8em; font-weight: bold; color: #007bff; }}
        
        .section {{ margin: 20px 30px; }}
        .section-header {{ background: #e9ecef; padding: 15px; border-radius: 8px; font-weight: bold; font-size: 1.2em; color: #495057; }}
        .section-content {{ padding: 20px 0; }}
        
        .instance-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .instance-table th, .instance-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }}
        .instance-table th {{ background: #f8f9fa; font-weight: bold; }}
        
        .role-badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .role-badge.writer {{ background: #d4edda; color: #155724; }}
        .role-badge.reader {{ background: #d1ecf1; color: #0c5460; }}
        
        .detail-link {{ color: #007bff; text-decoration: none; font-weight: bold; }}
        .detail-link:hover {{ text-decoration: underline; }}
        
        .recommendation {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 15px; margin: 15px 0; }}
        .recommendation h4 {{ margin: 0 0 10px 0; color: #856404; }}
        
        .resource-grid {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0; }}
        .resource-card {{ background: #f8f9fa; border-radius: 8px; padding: 15px; }}
        .resource-card h4 {{ margin: 0 0 15px 0; color: #495057; }}
        .metric-bar {{ background: #e9ecef; height: 20px; border-radius: 10px; margin: 5px 0; position: relative; }}
        .metric-fill {{ height: 100%; border-radius: 10px; }}
        .metric-label {{ font-size: 0.9em; color: #6c757d; }}
        
        @media (max-width: 768px) {{
            .summary-grid {{ grid-template-columns: 1fr; }}
            .resource-grid {{ grid-template-columns: 1fr; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üèóÔ∏è Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ±Îä• Î≥¥Í≥†ÏÑú</h1>
            <div class="subtitle">ÌÅ¥Îü¨Ïä§ÌÑ∞: {cluster_id} | ÏóîÏßÑ: {engine_info} | ÏÉùÏÑ±ÏùºÏãú: {timestamp}</div>
        </div>
        
        <div class="summary-grid">
            <div class="summary-card">
                <h3>üìä ÌÅ¥Îü¨Ïä§ÌÑ∞ Íµ¨ÏÑ±</h3>
                <div class="value">{len(cluster_info['DBClusterMembers'])}Í∞ú Ïù∏Ïä§ÌÑ¥Ïä§</div>
                <div>Writer: 1Í∞ú, Reader: {len([m for m in cluster_info['DBClusterMembers'] if not m['IsClusterWriter']])}Í∞ú</div>
            </div>
            <div class="summary-card">
                <h3>üîÑ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÉÅÌÉú</h3>
                <div class="value" style="color: #28a745">{cluster_info.get('Status', 'AVAILABLE')}</div>
                <div>Multi-AZ: {'Yes' if cluster_info.get('MultiAZ', False) else 'No'}</div>
            </div>
            <div class="summary-card">
                <h3>üîê Î≥¥Ïïà ÏÑ§Ï†ï</h3>
                <div class="value">üîí</div>
                <div>ÏïîÌò∏Ìôî: {'ÌôúÏÑ±Ìôî' if cluster_info.get('StorageEncrypted', False) else 'ÎπÑÌôúÏÑ±Ìôî'}</div>
            </div>
            <div class="summary-card">
                <h3>üíæ Î∞±ÏóÖ ÏÑ§Ï†ï</h3>
                <div class="value">{cluster_info.get('BackupRetentionPeriod', 0)}Ïùº</div>
                <div>ÏûêÎèô Î∞±ÏóÖ: {'ÌôúÏÑ±Ìôî' if cluster_info.get('BackupRetentionPeriod', 0) > 0 else 'ÎπÑÌôúÏÑ±Ìôî'}</div>
            </div>
        </div>
        
        <div class="section">
            <div class="section-header">üìã Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú</div>
            <div class="section-content">
                <table class="instance-table">
                    <thead>
                        <tr>
                            <th>Ïó≠Ìï†</th>
                            <th>Ïù∏Ïä§ÌÑ¥Ïä§ ID</th>
                            <th>ÏÉÅÏÑ∏ Î∂ÑÏÑù</th>
                        </tr>
                    </thead>
                    <tbody>
                        {''.join(instance_links)}
                    </tbody>
                </table>
                <div class="recommendation">
                    <h4>üí° ÏÇ¨Ïö© Í∞ÄÏù¥Îìú</h4>
                    <p>Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Ïùò "üìä ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú Î≥¥Í∏∞" ÎßÅÌÅ¨Î•º ÌÅ¥Î¶≠ÌïòÎ©¥ Ìï¥Îãπ Ïù∏Ïä§ÌÑ¥Ïä§Ïùò ÏÉÅÏÑ∏Ìïú ÏÑ±Îä• Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.</p>
                </div>
            </div>
        </div>
        
        {load_analysis_html}
        
        {resource_comparison_html}
    </div>
</body>
</html>"""

    def _generate_load_analysis_html(self, load_distribution: Dict) -> str:
        """Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù HTML ÏÉùÏÑ± (ÎπÑÌôúÏÑ±Ìôî)"""
        return ""

    def _generate_resource_comparison_html(self, resource_comparison: Dict) -> str:
        """Î¶¨ÏÜåÏä§ ÎπÑÍµê HTML ÏÉùÏÑ±"""
        if not resource_comparison:
            return ""
        
        cards_html = ""
        for instance_id, metrics in resource_comparison.items():
            cpu_usage = metrics.get('cpu_avg', 0)
            memory_usage = metrics.get('memory_usage_percent', 0)
            connections_avg = metrics.get('connections_avg', 0)
            read_iops = metrics.get('read_iops', 0)
            write_iops = metrics.get('write_iops', 0)
            
            cards_html += f"""
            <div class="resource-card">
                <h4>{instance_id}</h4>
                <div class="metric-label">CPU ÏÇ¨Ïö©Î•†: {cpu_usage:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(cpu_usage, 100)}%; background: #007bff;"></div>
                </div>
                
                <div class="metric-label">Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Î•†: {memory_usage:.1f}%</div>
                <div class="metric-bar">
                    <div class="metric-fill" style="width: {min(memory_usage, 100)}%; background: #28a745;"></div>
                </div>
                
                <div class="metric-label">ÌèâÍ∑† Ïó∞Í≤∞ Ïàò: {connections_avg:.1f}</div>
                <div class="metric-label">Read IOPS: {read_iops:.1f}</div>
                <div class="metric-label">Write IOPS: {write_iops:.1f}</div>
            </div>"""
        
        return f"""
        <div class="section">
            <div class="section-header">üìä Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Î•† ÎπÑÍµê</div>
            <div class="section-content">
                <div class="resource-grid">
                    {cards_html}
                </div>
            </div>
        </div>"""

    def _generate_cluster_metrics_table(self, cluster_metrics: Dict) -> str:
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ Ìëú HTML ÏÉùÏÑ±"""
        if not cluster_metrics:
            return '<div class="no-data">ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.</div>'

        # Ï£ºÏöî Î©îÌä∏Î¶≠Îì§Ïóê ÎåÄÌïú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        important_metrics = [
            ("CPUUtilization", "CPU ÏÇ¨Ïö©Î•†", "%"),
            ("FreeableMemory", "ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÎ™®Î¶¨", "MB"),
            ("ReadIOPS", "ÏùΩÍ∏∞ IOPS", "IOPS"),
            ("WriteIOPS", "Ïì∞Í∏∞ IOPS", "IOPS"),
            ("DatabaseConnections", "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïàò", "Í∞ú"),
            ("AuroraReplicaLag", "Aurora Î≥µÏ†ú ÏßÄÏó∞", "ms"),
        ]

        table_html = """
        <table class="metrics-table">
            <thead>
                <tr>
                    <th>Î©îÌä∏Î¶≠</th>
                    <th>ÌèâÍ∑†</th>
                    <th>ÏµúÎåÄÍ∞í</th>
                    <th>ÏµúÏÜåÍ∞í</th>
                    <th>Îã®ÏúÑ</th>
                </tr>
            </thead>
            <tbody>
        """

        for metric_name, display_name, unit in important_metrics:
            if metric_name in cluster_metrics:
                datapoints = cluster_metrics[metric_name]

                if datapoints:
                    avg_values = [float(point["Average"]) for point in datapoints]
                    max_values = [float(point["Maximum"]) for point in datapoints]
                    min_values = [float(point["Minimum"]) for point in datapoints]

                    overall_avg = sum(avg_values) / len(avg_values)
                    overall_max = max(max_values)
                    overall_min = min(min_values)

                    # Î©îÎ™®Î¶¨Îäî MB Îã®ÏúÑÎ°ú Î≥ÄÌôò
                    if metric_name == "FreeableMemory":
                        overall_avg = overall_avg / (1024 * 1024)
                        overall_max = overall_max / (1024 * 1024)
                        overall_min = overall_min / (1024 * 1024)

                    table_html += f"""
                    <tr>
                        <td class="metric-name">{display_name}</td>
                        <td class="metric-value">{overall_avg:.2f}</td>
                        <td class="metric-value">{overall_max:.2f}</td>
                        <td class="metric-value">{overall_min:.2f}</td>
                        <td>{unit}</td>
                    </tr>
                    """

        table_html += "</tbody></table>"

        if len([m for m, _, _ in important_metrics if m in cluster_metrics]) == 0:
            return '<div class="no-data">ÌëúÏãúÌï† Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.</div>'

        return table_html

    def _generate_events_table(self, events: List[Dict]) -> str:
        """Ïù¥Î≤§Ìä∏ ÌÖåÏù¥Î∏î HTML ÏÉùÏÑ±"""
        if not events:
            return '<div class="no-data">ÏµúÍ∑º 7ÏùºÍ∞Ñ Ïù¥Î≤§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.</div>'

        # ÏµúÍ∑º 20Í∞ú Ïù¥Î≤§Ìä∏Îßå ÌëúÏãú
        recent_events = events[:20]

        table_html = """
        <table class="events-table">
            <thead>
                <tr>
                    <th>ÏùºÏãú</th>
                    <th>Ïã¨Í∞ÅÎèÑ</th>
                    <th>ÏÜåÏä§</th>
                    <th>Î©îÏãúÏßÄ</th>
                    <th>Ïπ¥ÌÖåÍ≥†Î¶¨</th>
                </tr>
            </thead>
            <tbody>
        """

        for event in recent_events:
            severity_class = event["severity"].lower()
            categories = ", ".join(event.get("event_categories", []))

            table_html += f"""
            <tr>
                <td>{event['date']}</td>
                <td><span class="severity-badge {event['severity']}">{event['severity']}</span></td>
                <td>{event.get('source_id', 'N/A')}</td>
                <td>{event['message']}</td>
                <td>{categories}</td>
            </tr>
            """

        table_html += "</tbody></table>"

        if len(events) > 20:
            table_html += f'<div style="text-align: center; margin-top: 10px; color: #6c757d;">Ï¥ù {len(events)}Í∞ú Ïù¥Î≤§Ìä∏ Ï§ë ÏµúÍ∑º 20Í∞úÎßå ÌëúÏãú</div>'

        return table_html

    def _generate_chart_scripts(self, cluster_metrics: Dict) -> str:
        """Ï∞®Ìä∏ ÏÉùÏÑ± ÎπÑÌôúÏÑ±Ìôî - Í∞ÑÎã®Ìïú Î©îÌä∏Î¶≠ ÌëúÏãúÎßå"""
        return ""

    def _get_balance_color(self, score):
        """Î∂ÄÌïò Î∂ÑÏÇ∞ Ï†êÏàòÏóê Îî∞Î•∏ ÏÉâÏÉÅ Î∞òÌôò"""
        if score >= 80:
            return "#28a745"  # ÎÖπÏÉâ
        elif score >= 60:
            return "#ffc107"  # ÎÖ∏ÎûÄÏÉâ
        else:
            return "#dc3545"  # Îπ®Í∞ÑÏÉâ

    def _get_balance_status(self, score):
        """Î∂ÄÌïò Î∂ÑÏÇ∞ Ï†êÏàòÏóê Îî∞Î•∏ ÏÉÅÌÉú Î©îÏãúÏßÄ"""
        if score >= 80:
            return "Ïö∞ÏàòÌïú Î∂ÑÏÇ∞"
        elif score >= 60:
            return "Î≥¥ÌÜµ Î∂ÑÏÇ∞"
        else:
            return "Í∞úÏÑ† ÌïÑÏöî"

    def _generate_load_distribution_html(self, load_analysis):
        """Î∂ÄÌïò Î∂ÑÏÇ∞ Î∂ÑÏÑù HTML ÏÉùÏÑ±"""
        if not load_analysis:
            return """
            <div class="recommendation">
                <h4 style="color: #dc3545;">‚ö†Ô∏è Î∂ÄÌïò Î∂ÑÏÇ∞ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Î∂àÍ∞Ä</h4>
                <p><strong>ÏõêÏù∏:</strong> ÌÅ¥Îü¨Ïä§ÌÑ∞ Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï∂©Î∂ÑÌïòÏßÄ ÏïäÍ±∞ÎÇò Î∂ÑÏÑù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.</p>
                <p><strong>ÌôïÏù∏ÏÇ¨Ìï≠:</strong></p>
                <ul>
                    <li>Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Ïùò CloudWatch Î©îÌä∏Î¶≠Ïù¥ Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏàòÏßëÎêòÍ≥† ÏûàÎäîÏßÄ ÌôïÏù∏</li>
                    <li>WriterÏôÄ Reader Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä Î™®Îëê ÌôúÏÑ± ÏÉÅÌÉúÏù∏ÏßÄ ÌôïÏù∏</li>
                    <li>Î©îÌä∏Î¶≠ ÏàòÏßë Í∏∞Í∞Ñ ÎèôÏïà Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏</li>
                </ul>
                <p><strong>Í∂åÏû•Ï°∞Ïπò:</strong> Í∞úÎ≥Ñ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑúÎ•º ÌôïÏù∏ÌïòÏó¨ Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Ïùò ÏÑ±Îä• ÏÉÅÌÉúÎ•º Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.</p>
            </div>
            """

        writer = load_analysis.get("writer", {})
        readers = load_analysis.get("readers", [])

        html = f"""
        <div style="margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 5px;">
            <h4>Writer Ïù∏Ïä§ÌÑ¥Ïä§</h4>
            <p>CPU: {writer.get('cpu', 0):.1f}% | Ïó∞Í≤∞ Ïàò: {writer.get('connections', 0):.1f}</p>
        </div>
        """

        if readers:
            html += "<h4>Reader Ïù∏Ïä§ÌÑ¥Ïä§Îì§</h4>"
            for reader in readers:
                html += f"""
                <div style="margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 5px;">
                    <strong>{reader['instance_id']}</strong><br>
                    CPU: {reader['cpu']:.1f}% | Ïó∞Í≤∞ Ïàò: {reader['connections']:.1f}
                </div>
                """

        return html

    def _generate_cluster_recommendations(self, cluster_analysis, cluster_info):
        """ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []

        # Î∂ÄÌïò Î∂ÑÏÇ∞ Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        load_dist = cluster_analysis.get("load_distribution", {})
        balance_score = load_dist.get("balance_score", 0)

        if balance_score < 60:
            recommendations.append(
                {
                    "priority": "ÎÜíÏùå",
                    "title": "Î∂ÄÌïò Î∂ÑÏÇ∞ Í∞úÏÑ† ÌïÑÏöî",
                    "description": "WriterÏôÄ Reader Í∞Ñ Î∂ÄÌïò Î∂ÑÏÇ∞Ïù¥ Î∂àÍ∑†ÌòïÌï©ÎãàÎã§. ÏùΩÍ∏∞ ÏøºÎ¶¨Î•º Reader ÏóîÎìúÌè¨Ïù∏Ìä∏Î°ú Î∂ÑÏÇ∞ÌïòÏÑ∏Ïöî.",
                    "action": "Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú ÏùΩÍ∏∞ Ï†ÑÏö© ÏøºÎ¶¨Î•º Reader ÏóîÎìúÌè¨Ïù∏Ìä∏Î°ú ÎùºÏö∞ÌåÖ",
                }
            )

        # ÏïîÌò∏Ìôî Í∂åÏû•ÏÇ¨Ìï≠
        if not cluster_info.get("StorageEncrypted"):
            recommendations.append(
                {
                    "priority": "Ï§ëÍ∞Ñ",
                    "title": "Ïä§ÌÜ†Î¶¨ÏßÄ ÏïîÌò∏Ìôî ÌôúÏÑ±Ìôî",
                    "description": "Îç∞Ïù¥ÌÑ∞ Î≥¥ÏïàÏùÑ ÏúÑÌï¥ Ïä§ÌÜ†Î¶¨ÏßÄ ÏïîÌò∏ÌôîÎ•º ÌôúÏÑ±ÌôîÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.",
                    "action": "ÏÉà ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÉùÏÑ± Ïãú ÏïîÌò∏Ìôî ÏòµÏÖò ÌôúÏÑ±Ìôî",
                }
            )

        # Î∞±ÏóÖ ÏÑ§Ï†ï Í∂åÏû•ÏÇ¨Ìï≠
        backup_retention = cluster_info.get("BackupRetentionPeriod", 0)
        if backup_retention < 7:
            recommendations.append(
                {
                    "priority": "Ï§ëÍ∞Ñ",
                    "title": "Î∞±ÏóÖ Î≥¥Ï°¥ Í∏∞Í∞Ñ Ïó∞Ïû•",
                    "description": f"ÌòÑÏû¨ Î∞±ÏóÖ Î≥¥Ï°¥ Í∏∞Í∞ÑÏù¥ {backup_retention}ÏùºÏûÖÎãàÎã§. ÏµúÏÜå 7Ïùº Ïù¥ÏÉÅ Í∂åÏû•Ìï©ÎãàÎã§.",
                    "action": "Î∞±ÏóÖ Î≥¥Ï°¥ Í∏∞Í∞ÑÏùÑ 7-35ÏùºÎ°ú ÏÑ§Ï†ï",
                }
            )

        # HTML ÏÉùÏÑ±
        if not recommendations:
            return "<div class='recommendation'><h4>‚úÖ Ïö∞ÏàòÌïú ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ§Ï†ï</h4><p>ÌòÑÏû¨ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ§Ï†ïÏù¥ Î™®Î≤î ÏÇ¨Î°ÄÎ•º Ïûò Îî∞Î•¥Í≥† ÏûàÏäµÎãàÎã§.</p></div>"

        html = ""
        for rec in recommendations:
            priority_color = {
                "ÎÜíÏùå": "#dc3545",
                "Ï§ëÍ∞Ñ": "#ffc107",
                "ÎÇÆÏùå": "#28a745",
            }.get(rec["priority"], "#6c757d")
            html += f"""
            <div class="recommendation">
                <h4 style="color: {priority_color};">üéØ {rec['title']} (Ïö∞ÏÑ†ÏàúÏúÑ: {rec['priority']})</h4>
                <p><strong>ÏÑ§Î™Ö:</strong> {rec['description']}</p>
                <p><strong>Í∂åÏû• Ï°∞Ïπò:</strong> {rec['action']}</p>
            </div>
            """

        return html

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL ÌååÏùºÏùÑ sql ÎîîÎ†âÌÜ†Î¶¨Î°ú Î≥µÏÇ¨"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ÏÜåÏä§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL ÌååÏùºÏù¥ ÏïÑÎãôÎãàÎã§: {source_path}"

            # ÎåÄÏÉÅ ÌååÏùºÎ™Ö Í≤∞Ï†ï
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # ÌååÏùº Î≥µÏÇ¨
            import shutil

            shutil.copy2(source, target_path)

            return f"‚úÖ SQL ÌååÏùºÏù¥ Î≥µÏÇ¨ÎêòÏóàÏäµÎãàÎã§: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL ÌååÏùº Î≥µÏÇ¨ Ïã§Ìå®: {str(e)}"

    # === Î∂ÑÏÑù Í¥ÄÎ†® Î©îÏÑúÎìú ===

    def setup_cloudwatch_client(self, region_name: str = "us-east-1"):
        """CloudWatch ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï"""
        try:
            self.cloudwatch = boto3.client("cloudwatch", region_name=region_name)
            return True
        except Exception as e:
            logger.error(f"CloudWatch ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            return False

    async def collect_db_metrics(
        self,
        db_instance_identifier: str,
        hours: int = 24,
        metrics: Optional[List[str]] = None,
        region: str = "us-east-1",
    ) -> str:
        """CloudWatchÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î©îÌä∏Î¶≠ ÏàòÏßë"""
        if not ANALYSIS_AVAILABLE:
            return "‚ùå Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. pip install pandas numpy scikit-learnÏùÑ Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî."

        try:
            if not self.setup_cloudwatch_client(region):
                return "CloudWatch ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ïÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."

            # ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌôïÏù∏ Î∞è Ïù∏Ïä§ÌÑ¥Ïä§ identifier Î≥ÄÌôò
            try:
                rds_client = boto3.client("rds", region_name=region)
                cluster_response = rds_client.describe_db_clusters(
                    DBClusterIdentifier=db_instance_identifier
                )
                if cluster_response["DBClusters"]:
                    cluster = cluster_response["DBClusters"][0]
                    if cluster["DBClusterMembers"]:
                        # Ï≤´ Î≤àÏß∏ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÇ¨Ïö© (Î≥¥ÌÜµ writer Ïù∏Ïä§ÌÑ¥Ïä§)
                        original_id = db_instance_identifier
                        db_instance_identifier = cluster["DBClusterMembers"][0][
                            "DBInstanceIdentifier"
                        ]
                        logger.info(
                            f"ÌÅ¥Îü¨Ïä§ÌÑ∞ {original_id}ÏóêÏÑú Ïù∏Ïä§ÌÑ¥Ïä§ {db_instance_identifier}Î°ú Î≥ÄÌôò"
                        )
            except rds_client.exceptions.DBClusterNotFoundFault:
                logger.debug(f"{db_instance_identifier}Îäî Ïù∏Ïä§ÌÑ¥Ïä§ IDÏûÖÎãàÎã§")
            except Exception as e:
                logger.debug(f"ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌôïÏù∏ Ïã§Ìå®: {str(e)}")

            # Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥ ÏàòÏßë
            try:
                instance_response = rds_client.describe_db_instances(
                    DBInstanceIdentifier=db_instance_identifier
                )
                if instance_response["DBInstances"]:
                    instance_class = instance_response["DBInstances"][0]["DBInstanceClass"]
                    # ÌòÑÏû¨ Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¥ÎûòÏä§Î•º Ï†ÄÏû•ÌïòÏó¨ ÎèôÏ†Å ÏûÑÍ≥ÑÍ∞íÏóê ÏÇ¨Ïö©
                    self.current_instance_class = instance_class
                    logger.info(f"Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¥ÎûòÏä§: {instance_class}")
            except Exception as e:
                logger.warning(f"Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {str(e)}")
                self.current_instance_class = 'r5.large'  # Í∏∞Î≥∏Í∞í

            if not metrics:
                metrics = self.default_metrics

            # ÏãúÍ∞Ñ Î≤îÏúÑ ÏÑ§Ï†ï
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=hours)

            # Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏
            data = []

            # Í∞Å Î©îÌä∏Î¶≠Ïóê ÎåÄÌï¥ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            for metric in metrics:
                try:
                    response = self.cloudwatch.get_metric_statistics(
                        Namespace="AWS/RDS",
                        MetricName=metric,
                        Dimensions=[
                            {
                                "Name": "DBInstanceIdentifier",
                                "Value": db_instance_identifier,
                            },
                        ],
                        StartTime=start_time,
                        EndTime=end_time,
                        Period=300,  # 5Î∂Ñ Í∞ÑÍ≤©
                        Statistics=["Average"],
                    )

                    # ÏùëÎãµÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
                    for point in response["Datapoints"]:
                        data.append(
                            {
                                "Timestamp": point["Timestamp"].replace(tzinfo=None),
                                "Metric": metric,
                                "Value": point["Average"],
                            }
                        )
                except Exception as e:
                    logger.error(f"Î©îÌä∏Î¶≠ {metric} ÏàòÏßë Ïã§Ìå®: {str(e)}")

            if not data:
                return "ÏàòÏßëÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."

            # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±
            df = pd.DataFrame(data)
            df = df.sort_values("Timestamp")

            # ÌîºÎ≤ó ÌÖåÏù¥Î∏î ÏÉùÏÑ±
            pivot_df = df.pivot(index="Timestamp", columns="Metric", values="Value")

            # CSV ÌååÏùºÎ°ú Ï†ÄÏû•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = (
                DATA_DIR / f"database_metrics_{db_instance_identifier}_{timestamp}.csv"
            )
            pivot_df.to_csv(csv_file)

            return f"‚úÖ Î©îÌä∏Î¶≠ ÏàòÏßë ÏôÑÎ£å\nüìä ÏàòÏßëÎêú Î©îÌä∏Î¶≠: {len(metrics)}Í∞ú\nüìà Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏: {len(data)}Í∞ú\nüíæ Ï†ÄÏû• ÏúÑÏπò: {csv_file}"

        except Exception as e:
            return f"Î©îÌä∏Î¶≠ ÏàòÏßë Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    async def analyze_metric_correlation(
        self, csv_file: str, target_metric: str = "CPUUtilization", top_n: int = 10
    ) -> str:
        """Î©îÌä∏Î¶≠ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù"""
        if not ANALYSIS_AVAILABLE:
            return "‚ùå Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."

        try:
            # CSV ÌååÏùº Í≤ΩÎ°ú Ï≤òÎ¶¨
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}"

            # Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            if target_metric not in df.columns:
                return f"ÌÉÄÍ≤ü Î©îÌä∏Î¶≠ '{target_metric}'Ïù¥ Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÏäµÎãàÎã§.\nÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÌä∏Î¶≠: {list(df.columns)}"

            # ÏÉÅÍ¥Ä Î∂ÑÏÑù
            correlation_matrix = df.corr()
            target_correlations = correlation_matrix[target_metric].abs()
            target_correlations = target_correlations.drop(
                target_metric, errors="ignore"
            )
            top_correlations = target_correlations.nlargest(top_n)

            # Í≤∞Í≥º Î¨∏ÏûêÏó¥ ÏÉùÏÑ±
            result = f"üìä {target_metric}Í≥º ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÎÜíÏùÄ ÏÉÅÏúÑ {top_n}Í∞ú Î©îÌä∏Î¶≠:\n\n"
            for metric, correlation in top_correlations.items():
                result += f"‚Ä¢ {metric}: {correlation:.4f}\n"

            # Í∑∏ÎûòÌîÑ ÏÉùÏÑ± Ï†úÍ±∞Îê® - ÌÖçÏä§Ìä∏ Í≤∞Í≥ºÎßå Î∞òÌôò
            return result

        except Exception as e:
            return f"ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    def load_metric_thresholds(self) -> dict:
        """input Ìè¥ÎçîÏóêÏÑú ÏµúÏã† ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï ÌååÏùº Î°úÎìú"""
        try:
            input_dir = Path(__file__).parent / "input"
            if not input_dir.exists():
                input_dir.mkdir(exist_ok=True)
            
            # metric_thresholds_*.txt ÌååÏùº Ï§ë ÏµúÏã† ÌååÏùº Ï∞æÍ∏∞
            threshold_files = list(input_dir.glob("metric_thresholds_*.txt"))
            if not threshold_files:
                return self.get_default_thresholds()
            
            latest_file = max(threshold_files, key=lambda f: f.stat().st_mtime)
            
            thresholds = {}
            current_metric = None
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    if line.startswith('[') and line.endswith(']'):
                        current_metric = line[1:-1]
                        thresholds[current_metric] = {}
                    elif '=' in line and current_metric:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        
                        # Ïà´Ïûê Î≥ÄÌôò
                        if key in ['min', 'max', 'high_threshold', 'low_threshold', 'spike_factor']:
                            try:
                                thresholds[current_metric][key] = float(value) if value != 'None' else None
                            except ValueError:
                                thresholds[current_metric][key] = None
                        else:
                            thresholds[current_metric][key] = value
            
            return thresholds
            
        except Exception as e:
            debug_log(f"ÏûÑÍ≥ÑÍ∞í ÌååÏùº Î°úÎìú Ïã§Ìå®: {e}")
            return self.get_default_thresholds()
    
    def get_default_thresholds(self) -> dict:
        """Í∏∞Î≥∏ ÏûÑÍ≥ÑÍ∞í Î∞òÌôò"""
        return {
            'CPUUtilization': {'min': 0, 'max': 100, 'high_threshold': 80, 'method': 'absolute'},
            'DatabaseConnections': {'min': 0, 'max': None, 'spike_factor': 3.0, 'method': 'spike'},
            'FreeableMemory': {'min': 0, 'max': None, 'low_threshold': 0.1, 'method': 'percentage'},
            'ReadLatency': {'min': 0, 'max': None, 'high_threshold': 0.01, 'method': 'absolute'},
            'WriteLatency': {'min': 0, 'max': None, 'high_threshold': 0.01, 'method': 'absolute'},
            'ReadIOPS': {'min': 0, 'max': None, 'spike_factor': 5.0, 'method': 'spike'},
            'WriteIOPS': {'min': 0, 'max': None, 'spike_factor': 5.0, 'method': 'spike'},
            'NetworkReceiveThroughput': {'min': 0, 'max': None, 'spike_factor': 3.0, 'method': 'spike'},
            'NetworkTransmitThroughput': {'min': 0, 'max': None, 'spike_factor': 3.0, 'method': 'spike'},
            'DBLoad': {'min': 0, 'max': None, 'high_threshold': 2.0, 'method': 'dynamic'},
            'DBLoadCPU': {'min': 0, 'max': None, 'high_threshold': 2.0, 'method': 'dynamic'},
            'DBLoadNonCPU': {'min': 0, 'max': None, 'high_threshold': 1.0, 'method': 'dynamic'},
            'BufferCacheHitRatio': {'min': 0, 'max': 100, 'low_threshold': 80, 'method': 'percentage'}
        }

    def get_dynamic_dbload_threshold(self, instance_class: str) -> float:
        """Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¥ÎûòÏä§Î≥Ñ DBLoad ÏûÑÍ≥ÑÍ∞í Î∞òÌôò"""
        # vCPU Ïàò Í∏∞Î∞ò ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
        vcpu_mapping = {
            # t3/t4g ÏãúÎ¶¨Ï¶à
            't3.micro': 2, 't3.small': 2, 't3.medium': 2, 't3.large': 2, 't3.xlarge': 4, 't3.2xlarge': 8,
            't4g.micro': 2, 't4g.small': 2, 't4g.medium': 2, 't4g.large': 2, 't4g.xlarge': 4, 't4g.2xlarge': 8,
            # r5/r6i ÏãúÎ¶¨Ï¶à
            'r5.large': 2, 'r5.xlarge': 4, 'r5.2xlarge': 8, 'r5.4xlarge': 16, 'r5.8xlarge': 32, 'r5.12xlarge': 48, 'r5.16xlarge': 64, 'r5.24xlarge': 96,
            'r6i.large': 2, 'r6i.xlarge': 4, 'r6i.2xlarge': 8, 'r6i.4xlarge': 16, 'r6i.8xlarge': 32, 'r6i.12xlarge': 48, 'r6i.16xlarge': 64, 'r6i.24xlarge': 96, 'r6i.32xlarge': 128,
            # m5/m6i ÏãúÎ¶¨Ï¶à
            'm5.large': 2, 'm5.xlarge': 4, 'm5.2xlarge': 8, 'm5.4xlarge': 16, 'm5.8xlarge': 32, 'm5.12xlarge': 48, 'm5.16xlarge': 64, 'm5.24xlarge': 96,
            'm6i.large': 2, 'm6i.xlarge': 4, 'm6i.2xlarge': 8, 'm6i.4xlarge': 16, 'm6i.8xlarge': 32, 'm6i.12xlarge': 48, 'm6i.16xlarge': 64, 'm6i.24xlarge': 96, 'm6i.32xlarge': 128,
        }
        
        vcpu_count = vcpu_mapping.get(instance_class, 2)  # Í∏∞Î≥∏Í∞í 2 vCPU
        # DBLoad ÏûÑÍ≥ÑÍ∞í = vCPU Ïàò * 0.8 (80% ÌôúÏö©Î•† Í∏∞Ï§Ä)
        return vcpu_count * 0.8

    async def detect_metric_outliers(
        self, csv_file: str, std_threshold: float = 3.0, skip_html_report: bool = False
    ) -> str:
        """Í∞úÏÑ†Îêú ÏïÑÏõÉÎùºÏù¥Ïñ¥ ÌÉêÏßÄ - Î©îÌä∏Î¶≠Î≥Ñ ÎßûÏ∂§ Í∏∞Ï§Ä Ï†ÅÏö©"""
        if not ANALYSIS_AVAILABLE:
            return "‚ùå Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."

        try:
            # CSV ÌååÏùº Í≤ΩÎ°ú Ï≤òÎ¶¨
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}"

            # Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)
            df = df.dropna()

            # ÏûÑÍ≥ÑÍ∞í ÌååÏùºÏóêÏÑú Î°úÎìú
            metric_thresholds = self.load_metric_thresholds()

            result = f"üîç Í∞úÏÑ†Îêú ÏïÑÏõÉÎùºÏù¥Ïñ¥ ÌÉêÏßÄ Í≤∞Í≥º:\n\n"
            outlier_summary = []
            critical_issues = []

            # Í∞Å Î©îÌä∏Î¶≠Ïóê ÎåÄÌï¥ ÎßûÏ∂§ ÏïÑÏõÉÎùºÏù¥Ïñ¥ ÌÉêÏßÄ
            for column in df.columns:
                series = df[column]
                config = metric_thresholds.get(column, {'method': 'iqr'})
                outliers = pd.Series(dtype=float)
                
                if config['method'] == 'dynamic':
                    # ÎèôÏ†Å ÏûÑÍ≥ÑÍ∞í Í∏∞Ï§Ä (DBLoad Îì±)
                    if column in ['DBLoad', 'DBLoadCPU', 'DBLoadNonCPU']:
                        instance_class = getattr(self, 'current_instance_class', 'r5.large')
                        dynamic_threshold = self.get_dynamic_dbload_threshold(instance_class)
                        outliers = series[series > dynamic_threshold]
                    else:
                        # Îã§Î•∏ Î©îÌä∏Î¶≠ÏùÄ Í∏∞Î≥∏ ÏûÑÍ≥ÑÍ∞í ÏÇ¨Ïö©
                        if 'high_threshold' in config:
                            outliers = series[series > config['high_threshold']]
                        
                elif config['method'] == 'absolute':
                    # Ï†àÎåÄÍ∞í Í∏∞Ï§Ä (CPU, Latency Îì±)
                    if 'high_threshold' in config:
                        outliers = series[series > config['high_threshold']]
                    if 'low_threshold' in config:
                        low_outliers = series[series < config['low_threshold']]
                        outliers = pd.concat([outliers, low_outliers])
                        
                elif config['method'] == 'spike':
                    # Í∏âÍ≤©Ìïú Î≥ÄÌôî ÌÉêÏßÄ (Connections, IOPS, Network Îì±)
                    median = series.median()
                    mad = (series - median).abs().median()
                    threshold = median + config.get('spike_factor', 3.0) * mad
                    outliers = series[series > threshold]
                    
                elif config['method'] == 'percentage':
                    # Î∞±Î∂ÑÏú® Í∏∞Ï§Ä (Memory, Cache Hit Ratio Îì±)
                    if 'low_threshold' in config:
                        outliers = series[series < config['low_threshold']]
                        
                else:
                    # IQR Î∞©Ïãù (Í∏∞Î≥∏Í∞í)
                    Q1 = series.quantile(0.25)
                    Q3 = series.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    outliers = series[(series < lower_bound) | (series > upper_bound)]

                # Î¨ºÎ¶¨Ï†Å Ï†úÏïΩ Ï†ÅÏö©
                if config.get('min') is not None:
                    outliers = outliers[outliers >= config['min']]
                if config.get('max') is not None:
                    outliers = outliers[outliers <= config['max']]

                if not outliers.empty:
                    severity = "üî•" if len(outliers) > len(series) * 0.1 else "‚ö†Ô∏è"
                    result += f"{severity} {column} Ïù¥ÏÉÅ ÌÉêÏßÄ ({len(outliers)}Í∞ú):\n"
                    
                    # Ïã¨Í∞ÅÎèÑ ÌåêÏ†ï
                    if column == 'CPUUtilization' and outliers.max() > 90:
                        critical_issues.append(f"CPU ÏÇ¨Ïö©Î•† ÏúÑÌóò ÏàòÏ§Ä: {outliers.max():.1f}%")
                    elif column in ['ReadLatency', 'WriteLatency'] and outliers.max() > 0.1:
                        critical_issues.append(f"{column} ÏßÄÏó∞ÏãúÍ∞Ñ Í∏âÏ¶ù: {outliers.max():.3f}Ï¥à")
                    elif column in ['DBLoad', 'DBLoadCPU', 'DBLoadNonCPU']:
                        # ÎèôÏ†Å ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò ÌåêÏ†ï
                        instance_class = getattr(self, 'current_instance_class', 'r5.large')
                        dynamic_threshold = self.get_dynamic_dbload_threshold(instance_class)
                        if outliers.max() > dynamic_threshold * 1.5:  # ÏûÑÍ≥ÑÍ∞íÏùò 150% Ï¥àÍ≥º Ïãú Ïã¨Í∞Å
                            critical_issues.append(f"{column} Î∂ÄÌïò Í≥ºÎã§ (Ïù∏Ïä§ÌÑ¥Ïä§: {instance_class}): {outliers.max():.1f} (ÏûÑÍ≥ÑÍ∞í: {dynamic_threshold:.1f})")

                    # ÏÉÅÏúÑ 3Í∞ú Ïù¥ÏÉÅÍ∞íÎßå ÌëúÏãú
                    top_outliers = outliers.nlargest(3)
                    for timestamp, value in top_outliers.items():
                        result += f"   ‚Ä¢ {timestamp}: {value:.2f}\n"
                    
                    if len(outliers) > 3:
                        result += f"   ... Î∞è {len(outliers) - 3}Í∞ú Îçî\n"
                    result += "\n"

                    outlier_summary.append({
                        "metric": column,
                        "count": len(outliers),
                        "max_value": outliers.max(),
                        "severity": "Critical" if len(outliers) > len(series) * 0.1 else "Warning"
                    })
                else:
                    result += f"‚úÖ {column}: Ï†ïÏÉÅ Î≤îÏúÑ\n"

            # Ïã¨Í∞ÅÌïú Î¨∏Ï†ú ÏöîÏïΩ
            if critical_issues:
                result += "\nüö® Ï¶âÏãú Ï°∞Ïπò ÌïÑÏöî:\n"
                for issue in critical_issues:
                    result += f"‚Ä¢ {issue}\n"

            # Ï†ÑÏ≤¥ ÏöîÏïΩ
            if outlier_summary:
                result += "\nüìä ÌÉêÏßÄ ÏöîÏïΩ:\n"
                critical_count = sum(1 for s in outlier_summary if s['severity'] == 'Critical')
                warning_count = len(outlier_summary) - critical_count
                result += f"‚Ä¢ Ïã¨Í∞Å: {critical_count}Í∞ú Î©îÌä∏Î¶≠\n"
                result += f"‚Ä¢ Í≤ΩÍ≥†: {warning_count}Í∞ú Î©îÌä∏Î¶≠\n"
            else:
                result += "\n‚úÖ Î™®Îì† Î©îÌä∏Î¶≠Ïù¥ Ï†ïÏÉÅ Î≤îÏúÑ ÎÇ¥Ïóê ÏûàÏäµÎãàÎã§.\n"

            # ÏûÑÍ≥ÑÍ∞í Ï†ïÎ≥¥ HTML ÏÉùÏÑ±
            threshold_html = self.generate_threshold_html(metric_thresholds)
            
            # HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± (ÏÑ†ÌÉùÏ†Å)
            debug_log(f"skip_html_report: {skip_html_report}")
            if not skip_html_report:
                debug_log("HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï§ë...")
                html_report_path = OUTPUT_DIR / f"outlier_analysis_{csv_file.replace('.csv', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                self.save_outlier_html_report(result, threshold_html, html_report_path)
                result += f"\nüìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú: {self.format_file_link(str(html_report_path), 'ÏïÑÏõÉÎùºÏù¥Ïñ¥ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú Ïó¥Í∏∞')}\n"
                result += "üí° Î≥¥Í≥†ÏÑúÏóêÏÑú 'ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï' Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠ÌïòÏó¨ ÏÉÅÏÑ∏ ÏÑ§Ï†ïÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\n"
            else:
                debug_log("HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Í±¥ÎÑàÎúÄ")

            return result

        except Exception as e:
            return f"ÏïÑÏõÉÎùºÏù¥Ïñ¥ ÌÉêÏßÄ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    def generate_threshold_html(self, thresholds: dict) -> str:
        """ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ïÏùÑ HTML ÌÖåÏù¥Î∏îÎ°ú ÏÉùÏÑ±"""
        html = """
        <div id="thresholdModal" class="modal">
            <div class="modal-content">
                <span class="close">&times;</span>
                <h2>üìä Î©îÌä∏Î¶≠ ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï</h2>
                <table class="threshold-table">
                    <thead>
                        <tr>
                            <th>Î©îÌä∏Î¶≠</th>
                            <th>ÌÉêÏßÄ Î∞©Ïãù</th>
                            <th>ÏûÑÍ≥ÑÍ∞í</th>
                            <th>ÏÑ§Î™Ö</th>
                        </tr>
                    </thead>
                    <tbody>
        """
        
        for metric, config in thresholds.items():
            method = config.get('method', 'iqr')
            threshold_info = []
            
            if method == 'absolute':
                if config.get('high_threshold'):
                    threshold_info.append(f"ÏÉÅÌïú: {config['high_threshold']}")
                if config.get('low_threshold'):
                    threshold_info.append(f"ÌïòÌïú: {config['low_threshold']}")
            elif method == 'spike':
                threshold_info.append(f"Í∏âÏ¶ù Î∞∞Ïàò: {config.get('spike_factor', 3.0)}")
            elif method == 'percentage':
                if config.get('low_threshold'):
                    threshold_info.append(f"ÏµúÏÜå: {config['low_threshold']}%")
            
            threshold_str = ", ".join(threshold_info) if threshold_info else "IQR Î∞©Ïãù"
            description = config.get('description', f"{metric} Î©îÌä∏Î¶≠")
            
            html += f"""
                        <tr>
                            <td>{metric}</td>
                            <td>{method}</td>
                            <td>{threshold_str}</td>
                            <td>{description}</td>
                        </tr>
            """
        
        html += """
                    </tbody>
                </table>
                <p><strong>üìÅ ÏÑ§Ï†ï ÌååÏùº:</strong> input/metric_thresholds_*.txt</p>
                <p><strong>üí° ÏàòÏ†ï Î∞©Î≤ï:</strong> input Ìè¥ÎçîÏùò ÏµúÏã† ÏûÑÍ≥ÑÍ∞í ÌååÏùºÏùÑ Ìé∏ÏßëÌïòÏÑ∏Ïöî.</p>
            </div>
        </div>
        """
        return html

    def save_outlier_html_report(self, result: str, threshold_html: str, report_path: Path):
        """ÏïÑÏõÉÎùºÏù¥Ïñ¥ Î∂ÑÏÑù HTML Î≥¥Í≥†ÏÑú Ï†ÄÏû•"""
        html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÏïÑÏõÉÎùºÏù¥Ïñ¥ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; }}
        .btn {{ background-color: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 10px; }}
        .btn:hover {{ background-color: #0056b3; }}
        .result-content {{ white-space: pre-wrap; font-family: monospace; background: #f8f9fa; padding: 20px; border-radius: 5px; }}
        .modal {{ display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.4); }}
        .modal-content {{ background-color: #fefefe; margin: 5% auto; padding: 20px; border: none; border-radius: 10px; width: 80%; max-width: 800px; }}
        .close {{ color: #aaa; float: right; font-size: 28px; font-weight: bold; cursor: pointer; }}
        .close:hover {{ color: black; }}
        .threshold-table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
        .threshold-table th, .threshold-table td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        .threshold-table th {{ background-color: #f2f2f2; font-weight: bold; }}
        .threshold-table tr:nth-child(even) {{ background-color: #f9f9f9; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üîç ÏïÑÏõÉÎùºÏù¥Ïñ¥ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú</h1>
            <p>ÏÉùÏÑ± ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <button class="btn" onclick="document.getElementById('thresholdModal').style.display='block'">
                üìä ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï Î≥¥Í∏∞
            </button>
        </div>
        
        <div class="result-content">{result}</div>
        
        {threshold_html}
    </div>

    <script>
        // Î™®Îã¨ Ï∞Ω Ï†úÏñ¥
        var modal = document.getElementById('thresholdModal');
        var span = document.getElementsByClassName('close')[0];
        
        span.onclick = function() {{
            modal.style.display = 'none';
        }}
        
        window.onclick = function(event) {{
            if (event.target == modal) {{
                modal.style.display = 'none';
            }}
        }}
    </script>
</body>
</html>
        """
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
        except Exception as e:
            debug_log(f"HTML Î≥¥Í≥†ÏÑú Ï†ÄÏû• Ïã§Ìå®: {e}")

    async def perform_regression_analysis(
        self,
        csv_file: str,
        predictor_metric: str,
        target_metric: str = "CPUUtilization",
    ) -> str:
        """ÌöåÍ∑Ä Î∂ÑÏÑù ÏàòÌñâ"""
        if not ANALYSIS_AVAILABLE:
            return "‚ùå Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."

        try:
            # CSV ÌååÏùº Í≤ΩÎ°ú Ï≤òÎ¶¨
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}"

            # Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            # ÌïÑÏöîÌïú Î©îÌä∏Î¶≠ ÌôïÏù∏
            if predictor_metric not in df.columns or target_metric not in df.columns:
                return f"ÌïÑÏöîÌïú Î©îÌä∏Î¶≠Ïù¥ Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÏäµÎãàÎã§.\nÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î©îÌä∏Î¶≠: {list(df.columns)}"

            # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
            X = df[predictor_metric].values.reshape(-1, 1)
            y = df[target_metric].values

            # NaN Í∞í Ï≤òÎ¶¨
            imputer = SimpleImputer(strategy="mean")
            X = imputer.fit_transform(X)
            y = imputer.fit_transform(y.reshape(-1, 1)).ravel()

            # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Îã§Ìï≠ ÌöåÍ∑Ä Î™®Îç∏ ÏÉùÏÑ± (2Ï∞®)
            poly_features = PolynomialFeatures(degree=2, include_bias=False)
            X_poly_train = poly_features.fit_transform(X_train)
            X_poly_test = poly_features.transform(X_test)

            # Î™®Îç∏ ÌïôÏäµ
            model = LinearRegression()
            model.fit(X_poly_train, y_train)

            # ÏòàÏ∏°
            y_pred = model.predict(X_poly_test)

            # Î™®Îç∏ ÌèâÍ∞Ä
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # Í≥ÑÏàò Ï∂úÎ†•
            coefficients = model.coef_
            intercept = model.intercept_

            result = f"üìà ÌöåÍ∑Ä Î∂ÑÏÑù Í≤∞Í≥º ({predictor_metric} ‚Üí {target_metric}):\n\n"
            result += f"üìä Î™®Îç∏ ÏÑ±Îä•:\n"
            result += f"‚Ä¢ Mean Squared Error: {mse:.4f}\n"
            result += f"‚Ä¢ R-squared Score: {r2:.4f}\n\n"

            result += f"üî¢ Îã§Ìï≠ ÌöåÍ∑Ä Î™®Îç∏ (2Ï∞®):\n"
            result += f"y = {coefficients[1]:.4f}x¬≤ + {coefficients[0]:.4f}x + {intercept:.4f}\n\n"

            # Ìï¥ÏÑù Ï∂îÍ∞Ä
            result += "Ìï¥ÏÑù:\n"
            if r2 > 0.7:
                result += f"‚Ä¢ Î™®Îç∏ ÏÑ§Î™ÖÎ†•: {r2*100:.1f}% (ÎÜíÏùÄ ÏòàÏ∏° Ï†ïÌôïÎèÑ)\n"
            elif r2 > 0.5:
                result += f"‚Ä¢ Î™®Îç∏ ÏÑ§Î™ÖÎ†•: {r2*100:.1f}% (Ï§ëÍ∞Ñ ÏòàÏ∏° Ï†ïÌôïÎèÑ)\n"
            else:
                result += f"‚Ä¢ Î™®Îç∏ ÏÑ§Î™ÖÎ†•: {r2*100:.1f}% (ÎÇÆÏùÄ ÏòàÏ∏° Ï†ïÌôïÎèÑ)\n"

            # Í∑∏ÎûòÌîÑ ÏÉùÏÑ± Ï†úÍ±∞Îê® - ÌÖçÏä§Ìä∏ Í≤∞Í≥ºÎßå Î∞òÌôò
            return result

        except Exception as e:
            return f"ÌöåÍ∑Ä Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    async def list_data_files(self) -> str:
        """Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
        try:
            csv_files = list(DATA_DIR.glob("*.csv"))
            if not csv_files:
                return "data ÎîîÎ†âÌÜ†Î¶¨Ïóê CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§."

            result = "üìÅ Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î™©Î°ù:\n\n"
            for file in csv_files:
                file_size = file.stat().st_size
                modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                result += f"‚Ä¢ {file.name}\n"
                result += f"  ÌÅ¨Í∏∞: {file_size:,} bytes\n"
                result += f"  ÏàòÏ†ïÏùº: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            return result
        except Exception as e:
            return f"Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}"

    async def get_metric_summary(self, csv_file: str) -> str:
        """Î©îÌä∏Î¶≠ ÏöîÏïΩ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        if not ANALYSIS_AVAILABLE:
            return "‚ùå Î∂ÑÏÑù ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."

        try:
            # CSV ÌååÏùº Í≤ΩÎ°ú Ï≤òÎ¶¨
            if not csv_file.startswith("/"):
                csv_path = DATA_DIR / csv_file
            else:
                csv_path = Path(csv_file)

            if not csv_path.exists():
                return f"CSV ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {csv_path}"

            # Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞
            df = pd.read_csv(csv_path, index_col="Timestamp", parse_dates=True)

            result = f"üìä Î©îÌä∏Î¶≠ ÏöîÏïΩ Ï†ïÎ≥¥ ({csv_file}):\n\n"
            result += f"üìÖ Îç∞Ïù¥ÌÑ∞ Í∏∞Í∞Ñ: {df.index.min()} ~ {df.index.max()}\n"
            result += f"üìà Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏: {len(df)}Í∞ú\n"
            result += f"üìã Î©îÌä∏Î¶≠ Ïàò: {len(df.columns)}Í∞ú\n\n"

            result += "üìä Î©îÌä∏Î¶≠ Î™©Î°ù:\n"
            for i, column in enumerate(df.columns, 1):
                non_null_count = df[column].count()
                result += f"{i:2d}. {column} ({non_null_count}Í∞ú Îç∞Ïù¥ÌÑ∞)\n"

            # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
            result += f"\nüìà Í∏∞Î≥∏ ÌÜµÍ≥Ñ:\n"
            stats = df.describe()
            result += stats.to_string()

            return result

        except Exception as e:
            return f"Î©îÌä∏Î¶≠ ÏöîÏïΩ Ï†ïÎ≥¥ Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    def validate_column_type_compatibility(
        self, existing_column: Dict[str, Any], new_definition: str, debug_log
    ) -> Dict[str, Any]:
        """Ïª¨Îüº Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"""
        debug_log(
            f"ÌÉÄÏûÖ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù ÏãúÏûë: Í∏∞Ï°¥={existing_column['data_type']}, ÏÉàÎ°úÏö¥={new_definition}"
        )

        issues = []

        # ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌååÏã±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        debug_log(f"ÌååÏã±Îêú ÏÉà ÌÉÄÏûÖ: {new_type_info}")

        # Ìò∏ÌôòÎêòÏßÄ ÏïäÎäî ÌÉÄÏûÖ Î≥ÄÍ≤Ω Í≤ÄÏÇ¨
        incompatible_changes = [
            # Î¨∏ÏûêÏó¥ -> Ïà´Ïûê
            (
                ["VARCHAR", "CHAR", "TEXT", "LONGTEXT", "MEDIUMTEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # Ïà´Ïûê -> Î¨∏ÏûêÏó¥ (Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Í∞ÄÎä•)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌÉÄÏûÖ Î≥ÄÍ≤Ω
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏùÑ {existing_type}ÏóêÏÑú {new_type_info['type']}Î°ú Î≥ÄÍ≤ΩÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )
                debug_log(f"Ìò∏ÌôòÏÑ± Î¨∏Ï†ú: {existing_type} -> {new_type_info['type']}")

        # Í∏∏Ïù¥ Ï∂ïÏÜå Í≤ÄÏÇ¨
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"Ïª¨Îüº Í∏∏Ïù¥Î•º {existing_length}ÏóêÏÑú {new_length}Î°ú Ï∂ïÏÜåÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )
                debug_log(f"Í∏∏Ïù¥ Ï∂ïÏÜå Î¨∏Ï†ú: {existing_length} -> {new_length}")

        # Ï†ïÎ∞ÄÎèÑ Ï∂ïÏÜå Í≤ÄÏÇ¨ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL Ï†ïÎ∞ÄÎèÑÎ•º ({existing_precision},{existing_scale})ÏóêÏÑú ({new_precision},{new_scale})Î°ú Ï∂ïÏÜåÌïòÎäî Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ ÏïºÍ∏∞Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                )
                debug_log(
                    f"Ï†ïÎ∞ÄÎèÑ Ï∂ïÏÜå Î¨∏Ï†ú: ({existing_precision},{existing_scale}) -> ({new_precision},{new_scale})"
                )

        result = {"compatible": len(issues) == 0, "issues": issues}

        debug_log(
            f"ÌÉÄÏûÖ Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù ÏôÑÎ£å: compatible={result['compatible']}, issues={len(issues)}"
        )
        return result

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î¨∏ÏûêÏó¥ÏùÑ ÌååÏã±ÌïòÏó¨ ÌÉÄÏûÖÍ≥º Í∏∏Ïù¥ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) Îì±ÏùÑ ÌååÏã±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) ÌòïÌÉú
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) ÌòïÌÉú
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def debug_cloudwatch_collection(self, database_secret: str, start_time: str, end_time: str) -> str:
        """CloudWatch ÏàòÏßë ÎîîÎ≤ÑÍ∑∏ Ìï®Ïàò"""
        try:
            # ÏãúÍ∞Ñ Î≥ÄÌôò (KST -> UTC)
            start_dt = self.convert_kst_to_utc(start_time)
            end_dt = self.convert_kst_to_utc(end_time)
            
            # ÏãúÌÅ¨Î¶øÏóêÏÑú DB Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            secrets_client = boto3.client('secretsmanager', region_name='ap-northeast-2')
            secret_response = secrets_client.get_secret_value(SecretId=database_secret)
            secret_data = json.loads(secret_response['SecretString'])
            
            # DB ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏãùÎ≥ÑÏûê Ï∂îÏ∂ú
            db_host = secret_data.get('host', '')
            if '.cluster-' in db_host:
                cluster_identifier = db_host.split('.cluster-')[0]
            else:
                return "‚ùå Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"
            
            # CloudWatch ÏàòÏßë ÏãúÎèÑ
            logs_client = boto3.client('logs', region_name='ap-northeast-2')
            log_group_name = f"/aws/rds/cluster/{cluster_identifier}/slowquery"
            
            start_time_ms = int(start_dt.timestamp() * 1000)
            end_time_ms = int(end_dt.timestamp() * 1000)
            
            print(f"DEBUG: ÌÅ¥Îü¨Ïä§ÌÑ∞ ID: {cluster_identifier}")
            print(f"DEBUG: Î°úÍ∑∏ Í∑∏Î£π: {log_group_name}")
            print(f"DEBUG: ÏãúÍ∞Ñ Î≤îÏúÑ: {start_dt} ~ {end_dt} (UTC)")
            print(f"DEBUG: ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ: {start_time_ms} ~ {end_time_ms}")
            
            response = logs_client.filter_log_events(
                logGroupName=log_group_name,
                startTime=start_time_ms,
                endTime=end_time_ms
            )
            
            events_count = len(response.get('events', []))
            print(f"DEBUG: Í≤ÄÏÉâÎêú Ïù¥Î≤§Ìä∏ Ïàò: {events_count}")
            
            if events_count > 0:
                # Ï≤´ Î≤àÏß∏ Ïù¥Î≤§Ìä∏ ÌôïÏù∏
                first_event = response['events'][0]
                print(f"DEBUG: Ï≤´ Î≤àÏß∏ Ïù¥Î≤§Ìä∏ ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ: {first_event['timestamp']}")
                print(f"DEBUG: Ï≤´ Î≤àÏß∏ Ïù¥Î≤§Ìä∏ Î©îÏãúÏßÄ ÎØ∏Î¶¨Î≥¥Í∏∞: {first_event['message'][:100]}...")
                
                # ÌååÏã± ÌÖåÏä§Ìä∏
                message = first_event['message'].replace('\\n', '\n')
                print(f"DEBUG: Query_time Ìå®ÌÑ¥ Ï°¥Ïû¨? {'# Query_time: ' in message}")
                
                if '# Query_time: ' in message:
                    lines = message.split('\n')
                    print(f"DEBUG: Î∂ÑÌï†Îêú ÎùºÏù∏ Ïàò: {len(lines)}")
                    for i, line in enumerate(lines[:5]):  # Ï≤òÏùå 5Ï§ÑÎßå
                        print(f"DEBUG: Line {i}: {repr(line)}")
            
            return f"‚úÖ ÎîîÎ≤ÑÍ∑∏ ÏôÑÎ£å: {events_count}Í∞ú Ïù¥Î≤§Ìä∏ Î∞úÍ≤¨"
            
        except Exception as e:
            import traceback
            return f"‚ùå ÎîîÎ≤ÑÍ∑∏ Ïã§Ìå®: {str(e)}\n{traceback.format_exc()}"

    async def collect_slow_queries(
        self, database_secret: str, start_time: str = None, end_time: str = None
    ) -> str:
        """Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë (CloudWatch ‚Üí Î°úÏª¨ÌååÏùº ‚Üí Performance Schema ÏàúÏÑú)"""
        try:
            # ÏãúÍ∞ÑÎåÄ Ï≤òÎ¶¨ (KST -> UTC)
            if start_time and end_time:
                start_dt = self.convert_kst_to_utc(start_time)
                end_dt = self.convert_kst_to_utc(end_time)
            else:
                # Í∏∞Î≥∏Í∞í: 24ÏãúÍ∞Ñ Ï†ÑÎ∂ÄÌÑ∞ ÌòÑÏû¨ÍπåÏßÄ (UTC Í∏∞Ï§Ä)
                end_dt = datetime.utcnow()
                start_dt = end_dt - timedelta(hours=24)

            # ÏãúÌÅ¨Î¶øÏóêÏÑú DB Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
            secrets_client = boto3.client('secretsmanager', region_name=self.default_region)
            secret_response = secrets_client.get_secret_value(SecretId=database_secret)
            secret_data = json.loads(secret_response['SecretString'])
            
            # DB ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏãùÎ≥ÑÏûê Ï∂îÏ∂ú
            db_host = secret_data.get('host', '')
            if '.cluster-' in db_host:
                cluster_identifier = db_host.split('.cluster-')[0]
            else:
                return "‚ùå Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"

            # 1Îã®Í≥Ñ: CloudWatch LogsÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë ÏãúÎèÑ
            cloudwatch_result = await self._collect_from_cloudwatch(
                cluster_identifier, start_dt, end_dt
            )
            
            if cloudwatch_result['success']:
                return cloudwatch_result['message']
            
            # 2Îã®Í≥Ñ: Î°úÏª¨ ÌååÏùºÏóêÏÑú ÏàòÏßë ÏãúÎèÑ
            local_file_result = await self._collect_from_local_file(
                database_secret, start_dt, end_dt
            )
            
            if local_file_result['success']:
                return local_file_result['message']
            
            # 3Îã®Í≥Ñ: Performance SchemaÏóêÏÑú ÏàòÏßë ÏãúÎèÑ
            performance_result = await self._collect_from_performance_schema(
                database_secret, start_dt, end_dt
            )
            
            if performance_result['success']:
                return performance_result['message']
            
            # 4Îã®Í≥Ñ: Log exports ÏÑ§Ï†ï Ï†úÏïà
            return await self._suggest_log_exports_setup(cluster_identifier, cloudwatch_result['message'])
            
        except ValueError as ve:
            return f"‚ùå {str(ve)}"
        except Exception as e:
            import traceback
            return f"‚ùå Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë Ïã§Ìå®: {str(e)}\n{traceback.format_exc()}"

    async def _collect_from_local_file(self, database_secret: str, start_dt: datetime, end_dt: datetime) -> dict:
        """Î°úÏª¨ Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÌååÏùºÏóêÏÑú ÏàòÏßë"""
        try:
            # Í∏∞Ï°¥ Ïó∞Í≤∞ Ï†ïÎ¶¨
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
            if not self.setup_shared_connection(database_secret, None, True):
                return {'success': False, 'message': "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"}

            cursor = self.shared_cursor
            
            # Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°ú ÌôïÏù∏
            cursor.execute("SHOW VARIABLES LIKE 'slow_query_log_file'")
            result = cursor.fetchone()
            if not result:
                return {'success': False, 'message': "Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°úÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå"}
            
            log_file_path = result[1]
            
            # ÌååÏùº ÎÇ¥Ïö© ÏùΩÍ∏∞ ÏãúÎèÑ (LOAD_FILE Ìï®Ïàò ÏÇ¨Ïö©)
            try:
                cursor.execute(f"SELECT LOAD_FILE('{log_file_path}')")
                file_content = cursor.fetchone()
                
                if not file_content or not file_content[0]:
                    return {'success': False, 'message': f"Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ ÌååÏùºÏùÑ ÏùΩÏùÑ Ïàò ÏóÜÏùå: {log_file_path}"}
                
                content = file_content[0].decode('utf-8') if isinstance(file_content[0], bytes) else str(file_content[0])
                
                # ÏãúÍ∞Ñ Î≤îÏúÑ ÌïÑÌÑ∞ÎßÅÏùÑ ÏúÑÌïú Î°úÍ∑∏ ÌååÏã±
                slow_queries = self._parse_slow_query_log(content, start_dt, end_dt)
                
                if slow_queries:
                    # ÌååÏùº ÏÉùÏÑ±
                    current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"slow_queries_local_file_{current_date}.sql"
                    file_path = SQL_DIR / filename
                    
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(f"-- Î°úÏª¨ Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÌååÏùº ÏàòÏßë Í≤∞Í≥º\n")
                        f.write(f"-- ÌååÏùº Í≤ΩÎ°ú: {log_file_path}\n")
                        f.write(f"-- ÏàòÏßë Í∏∞Í∞Ñ: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (KST)\n")
                        f.write(f"-- Ï¥ù {len(slow_queries)}Í∞úÏùò ÏøºÎ¶¨\n\n")
                        
                        for i, query in enumerate(slow_queries, 1):
                            f.write(f"-- Ïä¨Î°úÏö∞ ÏøºÎ¶¨ #{i}\n")
                            if 'time' in query:
                                f.write(f"-- {query['time']}\n")
                            if 'query_time' in query:
                                f.write(f"-- {query['query_time']}\n")
                            if 'user_host' in query:
                                f.write(f"-- {query['user_host']}\n")
                            f.write(f"{query['sql']};\n\n")
                    
                    return {
                        'success': True,
                        'message': f"‚úÖ Î°úÏª¨ ÌååÏùºÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨ {len(slow_queries)}Í∞ú ÏàòÏßë ÏôÑÎ£å: {filename}\nÌååÏùº Í≤ΩÎ°ú: {log_file_path}"
                    }
                else:
                    return {
                        'success': False,
                        'message': f"Î°úÏª¨ ÌååÏùºÏóêÏÑú Ìï¥Îãπ ÏãúÍ∞Ñ Î≤îÏúÑÏùò Ïä¨Î°úÏö∞ ÏøºÎ¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå"
                    }
                    
            except Exception as e:
                return {
                    'success': False,
                    'message': f"Î°úÏª¨ ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {str(e)}"
                }
                
        except Exception as e:
            return {
                'success': False,
                'message': f"Î°úÏª¨ ÌååÏùº ÏàòÏßë Ïã§Ìå®: {str(e)}"
            }

    def _parse_slow_query_log(self, content: str, start_dt: datetime, end_dt: datetime) -> list:
        """Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ ÎÇ¥Ïö© ÌååÏã±"""
        slow_queries = []
        lines = content.split('\n')
        current_query = {}
        sql_lines = []
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('# Time:'):
                # Ïù¥Ï†Ñ ÏøºÎ¶¨ Ï†ÄÏû•
                if current_query and sql_lines:
                    current_query['sql'] = ' '.join(sql_lines)
                    
                    # ÏãúÍ∞Ñ Î≤îÏúÑ Ï≤¥ÌÅ¨
                    if self._is_within_time_range(current_query.get('time', ''), start_dt, end_dt):
                        slow_queries.append(current_query.copy())
                
                # ÏÉà ÏøºÎ¶¨ ÏãúÏûë
                current_query = {'time': line}
                sql_lines = []
                
            elif line.startswith('# Query_time:'):
                current_query['query_time'] = line
            elif line.startswith('# User@Host:'):
                current_query['user_host'] = line
            elif not line.startswith('#') and line and not line.startswith('SET timestamp'):
                if not line.startswith('use '):
                    sql_lines.append(line)
        
        # ÎßàÏßÄÎßâ ÏøºÎ¶¨ Ï≤òÎ¶¨
        if current_query and sql_lines:
            current_query['sql'] = ' '.join(sql_lines)
            if self._is_within_time_range(current_query.get('time', ''), start_dt, end_dt):
                slow_queries.append(current_query)
        
        return slow_queries

    def _is_within_time_range(self, time_str: str, start_dt: datetime, end_dt: datetime) -> bool:
        """ÏãúÍ∞Ñ Î¨∏ÏûêÏó¥Ïù¥ Î≤îÏúÑ ÎÇ¥Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏"""
        try:
            # # Time: 2025-09-05T14:30:45.123456Z ÌòïÏãù ÌååÏã±
            if 'Time:' in time_str:
                time_part = time_str.split('Time:')[1].strip()
                # ZÎ•º Ï†úÍ±∞ÌïòÍ≥† ÎßàÏù¥ÌÅ¨Î°úÏ¥à Î∂ÄÎ∂Ñ Ï≤òÎ¶¨
                if 'Z' in time_part:
                    time_part = time_part.replace('Z', '')
                if '.' in time_part:
                    time_part = time_part.split('.')[0]
                
                query_time = datetime.strptime(time_part, '%Y-%m-%dT%H:%M:%S')
                return start_dt <= query_time <= end_dt
        except:
            pass
        return True  # ÌååÏã± Ïã§Ìå® Ïãú Ìè¨Ìï®

    async def _collect_from_cloudwatch(self, cluster_identifier: str, start_dt: datetime, end_dt: datetime) -> dict:
        """CloudWatch LogsÏóêÏÑú Ïù∏Ïä§ÌÑ¥Ïä§Î≥ÑÎ°ú Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë"""
        try:
            logs_client = boto3.client('logs', region_name=self.default_region)
            log_group_name = f"/aws/rds/cluster/{cluster_identifier}/slowquery"
            
            start_time_ms = int(start_dt.timestamp() * 1000)
            end_time_ms = int(end_dt.timestamp() * 1000)
            
            # Î®ºÏ†Ä Î°úÍ∑∏ Ïä§Ìä∏Î¶º Î™©Î°ù Ï°∞Ìöå
            streams_response = logs_client.describe_log_streams(
                logGroupName=log_group_name,
                orderBy='LastEventTime',
                descending=True
            )
            
            log_streams = streams_response.get('logStreams', [])
            if not log_streams:
                return {
                    'success': False,
                    'message': f"Î°úÍ∑∏ Ïä§Ìä∏Î¶ºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå: {log_group_name}"
                }
            
            instance_files = []
            total_queries = 0
            
            # Í∞Å Î°úÍ∑∏ Ïä§Ìä∏Î¶º(Ïù∏Ïä§ÌÑ¥Ïä§)Î≥ÑÎ°ú Ï≤òÎ¶¨
            for stream in log_streams:
                stream_name = stream['logStreamName']
                
                # Ïù∏Ïä§ÌÑ¥Ïä§ ID Ï∂îÏ∂ú (Ïòà: cluster-instance-1)
                instance_id = stream_name.split('/')[-1] if '/' in stream_name else stream_name
                
                try:
                    # Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ Ïù¥Î≤§Ìä∏ Ï°∞Ìöå
                    response = logs_client.filter_log_events(
                        logGroupName=log_group_name,
                        logStreamNames=[stream_name],
                        startTime=start_time_ms,
                        endTime=end_time_ms
                    )
                    
                    events = response.get('events', [])
                    if not events:
                        continue
                    
                    # Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÌååÏã±
                    slow_queries = []
                    for event in events:
                        message = event['message'].replace('\\n', '\n')
                        
                        if '# Query_time: ' in message:
                            lines = message.split('\n')
                            query_info = {'instance_id': instance_id}
                            sql_lines = []
                            
                            for line in lines:
                                if line.startswith('# Query_time:'):
                                    query_info['query_time'] = line
                                elif line.startswith('# User@Host:'):
                                    query_info['user_host'] = line
                                elif line.startswith('# Time:'):
                                    query_info['time'] = line
                                elif not line.startswith('#') and line.strip() and not line.startswith('SET timestamp'):
                                    if not line.startswith('use '):
                                        sql_lines.append(line.strip())
                            
                            if sql_lines:
                                query_info['sql'] = ' '.join(sql_lines)
                                slow_queries.append(query_info)
                    
                    if slow_queries:
                        # Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÌååÏùº ÏÉùÏÑ±
                        current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"slow_queries_{instance_id}_{current_date}.sql"
                        file_path = SQL_DIR / filename
                        
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(f"-- CloudWatch Logs Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë Í≤∞Í≥º (Ïù∏Ïä§ÌÑ¥Ïä§: {instance_id})\n")
                            f.write(f"-- ÏàòÏßë Í∏∞Í∞Ñ: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (Local)\n")
                            f.write(f"-- Î°úÍ∑∏ Ïä§Ìä∏Î¶º: {stream_name}\n")
                            f.write(f"-- Ï¥ù {len(slow_queries)}Í∞úÏùò ÏøºÎ¶¨\n\n")
                            
                            for i, query in enumerate(slow_queries, 1):
                                f.write(f"-- Ïä¨Î°úÏö∞ ÏøºÎ¶¨ #{i} (Ïù∏Ïä§ÌÑ¥Ïä§: {instance_id})\n")
                                if 'time' in query:
                                    f.write(f"-- {query['time']}\n")
                                if 'query_time' in query:
                                    f.write(f"-- {query['query_time']}\n")
                                if 'user_host' in query:
                                    f.write(f"-- {query['user_host']}\n")
                                f.write(f"{query['sql']};\n\n")
                        
                        instance_files.append(f"{instance_id}: {self.format_file_link(str(file_path), filename)} ({len(slow_queries)}Í∞ú)")
                        total_queries += len(slow_queries)
                
                except Exception as e:
                    print(f"Ïù∏Ïä§ÌÑ¥Ïä§ {instance_id} Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                    continue
            
            if instance_files:
                return {
                    'success': True,
                    'message': f"‚úÖ CloudWatchÏóêÏÑú Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë ÏôÑÎ£å (Ï¥ù {total_queries}Í∞ú)\n" + 
                              "\n".join([f"‚Ä¢ {file_info}" for file_info in instance_files])
                }
            else:
                return {
                    'success': False,
                    'message': f"CloudWatch LogsÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå (Ïä§Ìä∏Î¶º Ïàò: {len(log_streams)})"
                }
                
        except Exception as e:
            return {
                'success': False,
                'message': f"CloudWatch Logs Ï°∞Ìöå Ïã§Ìå®: {str(e)}"
            }

    async def _collect_from_performance_schema(self, database_secret: str, start_dt: datetime, end_dt: datetime) -> dict:
        """Performance SchemaÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë"""
        try:
            # Í∏∞Ï°¥ Ïó∞Í≤∞ Ï†ïÎ¶¨
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
            if not self.setup_shared_connection(database_secret, None, True):
                return {'success': False, 'message': "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"}

            cursor = self.shared_cursor
            
            # Performance SchemaÏóêÏÑú ÎäêÎ¶∞ ÏøºÎ¶¨ Ï°∞Ìöå
            query = """
            SELECT 
                DIGEST_TEXT as sql_text,
                COUNT_STAR as exec_count,
                AVG_TIMER_WAIT/1000000000000 as avg_time_sec,
                MAX_TIMER_WAIT/1000000000000 as max_time_sec,
                SUM_TIMER_WAIT/1000000000000 as total_time_sec,
                FIRST_SEEN,
                LAST_SEEN
            FROM performance_schema.events_statements_summary_by_digest 
            WHERE AVG_TIMER_WAIT/1000000000000 > 1.0
            AND LAST_SEEN >= %s
            ORDER BY AVG_TIMER_WAIT DESC 
            LIMIT 50
            """
            
            cursor.execute(query, (start_dt,))
            results = cursor.fetchall()
            
            if results:
                # ÌååÏùº ÏÉùÏÑ±
                current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"slow_queries_performance_schema_{current_date}.sql"
                file_path = SQL_DIR / filename
                
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- Performance Schema Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë Í≤∞Í≥º\n")
                    f.write(f"-- ÏàòÏßë Í∏∞Í∞Ñ: {self.convert_utc(start_dt)} ~ {self.convert_utc(end_dt)} (Local)\n")
                    f.write(f"-- Ï¥ù {len(results)}Í∞úÏùò ÏøºÎ¶¨\n\n")
                    
                    for i, row in enumerate(results, 1):
                        sql_text, exec_count, avg_time, max_time, total_time, first_seen, last_seen = row
                        f.write(f"-- Ïä¨Î°úÏö∞ ÏøºÎ¶¨ #{i}\n")
                        f.write(f"-- Ïã§ÌñâÌöüÏàò: {exec_count}, ÌèâÍ∑†ÏãúÍ∞Ñ: {avg_time:.3f}Ï¥à, ÏµúÎåÄÏãúÍ∞Ñ: {max_time:.3f}Ï¥à\n")
                        f.write(f"-- Ï¥ù ÏãúÍ∞Ñ: {total_time:.3f}Ï¥à, ÎßàÏßÄÎßâ Ïã§Ìñâ: {last_seen}\n")
                        f.write(f"{sql_text};\n\n")
                
                return {
                    'success': True,
                    'message': f"‚úÖ Performance SchemaÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨ {len(results)}Í∞ú ÏàòÏßë ÏôÑÎ£å: {filename}\nÍ≤ÄÏÉâ Í∏∞Í∞Ñ: {start_dt} ~ {end_dt} (UTC)"
                }
            else:
                return {
                    'success': False,
                    'message': f"Performance SchemaÏóêÏÑúÎèÑ Ïä¨Î°úÏö∞ ÏøºÎ¶¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå"
                }
                
        except Exception as e:
            return {
                'success': False,
                'message': f"Performance Schema Ï°∞Ìöå Ïã§Ìå®: {str(e)}"
            }

    async def _suggest_log_exports_setup(self, cluster_identifier: str, cloudwatch_error: str) -> str:
        """Log exports ÏÑ§Ï†ï Ï†úÏïà Î∞è ÏûêÎèô ÏÑ§Ï†ï"""
        try:
            # RDS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú ÌòÑÏû¨ ÏÑ§Ï†ï ÌôïÏù∏
            rds_client = boto3.client('rds', region_name='ap-northeast-2')
            
            try:
                response = rds_client.describe_db_clusters(
                    DBClusterIdentifier=cluster_identifier
                )
                cluster = response['DBClusters'][0]
                enabled_logs = cluster.get('EnabledCloudwatchLogsExports', [])
                
                result_msg = f"üîç **Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë Í≤∞Í≥º**\n\n"
                result_msg += f"‚ùå CloudWatch Logs: {cloudwatch_error}\n"
                result_msg += f"‚ùå Performance Schema: Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏóÜÏùå\n\n"
                result_msg += f"üìä **ÌòÑÏû¨ Log Exports ÏÑ§Ï†ï**\n"
                result_msg += f"ÌÅ¥Îü¨Ïä§ÌÑ∞: {cluster_identifier}\n"
                result_msg += f"ÌôúÏÑ±ÌôîÎêú Î°úÍ∑∏: {enabled_logs if enabled_logs else 'ÏóÜÏùå'}\n\n"
                
                if 'slowquery' not in enabled_logs:
                    result_msg += f"üí° **Ìï¥Í≤∞ Î∞©Ïïà**\n"
                    result_msg += f"Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ÏóêÏÑú SlowQuery Î°úÍ∑∏Î•º CloudWatchÎ°ú Ï†ÑÏÜ°ÌïòÎèÑÎ°ù ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.\n\n"
                    result_msg += f"**ÏûêÎèô ÏÑ§Ï†ïÏùÑ ÏßÑÌñâÌïòÏãúÍ≤†ÏäµÎãàÍπå?**\n"
                    result_msg += f"Îã§Ïùå Î™ÖÎ†πÏù¥ Ïã§ÌñâÎê©ÎãàÎã§:\n"
                    result_msg += f"```\n"
                    result_msg += f"aws rds modify-db-cluster \\\n"
                    result_msg += f"  --db-cluster-identifier {cluster_identifier} \\\n"
                    result_msg += f"  --cloudwatch-logs-configuration 'EnableLogTypes=slowquery'\n"
                    result_msg += f"```\n\n"
                    result_msg += f"ÏÑ§Ï†ï ÌõÑ ÏïΩ 5-10Î∂Ñ ÌõÑÎ∂ÄÌÑ∞ CloudWatch LogsÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n"
                    result_msg += f"**ÏÑ§Ï†ïÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/n)**"
                    
                    # ÏÇ¨Ïö©Ïûê ÏûÖÎ†• ÎåÄÍ∏∞Îäî MCPÏóêÏÑú ÏßÄÏõêÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú, Î≥ÑÎèÑ Ìï®ÏàòÎ°ú Î∂ÑÎ¶¨
                    return result_msg
                else:
                    result_msg += f"‚úÖ SlowQuery Î°úÍ∑∏ Ï†ÑÏÜ°Ïù¥ Ïù¥ÎØ∏ ÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n"
                    result_msg += f"Î°úÍ∑∏Í∞Ä ÎÇòÌÉÄÎÇòÏßÄ ÏïäÎäî Ïù¥Ïú†:\n"
                    result_msg += f"1. Ïã§Ï†úÎ°ú 1Ï¥à Ïù¥ÏÉÅ Ïã§ÌñâÎêòÎäî ÏøºÎ¶¨Í∞Ä ÏóÜÏùå\n"
                    result_msg += f"2. Î°úÍ∑∏ Ï†ÑÏÜ°Ïóê ÏßÄÏó∞Ïù¥ ÏûàÏùå (ÏµúÎåÄ 10Î∂Ñ)\n"
                    result_msg += f"3. Î°úÍ∑∏ Î≥¥Ï°¥ Ï†ïÏ±ÖÏúºÎ°ú Ïù∏Ìïú ÏÇ≠Ï†ú\n"
                    return result_msg
                    
            except Exception as e:
                return f"‚ùå ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}"
                
        except Exception as e:
            return f"‚ùå Log exports ÏÑ§Ï†ï ÌôïÏù∏ Ïã§Ìå®: {str(e)}"

    async def enable_slow_query_log_exports(self, cluster_identifier: str) -> str:
        """Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞Ïùò SlowQuery Î°úÍ∑∏ CloudWatch Ï†ÑÏÜ° ÌôúÏÑ±Ìôî"""
        try:
            rds_client = boto3.client('rds', region_name='ap-northeast-2')
            
            response = rds_client.modify_db_cluster(
                DBClusterIdentifier=cluster_identifier,
                CloudwatchLogsConfiguration={
                    'EnableLogTypes': ['slowquery']
                },
                ApplyImmediately=True
            )
            
            return f"‚úÖ SlowQuery Î°úÍ∑∏ CloudWatch Ï†ÑÏÜ°Ïù¥ ÌôúÏÑ±ÌôîÎêòÏóàÏäµÎãàÎã§.\n" \
                   f"ÌÅ¥Îü¨Ïä§ÌÑ∞: {cluster_identifier}\n" \
                   f"ÏÉÅÌÉú: {response['DBCluster']['Status']}\n" \
                   f"ÏïΩ 5-10Î∂Ñ ÌõÑÎ∂ÄÌÑ∞ CloudWatch LogsÏóêÏÑú Ïä¨Î°úÏö∞ ÏøºÎ¶¨Î•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§."
                   
        except Exception as e:
            return f"‚ùå SlowQuery Î°úÍ∑∏ ÌôúÏÑ±Ìôî Ïã§Ìå®: {str(e)}"

    async def collect_memory_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None,
        start_time: str = None, end_time: str = None
    ) -> str:
        """Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Î∞è SQL ÌååÏùº ÏÉùÏÑ±"""
        try:
            # Í∏∞Ï°¥ Ïó∞Í≤∞Ïù¥ ÏûàÏúºÎ©¥ Ï†ïÎ¶¨
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§Î°ú ÏÉàÎ°ú Ïó∞Í≤∞
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"

            cursor = self.shared_cursor

            # ÌòÑÏû¨ ÎÇ†ÏßúÏôÄ Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ°ú ÌååÏùºÎ™Ö ÏÉùÏÑ±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = (
                f"_{db_instance_identifier}" if db_instance_identifier else ""
            )
            filename = f"memory_intensive_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # Ï§ëÎ≥µ Ï†úÍ±∞Ïö©

            # ÏãúÍ∞Ñ ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÉùÏÑ±
            time_filter = ""
            if start_time and end_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}' AND LAST_SEEN <= '{end_time}'"
            elif start_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}'"
            elif end_time:
                time_filter = f"AND LAST_SEEN <= '{end_time}'"

            # 1. performance_schemaÏóêÏÑú Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë ÏãúÎèÑ
            try:
                query_sql = f"""
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND MAX_MEMORY_USED > 100*1024*1024
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                        {time_filter}
                    ORDER BY MAX_MEMORY_USED DESC 
                    LIMIT 10
                """
                cursor.execute(query_sql)

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINÏúºÎ°ú ÏãúÏûëÌïòÎäî ÏøºÎ¶¨ Ï†úÏô∏
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema Í¥ÄÎ†® ÏøºÎ¶¨ Ï†úÏô∏
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema Î©îÎ™®Î¶¨ ÏøºÎ¶¨ Ï†ëÍ∑º Ïã§Ìå®: {e}")

            # ÏøºÎ¶¨Í∞Ä ÏûàÏùÑ ÎïåÎßå ÌååÏùº ÏÉùÏÑ±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(
                        f"-- Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ Î™®Ïùå (ÏàòÏßëÏùºÏãú: {datetime.now()})\n"
                    )
                    f.write(f"-- Ï¥ù {len(collected_queries)}Í∞úÏùò ÏøºÎ¶¨\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"‚úÖ Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ {len(collected_queries)}Í∞ú ÏàòÏßë ÏôÑÎ£å: {filename}"
            else:
                return (
                    f"‚úÖ Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨Í∞Ä Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§ (ÌååÏùº ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏùå)"
                )

        except Exception as e:
            return f"‚ùå Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Ïã§Ìå®: {str(e)}"

    async def collect_cpu_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None, 
        start_time: str = None, end_time: str = None
    ) -> str:
        """CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Î∞è SQL ÌååÏùº ÏÉùÏÑ±"""
        try:
            # Í∏∞Ï°¥ Ïó∞Í≤∞Ïù¥ ÏûàÏúºÎ©¥ Ï†ïÎ¶¨
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§Î°ú ÏÉàÎ°ú Ïó∞Í≤∞
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"

            cursor = self.shared_cursor

            # ÌòÑÏû¨ ÎÇ†ÏßúÏôÄ Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ°ú ÌååÏùºÎ™Ö ÏÉùÏÑ±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = (
                f"_{db_instance_identifier}" if db_instance_identifier else ""
            )
            filename = f"cpu_intensive_queries{instance_suffix}_{current_date}.sql"
            file_path = SQL_DIR / filename

            collected_queries = set()  # Ï§ëÎ≥µ Ï†úÍ±∞Ïö©

            # ÏãúÍ∞Ñ ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÉùÏÑ±
            time_filter = ""
            if start_time and end_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}' AND LAST_SEEN <= '{end_time}'"
            elif start_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}'"
            elif end_time:
                time_filter = f"AND LAST_SEEN <= '{end_time}'"

            # 1. performance_schemaÏóêÏÑú CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë ÏãúÎèÑ
            try:
                query_sql = f"""
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND SUM_TIMER_WAIT > 10000000000000
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                        {time_filter}
                    ORDER BY SUM_TIMER_WAIT DESC 
                    LIMIT 10
                """
                cursor.execute(query_sql)

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINÏúºÎ°ú ÏãúÏûëÌïòÎäî ÏøºÎ¶¨ Ï†úÏô∏
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema Í¥ÄÎ†® ÏøºÎ¶¨ Ï†úÏô∏
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema CPU ÏøºÎ¶¨ Ï†ëÍ∑º Ïã§Ìå®: {e}")

            # 2. information_schemaÏóêÏÑú ÌòÑÏû¨ CPU ÏÇ¨Ïö© Ï§ëÏù∏ ÏøºÎ¶¨ ÏàòÏßë
            try:
                cursor.execute(
                    """
                    SELECT INFO 
                    FROM information_schema.PROCESSLIST 
                    WHERE COMMAND = 'Query' 
                        AND STATE IN ('Sending data', 'Sorting result', 'Creating sort index', 'Copying to tmp table')
                        AND INFO IS NOT NULL
                        AND INFO NOT LIKE '%PROCESSLIST%'
                        AND INFO NOT LIKE 'EXPLAIN%'
                    ORDER BY TIME DESC
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINÏúºÎ°ú ÏãúÏûëÌïòÎäî ÏøºÎ¶¨ Ï†úÏô∏
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema Í¥ÄÎ†® ÏøºÎ¶¨ Ï†úÏô∏
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"PROCESSLIST CPU ÏøºÎ¶¨ Ï†ëÍ∑º Ïã§Ìå®: {e}")

            # ÏøºÎ¶¨Í∞Ä ÏûàÏùÑ ÎïåÎßå ÌååÏùº ÏÉùÏÑ±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"-- CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ Î™®Ïùå (ÏàòÏßëÏùºÏãú: {datetime.now()})\n")
                    f.write(f"-- Ï¥ù {len(collected_queries)}Í∞úÏùò ÏøºÎ¶¨\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"‚úÖ CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ {len(collected_queries)}Í∞ú ÏàòÏßë ÏôÑÎ£å: {self.format_file_link(str(file_path), filename)}"
            else:
                return f"‚úÖ CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨Í∞Ä Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§ (ÌååÏùº ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏùå)"

        except Exception as e:
            return f"‚ùå CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Ïã§Ìå®: {str(e)}"

    async def collect_temp_space_intensive_queries(
        self, database_secret: str, db_instance_identifier: str = None,
        start_time: str = None, end_time: str = None
    ) -> str:
        """ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Î∞è SQL ÌååÏùº ÏÉùÏÑ±"""
        try:
            # Í∏∞Ï°¥ Ïó∞Í≤∞Ïù¥ ÏûàÏúºÎ©¥ Ï†ïÎ¶¨
            if self.shared_connection or self.shared_cursor:
                self.cleanup_shared_connection()

            # ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§Î°ú ÏÉàÎ°ú Ïó∞Í≤∞
            if not self.setup_shared_connection(
                database_secret, None, True, db_instance_identifier
            ):
                return "‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®"

            cursor = self.shared_cursor

            # ÌòÑÏû¨ ÎÇ†ÏßúÏôÄ Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ°ú ÌååÏùºÎ™Ö ÏÉùÏÑ±
            current_date = datetime.now().strftime("%Y%m%d")
            instance_suffix = (
                f"_{db_instance_identifier}" if db_instance_identifier else ""
            )
            filename = (
                f"temp_space_intensive_queries{instance_suffix}_{current_date}.sql"
            )
            file_path = SQL_DIR / filename

            collected_queries = set()  # Ï§ëÎ≥µ Ï†úÍ±∞Ïö©

            # ÏãúÍ∞Ñ ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÉùÏÑ±
            time_filter = ""
            if start_time and end_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}' AND LAST_SEEN <= '{end_time}'"
            elif start_time:
                time_filter = f"AND FIRST_SEEN >= '{start_time}'"
            elif end_time:
                time_filter = f"AND LAST_SEEN <= '{end_time}'"

            # 1. performance_schemaÏóêÏÑú ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë ÏãúÎèÑ
            try:
                query_sql = f"""
                    SELECT QUERY_SAMPLE_TEXT 
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL 
                        AND (SUM_CREATED_TMP_TABLES > 0 OR SUM_CREATED_TMP_DISK_TABLES > 0 OR SUM_SORT_MERGE_PASSES > 0)
                        AND DIGEST_TEXT NOT LIKE '%performance_schema%'
                        AND DIGEST_TEXT NOT LIKE '%information_schema%'
                        AND DIGEST_TEXT NOT LIKE 'EXPLAIN%'
                        {time_filter}
                    ORDER BY (SUM_CREATED_TMP_DISK_TABLES + SUM_SORT_MERGE_PASSES) DESC 
                    LIMIT 10
                """
                cursor.execute(query_sql)

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINÏúºÎ°ú ÏãúÏûëÌïòÎäî ÏøºÎ¶¨ Ï†úÏô∏
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema Í¥ÄÎ†® ÏøºÎ¶¨ Ï†úÏô∏
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"performance_schema ÏûÑÏãúÍ≥µÍ∞Ñ ÏøºÎ¶¨ Ï†ëÍ∑º Ïã§Ìå®: {e}")

            # 2. information_schemaÏóêÏÑú ÌòÑÏû¨ ÏûÑÏãú ÌÖåÏù¥Î∏î ÏÇ¨Ïö© Ï§ëÏù∏ ÏøºÎ¶¨ ÏàòÏßë
            try:
                cursor.execute(
                    """
                    SELECT INFO 
                    FROM information_schema.PROCESSLIST 
                    WHERE COMMAND = 'Query' 
                        AND STATE IN ('Copying to tmp table', 'Sorting for group', 'Sorting for order', 'Creating sort index')
                        AND INFO IS NOT NULL
                        AND INFO NOT LIKE '%PROCESSLIST%'
                        AND INFO NOT LIKE 'EXPLAIN%'
                    ORDER BY TIME DESC
                    LIMIT 10
                """
                )

                for (query,) in cursor.fetchall():
                    if query and query.strip():
                        query_clean = query.strip()
                        # EXPLAINÏúºÎ°ú ÏãúÏûëÌïòÎäî ÏøºÎ¶¨ Ï†úÏô∏
                        if not query_clean.upper().startswith("EXPLAIN"):
                            # performance_schema, information_schema Í¥ÄÎ†® ÏøºÎ¶¨ Ï†úÏô∏
                            if (
                                "performance_schema" not in query_clean.lower()
                                and "information_schema" not in query_clean.lower()
                            ):
                                collected_queries.add(query_clean)

            except Exception as e:
                print(f"PROCESSLIST ÏûÑÏãúÍ≥µÍ∞Ñ ÏøºÎ¶¨ Ï†ëÍ∑º Ïã§Ìå®: {e}")

            # ÏøºÎ¶¨Í∞Ä ÏûàÏùÑ ÎïåÎßå ÌååÏùº ÏÉùÏÑ±
            if collected_queries:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(
                        f"-- ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ Î™®Ïùå (ÏàòÏßëÏùºÏãú: {datetime.now()})\n"
                    )
                    f.write(f"-- Ï¥ù {len(collected_queries)}Í∞úÏùò ÏøºÎ¶¨\n\n")

                    for i, query in enumerate(collected_queries, 1):
                        f.write(f"-- ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ #{i}\n")
                        f.write(f"{query};\n\n")

                return f"‚úÖ ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ {len(collected_queries)}Í∞ú ÏàòÏßë ÏôÑÎ£å: {self.format_file_link(str(file_path), filename)}"
            else:
                return f"‚úÖ ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨Í∞Ä Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§ (ÌååÏùº ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏùå)"

        except Exception as e:
            return f"‚ùå ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ ÏàòÏßë Ïã§Ìå®: {str(e)}"

    async def analyze_aurora_mysql_error_logs(
        self, keyword: str, start_datetime_str: str, end_datetime_str: str
    ) -> str:
        """Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù"""
        try:
            # ÏãúÍ∞Ñ Î≥ÄÌôò (KST -> UTC)
            start_time_utc = self.convert_kst_to_utc(start_datetime_str)
            end_time_utc = self.convert_kst_to_utc(end_datetime_str)

            logger.info(f"ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù ÏãúÏûë: {start_time_utc} ~ {end_time_utc} (UTC)")

            # AWS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
            s3_client = boto3.client("s3", region_name=self.default_region)
            rds_client = boto3.client("rds", region_name=self.default_region)

            # ÌÇ§ÏõåÎìúÎ°ú ÏãúÌÅ¨Î¶ø Î¶¨Ïä§Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞
            secret_lists = await self._get_secrets_by_keyword(keyword)
            if not secret_lists:
                return f"‚ùå '{keyword}' ÌÇ§ÏõåÎìúÎ°ú Ï∞æÏùÄ ÏãúÌÅ¨Î¶øÏù¥ ÏóÜÏäµÎãàÎã§."

            results = []
            s3_bucket_name = "your-s3-bucket-name"  # Ïã§Ï†ú S3 Î≤ÑÌÇ∑Î™ÖÏúºÎ°ú Î≥ÄÍ≤Ω ÌïÑÏöî

            for secret_name in secret_lists:
                try:
                    # DB Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê Í∞ÄÏ†∏Ïò§Í∏∞
                    response = rds_client.describe_db_instances(
                        Filters=[{"Name": "db-cluster-id", "Values": [secret_name]}]
                    )
                    instances = [
                        instance["DBInstanceIdentifier"]
                        for instance in response["DBInstances"]
                    ]

                    for instance in instances:
                        log_content = []
                        output_file = f"error_log_{instance}_{start_time_utc.strftime('%Y%m%d%H%M')}_to_{end_time_utc.strftime('%Y%m%d%H%M')}.log"

                        # ÏóêÎü¨ Î°úÍ∑∏ ÌååÏùº Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
                        log_file_list = rds_client.describe_db_log_files(
                            DBInstanceIdentifier=instance, FilenameContains="error"
                        )

                        for log_file_info in log_file_list["DescribeDBLogFiles"]:
                            log_filename = log_file_info["LogFileName"]
                            last_written = datetime.fromtimestamp(
                                log_file_info["LastWritten"] / 1000
                            )

                            if start_time_utc <= last_written <= end_time_utc:
                                # Î°úÍ∑∏ ÌååÏùº ÎÇ¥Ïö© Îã§Ïö¥Î°úÎìú
                                response = rds_client.download_db_log_file_portion(
                                    DBInstanceIdentifier=instance,
                                    LogFileName=log_filename,
                                    Marker="0",
                                )

                                log_data = response.get("LogFileData", "")
                                lines = log_data.splitlines()

                                # Ï§ëÏöîÌïú ÏóêÎü¨ Î°úÍ∑∏ Ìï≠Î™© ÌïÑÌÑ∞ÎßÅ
                                error_keywords = [
                                    "error",
                                    "warning",
                                    "critical",
                                    "failed",
                                    "crash",
                                    "exception",
                                    "fatal",
                                    "corruption",
                                ]

                                for line in lines:
                                    if any(kw in line.lower() for kw in error_keywords):
                                        log_content.append(line)

                        # Î°úÍ∑∏ ÎÇ¥Ïö©Ïù¥ ÏûàÏúºÎ©¥ Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
                        if log_content:
                            # Ï†ÅÏ†àÌïú ÌÅ¨Í∏∞Î°ú Î∂ÑÌï† (ÏµúÎåÄ 5000Ïûê)
                            content_chunks = self._split_log_content(log_content, 5000)
                            for i, chunk in enumerate(content_chunks):
                                chunk_header = (
                                    f"<{instance}_chunk_{i+1}>"
                                    if len(content_chunks) > 1
                                    else f"<{instance}>"
                                )
                                chunk_footer = (
                                    f"</{instance}_chunk_{i+1}>"
                                    if len(content_chunks) > 1
                                    else f"</{instance}>"
                                )
                                results.append(
                                    f"{chunk_header}\n{chunk}\n{chunk_footer}"
                                )
                        else:
                            results.append(
                                f"<{instance}>\nÌï¥Îãπ Í∏∞Í∞ÑÏóê ÏóêÎü¨ Î°úÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§.\n</{instance}>"
                            )

                except Exception as e:
                    logger.error(f"Ïù∏Ïä§ÌÑ¥Ïä§ {secret_name} Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                    results.append(
                        f"<{secret_name}>\nÎ°úÍ∑∏ ÏàòÏßë Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}\n</{secret_name}>"
                    )

            if not results:
                return "‚ùå Î∂ÑÏÑùÌï† ÏóêÎü¨ Î°úÍ∑∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            # ClaudeÎ•º ÌÜµÌïú ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù
            analysis_result = await self._analyze_error_logs_with_claude(results)

            # Í≤∞Í≥º Ï†ÄÏû•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path("output") / f"error_log_analysis_{timestamp}.html"

            # HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
            html_report = await self._generate_error_log_html_report(
                results,
                analysis_result,
                keyword,
                start_datetime_str,
                end_datetime_str,
                output_path,
            )

            return f"""‚úÖ Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù ÏôÑÎ£å

üìä Î∂ÑÏÑù ÏöîÏïΩ:
‚Ä¢ Î∂ÑÏÑù Í∏∞Í∞Ñ: {start_datetime_str} ~ {end_datetime_str}
‚Ä¢ ÎåÄÏÉÅ ÌÇ§ÏõåÎìú: {keyword}
‚Ä¢ Î∂ÑÏÑùÎêú Ïù∏Ïä§ÌÑ¥Ïä§: {len(secret_lists)}Í∞ú
‚Ä¢ ÏàòÏßëÎêú Î°úÍ∑∏ Ï≤≠ÌÅ¨: {len(results)}Í∞ú

ü§ñ Claude AI Î∂ÑÏÑù Í≤∞Í≥º:
{analysis_result}

üìÑ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú: {html_report}
"""

        except Exception as e:
            logger.error(f"ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù Ï§ë Ïò§Î•ò: {e}")
            return f"‚ùå ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù Ïã§Ìå®: {str(e)}"

    def _split_log_content(self, log_lines: List[str], max_chars: int) -> List[str]:
        """Î°úÍ∑∏ ÎÇ¥Ïö©ÏùÑ Ï†ÅÏ†àÌïú ÌÅ¨Í∏∞Î°ú Î∂ÑÌï†"""
        chunks = []
        current_chunk = []
        current_size = 0

        for line in log_lines:
            line_size = len(line) + 1  # +1 for newline

            if current_size + line_size > max_chars and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size

        if current_chunk:
            chunks.append("\n".join(current_chunk))

        return chunks

    async def _get_secrets_by_keyword(self, keyword: str) -> List[str]:
        """ÌÇ§ÏõåÎìúÎ°ú ÏãúÌÅ¨Î¶ø Î™©Î°ù Ï°∞Ìöå"""
        try:
            secrets_client = boto3.client(
                "secretsmanager", region_name="ap-northeast-2"
            )
            response = secrets_client.list_secrets()

            matching_secrets = []
            for secret in response.get("SecretList", []):
                secret_name = secret["Name"]
                if keyword.lower() in secret_name.lower():
                    matching_secrets.append(secret_name)

            return matching_secrets

        except Exception as e:
            logger.error(f"ÏãúÌÅ¨Î¶ø Î™©Î°ù Ï°∞Ìöå Ï§ë Ïò§Î•ò: {e}")
            return []

    async def _analyze_error_logs_with_claude(self, log_results: List[str]) -> str:
        """ClaudeÎ•º ÌÜµÌïú ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù"""
        try:
            # Î°úÍ∑∏ ÎÇ¥Ïö© Í≤∞Ìï©
            combined_logs = "\n".join(log_results)

            prompt = f"""ÏïÑÎûòÎäî Aurora MySQL 3.5 Ïù∏Ïä§ÌÑ¥Ïä§Ïùò ÏóêÎü¨ Î°úÍ∑∏ÏûÖÎãàÎã§. Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Ïóê ÎåÄÌïú ÏóêÎü¨Î°úÍ∑∏Î•º Î∂ÑÏÑùÌïòÍ≥† Îã§Ïùå ÏÇ¨Ìï≠Ïóê ÎåÄÌïú ÏöîÏïΩÏùÑ Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:

<instanceÎ™Ö>Í≥º </instanceÎ™Ö> ÏÇ¨Ïù¥Ïóê ÏûàÎäî Î°úÍ∑∏Îäî Ìï¥Îãπ Ïù∏Ïä§ÌÑ¥Ïä§Ïùò error logÏûÖÎãàÎã§.

Ïñ¥Îñ§ ÌÇ§ÏõåÎìúÏùò ÏóêÎü¨Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇòÌÉÄÎÇ¨ÎäîÏßÄ, ÏóêÎü¨Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú ÏßëÍ≥ÑÎèÑ Î∂ÄÌÉÅÌï©ÎãàÎã§.
ÏïÑÎûòÏôÄ Í∞ôÏùÄ Ìè¨Îß∑ÏúºÎ°ú Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§Î≥ÑÎ°ú ÏóêÎü¨ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú ÏßëÍ≥ÑÌïòÍ≥†, Î∂ÑÏÑùÌïú ÎÇ¥Ïö©ÏùÑ ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî.

ÏòàÎ•º Îì§Ïñ¥ aborted connectionÏù¥ Î™áÍ±¥ÏûàÏóàÍ≥†, Í∑∏Í≤ÉÏùÄ Ïñ¥Îñ§ ÏòÅÌñ•ÏùÑ Í∞ÄÏßÄÎäîÏßÄ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
Î∂ÑÏÑùÌï†Îïå Ïñ¥Îäê Ïù∏Ïä§ÌÑ¥Ïä§Ïóê ÏûàÎäî Ïñ¥Îñ§ ÎÇ¥Ïö©ÏùÑ Í∑ºÍ±∞Î°ú ÌñàÎäîÏßÄ Î™ÖÌôïÌïòÍ≤å ÌïòÍ≥†, Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Î™®Î•¥Í≤†Îã§Í≥† Ìï©ÎãàÎã§.

**Î∂ÑÏÑù Í≤∞Í≥º:**

1. **Ï†ÑÏ≤¥ ÏöîÏïΩ**
   - Ï¥ù ÏóêÎü¨ Í±¥Ïàò: XÍ±¥
   - Ïã¨Í∞ÅÎèÑÎ≥Ñ Î∂ÑÎ•ò: ÎÜíÏùå/Ï§ëÍ∞Ñ/ÎÇÆÏùå
   - Ï£ºÏöî ÏóêÎü¨ Ìå®ÌÑ¥: 

2. **Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÏÉÅÏÑ∏ Î∂ÑÏÑù**

3. **Í∂åÏû• Ï°∞ÏπòÏÇ¨Ìï≠**

<context>
Ïã¨Í∞ÅÌïú ÏóêÎü¨ ÌÇ§ÏõåÎìú:
1. "Fatal error" - ÏòÅÌñ•ÎèÑ: Îß§Ïö∞ ÎÜíÏùå (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑúÎ≤Ñ Ï§ëÏßÄ/Ïû¨ÏãúÏûë Í∞ÄÎä•)
2. "Out of memory" - ÏòÅÌñ•ÎèÑ: ÎÜíÏùå (Î©îÎ™®Î¶¨ Î∂ÄÏ°±ÏúºÎ°ú ÏÑ±Îä• Ï†ÄÌïò/ÏøºÎ¶¨ Ïã§Ìå®)
3. "Disk full" - ÏòÅÌñ•ÎèÑ: ÎÜíÏùå (ÎîîÏä§ÌÅ¨ Í≥µÍ∞Ñ Î∂ÄÏ°±ÏúºÎ°ú Ïì∞Í∏∞ ÏûëÏóÖ Ïã§Ìå®)
4. "Connection refused" - ÏòÅÌñ•ÎèÑ: Ï§ëÍ∞Ñ~ÎÜíÏùå (ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞ Î¨∏Ï†ú)
5. "InnoDB: Corruption" - ÏòÅÌñ•ÎèÑ: ÎÜíÏùå (Îç∞Ïù¥ÌÑ∞ Î¨¥Í≤∞ÏÑ± Î¨∏Ï†ú, Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Í∞ÄÎä•)

Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïú ÏóêÎü¨ ÌÇ§ÏõåÎìú:
6. "Slow query" - ÏòÅÌñ•ÎèÑ: Ï§ëÍ∞Ñ (ÏÑ±Îä• Ï†ÄÌïò)
7. "Lock wait timeout exceeded" - ÏòÅÌñ•ÎèÑ: Ï§ëÍ∞Ñ (ÎèôÏãúÏÑ± Î¨∏Ï†ú)
8. "Warning" - ÏòÅÌñ•ÎèÑ: ÎÇÆÏùå~Ï§ëÍ∞Ñ (Ïû†Ïû¨Ï†Å Î¨∏Ï†ú)
9. "Table is full" - ÏòÅÌñ•ÎèÑ: Ï§ëÍ∞Ñ (ÌÖåÏù¥Î∏î Ïö©Îüâ Ï¥àÍ≥º)
10. "Deadlock found" - ÏòÅÌñ•ÎèÑ: Ï§ëÍ∞Ñ (Ìä∏ÎûúÏû≠ÏÖò Ï∂©Îèå)
</context>

{combined_logs}
"""

            claude_input = json.dumps(
                {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4096,
                    "messages": [
                        {"role": "user", "content": [{"type": "text", "text": prompt}]}
                    ],
                    "temperature": 0.3,
                }
            )

            sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
            sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            # Claude Sonnet 4 Ìò∏Ï∂ú ÏãúÎèÑ
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_4_model_id, body=claude_input
                )
                response_body = json.loads(response.get("body").read())
                claude_response = response_body.get("content", [{}])[0].get("text", "")
                logger.info("Claude Sonnet 4Î°ú ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù ÏôÑÎ£å")
                return claude_response

            except Exception as e:
                logger.warning(f"Claude Sonnet 4 Ìò∏Ï∂ú Ïã§Ìå®, fallback ÏãúÎèÑ: {e}")

                # Claude 3.7 Sonnet Ìò∏Ï∂ú (fallback)
                try:
                    response = self.bedrock_client.invoke_model(
                        modelId=sonnet_3_7_model_id, body=claude_input
                    )
                    response_body = json.loads(response.get("body").read())
                    claude_response = response_body.get("content", [{}])[0].get(
                        "text", ""
                    )
                    logger.info("Claude 3.7 SonnetÏúºÎ°ú ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù ÏôÑÎ£å")
                    return claude_response

                except Exception as e2:
                    logger.error(f"Claude Ìò∏Ï∂ú ÏôÑÏ†Ñ Ïã§Ìå®: {e2}")
                    return f"Claude Î∂ÑÏÑù Ïã§Ìå®: {str(e2)}"

        except Exception as e:
            logger.error(f"ÏóêÎü¨ Î°úÍ∑∏ Claude Î∂ÑÏÑù Ï§ë Ïò§Î•ò: {e}")
            return f"Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

    async def _generate_error_log_html_report(
        self,
        log_results: List[str],
        analysis_result: str,
        keyword: str,
        start_time: str,
        end_time: str,
        output_path: Path,
    ) -> str:
        """ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ±"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.5em; }}
        .header .subtitle {{ margin-top: 10px; opacity: 0.9; }}
        .content {{ padding: 30px; }}
        .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .summary-card {{ background: #f8f9fa; border-left: 4px solid #007bff; padding: 20px; border-radius: 5px; }}
        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .summary-card .value {{ font-size: 1.5em; font-weight: bold; color: #007bff; }}
        .analysis-section {{ margin-bottom: 30px; }}
        .analysis-section h2 {{ color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
        .log-content {{ background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: monospace; font-size: 0.9em; max-height: 400px; overflow-y: auto; }}
        .error-high {{ color: #dc3545; font-weight: bold; }}
        .error-medium {{ color: #fd7e14; }}
        .error-low {{ color: #6c757d; }}
        .footer {{ text-align: center; padding: 20px; color: #6c757d; border-top: 1px solid #dee2e6; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üîç Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù</h1>
            <div class="subtitle">ÏÉùÏÑ±ÏùºÏãú: {timestamp}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-card">
                    <h3>Î∂ÑÏÑù Í∏∞Í∞Ñ</h3>
                    <div class="value">{start_time}<br>~<br>{end_time}</div>
                </div>
                <div class="summary-card">
                    <h3>ÎåÄÏÉÅ ÌÇ§ÏõåÎìú</h3>
                    <div class="value">{keyword}</div>
                </div>
                <div class="summary-card">
                    <h3>Î°úÍ∑∏ Ï≤≠ÌÅ¨ Ïàò</h3>
                    <div class="value">{len(log_results)}Í∞ú</div>
                </div>
            </div>
            
            <div class="analysis-section">
                <h2>ü§ñ Claude AI Î∂ÑÏÑù Í≤∞Í≥º</h2>
                <div class="log-content">
                    {analysis_result.replace(chr(10), '<br>')}
                </div>
            </div>
            
            <div class="analysis-section">
                <h2>üìã ÏõêÎ≥∏ Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞</h2>
                {''.join([f'<div class="log-content">{log.replace(chr(10), "<br>")}</div>' for log in log_results])}
            </div>
        </div>
        
        <div class="footer">
            <p>DB Assistant MCP Server - Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú</p>
        </div>
    </div>
</body>
</html>"""

            # Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # HTML ÌååÏùº Ï†ÄÏû•
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            return str(output_path)

        except Exception as e:
            logger.error(f"HTML Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {e}")
            return f"Î≥¥Í≥†ÏÑú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"

    async def save_to_vector_store(
        self,
        content: str,
        topic: str,
        category: str = "examples",
        tags: list = None,
        force_save: bool = False,
    ) -> str:
        """ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÏóê Ï†ÄÏû• (Ï§ëÎ≥µ Î∞è ÏÉÅÏ∂© Í≤ÄÏÇ¨ Ìè¨Ìï®)"""
        try:
            import os
            from datetime import datetime
            import re

            # 1. Í∞ïÏ†ú Ï†ÄÏû•Ïù¥ ÏïÑÎãå Í≤ΩÏö∞ÏóêÎßå Ï§ëÎ≥µ/ÏÉÅÏ∂© Í≤ÄÏÇ¨
            if not force_save:
                duplicate_check = await self._check_content_similarity(
                    content, category
                )

                if duplicate_check["is_duplicate"]:
                    return f"""‚ö†Ô∏è Ï§ëÎ≥µÎêú ÎÇ¥Ïö©Ïù¥ Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§!

üîç Ïú†ÏÇ¨Ìïú Í∏∞Ï°¥ Î¨∏ÏÑú:
üìÑ ÌååÏùº: {duplicate_check["similar_file"]}
üìä Ïú†ÏÇ¨ÎèÑ: {duplicate_check["similarity_score"]:.1%}

üí° Îã§Ïùå Ï§ë ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:
1. 'update_vector_content' ÎèÑÍµ¨Î°ú Í∏∞Ï°¥ Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏
2. 'save_to_vector_store'Ïóê force_save=trueÎ°ú Í∞ïÏ†ú Ï†ÄÏû•
3. Ï†ÄÏû• Ï∑®ÏÜå"""

                if duplicate_check["has_conflict"]:
                    return f"""üö® Í∏∞Ï°¥ ÎÇ¥Ïö©Í≥º ÏÉÅÏ∂©ÎêòÎäî Ï†ïÎ≥¥Í∞Ä Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§!

‚ö†Ô∏è ÏÉÅÏ∂© ÎÇ¥Ïö©:
{duplicate_check["conflict_details"]}

ü§î Îã§Ïùå Ï§ë ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:
1. ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Í∞Ä ÎßûÎã§Î©¥ 'update_vector_content'Î°ú Í∏∞Ï°¥ Î¨∏ÏÑú ÍµêÏ≤¥
2. Í∏∞Ï°¥ Ï†ïÎ≥¥Í∞Ä ÎßûÎã§Î©¥ Ï†ÄÏû• Ï∑®ÏÜå
3. Îëò Îã§ ÎßûÎã§Î©¥ 'save_to_vector_store'Ïóê force_save=trueÎ°ú Î≥ÑÎèÑ Ï†ÄÏû•"""

            # 2. Ï§ëÎ≥µ/ÏÉÅÏ∂©Ïù¥ ÏóÜÏúºÎ©¥ Ï†ïÏÉÅ Ï†ÄÏû• ÏßÑÌñâ
            date_str = datetime.now().strftime("%Y%m%d")
            clean_topic = re.sub(r"[^a-zA-Z0-9]", "", topic.lower())[:10]
            if not clean_topic:
                clean_topic = "content"

            filename = f"{date_str}_{clean_topic}.md"

            # vector Ìè¥Îçî ÏÉùÏÑ±
            vector_dir = "vector"
            os.makedirs(vector_dir, exist_ok=True)

            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
            if tags is None:
                tags = ["conversation", "analysis"]

            metadata_tags = tags + ["database", "optimization", "best-practices"]

            # YAML Ìó§Îçî ÏÉùÏÑ±
            yaml_header = f"""---
title: "{topic}"
category: "{category}"
tags: {metadata_tags}
version: "1.0"
last_updated: "{datetime.now().strftime('%Y-%m-%d')}"
author: "DB Assistant"
source: "conversation"
similarity_checked: true
---

"""

            # ÌååÏùº ÎÇ¥Ïö© ÏÉùÏÑ±
            file_content = yaml_header + content

            # Î°úÏª¨ ÌååÏùº Ï†ÄÏû•
            local_path = os.path.join(vector_dir, filename)
            with open(local_path, "w", encoding="utf-8") as f:
                f.write(file_content)

            # S3Ïóê ÏóÖÎ°úÎìú
            s3_client = boto3.client("s3", region_name="us-east-1")
            s3_key = f"{category}/{filename}"

            s3_client.upload_file(
                local_path,
                "bedrockagent-hhs",
                s3_key,
                ExtraArgs={"ContentType": "text/markdown"},
            )

            logger.info(f"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÏóê ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: {s3_key}")

            # ÏûêÎèôÏúºÎ°ú Knowledge Base ÎèôÍ∏∞Ìôî Ïã§Ìñâ
            sync_result = await self.sync_knowledge_base()

            return f"""‚úÖ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÏóê Ï†ÄÏû• ÏôÑÎ£å!

üìÅ Î°úÏª¨ Ï†ÄÏû•: {local_path}
‚òÅÔ∏è S3 Ï†ÄÏû•: s3://bedrockagent-hhs/{s3_key}
üè∑Ô∏è Ïπ¥ÌÖåÍ≥†Î¶¨: {category}
üîñ ÌÉúÍ∑∏: {', '.join(metadata_tags)}
‚úÖ Ï§ëÎ≥µ/ÏÉÅÏ∂© Í≤ÄÏÇ¨: ÌÜµÍ≥º

üîÑ Knowledge Base ÎèôÍ∏∞Ìôî ÏûêÎèô Ïã§Ìñâ:
{sync_result}"""

        except Exception as e:
            logger.error(f"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Ï†ÄÏû• Ïò§Î•ò: {e}")
            return f"‚ùå Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Ï†ÄÏû• Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"

    async def _check_content_similarity(self, new_content: str, category: str) -> dict:
        """Í∏∞Ï°¥ ÎÇ¥Ïö©Í≥º Ï§ëÎ≥µ/ÏÉÅÏ∂© Í≤ÄÏÇ¨"""
        try:
            # 1. Knowledge BaseÏóêÏÑú Ïú†ÏÇ¨Ìïú ÎÇ¥Ïö© Í≤ÄÏÉâ
            similar_docs = await self._search_similar_content(new_content, category)

            # 2. Claude AIÎ°ú Ï§ëÎ≥µ/ÏÉÅÏ∂© Î∂ÑÏÑù
            if similar_docs:
                analysis = await self._analyze_content_conflicts(
                    new_content, similar_docs
                )
                return analysis

            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

        except Exception as e:
            logger.error(f"ÎÇ¥Ïö© Ïú†ÏÇ¨ÏÑ± Í≤ÄÏÇ¨ Ïò§Î•ò: {e}")
            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

    async def _search_similar_content(self, content: str, category: str) -> list:
        """Knowledge BaseÏóêÏÑú Ïú†ÏÇ¨Ìïú ÎÇ¥Ïö© Í≤ÄÏÉâ"""
        try:
            # ÎÇ¥Ïö©Ïùò ÌïµÏã¨ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
            keywords = self._extract_keywords(content)
            search_query = " ".join(keywords[:5])  # ÏÉÅÏúÑ 5Í∞ú ÌÇ§ÏõåÎìú ÏÇ¨Ïö©

            response = self.bedrock_agent_client.retrieve(
                knowledgeBaseId=self.knowledge_base_id,
                retrievalQuery={"text": search_query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {
                        "numberOfResults": 3,
                        "overrideSearchType": "SEMANTIC",
                    }
                },
            )

            similar_docs = []
            for result in response.get("retrievalResults", []):
                if result["score"] > 0.7:  # 70% Ïù¥ÏÉÅ Ïú†ÏÇ¨ÎèÑ
                    similar_docs.append(
                        {
                            "content": result["content"]["text"],
                            "score": result["score"],
                            "source": result["location"]["s3Location"]["uri"],
                        }
                    )

            return similar_docs

        except Exception as e:
            logger.error(f"Ïú†ÏÇ¨ ÎÇ¥Ïö© Í≤ÄÏÉâ Ïò§Î•ò: {e}")
            return []

    def _extract_keywords(self, content: str) -> list:
        """ÎÇ¥Ïö©ÏóêÏÑú ÌïµÏã¨ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        import re

        # Í∏∞Î≥∏Ï†ÅÏù∏ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú NLP Í∏∞Î≤ï ÏÇ¨Ïö© Í∞ÄÎä•)
        words = re.findall(r"\b[a-zA-ZÍ∞Ä-Ìû£]{3,}\b", content.lower())

        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í¥ÄÎ†® Ï§ëÏöî ÌÇ§ÏõåÎìú Ïö∞ÏÑ†ÏàúÏúÑ
        db_keywords = [
            "mysql",
            "aurora",
            "index",
            "query",
            "performance",
            "optimization",
            "table",
            "database",
            "sql",
            "schema",
            "connection",
            "error",
            "log",
        ]

        # Ï§ëÏöî ÌÇ§ÏõåÎìú Ïö∞ÏÑ† Ï†ïÎ†¨
        keywords = []
        for keyword in db_keywords:
            if keyword in words:
                keywords.append(keyword)

        # ÎÇòÎ®∏ÏßÄ ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä (ÎπàÎèÑÏàú)
        word_freq = {}
        for word in words:
            if word not in keywords and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1

        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords.extend([word for word, freq in sorted_words[:10]])

        return keywords

    async def _analyze_content_conflicts(
        self, new_content: str, similar_docs: list
    ) -> dict:
        """Claude AIÎ°ú ÎÇ¥Ïö© Ï§ëÎ≥µ/ÏÉÅÏ∂© Î∂ÑÏÑù"""
        try:
            similar_content = "\n\n".join(
                [
                    f"Î¨∏ÏÑú {i+1}: {doc['content'][:500]}..."
                    for i, doc in enumerate(similar_docs)
                ]
            )

            prompt = f"""Îã§Ïùå ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©Í≥º Í∏∞Ï°¥ Î¨∏ÏÑúÎì§ÏùÑ ÎπÑÍµêÌïòÏó¨ Ï§ëÎ≥µÏÑ±Í≥º ÏÉÅÏ∂©ÏÑ±ÏùÑ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.

ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©:
{new_content}

Í∏∞Ï°¥ Î¨∏ÏÑúÎì§:
{similar_content}

Îã§Ïùå Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
1. Ï§ëÎ≥µÏÑ±: ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©Ïù¥ Í∏∞Ï°¥ Î¨∏ÏÑúÏôÄ 80% Ïù¥ÏÉÅ Ïú†ÏÇ¨ÌïúÍ∞Ä?
2. ÏÉÅÏ∂©ÏÑ±: ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©Ïù¥ Í∏∞Ï°¥ Î¨∏ÏÑúÏôÄ Î™®ÏàúÎêòÎäî Ï†ïÎ≥¥Î•º Ìè¨Ìï®ÌïòÎäîÍ∞Ä?

ÏùëÎãµ ÌòïÏãù:
DUPLICATE: true/false
CONFLICT: true/false
SIMILARITY_SCORE: 0.0-1.0
SIMILAR_FILE: Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú Î≤àÌò∏
CONFLICT_DETAILS: ÏÉÅÏ∂©ÎêòÎäî ÎÇ¥Ïö© ÏÑ§Î™Ö (ÏÉÅÏ∂©Ïù¥ ÏûàÏùÑ Í≤ΩÏö∞Îßå)"""

            response = self.bedrock_runtime.invoke_model(
                modelId="us.anthropic.claude-sonnet-4-20250514-v1:0",
                body=json.dumps(
                    {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 1000,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                ),
            )

            response_body = json.loads(response.get("body").read())
            analysis_text = response_body.get("content", [{}])[0].get("text", "")

            # ÏùëÎãµ ÌååÏã±
            is_duplicate = "DUPLICATE: true" in analysis_text.lower()
            has_conflict = "CONFLICT: true" in analysis_text.lower()

            # Ïú†ÏÇ¨ÎèÑ Ï†êÏàò Ï∂îÏ∂ú
            similarity_score = 0.0
            if similar_docs:
                similarity_score = similar_docs[0]["score"]

            # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÌååÏùº Ï∂îÏ∂ú
            similar_file = None
            if similar_docs:
                similar_file = similar_docs[0]["source"].split("/")[-1]

            # ÏÉÅÏ∂© ÎÇ¥Ïö© Ï∂îÏ∂ú
            conflict_details = None
            if has_conflict:
                lines = analysis_text.split("\n")
                for line in lines:
                    if "CONFLICT_DETAILS:" in line:
                        conflict_details = line.split("CONFLICT_DETAILS:")[1].strip()
                        break

            return {
                "is_duplicate": is_duplicate,
                "has_conflict": has_conflict,
                "similarity_score": similarity_score,
                "similar_file": similar_file,
                "conflict_details": conflict_details,
            }

        except Exception as e:
            logger.error(f"ÎÇ¥Ïö© ÏÉÅÏ∂© Î∂ÑÏÑù Ïò§Î•ò: {e}")
            return {
                "is_duplicate": False,
                "has_conflict": False,
                "similarity_score": 0.0,
                "similar_file": None,
                "conflict_details": None,
            }

    async def sync_knowledge_base(self) -> str:
        """Knowledge Base Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÎèôÍ∏∞Ìôî"""
        try:
            bedrock_agent_client = boto3.client(
                "bedrock-agent", region_name="us-east-1"
            )

            response = bedrock_agent_client.start_ingestion_job(
                knowledgeBaseId="0WQUBRHVR8", dataSourceId="A8VCUHOHEQ"
            )

            job_id = response["ingestionJob"]["ingestionJobId"]
            status = response["ingestionJob"]["status"]

            logger.info(f"Knowledge Base ÎèôÍ∏∞Ìôî ÏãúÏûë: {job_id}")

            return f"""‚úÖ Knowledge Base ÎèôÍ∏∞Ìôî ÏãúÏûë!

üîÑ ÏûëÏóÖ ID: {job_id}
üìä ÏÉÅÌÉú: {status}
‚è∞ ÏãúÏûë ÏãúÍ∞Ñ: {response['ingestionJob']['startedAt']}

üí° ÎèôÍ∏∞ÌôîÍ∞Ä ÏôÑÎ£åÎêòÎ©¥ ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©ÏùÑ Knowledge BaseÏóêÏÑú Í≤ÄÏÉâÌï† Ïàò ÏûàÏäµÎãàÎã§.
ÏÉÅÌÉú ÌôïÏù∏: AWS ÏΩòÏÜî > Bedrock > Knowledge Base > Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§"""

        except Exception as e:
            logger.error(f"Knowledge Base ÎèôÍ∏∞Ìôî Ïò§Î•ò: {e}")
            return f"‚ùå Knowledge Base ÎèôÍ∏∞Ìôî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"

    async def query_vector_store(self, query: str, max_results: int = 5) -> str:
        """Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÏóêÏÑú ÎÇ¥Ïö©ÏùÑ Í≤ÄÏÉâÌï©ÎãàÎã§"""
        try:
            bedrock_agent_runtime = boto3.client(
                "bedrock-agent-runtime", region_name="us-east-1"
            )

            # Knowledge BaseÏóêÏÑú Í≤ÄÏÉâ
            response = bedrock_agent_runtime.retrieve(
                knowledgeBaseId="0WQUBRHVR8",
                retrievalQuery={"text": query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": max_results}
                },
            )

            if not response.get("retrievalResults"):
                return f"""üîç Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.

üîé Í≤ÄÏÉâÏñ¥: '{query}'
üí° Îã§Î•∏ ÌÇ§ÏõåÎìúÎ°ú ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî.
üìù ÏòàÏãú: 'HLL', 'lock', 'performance', 'SQL' Îì±"""

            results = []
            for i, result in enumerate(response["retrievalResults"], 1):
                content = result["content"]["text"]
                score = result.get("score", 0)

                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
                metadata = result.get("metadata", {})
                source = metadata.get("source", "Ïïå Ïàò ÏóÜÏùå")

                # ÎÇ¥Ïö© Í∏∏Ïù¥ Ï†úÌïú
                preview = content[:300] + "..." if len(content) > 300 else content

                results.append(
                    f"""üìÑ **Í≤∞Í≥º {i}** (Í¥ÄÎ†®ÎèÑ: {score:.2f})
üìÅ Ï∂úÏ≤ò: {source}
üìù ÎÇ¥Ïö©:
{preview}
"""
                )

            return f"""üîç **Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Í≤ÄÏÉâ Í≤∞Í≥º**

üîé Í≤ÄÏÉâÏñ¥: "{query}"
üìä Ï¥ù {len(results)}Í∞ú Í≤∞Í≥º Î∞úÍ≤¨

{chr(10).join(results)}

üí° Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Í≤ÄÏÉâÏùÑ ÏõêÌïòÏãúÎ©¥ ÌÇ§ÏõåÎìúÎ•º ÏÑ∏Î∂ÑÌôîÌï¥Î≥¥ÏÑ∏Ïöî."""

        except Exception as e:
            logger.error(f"Î≤°ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}")
            return f"Î≤°ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}"

    async def update_vector_content(
        self, filename: str, new_content: str, update_mode: str = "append"
    ) -> str:
        """Í∏∞Ï°¥ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            import os
            from datetime import datetime

            # Î°úÏª¨ ÌååÏùº Í≤ΩÎ°ú
            local_path = os.path.join("vector", filename)

            if not os.path.exists(local_path):
                return f"‚ùå ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {filename}"

            # Í∏∞Ï°¥ ÌååÏùº ÏùΩÍ∏∞
            with open(local_path, "r", encoding="utf-8") as f:
                existing_content = f.read()

            # YAML Ìó§ÎçîÏôÄ Î≥∏Î¨∏ Î∂ÑÎ¶¨
            if existing_content.startswith("---"):
                parts = existing_content.split("---", 2)
                if len(parts) >= 3:
                    yaml_header = f"---{parts[1]}---"
                    existing_body = parts[2].strip()
                else:
                    yaml_header = ""
                    existing_body = existing_content
            else:
                yaml_header = ""
                existing_body = existing_content

            # ÏóÖÎç∞Ïù¥Ìä∏ Î™®ÎìúÏóê Îî∞Î•∏ ÎÇ¥Ïö© Ï≤òÎ¶¨
            if update_mode == "replace":
                updated_body = new_content
            else:  # append
                updated_body = f"{existing_body}\n\n## ÏóÖÎç∞Ïù¥Ìä∏ ({datetime.now().strftime('%Y-%m-%d %H:%M')})\n\n{new_content}"

            # YAML Ìó§Îçî ÏóÖÎç∞Ïù¥Ìä∏
            if yaml_header:
                # last_updated ÌïÑÎìú ÏóÖÎç∞Ïù¥Ìä∏
                import re

                yaml_header = re.sub(
                    r'last_updated: "[^"]*"',
                    f'last_updated: "{datetime.now().strftime("%Y-%m-%d")}"',
                    yaml_header,
                )
                # version ÏóÖÎç∞Ïù¥Ìä∏
                version_match = re.search(r'version: "([^"]*)"', yaml_header)
                if version_match:
                    current_version = version_match.group(1)
                    try:
                        version_num = float(current_version) + 0.1
                        yaml_header = re.sub(
                            r'version: "[^"]*"',
                            f'version: "{version_num:.1f}"',
                            yaml_header,
                        )
                    except:
                        pass

            # ÏÉàÎ°úÏö¥ ÌååÏùº ÎÇ¥Ïö© ÏÉùÏÑ±
            updated_content = (
                f"{yaml_header}\n\n{updated_body}" if yaml_header else updated_body
            )

            # Î°úÏª¨ ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏
            with open(local_path, "w", encoding="utf-8") as f:
                f.write(updated_content)

            # S3 ÏóÖÎç∞Ïù¥Ìä∏
            s3_client = boto3.client("s3", region_name="us-east-1")

            # Ïπ¥ÌÖåÍ≥†Î¶¨ Ï∂îÏ∂ú (ÌååÏùºÎ™ÖÏóêÏÑú ÎòêÎäî YAMLÏóêÏÑú)
            category = "examples"  # Í∏∞Î≥∏Í∞í
            if yaml_header:
                category_match = re.search(r'category: "([^"]*)"', yaml_header)
                if category_match:
                    category = category_match.group(1)

            s3_key = f"{category}/{filename}"

            s3_client.upload_file(
                local_path,
                "bedrockagent-hhs",
                s3_key,
                ExtraArgs={"ContentType": "text/markdown"},
            )

            logger.info(f"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {s3_key}")

            # ÏûêÎèôÏúºÎ°ú Knowledge Base ÎèôÍ∏∞Ìôî Ïã§Ìñâ
            sync_result = await self.sync_knowledge_base()

            return f"""‚úÖ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å!

üìÅ Î°úÏª¨ ÌååÏùº: {local_path}
‚òÅÔ∏è S3 ÌååÏùº: s3://bedrockagent-hhs/{s3_key}
üîÑ ÏóÖÎç∞Ïù¥Ìä∏ Î™®Îìú: {update_mode}
üìù ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M')}

üîÑ Knowledge Base ÎèôÍ∏∞Ìôî ÏûêÎèô Ïã§Ìñâ:
{sync_result}"""

        except Exception as e:
            logger.error(f"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÏóÖÎç∞Ïù¥Ìä∏ Ïò§Î•ò: {e}")
            return f"‚ùå Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÏóÖÎç∞Ïù¥Ìä∏ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"


# MCP ÏÑúÎ≤Ñ ÏÑ§Ï†ï
server = Server("db-assistant-mcp-server")
db_assistant = DBAssistantMCPServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ Î™©Î°ù Î∞òÌôò"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ÎîîÎ†âÌÜ†Î¶¨Ïùò SQL ÌååÏùº Î™©Î°ùÏùÑ Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets ManagerÏùò Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Î™©Î°ùÏùÑ Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "Í≤ÄÏÉâÌï† ÌÇ§ÏõåÎìú (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    }
                },
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ÏùÑ ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="list_databases",
            description="Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î™©Î°ùÏùÑ Ï°∞ÌöåÌïòÍ≥† ÏÑ†ÌÉùÌï† Ïàò ÏûàÏäµÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="select_database",
            description="ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º ÏÑ†ÌÉùÌï©ÎãàÎã§ (USE Î™ÖÎ†πÏñ¥ Ïã§Ìñâ). Î®ºÏ†Ä list_databasesÎ°ú Î™©Î°ùÏùÑ ÌôïÏù∏Ìïú ÌõÑ Î≤àÌò∏ÎÇò Ïù¥Î¶ÑÏúºÎ°ú ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ÏÑ†ÌÉùÌï† Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ (Î≤àÌò∏ ÎòêÎäî Ïù¥Î¶Ñ)",
                    },
                },
                "required": ["database_secret", "database_selection"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïä§ÌÇ§ÎßàÏùò ÏöîÏïΩ Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_schema",
            description="ÌäπÏ†ï ÌÖåÏù¥Î∏îÏùò ÏÉÅÏÑ∏ Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Î•º Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "Ï°∞ÌöåÌï† ÌÖåÏù¥Î∏î Ïù¥Î¶Ñ",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="get_table_index",
            description="ÌäπÏ†ï ÌÖåÏù¥Î∏îÏùò Ïù∏Îç±Ïä§ Ï†ïÎ≥¥Î•º Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "Ï°∞ÌöåÌï† ÌÖåÏù¥Î∏î Ïù¥Î¶Ñ",
                    },
                },
                "required": ["database_secret", "table_name"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• Î©îÌä∏Î¶≠ÏùÑ Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "Î©îÌä∏Î¶≠ ÌÉÄÏûÖ (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_db_metrics",
            description="CloudWatchÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î©îÌä∏Î¶≠ÏùÑ ÏàòÏßëÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ÏàòÏßëÌï† ÏãúÍ∞Ñ Î≤îÏúÑ (ÏãúÍ∞Ñ Îã®ÏúÑ, Í∏∞Î≥∏Í∞í: 24)",
                        "default": 24,
                    },
                    "metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ÏàòÏßëÌï† Î©îÌä∏Î¶≠ Î™©Î°ù (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS Î¶¨Ï†Ñ (Í∏∞Î≥∏Í∞í: us-east-1)",
                        "default": "us-east-1",
                    },
                },
                "required": ["db_instance_identifier"],
            },
        ),
        types.Tool(
            name="analyze_metric_correlation",
            description="Î©îÌä∏Î¶≠ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Î∂ÑÏÑùÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "Î∂ÑÏÑùÌï† CSV ÌååÏùºÎ™Ö"},
                    "target_metric": {
                        "type": "string",
                        "description": "ÌÉÄÍ≤ü Î©îÌä∏Î¶≠ (Í∏∞Î≥∏Í∞í: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "ÏÉÅÏúÑ NÍ∞ú Î©îÌä∏Î¶≠ (Í∏∞Î≥∏Í∞í: 10)",
                        "default": 10,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="detect_metric_outliers",
            description="Í∞úÏÑ†Îêú ÏïÑÏõÉÎùºÏù¥Ïñ¥ ÌÉêÏßÄ - Î©îÌä∏Î¶≠Î≥Ñ ÎßûÏ∂§ ÏûÑÍ≥ÑÍ∞íÍ≥º Î¨ºÎ¶¨Ï†Å Ï†úÏïΩ Ï†ÅÏö©",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "Î∂ÑÏÑùÌï† CSV ÌååÏùºÎ™Ö"},
                    "std_threshold": {
                        "type": "number",
                        "description": "IQR Î∞©ÏãùÏö© ÏûÑÍ≥ÑÍ∞í (Í∏∞Î≥∏Í∞í: 3.0, Î©îÌä∏Î¶≠Î≥Ñ ÎßûÏ∂§ Í∏∞Ï§Ä Ïö∞ÏÑ† Ï†ÅÏö©)",
                        "default": 3.0,
                    },
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="perform_regression_analysis",
            description="Î©îÌä∏Î¶≠ Í∞Ñ ÌöåÍ∑Ä Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "Î∂ÑÏÑùÌï† CSV ÌååÏùºÎ™Ö"},
                    "predictor_metric": {
                        "type": "string",
                        "description": "ÏòàÏ∏° Î≥ÄÏàò Î©îÌä∏Î¶≠",
                    },
                    "target_metric": {
                        "type": "string",
                        "description": "ÌÉÄÍ≤ü Î©îÌä∏Î¶≠ (Í∏∞Î≥∏Í∞í: CPUUtilization)",
                        "default": "CPUUtilization",
                    },
                },
                "required": ["csv_file", "predictor_metric"],
            },
        ),
        types.Tool(
            name="list_data_files",
            description="Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨Ïùò CSV ÌååÏùº Î™©Î°ùÏùÑ Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="validate_sql_file",
            description="ÌäπÏ†ï SQL ÌååÏùºÏùÑ Í≤ÄÏ¶ùÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "Í≤ÄÏ¶ùÌï† SQL ÌååÏùºÎ™Ö"},
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL ÌååÏùºÏùÑ sql ÎîîÎ†âÌÜ†Î¶¨Î°ú Î≥µÏÇ¨Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "Î≥µÏÇ¨Ìï† SQL ÌååÏùºÏùò Í≤ΩÎ°ú",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ÎåÄÏÉÅ ÌååÏùºÎ™Ö (ÏÑ†ÌÉùÏÇ¨Ìï≠, Í∏∞Î≥∏Í∞íÏùÄ ÏõêÎ≥∏ ÌååÏùºÎ™Ö)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="get_metric_summary",
            description="CSV ÌååÏùºÏùò Î©îÌä∏Î¶≠ ÏöîÏïΩ Ï†ïÎ≥¥Î•º Ï°∞ÌöåÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "csv_file": {"type": "string", "description": "ÏöîÏïΩÌï† CSV ÌååÏùºÎ™Ö"}
                },
                "required": ["csv_file"],
            },
        ),
        types.Tool(
            name="debug_cloudwatch_collection",
            description="CloudWatch Ïä¨Î°úÏö∞ ÏøºÎ¶¨ ÏàòÏßë ÎîîÎ≤ÑÍ∑∏",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS, KST Í∏∞Ï§Ä)",
                    },
                    "end_time": {
                        "type": "string", 
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS, KST Í∏∞Ï§Ä)",
                    },
                },
                "required": ["database_secret", "start_time", "end_time"],
            },
        ),
        types.Tool(
            name="collect_slow_queries",
            description="Ïä¨Î°úÏö∞ ÏøºÎ¶¨ Î°úÍ∑∏ÏóêÏÑú ÎäêÎ¶∞ ÏøºÎ¶¨Î•º ÏàòÏßëÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS, KST Í∏∞Ï§Ä, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS, KST Í∏∞Ï§Ä, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="enable_slow_query_log_exports",
            description="Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞Ïùò SlowQuery Î°úÍ∑∏ CloudWatch Ï†ÑÏÜ°ÏùÑ ÌôúÏÑ±ÌôîÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_identifier": {
                        "type": "string",
                        "description": "Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏãùÎ≥ÑÏûê",
                    },
                },
                "required": ["cluster_identifier"],
            },
        ),
        types.Tool(
            name="collect_memory_intensive_queries",
            description="Î©îÎ™®Î¶¨ ÏßëÏïΩÏ†Å ÏøºÎ¶¨Î•º ÏàòÏßëÌïòÎäî SQLÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "end_time": {
                        "type": "string", 
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_cpu_intensive_queries",
            description="CPU ÏßëÏïΩÏ†Å ÏøºÎ¶¨Î•º ÏàòÏßëÌïòÎäî SQLÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="collect_temp_space_intensive_queries",
            description="ÏûÑÏãú Í≥µÍ∞Ñ ÏßëÏïΩÏ†Å ÏøºÎ¶¨Î•º ÏàòÏßëÌïòÎäî SQLÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "ÌäπÏ†ï Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "start_time": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "end_time": {
                        "type": "string",
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="analyze_aurora_mysql_error_logs",
            description="Aurora MySQL ÏóêÎü¨ Î°úÍ∑∏Î•º Î∂ÑÏÑùÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "Í≤ÄÏÉâÌï† ÌÇ§ÏõåÎìú (ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ ÌïÑÌÑ∞ÎßÅÏö©)",
                    },
                    "start_datetime_str": {
                        "type": "string",
                        "description": "ÏãúÏûë ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù)",
                    },
                    "end_datetime_str": {
                        "type": "string",
                        "description": "Ï¢ÖÎ£å ÏãúÍ∞Ñ (YYYY-MM-DD HH:MM:SS ÌòïÏãù)",
                    },
                },
                "required": ["keyword", "start_datetime_str", "end_datetime_str"],
            },
        ),
        types.Tool(
            name="test_individual_query_validation",
            description="Í∞úÎ≥Ñ ÏøºÎ¶¨ Í≤ÄÏ¶ù ÌÖåÏä§Ìä∏ (ÎîîÎ≤ÑÍ∑∏Ïö©)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "filename": {"type": "string", "description": "Í≤ÄÏ¶ùÌï† SQL ÌååÏùºÎ™Ö"},
                },
                "required": ["database_secret", "filename"],
            },
        ),
        types.Tool(
            name="generate_consolidated_report",
            description="Í∏∞Ï°¥ HTML Î≥¥Í≥†ÏÑúÎì§ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌÜµÌï© Î≥¥Í≥†ÏÑú ÏÉùÏÑ±",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ÌïÑÌÑ∞ÎßÅÌï† ÌÇ§ÏõåÎìú (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "report_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ÌäπÏ†ï Î≥¥Í≥†ÏÑú ÌååÏùºÎ™Ö Î™©Î°ù (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "date_filter": {
                        "type": "string",
                        "description": "ÎÇ†Ïßú ÌïÑÌÑ∞ (YYYYMMDD ÌòïÏãù, ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "latest_count": {
                        "type": "integer",
                        "description": "ÏµúÏã† ÌååÏùº Í∞úÏàò Ï†úÌïú (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                },
            },
        ),
        types.Tool(
            name="save_to_vector_store",
            description="ÎåÄÌôî ÎÇ¥Ïö©Ïù¥ÎÇò Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå(Knowledge Base)Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§ (Ï§ëÎ≥µ/ÏÉÅÏ∂© Í≤ÄÏÇ¨ Ìè¨Ìï®)",
            inputSchema={
                "type": "object",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "Ï†ÄÏû•Ìï† ÎÇ¥Ïö©",
                    },
                    "topic": {
                        "type": "string",
                        "description": "Ï£ºÏ†úÎ™Ö (10Ïûê Ïù¥ÎÇ¥ ÏòÅÎ¨∏)",
                    },
                    "category": {
                        "type": "string",
                        "description": "Ïπ¥ÌÖåÍ≥†Î¶¨ (database-standards, performance-optimization, troubleshooting, examples Ï§ë ÏÑ†ÌÉù)",
                        "enum": [
                            "database-standards",
                            "performance-optimization",
                            "troubleshooting",
                            "examples",
                        ],
                        "default": "examples",
                    },
                    "tags": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ÌÉúÍ∑∏ Î™©Î°ù (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                    },
                    "force_save": {
                        "type": "boolean",
                        "description": "Ï§ëÎ≥µ/ÏÉÅÏ∂© Í≤ÄÏÇ¨ Î¨¥ÏãúÌïòÍ≥† Í∞ïÏ†ú Ï†ÄÏû• (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                        "default": False,
                    },
                },
                "required": ["content", "topic"],
            },
        ),
        types.Tool(
            name="update_vector_content",
            description="Í∏∞Ï°¥ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Î¨∏ÏÑúÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {
                        "type": "string",
                        "description": "ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÌååÏùºÎ™Ö",
                    },
                    "new_content": {
                        "type": "string",
                        "description": "ÏÉàÎ°úÏö¥ ÎÇ¥Ïö© (Í∏∞Ï°¥ ÎÇ¥Ïö©Ïóê Ï∂îÍ∞ÄÎê®)",
                    },
                    "update_mode": {
                        "type": "string",
                        "description": "ÏóÖÎç∞Ïù¥Ìä∏ Î™®Îìú (append: Ï∂îÍ∞Ä, replace: ÍµêÏ≤¥)",
                        "enum": ["append", "replace"],
                        "default": "append",
                    },
                },
                "required": ["filename", "new_content"],
            },
        ),
        types.Tool(
            name="sync_knowledge_base",
            description="Knowledge Base Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§Î•º ÎèôÍ∏∞ÌôîÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),
        types.Tool(
            name="query_vector_store",
            description="Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå(Knowledge Base)ÏóêÏÑú ÎÇ¥Ïö©ÏùÑ Í≤ÄÏÉâÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Í≤ÄÏÉâÌï† ÌÇ§ÏõåÎìúÎÇò ÏßàÎ¨∏",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "ÏµúÎåÄ Í≤ÄÏÉâ Í≤∞Í≥º Ïàò (Í∏∞Î≥∏Í∞í: 5)",
                        "default": 5,
                    },
                },
                "required": ["query"],
            },
        ),
        types.Tool(
            name="generate_comprehensive_performance_report",
            description="Oracle AWR Ïä§ÌÉÄÏùºÏùò Ï¢ÖÌï© ÏÑ±Îä• ÏßÑÎã® Î≥¥Í≥†ÏÑú ÏÉùÏÑ± (Î©îÌä∏Î¶≠ Î∂ÑÏÑù, ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ, ÎäêÎ¶∞ ÏøºÎ¶¨, Î¶¨ÏÜåÏä§ ÏßëÏïΩÏ†Å ÏøºÎ¶¨ Ìè¨Ìï®)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "db_instance_identifier": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïù∏Ïä§ÌÑ¥Ïä§ ÏãùÎ≥ÑÏûê",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS Î¶¨Ï†Ñ (Í∏∞Î≥∏Í∞í: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ÏàòÏßëÌï† ÏãúÍ∞Ñ Î≤îÏúÑ (ÏãúÍ∞Ñ Îã®ÏúÑ, Í∏∞Î≥∏Í∞í: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_instance_identifier"],
            },
        ),
        types.Tool(
            name="generate_cluster_performance_report",
            description="Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ÑÏö© ÏÑ±Îä• Î≥¥Í≥†ÏÑú ÏÉùÏÑ± (ÌÅ¥Îü¨Ïä§ÌÑ∞ Î†àÎ≤® Î∂ÑÏÑù + Ïù∏Ïä§ÌÑ¥Ïä§Î≥Ñ ÏÉÅÏÑ∏ Î≥¥Í≥†ÏÑú ÎßÅÌÅ¨)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏãúÌÅ¨Î¶ø Ïù¥Î¶Ñ",
                    },
                    "db_cluster_identifier": {
                        "type": "string",
                        "description": "Aurora ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏãùÎ≥ÑÏûê",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS Î¶¨Ï†Ñ (Í∏∞Î≥∏Í∞í: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                    "hours": {
                        "type": "integer",
                        "description": "ÏàòÏßëÌï† ÏãúÍ∞Ñ Î≤îÏúÑ (ÏãúÍ∞Ñ Îã®ÏúÑ, Í∏∞Î≥∏Í∞í: 24)",
                        "default": 24,
                    },
                },
                "required": ["database_secret", "db_cluster_identifier"],
            },
        ),
        types.Tool(
            name="set_default_region",
            description="Í∏∞Î≥∏ AWS Î¶¨Ï†ÑÏùÑ Î≥ÄÍ≤ΩÌï©ÎãàÎã§",
            inputSchema={
                "type": "object",
                "properties": {
                    "region_name": {
                        "type": "string",
                        "description": "ÏÑ§Ï†ïÌï† AWS Î¶¨Ï†Ñ (Ïòà: ap-northeast-2, us-east-1, eu-west-1)",
                    }
                },
                "required": ["region_name"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ÎèÑÍµ¨ Ìò∏Ï∂ú Ï≤òÎ¶¨"""
    try:
        if name == "list_sql_files":
            result = await db_assistant.list_sql_files()
        elif name == "list_database_secrets":
            result = await db_assistant.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "test_database_connection":
            result = await db_assistant.test_database_connection(
                arguments["database_secret"]
            )
        elif name == "list_databases":
            result = await db_assistant.list_databases(arguments["database_secret"])
        elif name == "select_database":
            result = await db_assistant.select_database(
                arguments["database_secret"], arguments["database_selection"]
            )
        elif name == "get_schema_summary":
            result = await db_assistant.get_schema_summary(arguments["database_secret"])
        elif name == "get_table_schema":
            result = await db_assistant.get_table_schema(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "get_table_index":
            result = await db_assistant.get_table_index(
                arguments["database_secret"], arguments["table_name"]
            )
        elif name == "get_performance_metrics":
            result = await db_assistant.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "collect_db_metrics":
            result = await db_assistant.collect_db_metrics(
                arguments["db_instance_identifier"],
                arguments.get("hours", 24),
                arguments.get("metrics"),
                arguments.get("region", "us-east-1"),
            )
        elif name == "analyze_metric_correlation":
            result = await db_assistant.analyze_metric_correlation(
                arguments["csv_file"],
                arguments.get("target_metric", "CPUUtilization"),
                arguments.get("top_n", 10),
            )
        elif name == "detect_metric_outliers":
            result = await db_assistant.detect_metric_outliers(
                arguments["csv_file"], arguments.get("std_threshold", 3.0)
            )
        elif name == "perform_regression_analysis":
            result = await db_assistant.perform_regression_analysis(
                arguments["csv_file"],
                arguments["predictor_metric"],
                arguments.get("target_metric", "CPUUtilization"),
            )
        elif name == "list_data_files":
            result = await db_assistant.list_data_files()
        elif name == "validate_sql_file":
            result = await db_assistant.validate_sql_file(
                arguments["filename"], arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await db_assistant.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "get_metric_summary":
            result = await db_assistant.get_metric_summary(arguments["csv_file"])
        elif name == "debug_cloudwatch_collection":
            result = await db_assistant.debug_cloudwatch_collection(
                arguments["database_secret"],
                arguments["start_time"],
                arguments["end_time"]
            )
        elif name == "collect_slow_queries":
            result = await db_assistant.collect_slow_queries(
                arguments["database_secret"], 
                arguments.get("start_time"), 
                arguments.get("end_time")
            )
        elif name == "enable_slow_query_log_exports":
            result = await db_assistant.enable_slow_query_log_exports(
                arguments["cluster_identifier"]
            )
        elif name == "collect_memory_intensive_queries":
            result = await db_assistant.collect_memory_intensive_queries(
                arguments["database_secret"], 
                arguments.get("db_instance_identifier"),
                arguments.get("start_time"),
                arguments.get("end_time")
            )
        elif name == "collect_cpu_intensive_queries":
            result = await db_assistant.collect_cpu_intensive_queries(
                arguments["database_secret"], 
                arguments.get("db_instance_identifier"),
                arguments.get("start_time"),
                arguments.get("end_time")
            )
        elif name == "collect_temp_space_intensive_queries":
            result = await db_assistant.collect_temp_space_intensive_queries(
                arguments["database_secret"], 
                arguments.get("db_instance_identifier"),
                arguments.get("start_time"),
                arguments.get("end_time")
            )
        elif name == "analyze_aurora_mysql_error_logs":
            result = await db_assistant.analyze_aurora_mysql_error_logs(
                arguments["keyword"],
                arguments["start_datetime_str"],
                arguments["end_datetime_str"],
            )
        elif name == "save_to_vector_store":
            result = await db_assistant.save_to_vector_store(
                arguments["content"],
                arguments["topic"],
                arguments.get("category", "examples"),
                arguments.get("tags"),
                arguments.get("force_save", False),
            )
        elif name == "update_vector_content":
            result = await db_assistant.update_vector_content(
                arguments["filename"],
                arguments["new_content"],
                arguments.get("update_mode", "append"),
            )
        elif name == "sync_knowledge_base":
            result = await db_assistant.sync_knowledge_base()
        elif name == "query_vector_store":
            result = await db_assistant.query_vector_store(
                arguments["query"], arguments.get("max_results", 5)
            )
        elif name == "test_individual_query_validation":
            result = await db_assistant.test_individual_query_validation(
                arguments["database_secret"], arguments["filename"]
            )
        elif name == "generate_consolidated_report":
            result = await db_assistant.generate_consolidated_report(
                arguments.get("keyword"),
                arguments.get("report_files"),
                arguments.get("date_filter"),
                arguments.get("latest_count"),
            )
        elif name == "generate_comprehensive_performance_report":
            result = await db_assistant.generate_comprehensive_performance_report(
                arguments["database_secret"],
                arguments["db_instance_identifier"],
                arguments.get("region", "ap-northeast-2"),
                arguments.get("hours", 24),
            )
        elif name == "generate_cluster_performance_report":
            result = await db_assistant.generate_cluster_performance_report(
                arguments["database_secret"],
                arguments["db_cluster_identifier"],
                arguments.get("hours", 24),
                arguments.get("region", "ap-northeast-2"),
            )
        elif name == "set_default_region":
            result = db_assistant.set_default_region(arguments["region_name"])
        else:
            result = f"Ïïå Ïàò ÏóÜÎäî ÎèÑÍµ¨: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ÎèÑÍµ¨ Ïã§Ìñâ Ïò§Î•ò: {e}")
        return [types.TextContent(type="text", text=f"Ïò§Î•ò: {str(e)}")]


async def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="db-assistant-mcp-server",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ÏÑúÎ≤Ñ Ïã§Ìñâ Ïò§Î•ò: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())

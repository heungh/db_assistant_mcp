#!/usr/bin/env python3
"""
SQL ê²€ì¦ ë° ì„±ëŠ¥ë¶„ì„ Amazon Q CLI MCP ì„œë²„
"""

import asyncio
import json
import os
import re
import subprocess
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

import boto3

try:
    import mysql.connector
    from mysql.connector import Error as MySQLError
except ImportError:
    mysql = None
    MySQLError = Exception

from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import logging

# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
CURRENT_DIR = Path(__file__).parent
OUTPUT_DIR = CURRENT_DIR / "output"
SQL_DIR = CURRENT_DIR / "sql"
DATA_DIR = CURRENT_DIR / "data"
LOG_DIR = CURRENT_DIR / "logs"

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR.mkdir(exist_ok=True)

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ëª¨ë‘ì— ì¶œë ¥
log_file = LOG_DIR / "ddl_validation.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ë””ë ‰í† ë¦¬ ìƒì„±
OUTPUT_DIR.mkdir(exist_ok=True)
SQL_DIR.mkdir(exist_ok=True)
DATA_DIR.mkdir(exist_ok=True)


class DDLValidationQCLIServer:
    def __init__(self):
        self.bedrock_client = boto3.client(
            "bedrock-runtime", region_name="us-east-1", verify=False
        )
        self.knowledge_base_id = "0WQUBRHVR8"
        self.current_plan = None
        self.selected_cluster = None
        self.selected_database = None

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            try:
                subprocess.run(
                    ["pkill", "-f", "ssh.*54.180.79.255"],
                    capture_output=True,
                    timeout=5,
                )
            except:
                pass

            # SSH í„°ë„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤)
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-o",
                "ConnectTimeout=10",
                "-o",
                "ServerAliveInterval=60",
                "-o",
                "ServerAliveCountMax=3",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "-i",
                "/Users/heungh/test.pem",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ SSH í„°ë„ ì‹œì‘
            process = subprocess.Popen(
                ssh_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            time.sleep(3)

            # í”„ë¡œì„¸ìŠ¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            if process.poll() is None:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                stdout, stderr = process.communicate()
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {stderr.decode()}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        try:
            import subprocess

            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)
            logger.info("SSH í„°ë„ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SSH í„°ë„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def get_db_connection(
        self,
        database_secret: str,
        selected_database: str = None,
        use_ssh_tunnel: bool = True,
    ):
        """ê³µí†µ DB ì—°ê²° í•¨ìˆ˜"""
        try:
            if mysql is None:
                raise Exception(
                    "mysql-connector-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install mysql-connector-pythonì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                )

            # Secretì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(
                SecretId=database_secret
            )
            if (
                not get_secret_value_response
                or "SecretString" not in get_secret_value_response
            ):
                raise Exception(f"ì‹œí¬ë¦¿ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {database_secret}")
            secret = get_secret_value_response["SecretString"]
            db_config = json.loads(secret)

            connection_config = None
            tunnel_used = False

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            database_name = selected_database or db_config.get(
                "dbname", db_config.get("database")
            )

            if use_ssh_tunnel:
                if self.setup_ssh_tunnel(db_config.get("host")):
                    connection_config = {
                        "host": "localhost",
                        "port": 3307,
                        "user": str(db_config.get("username", "")),
                        "password": str(db_config.get("password", "")),
                        "connection_timeout": 10,
                    }
                    # databaseëŠ” Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
                    if database_name:
                        connection_config["database"] = str(database_name)
                    tunnel_used = True
        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            raise Exception(f"DB ì—°ê²° ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}\në””ë²„ê·¸: {error_details}")

        # ë‚˜ë¨¸ì§€ ì—°ê²° ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

        if not connection_config:
            # MySQL ì—°ê²°ì— í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë§Œ í¬í•¨
            connection_config = {
                "host": str(db_config.get("host", "")),
                "port": int(db_config.get("port", 3306)),
                "user": str(db_config.get("username", "")),
                "password": str(db_config.get("password", "")),
                "connection_timeout": 10,
            }
            # databaseëŠ” Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
            if database_name:
                connection_config["database"] = str(database_name)

        connection = mysql.connector.connect(**connection_config)
        return connection, tunnel_used

    def get_secret(self, secret_name):
        """Secrets Managerì—ì„œ DB ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            session = boto3.session.Session()
            client = session.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )
            get_secret_value_response = client.get_secret_value(SecretId=secret_name)
            if (
                not get_secret_value_response
                or "SecretString" not in get_secret_value_response
            ):
                raise Exception(f"ì‹œí¬ë¦¿ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {secret_name}")
            secret = get_secret_value_response["SecretString"]
            return json.loads(secret)
        except Exception as e:
            logger.error(f"Secret ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise e

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def list_sql_files(self) -> str:
        """SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            file_list = "\n".join([f"- {f.name}" for f in sql_files])
            return f"SQL íŒŒì¼ ëª©ë¡:\n{file_list}"
        except Exception as e:
            return f"SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def list_database_secrets(self, keyword: str = "") -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            secrets = self.get_secrets_by_keyword(keyword)
            if not secrets:
                return (
                    f"'{keyword}' í‚¤ì›Œë“œë¡œ ì°¾ì€ ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                    if keyword
                    else "ì‹œí¬ë¦¿ì´ ì—†ìŠµë‹ˆë‹¤."
                )

            secret_list = "\n".join([f"- {secret}" for secret in secrets])
            return f"ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡:\n{secret_list}"
        except Exception as e:
            return f"ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def validate_sql_file(
        self, filename: str, database_secret: Optional[str] = None
    ) -> str:
        """íŠ¹ì • SQL íŒŒì¼ ê²€ì¦ - Edge ì—°ê²° ë°©ì‹"""
        try:
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            # Edge ì—°ê²° ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ì‹¤í–‰
            result = await self.execute_validation_workflow(
                ddl_content, database_secret, filename
            )
            return result
        except Exception as e:
            return f"SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def execute_validation_workflow(
        self, ddl_content: str, database_secret: Optional[str], filename: str
    ) -> str:
        """Edge ì—°ê²° ë°©ì‹ì˜ ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # ê²€ì¦ ìƒíƒœ ì´ˆê¸°í™”
            validation_state = {
                "ddl_content": ddl_content,
                "filename": filename,
                "database_secret": database_secret,
                "current_step": 1,
                "total_steps": (
                    7 if database_secret else 4
                ),  # DB ì—°ê²° ì—¬ë¶€ì— ë”°ë¼ ë‹¨ê³„ ìˆ˜ ì¡°ì •
                "issues": [],
                "warnings": [],
                "recommendations": [],
            }

            # 1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦
            validation_state = await self.step_1_syntax_check(validation_state)
            logger.info(f"Step 1 completed, state is None: {validation_state is None}")

            # ë¬¸ë²• ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
            if not validation_state.get("syntax_valid", False):
                return await self.generate_final_report(validation_state)

            # 2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦
            validation_state = await self.step_2_standard_check(validation_state)
            logger.info(f"Step 2 completed, state is None: {validation_state is None}")

            # 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ (database_secretì´ ìˆëŠ” ê²½ìš°ë§Œ)
            if database_secret:
                validation_state = await self.step_3_db_connection_test(
                    validation_state
                )
                logger.info(
                    f"Step 3 completed, state is None: {validation_state is None}"
                )

                # ì—°ê²° ì„±ê³µì‹œ ì¶”ê°€ ê²€ì¦ ì§„í–‰
                if validation_state and validation_state.get("db_connected", False):
                    # 4ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ê²€ì¦
                    validation_state = await self.step_4_schema_validation(
                        validation_state
                    )
                    logger.info(
                        f"Step 4 completed, state is None: {validation_state is None}"
                    )

                    # 5ë‹¨ê³„: ì œì•½ì¡°ê±´ ê²€ì¦
                    validation_state = await self.step_5_constraint_validation(
                        validation_state
                    )
                    logger.info(
                        f"Step 5 completed, state is None: {validation_state is None}"
                    )

            # 6ë‹¨ê³„: Claude AI ì¢…í•© ê²€ì¦ (ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)
            if validation_state is not None:
                validation_state = await self.step_6_claude_validation(validation_state)
            else:
                logger.error("validation_state is None before step 6")

            # 7ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            result = await self.generate_final_report(validation_state)

            # ë¡œì»¬ ê²€ì¦ì¸ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
            if not database_secret:
                result += "\n\nğŸ” **ë¡œì»¬ ê²€ì¦ ì™„ë£Œ**\nâ€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—†ì´ ë¬¸ë²• ë° í‘œì¤€ ê·œì¹™ë§Œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\nâ€¢ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ì›í•˜ì‹œë©´ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ì„ ì§€ì •í•´ì£¼ì„¸ìš”."

            return result

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            return f"ê²€ì¦ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\në””ë²„ê·¸ ì •ë³´:\n{error_details}"

    async def prompt_for_database_selection(
        self, ddl_content: str, filename: str
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ í”„ë¡¬í”„íŠ¸"""
        try:
            # ë¡œì»¬ ê²€ì¦ì„ ìœ„í•´ íŒŒì¼ ì •ë³´ ì €ì¥
            self.pending_validation = {"ddl_content": ddl_content, "filename": filename}

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ
            secrets_list = await self.get_available_database_secrets()

            if not secrets_list:
                # ì‹œí¬ë¦¿ì´ ì—†ìœ¼ë©´ ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰
                return await self.execute_local_validation_only(ddl_content, filename)

            # ì‚¬ìš©ìì—ê²Œ ì„ íƒ ì˜µì…˜ ì œê³µ
            prompt_message = f"""ğŸ” **DDL ê²€ì¦ ì˜µì…˜ ì„ íƒ**

ğŸ“„ **íŒŒì¼:** {filename}

**ê²€ì¦ ë°©ì‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”:**

**1ï¸âƒ£ ì™„ì „ ê²€ì¦ (ê¶Œì¥)**
   â€¢ ë¬¸ë²• + í‘œì¤€ ê·œì¹™ + ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê²€ì¦
   â€¢ ì‹¤ì œ DB ì—°ê²°í•˜ì—¬ í…Œì´ë¸”/ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
   â€¢ ì œì•½ì¡°ê±´ ë° ì¸ë±ìŠ¤ ê²€ì¦
   â€¢ Claude AI ì¢…í•© ë¶„ì„

**2ï¸âƒ£ ë¡œì»¬ ê²€ì¦ë§Œ**
   â€¢ ë¬¸ë²• + í‘œì¤€ ê·œì¹™ ê²€ì¦
   â€¢ Claude AI ë¶„ì„
   â€¢ DB ì—°ê²° ì—†ì´ ë¹ ë¥¸ ê²€ì¦

ğŸ—„ï¸ **ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤:**
{secrets_list}

ğŸ’¡ **ì‚¬ìš©ë²•:**
   â€¢ ì™„ì „ ê²€ì¦: validate_sql_with_database ë„êµ¬ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì§€ì •
   â€¢ ë¡œì»¬ ê²€ì¦: confirm_and_execute ë„êµ¬ë¡œ 'local' ì…ë ¥"""

            return prompt_message

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰
            return await self.execute_local_validation_only(ddl_content, filename)

    async def get_available_database_secrets(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ëª¨ë“  ì‹œí¬ë¦¿ ì¡°íšŒ (í•„í„°ë§ ì—†ì´)
            secrets = self.get_secrets_by_keyword("")

            if not secrets:
                return ""

            result = ""
            for i, secret in enumerate(secrets, 1):
                result += f"   {i}. {secret}\n"

            return result

        except Exception as e:
            return ""

    async def execute_local_validation_only(
        self, ddl_content: str, filename: str
    ) -> str:
        """ë¡œì»¬ ê²€ì¦ë§Œ ìˆ˜í–‰ (DB ì—°ê²° ì—†ìŒ)"""
        try:
            # ê²€ì¦ ìƒíƒœ ì´ˆê¸°í™” (database_secret ì—†ìŒ)
            validation_state = {
                "ddl_content": ddl_content,
                "filename": filename,
                "database_secret": None,
                "current_step": 1,
                "total_steps": 4,  # ë¡œì»¬ ê²€ì¦ë§Œì´ë¯€ë¡œ 4ë‹¨ê³„
                "issues": [],
                "warnings": [],
                "recommendations": [],
            }

            # 1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦
            validation_state = await self.step_1_syntax_check(validation_state)

            # 2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦
            validation_state = await self.step_2_standard_check(validation_state)

            # 3ë‹¨ê³„: Claude AI ê²€ì¦ (DB ì •ë³´ ì—†ì´)
            validation_state = await self.step_6_claude_validation(validation_state)

            # 4ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            result = await self.generate_final_report(validation_state)

            # ë¡œì»¬ ê²€ì¦ì„ì„ ëª…ì‹œ
            local_notice = "\n\nğŸ” **ë¡œì»¬ ê²€ì¦ ì™„ë£Œ**\nâ€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—†ì´ ë¬¸ë²• ë° í‘œì¤€ ê·œì¹™ë§Œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\nâ€¢ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ì›í•˜ì‹œë©´ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ì„ ì§€ì •í•´ì£¼ì„¸ìš”."

            return result + local_notice

        except Exception as e:
            return f"ë¡œì»¬ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def step_1_syntax_check(self, state: dict) -> dict:
        """1ë‹¨ê³„: ë¬¸ë²• ê²€ì¦"""
        ddl_content = state["ddl_content"]
        issues = []

        # DDL íƒ€ì… ì¶”ì¶œ
        ddl_type = self.extract_ddl_type(ddl_content)
        state["ddl_type"] = ddl_type

        # ê¸°ë³¸ ë¬¸ë²• ê²€ì¦
        if not ddl_content.strip().endswith(";"):
            issues.append("ì„¸ë¯¸ì½œë¡ ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

        state["syntax_valid"] = len(issues) == 0
        state["syntax_issues"] = issues
        state["current_step"] = 2

        return state

    async def step_2_standard_check(self, state: dict) -> dict:
        """2ë‹¨ê³„: í‘œì¤€ ê·œì¹™ ê²€ì¦"""
        # í˜„ì¬ëŠ” ê¸°ë³¸ êµ¬í˜„ë§Œ ì œê³µ
        state["standard_compliant"] = True
        state["standard_issues"] = []
        state["current_step"] = 3
        return state

    async def step_3_db_connection_test(self, state: dict) -> dict:
        """3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        database_secret = state["database_secret"]

        try:
            connection_result = await self.test_database_connection(database_secret)
            state["db_connected"] = connection_result["success"]
            state["db_connection_info"] = connection_result

            if connection_result["success"]:
                state["warnings"].append("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            else:
                state["issues"].append(
                    f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {connection_result.get('error', 'Unknown error')}"
                )

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            state["db_connected"] = False
            state["issues"].append(
                f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}\në””ë²„ê·¸: {error_details}"
            )

        state["current_step"] = 4
        return state

    async def step_4_schema_validation(self, state: dict) -> dict:
        """4ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        try:
            logger.info("ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")
            schema_result = await self.validate_schema(
                state["ddl_content"], state["database_secret"]
            )
            logger.info(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼: {schema_result}")

            if schema_result["success"]:
                schema_issues = []
                for result in schema_result["validation_results"]:
                    if result.get("issues"):
                        schema_issues.extend(result["issues"])

                state["schema_issues"] = schema_issues
                if schema_issues:
                    state["issues"].extend(schema_issues)
                else:
                    state["warnings"].append("âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ í†µê³¼")
                # ì„±ê³µí•œ ê²½ìš° ëª¨ë“  ìŠ¤í‚¤ë§ˆ ê´€ë ¨ ì˜¤ë¥˜ ì™„ì „ ì œê±°
                filtered_issues = []
                for issue in state["issues"]:
                    if not ("ìŠ¤í‚¤ë§ˆ ê²€ì¦" in issue or "argument 7" in issue):
                        filtered_issues.append(issue)
                state["issues"] = filtered_issues
            else:
                error_msg = (
                    f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {schema_result.get('error', 'Unknown error')}"
                )
                logger.error(error_msg)
                state["issues"].append(error_msg)

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            error_msg = f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"{error_msg}\n{error_details}")
            state["issues"].append(error_msg)

        state["current_step"] = 5
        return state

    async def step_5_constraint_validation(self, state: dict) -> dict:
        """5ë‹¨ê³„: ì œì•½ì¡°ê±´ ê²€ì¦"""
        try:
            logger.info("ì œì•½ì¡°ê±´ ê²€ì¦ ì‹œì‘")
            constraint_result = await self.validate_constraints(
                state["ddl_content"], state["database_secret"]
            )
            logger.info(f"ì œì•½ì¡°ê±´ ê²€ì¦ ê²°ê³¼: {constraint_result}")

            if constraint_result["success"]:
                constraint_issues = []
                for result in constraint_result["constraint_results"]:
                    if not result.get("valid", True):
                        constraint_issues.append(result.get("issue", "ì œì•½ì¡°ê±´ ìœ„ë°˜"))

                state["constraint_issues"] = constraint_issues
                if constraint_issues:
                    state["issues"].extend(constraint_issues)
                else:
                    state["warnings"].append("âœ… ì œì•½ì¡°ê±´ ê²€ì¦ í†µê³¼")
                # ì„±ê³µí•œ ê²½ìš° ëª¨ë“  ì œì•½ì¡°ê±´ ê´€ë ¨ ì˜¤ë¥˜ ì™„ì „ ì œê±°
                filtered_issues = []
                for issue in state["issues"]:
                    if not ("ì œì•½ì¡°ê±´ ê²€ì¦" in issue or "argument 7" in issue):
                        filtered_issues.append(issue)
                state["issues"] = filtered_issues
            else:
                error_msg = f"ì œì•½ì¡°ê±´ ê²€ì¦ ì‹¤íŒ¨: {constraint_result.get('error', 'Unknown error')}"
                logger.error(error_msg)
                state["issues"].append(error_msg)

        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            error_msg = f"ì œì•½ì¡°ê±´ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"{error_msg}\n{error_details}")
            state["issues"].append(error_msg)

        state["current_step"] = 6
        return state

    async def generate_html_report(
        self,
        report_path: Path,
        filename: str,
        ddl_content: str,
        ddl_type: str,
        status: str,
        summary: str,
        issues: List[str],
        db_connection_info: Optional[Dict],
        schema_validation: Optional[Dict],
        constraint_validation: Optional[Dict],
        database_secret: Optional[str],
    ):
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
            status_color = "#28a745" if status == "PASS" else "#dc3545"
            status_icon = "âœ…" if status == "PASS" else "âŒ"

            # DB ì—°ê²° ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­ì— ë”°ë¼)
            db_info_section = ""

            # ë°œê²¬ëœ ë¬¸ì œ ì„¹ì…˜ - Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
            claude_issues = []
            other_issues = []

            for issue in issues:
                if issue.startswith("Claude ê²€ì¦:"):
                    claude_issues.append(issue[12:].strip())  # "Claude ê²€ì¦:" ì œê±°
                else:
                    other_issues.append(issue)

            # ê¸°íƒ€ ê²€ì¦ ë¬¸ì œ ì„¹ì…˜
            other_issues_section = ""
            if other_issues:
                other_issues_section = """
                <div class="issues-section">
                    <h3>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h3>
                    <ul class="issues-list">
                """
                for issue in other_issues:
                    other_issues_section += f"<li>{issue}</li>"
                other_issues_section += """
                    </ul>
                </div>
                """

            # Claude ê²€ì¦ ê²°ê³¼ ì„¹ì…˜
            claude_section = ""
            if claude_issues:
                claude_section = """
                <div class="claude-section">
                    <h3>ğŸ¤– Claude AI ê²€ì¦ ê²°ê³¼</h3>
                """
                for claude_result in claude_issues:
                    # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìŠ¤íƒ€ì¼ ì ìš©
                    claude_section += f"""
                    <div class="claude-result">
                        <pre class="claude-text">{claude_result}</pre>
                    </div>
                    """
                claude_section += """
                </div>
                """

            # ì „ì²´ ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš°
            success_section = ""
            if not issues:
                success_section = """
                <div class="issues-section success">
                    <h3>âœ… ê²€ì¦ ê²°ê³¼</h3>
                    <p class="no-issues">ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</p>
                </div>
                """

            # HTML ë³´ê³ ì„œ ë‚´ìš©
            report_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DDL ê²€ì¦ ë³´ê³ ì„œ - {filename}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .status-badge {{
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            margin-top: 10px;
            background-color: {status_color};
        }}
        .content {{
            padding: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }}
        .summary-item h4 {{
            margin: 0 0 10px 0;
            color: #333;
        }}
        .summary-item p {{
            margin: 0;
            font-size: 1.1em;
            font-weight: 500;
        }}
        .info-section, .issues-section {{
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }}
        .info-section h3, .issues-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }}
        .info-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .info-table td {{
            padding: 10px;
            border-bottom: 1px solid #e9ecef;
        }}
        .info-table td:first-child {{
            width: 150px;
            background: #f8f9fa;
        }}
        .issues-list {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .issues-list li {{
            margin: 5px 0;
        }}
        .status-success {{
            color: #28a745;
            font-weight: bold;
        }}
        .status-error {{
            color: #dc3545;
            font-weight: bold;
        }}
        .no-issues {{
            color: #28a745;
            font-weight: 500;
        }}
        .issues-section.success {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .sql-code {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: none;
        }}
        .claude-section {{
            margin: 30px 0;
            padding: 25px;
            border-radius: 8px;
            border: 2px solid #667eea;
            background: #f8f9ff;
            box-shadow: 0 2px 10px rgba(102, 126, 234, 0.1);
        }}
        .claude-section h3 {{
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            font-size: 1.3em;
        }}
        .claude-result {{
            margin: 20px 0;
            padding: 0;
            background: white;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            box-shadow: 0 1px 5px rgba(0,0,0,0.05);
        }}
        .claude-text {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
            max-height: 800px;  /* 400pxì—ì„œ 800pxë¡œ ì¦ê°€ */
            overflow-y: auto;
            min-height: 100px;
            resize: vertical;  /* ì‚¬ìš©ìê°€ ìˆ˜ì§ìœ¼ë¡œ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥ */
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                margin: 10px;
            }}
            body {{
                padding: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{status_icon} DDL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <div class="status-badge">{status}</div>
        </div>
        
        <div class="content">
            <div class="summary-grid">
                <div class="summary-item">
                    <h4>ğŸ“„ íŒŒì¼ëª…</h4>
                    <p>{filename}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ•’ ê²€ì¦ ì¼ì‹œ</h4>
                    <p>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ”§ DDL íƒ€ì…</h4>
                    <p>{ddl_type}</p>
                </div>
                <div class="summary-item">
                    <h4>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤</h4>
                    <p>{database_secret or 'N/A'}</p>
                </div>
            </div>
            
            {db_info_section}
            
            <div class="info-section">
                <h3>ğŸ“ ì›ë³¸ DDL</h3>
                <div class="sql-code">{ddl_content}</div>
            </div>
            
            <div class="info-section">
                <h3>ğŸ“Š ê²€ì¦ ê²°ê³¼</h3>
                <p style="font-size: 1.2em; font-weight: 500; color: {status_color};">{summary}</p>
            </div>
            
            {claude_section}
            {other_issues_section}
            {success_section}
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

        except Exception as e:
            logger.error(f"HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")

    async def step_6_claude_validation(self, state: dict) -> dict:
        """6ë‹¨ê³„: Claude AI ì¢…í•© ê²€ì¦ (ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)"""
        # stateê°€ Noneì¸ ê²½ìš° ë°©ì–´ ì½”ë“œ
        if state is None:
            logger.error("step_6_claude_validation: state is None")
            return {
                "current_step": 7,
                "issues": ["ì´ì „ ë‹¨ê³„ì—ì„œ ìƒíƒœ ì •ë³´ê°€ ì†ì‹¤ë˜ì—ˆìŠµë‹ˆë‹¤."],
                "warnings": [],
                "claude_issues": [],
            }

        try:
            logger.info("Claude ê²€ì¦ ì‹œì‘")

            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
            existing_analysis = {
                "syntax_issues": state.get("syntax_issues", []),
                "schema_issues": state.get("schema_issues", []),
                "constraint_issues": state.get("constraint_issues", []),
                "db_connection_info": state.get("db_connection_info", {}),
            }

            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ (DB ì—°ê²°ì´ ì„±ê³µí•œ ê²½ìš°)
            schema_info = None
            if state.get("db_connected", False) and state.get("database_secret"):
                try:
                    schema_info = await self.extract_current_schema_info(
                        state["database_secret"]
                    )
                except Exception as e:
                    logger.warning(f"Claude ê²€ì¦ìš© ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            logger.info("Claude ê²€ì¦ ì‹¤í–‰ ì¤‘...")
            # Claude ê²€ì¦ ì‹¤í–‰
            claude_result = await self.validate_with_claude(
                state["ddl_content"],
                state.get("database_secret"),
                schema_info,
                existing_analysis,
            )
            logger.info(f"Claude ê²€ì¦ ê²°ê³¼ (ì „ì²´): {claude_result}")
            print(f"ğŸ¤– Claude AI ê²€ì¦ ê²°ê³¼:\n{claude_result}\n" + "=" * 50)

            # Claude ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            claude_issues = []
            if (
                "ë¬¸ì œ" in claude_result
                or "ì˜¤ë¥˜" in claude_result
                or "ìœ„ë°˜" in claude_result
            ):
                # "ê²€ì¦ í†µê³¼"ê°€ ì•„ë‹Œ ê²½ìš° ì´ìŠˆë¡œ ì²˜ë¦¬
                if "ê²€ì¦ í†µê³¼" not in claude_result:
                    claude_issues.append(f"Claude ê²€ì¦: {claude_result}")

            state["claude_issues"] = claude_issues
            if claude_issues:
                state["issues"].extend(claude_issues)
            else:
                state["warnings"].append("âœ… Claude AI ê²€ì¦ í†µê³¼")

            logger.info("Claude ê²€ì¦ ì™„ë£Œ")

        except Exception as e:
            import traceback

            error_trace = traceback.format_exc()
            logger.error(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_trace}")
            state["issues"].append(f"Claude ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        state["current_step"] = 7
        return state

    async def generate_final_report(self, state: dict) -> str:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        filename = state["filename"]
        ddl_content = state["ddl_content"]
        ddl_type = state.get("ddl_type", "UNKNOWN")

        # ëª¨ë“  ì´ìŠˆ ìˆ˜ì§‘ - ì„±ê³µí•œ ê²€ì¦ì˜ ì˜¤ë¥˜ëŠ” ì œì™¸
        all_issues = []
        all_issues.extend(state.get("syntax_issues", []))

        # MySQL ì—°ê²° ì˜¤ë¥˜ ì™„ì „ ì œê±°
        clean_issues = []
        for issue in state.get("issues", []):
            issue_str = str(issue)
            # MySQL ê´€ë ¨ ì˜¤ë¥˜ ëª¨ë‘ ì œê±°
            skip_keywords = [
                "argument 7 must be str or None, not bool",
                "ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: argument 7",
                "ì œì•½ì¡°ê±´ ê²€ì¦ ì‹¤íŒ¨: ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: argument 7",
            ]
            if any(keyword in issue_str for keyword in skip_keywords):
                continue
            clean_issues.append(issue)

        all_issues.extend(clean_issues)

        # ìƒíƒœ ê²°ì •
        if not state.get("syntax_valid", False):
            summary = f"âŒ ë¬¸ë²• ì˜¤ë¥˜ë¡œ ì¸í•œ ê²€ì¦ ì‹¤íŒ¨: {len(state.get('syntax_issues', []))}ê°œ"
            status = "FAIL"
        elif len(all_issues) == 0:
            summary = "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤."
            status = "PASS"
        else:
            summary = f"âŒ ë°œê²¬ëœ ë¬¸ì œ: {len(all_issues)}ê°œ"
            status = "FAIL"

        # HTML ë³´ê³ ì„œ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = OUTPUT_DIR / f"validation_report_{filename}_{timestamp}.html"

        # ìƒˆë¡œìš´ generate_html_report í•¨ìˆ˜ ì‚¬ìš©
        await self.generate_html_report(
            report_path=report_path,
            filename=filename,
            ddl_content=ddl_content,
            ddl_type=ddl_type,
            status=status,
            summary=summary,
            issues=all_issues,
            db_connection_info=state.get("db_connection_info"),
            schema_validation=state.get("schema_validation"),
            constraint_validation=state.get("constraint_validation"),
            database_secret=state.get("database_secret"),
        )

        # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
        result_message = f"""
{summary}

ğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}

ğŸ“Š ê²€ì¦ ê²°ê³¼:
"""

        # ê° ë‹¨ê³„ë³„ ê²°ê³¼ ì¶”ê°€
        if state.get("syntax_issues"):
            result_message += f"â€¢ ë¬¸ë²• ê²€ì¦: âŒ {len(state['syntax_issues'])}ê°œ ë¬¸ì œ\n"
        else:
            result_message += "â€¢ ë¬¸ë²• ê²€ì¦: âœ… í†µê³¼\n"

        if state.get("db_connected"):
            result_message += "â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ… ì„±ê³µ\n"

            if state.get("schema_issues"):
                result_message += (
                    f"â€¢ ìŠ¤í‚¤ë§ˆ ê²€ì¦: âŒ {len(state['schema_issues'])}ê°œ ë¬¸ì œ\n"
                )
            else:
                result_message += "â€¢ ìŠ¤í‚¤ë§ˆ ê²€ì¦: âœ… í†µê³¼\n"

            if state.get("constraint_issues"):
                result_message += (
                    f"â€¢ ì œì•½ì¡°ê±´ ê²€ì¦: âŒ {len(state['constraint_issues'])}ê°œ ë¬¸ì œ\n"
                )
            else:
                result_message += "â€¢ ì œì•½ì¡°ê±´ ê²€ì¦: âœ… í†µê³¼\n"
        elif state.get("database_secret"):
            result_message += "â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âŒ ì‹¤íŒ¨\n"

        if state.get("claude_issues"):
            result_message += (
                f"â€¢ Claude AI ê²€ì¦: âŒ {len(state['claude_issues'])}ê°œ ë¬¸ì œ\n"
            )
        else:
            result_message += "â€¢ Claude AI ê²€ì¦: âœ… í†µê³¼\n"

        return result_message

    def extract_ddl_type(self, ddl_content: str) -> str:
        """DDL íƒ€ì… ì¶”ì¶œ"""
        ddl_upper = ddl_content.upper().strip()
        if ddl_upper.startswith("CREATE TABLE"):
            return "CREATE_TABLE"
        elif ddl_upper.startswith("ALTER TABLE"):
            return "ALTER_TABLE"
        elif ddl_upper.startswith("CREATE INDEX"):
            return "CREATE_INDEX"
        elif ddl_upper.startswith("DROP"):
            return "DROP"
        else:
            return "UNKNOWN"

    def setup_ssh_tunnel(self, db_host: str, region: str = "ap-northeast-2") -> bool:
        """SSH í„°ë„ ì„¤ì •"""
        try:
            import subprocess
            import time

            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ
            subprocess.run(["pkill", "-f", "ssh.*54.180.79.255"], capture_output=True)

            # SSH í„°ë„ ì‹œì‘ (ssh_tunnel.sh ë°©ì‹ ì‚¬ìš©, SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ)
            ssh_command = [
                "ssh",
                "-F",
                "/dev/null",  # SSH ì„¤ì • íŒŒì¼ ë¬´ì‹œ
                "-o",
                "UserKnownHostsFile=/dev/null",
                "-o",
                "StrictHostKeyChecking=no",
                "-i",
                "/Users/heungh/test.pem",
                "-f",
                "-N",
                "-L",
                f"3307:{db_host}:3306",
                "ec2-user@54.180.79.255",
            ]

            logger.info(f"SSH í„°ë„ ì„¤ì • ì¤‘: {db_host} -> localhost:3307")

            process = subprocess.run(ssh_command, capture_output=True, text=True)

            # í„°ë„ì´ ì„¤ì •ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)

            if process.returncode == 0:
                logger.info("SSH í„°ë„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"SSH í„°ë„ ì„¤ì • ì‹¤íŒ¨: {process.stderr}")
                return False

        except Exception as e:
            logger.error(f"SSH í„°ë„ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False

    def get_secrets_by_keyword(self, keyword=""):
        """í‚¤ì›Œë“œë¡œ Secret ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            secrets_manager = boto3.client(
                service_name="secretsmanager",
                region_name="ap-northeast-2",
                verify=False,
            )

            all_secrets = []
            next_token = None

            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            while True:
                if next_token:
                    response = secrets_manager.list_secrets(NextToken=next_token)
                else:
                    response = secrets_manager.list_secrets()

                all_secrets.extend(
                    [secret["Name"] for secret in response["SecretList"]]
                )

                if "NextToken" not in response:
                    break
                next_token = response["NextToken"]

            # í‚¤ì›Œë“œ í•„í„°ë§
            if keyword:
                filtered_secrets = [
                    secret
                    for secret in all_secrets
                    if keyword.lower() in secret.lower()
                ]
                return filtered_secrets
            else:
                return all_secrets
        except Exception as e:
            logger.error(f"Secret ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def test_database_connection(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )

            if connection.is_connected():
                db_info = connection.get_server_info()
                cursor = connection.cursor()
                cursor.execute("SELECT DATABASE()")
                current_db_result = cursor.fetchone()
                current_db = (
                    current_db_result[0]
                    if current_db_result and current_db_result[0]
                    else "None"
                )

                # SHOW DATABASES ì‹¤í–‰
                cursor.execute("SHOW DATABASES")
                databases = [db[0] for db in cursor.fetchall()]

                # í˜„ì¬ DBì˜ í…Œì´ë¸” ëª©ë¡
                tables = []
                if current_db:
                    cursor.execute("SHOW TABLES")
                    tables = [table[0] for table in cursor.fetchall()]

                cursor.close()
                connection.close()

                result = {
                    "success": True,
                    "server_version": db_info,
                    "current_database": current_db,
                    "connection_method": "SSH Tunnel" if tunnel_used else "Direct",
                    "databases": databases,
                    "tables": tables,
                }

                # SSH í„°ë„ ì •ë¦¬
                if tunnel_used:
                    self.cleanup_ssh_tunnel()

                return result
            else:
                if tunnel_used:
                    self.cleanup_ssh_tunnel()
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        except MySQLError as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"MySQL ì˜¤ë¥˜: {str(e)}"}
        except Exception as e:
            import traceback

            error_details = traceback.format_exc()
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {
                "success": False,
                "error": f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}",
                "debug": error_details,
            }

    async def validate_schema(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL êµ¬ë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        try:
            # DDL êµ¬ë¬¸ ìœ í˜• ë° ìƒì„¸ ì •ë³´ íŒŒì‹±
            ddl_info = self.parse_ddl_detailed(ddl_content)
            if not ddl_info:
                return {
                    "success": False,
                    "error": "DDLì—ì„œ êµ¬ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }

            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            validation_results = []

            # DDL êµ¬ë¬¸ ìœ í˜•ë³„ ê²€ì¦
            for ddl_statement in ddl_info:
                ddl_type = ddl_statement["type"]
                table_name = ddl_statement["table"]

                if ddl_type == "CREATE_TABLE":
                    result = await self.validate_create_table(cursor, ddl_statement)
                elif ddl_type == "ALTER_TABLE":
                    result = await self.validate_alter_table(cursor, ddl_statement)
                elif ddl_type == "CREATE_INDEX":
                    result = await self.validate_create_index(cursor, ddl_statement)
                elif ddl_type == "DROP_TABLE":
                    result = await self.validate_drop_table(cursor, ddl_statement)
                elif ddl_type == "DROP_INDEX":
                    result = await self.validate_drop_index(cursor, ddl_statement)
                else:
                    result = {
                        "table": table_name,
                        "ddl_type": ddl_type,
                        "valid": False,
                        "issues": [f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DDL êµ¬ë¬¸ ìœ í˜•: {ddl_type}"],
                    }

                validation_results.append(result)

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "validation_results": validation_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    async def validate_constraints(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """ì œì•½ì¡°ê±´ ê²€ì¦ - FK, ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´ í™•ì¸"""
        try:
            # DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ
            constraints_info = self.parse_ddl_constraints(ddl_content)

            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            constraint_results = []

            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ê²€ì¦
            if constraints_info.get("foreign_keys"):
                for fk in constraints_info["foreign_keys"]:
                    # ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    cursor.execute(
                        """
                        SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() AND table_name = %s
                    """,
                        (fk["referenced_table"],),
                    )

                    ref_table_exists = cursor.fetchone()[0] > 0

                    if ref_table_exists:
                        # ì°¸ì¡° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        cursor.execute(
                            """
                            SELECT COUNT(*) FROM information_schema.columns 
                            WHERE table_schema = DATABASE() 
                            AND table_name = %s AND column_name = %s
                        """,
                            (fk["referenced_table"], fk["referenced_column"]),
                        )

                        ref_column_exists = cursor.fetchone()[0] > 0

                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": ref_column_exists,
                                "issue": (
                                    None
                                    if ref_column_exists
                                    else f"ì°¸ì¡° ì»¬ëŸ¼ '{fk['referenced_table']}.{fk['referenced_column']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                                ),
                            }
                        )
                    else:
                        constraint_results.append(
                            {
                                "type": "FOREIGN_KEY",
                                "constraint": f"{fk['column']} -> {fk['referenced_table']}.{fk['referenced_column']}",
                                "valid": False,
                                "issue": f"ì°¸ì¡° í…Œì´ë¸” '{fk['referenced_table']}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                            }
                        )

            cursor.close()
            connection.close()

            # SSH í„°ë„ ì •ë¦¬
            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "constraint_results": constraint_results}

        except Exception as e:
            if use_ssh_tunnel:
                self.cleanup_ssh_tunnel()
            return {"success": False, "error": f"ì œì•½ì¡°ê±´ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"}

    def parse_ddl_detailed(self, ddl_content: str) -> List[Dict[str, Any]]:
        """DDL êµ¬ë¬¸ì„ ìƒì„¸í•˜ê²Œ íŒŒì‹±í•˜ì—¬ êµ¬ë¬¸ ìœ í˜•ë³„ ì •ë³´ ì¶”ì¶œ"""
        ddl_statements = []

        # CREATE TABLE íŒŒì‹±
        create_table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?\s*\((.*?)\)(?:\s*ENGINE\s*=\s*\w+)?(?:\s*COMMENT\s*=\s*[\'"][^\'"]*[\'"])?'
        create_matches = re.findall(
            create_table_pattern, ddl_content, re.DOTALL | re.IGNORECASE
        )

        for table_name, columns_def in create_matches:
            columns_info = self.parse_create_table_columns(columns_def)
            ddl_statements.append(
                {
                    "type": "CREATE_TABLE",
                    "table": table_name.lower(),
                    "columns": columns_info["columns"],
                    "constraints": columns_info["constraints"],
                }
            )

        # ALTER TABLE íŒŒì‹±
        alter_patterns = [
            # ADD COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+ADD\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "ADD_COLUMN",
            ),
            # DROP COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+DROP\s+(?:COLUMN\s+)?`?(\w+)`?",
                "DROP_COLUMN",
            ),
            # MODIFY COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+MODIFY\s+(?:COLUMN\s+)?`?(\w+)`?\s+([^,;]+)",
                "MODIFY_COLUMN",
            ),
            # CHANGE COLUMN
            (
                r"ALTER\s+TABLE\s+`?(\w+)`?\s+CHANGE\s+(?:COLUMN\s+)?`?(\w+)`?\s+`?(\w+)`?\s+([^,;]+)",
                "CHANGE_COLUMN",
            ),
        ]

        for pattern, alter_type in alter_patterns:
            matches = re.findall(pattern, ddl_content, re.IGNORECASE)
            for match in matches:
                if alter_type == "CHANGE_COLUMN":
                    table_name, old_column, new_column, column_def = match
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "old_column": old_column.lower(),
                            "new_column": new_column.lower(),
                            "column_definition": column_def.strip(),
                        }
                    )
                else:
                    table_name, column_name = match[:2]
                    column_def = match[2] if len(match) > 2 else None
                    ddl_statements.append(
                        {
                            "type": "ALTER_TABLE",
                            "table": table_name.lower(),
                            "alter_type": alter_type,
                            "column": column_name.lower(),
                            "column_definition": (
                                column_def.strip() if column_def else None
                            ),
                        }
                    )

        # CREATE INDEX íŒŒì‹±
        create_index_pattern = (
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\(([^)]+)\)"
        )
        index_matches = re.findall(create_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name, columns in index_matches:
            ddl_statements.append(
                {
                    "type": "CREATE_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                    "columns": [
                        col.strip().strip("`").lower() for col in columns.split(",")
                    ],
                }
            )

        # DROP TABLE íŒŒì‹±
        drop_table_pattern = r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?"
        drop_table_matches = re.findall(drop_table_pattern, ddl_content, re.IGNORECASE)

        for table_name in drop_table_matches:
            ddl_statements.append({"type": "DROP_TABLE", "table": table_name.lower()})

        # DROP INDEX íŒŒì‹±
        drop_index_pattern = r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?"
        drop_index_matches = re.findall(drop_index_pattern, ddl_content, re.IGNORECASE)

        for index_name, table_name in drop_index_matches:
            ddl_statements.append(
                {
                    "type": "DROP_INDEX",
                    "table": table_name.lower(),
                    "index_name": index_name.lower(),
                }
            )

        return ddl_statements

    def parse_create_table_columns(self, columns_def: str) -> Dict[str, Any]:
        """CREATE TABLEì˜ ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±"""
        columns = []
        constraints = []

        # ì»¬ëŸ¼ ì •ì˜ì™€ ì œì•½ì¡°ê±´ì„ ë¶„ë¦¬
        lines = [line.strip() for line in columns_def.split(",")]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì œì•½ì¡°ê±´ í™•ì¸
            if re.match(
                r"(?:CONSTRAINT|PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY)",
                line,
                re.IGNORECASE,
            ):
                constraints.append(line)
            else:
                # ì»¬ëŸ¼ ì •ì˜ íŒŒì‹±
                column_match = re.match(
                    r"`?(\w+)`?\s+([^,\s]+)(?:\s+(.*))?", line, re.IGNORECASE
                )
                if column_match:
                    column_name = column_match.group(1).lower()
                    data_type = column_match.group(2).upper()
                    attributes = column_match.group(3) or ""

                    columns.append(
                        {
                            "name": column_name,
                            "data_type": data_type,
                            "attributes": attributes.strip(),
                        }
                    )

        return {"columns": columns, "constraints": constraints}

    def parse_data_type(self, data_type_str: str) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì…ê³¼ ê¸¸ì´ ì •ë³´ ì¶”ì¶œ"""
        # VARCHAR(255), INT(11), DECIMAL(10,2) ë“±ì„ íŒŒì‹±
        type_match = re.match(r"(\w+)(?:\(([^)]+)\))?", data_type_str.upper())
        if not type_match:
            return {
                "type": data_type_str.upper(),
                "length": None,
                "precision": None,
                "scale": None,
            }

        base_type = type_match.group(1)
        params = type_match.group(2)

        result = {"type": base_type, "length": None, "precision": None, "scale": None}

        if params:
            if "," in params:
                # DECIMAL(10,2) í˜•íƒœ
                parts = [p.strip() for p in params.split(",")]
                result["precision"] = int(parts[0]) if parts[0].isdigit() else None
                result["scale"] = (
                    int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else None
                )
            else:
                # VARCHAR(255), INT(11) í˜•íƒœ
                result["length"] = int(params) if params.isdigit() else None

        return result

    async def validate_create_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        columns = ddl_statement["columns"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "CREATE_TABLE",
            "valid": not table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists, "columns_count": len(columns)},
        }

    async def validate_alter_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ALTER TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        alter_type = ddl_statement["alter_type"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {
                "table": table_name,
                "ddl_type": "ALTER_TABLE",
                "alter_type": alter_type,
                "valid": False,
                "issues": issues,
            }

        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT column_name, data_type, character_maximum_length, numeric_precision, numeric_scale, is_nullable
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        existing_columns = {
            row[0].lower(): {
                "data_type": row[1].upper(),
                "max_length": row[2],
                "precision": row[3],
                "scale": row[4],
                "is_nullable": row[5],
            }
            for row in cursor.fetchall()
        }

        # ALTER ìœ í˜•ë³„ ê²€ì¦
        if alter_type == "ADD_COLUMN":
            column_name = ddl_statement["column"]
            if column_name in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        elif alter_type == "DROP_COLUMN":
            column_name = ddl_statement["column"]
            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        elif alter_type == "MODIFY_COLUMN":
            column_name = ddl_statement["column"]
            new_definition = ddl_statement["column_definition"]

            if column_name not in existing_columns:
                issues.append(f"ì»¬ëŸ¼ '{column_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[column_name], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        elif alter_type == "CHANGE_COLUMN":
            old_column = ddl_statement["old_column"]
            new_column = ddl_statement["new_column"]
            new_definition = ddl_statement["column_definition"]

            if old_column not in existing_columns:
                issues.append(f"ê¸°ì¡´ ì»¬ëŸ¼ '{old_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif new_column != old_column and new_column in existing_columns:
                issues.append(f"ìƒˆ ì»¬ëŸ¼ëª… '{new_column}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                # ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦
                validation_result = self.validate_column_type_change(
                    existing_columns[old_column], new_definition
                )
                if not validation_result["valid"]:
                    issues.extend(validation_result["issues"])

        return {
            "table": table_name,
            "ddl_type": "ALTER_TABLE",
            "alter_type": alter_type,
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"existing_columns": list(existing_columns.keys())},
        }

    async def validate_create_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """CREATE INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]
        columns = ddl_statement["columns"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if index_exists:
                issues.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT column_name FROM information_schema.columns 
                WHERE table_schema = DATABASE() AND table_name = %s
            """,
                (table_name,),
            )

            existing_columns = {row[0].lower() for row in cursor.fetchall()}

            for column in columns:
                if column not in existing_columns:
                    issues.append(
                        f"ì»¬ëŸ¼ '{column}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return {
            "table": table_name,
            "ddl_type": "CREATE_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name, "columns": columns},
        }

    async def validate_drop_table(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP TABLE êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0
        issues = []

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return {
            "table": table_name,
            "ddl_type": "DROP_TABLE",
            "valid": table_exists,
            "issues": issues,
            "details": {"table_exists": table_exists},
        }

    async def validate_drop_index(
        self, cursor, ddl_statement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """DROP INDEX êµ¬ë¬¸ ê²€ì¦"""
        table_name = ddl_statement["table"]
        index_name = ddl_statement["index_name"]

        issues = []

        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(
            """
            SELECT COUNT(*) FROM information_schema.tables 
            WHERE table_schema = DATABASE() AND table_name = %s
        """,
            (table_name,),
        )

        table_exists = cursor.fetchone()[0] > 0

        if not table_exists:
            issues.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            cursor.execute(
                """
                SELECT COUNT(*) FROM information_schema.statistics 
                WHERE table_schema = DATABASE() AND table_name = %s AND index_name = %s
            """,
                (table_name, index_name),
            )

            index_exists = cursor.fetchone()[0] > 0

            if not index_exists:
                issues.append(
                    f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )

        return {
            "table": table_name,
            "ddl_type": "DROP_INDEX",
            "valid": len(issues) == 0,
            "issues": issues,
            "details": {"index_name": index_name},
        }

    def validate_column_type_change(
        self, existing_column: Dict[str, Any], new_definition: str
    ) -> Dict[str, Any]:
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë³€ê²½ ê°€ëŠ¥ì„± ê²€ì¦"""
        issues = []

        # ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì… íŒŒì‹±
        new_type_info = self.parse_data_type(new_definition.split()[0])
        existing_type = existing_column["data_type"]

        # í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê²€ì‚¬
        incompatible_changes = [
            # ë¬¸ìì—´ -> ìˆ«ì
            (
                ["VARCHAR", "CHAR", "TEXT"],
                ["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"],
            ),
            # ìˆ«ì -> ë¬¸ìì—´ (ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•˜ì§€ë§Œ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥)
            (["INT", "BIGINT", "DECIMAL", "FLOAT", "DOUBLE"], ["VARCHAR", "CHAR"]),
            # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ë³€ê²½
            (["DATE", "DATETIME", "TIMESTAMP"], ["INT", "VARCHAR", "CHAR"]),
        ]

        for from_types, to_types in incompatible_changes:
            if existing_type in from_types and new_type_info["type"] in to_types:
                issues.append(
                    f"ë°ì´í„° íƒ€ì…ì„ {existing_type}ì—ì„œ {new_type_info['type']}ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ê¸¸ì´ ì¶•ì†Œ ê²€ì‚¬
        if existing_type in ["VARCHAR", "CHAR"] and new_type_info["type"] in [
            "VARCHAR",
            "CHAR",
        ]:
            existing_length = existing_column["max_length"]
            new_length = new_type_info["length"]

            if existing_length and new_length and new_length < existing_length:
                issues.append(
                    f"ì»¬ëŸ¼ ê¸¸ì´ë¥¼ {existing_length}ì—ì„œ {new_length}ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        # ì •ë°€ë„ ì¶•ì†Œ ê²€ì‚¬ (DECIMAL)
        if existing_type == "DECIMAL" and new_type_info["type"] == "DECIMAL":
            existing_precision = existing_column["precision"]
            existing_scale = existing_column["scale"]
            new_precision = new_type_info["precision"]
            new_scale = new_type_info["scale"]

            if (
                existing_precision
                and new_precision
                and new_precision < existing_precision
            ) or (existing_scale and new_scale and new_scale < existing_scale):
                issues.append(
                    f"DECIMAL ì •ë°€ë„ë¥¼ ({existing_precision},{existing_scale})ì—ì„œ ({new_precision},{new_scale})ë¡œ ì¶•ì†Œí•˜ëŠ” ê²ƒì€ ë°ì´í„° ì†ì‹¤ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )

        return {"valid": len(issues) == 0, "issues": issues}

    def parse_ddl_constraints(self, ddl_content: str) -> Dict[str, List[Dict]]:
        """DDLì—ì„œ ì œì•½ì¡°ê±´ ì •ë³´ ì¶”ì¶œ"""
        constraints = {"foreign_keys": [], "indexes": [], "primary_keys": []}

        # ì™¸ë˜í‚¤ íŒ¨í„´ ë§¤ì¹­
        fk_pattern = (
            r"FOREIGN\s+KEY\s*\(`?(\w+)`?\)\s*REFERENCES\s+`?(\w+)`?\s*\(`?(\w+)`?\)"
        )
        fk_matches = re.findall(fk_pattern, ddl_content, re.IGNORECASE)

        for column, ref_table, ref_column in fk_matches:
            constraints["foreign_keys"].append(
                {
                    "column": column,
                    "referenced_table": ref_table,
                    "referenced_column": ref_column,
                }
            )

        return constraints

    async def analyze_current_schema(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìƒì„¸ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            schema_analysis = {
                "current_database": current_db,
                "tables": {},
                "indexes": {},
                "foreign_keys": {},
                "constraints": {},
            }

            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT table_name, table_type, engine, table_rows, 
                       data_length, index_length, table_comment
                FROM information_schema.tables 
                WHERE table_schema = DATABASE()
                ORDER BY table_name
            """
            )

            tables_info = cursor.fetchall()

            for table_info in tables_info:
                table_name = table_info[0]
                schema_analysis["tables"][table_name] = {
                    "type": table_info[1],
                    "engine": table_info[2],
                    "rows": table_info[3],
                    "data_length": table_info[4],
                    "index_length": table_info[5],
                    "comment": table_info[6],
                    "columns": {},
                    "indexes": [],
                    "foreign_keys": [],
                }

                # ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT column_name, data_type, is_nullable, column_default,
                           column_key, extra, column_comment, character_maximum_length,
                           numeric_precision, numeric_scale
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table_name,),
                )

                columns_info = cursor.fetchall()
                for col_info in columns_info:
                    col_name = col_info[0]
                    schema_analysis["tables"][table_name]["columns"][col_name] = {
                        "data_type": col_info[1],
                        "is_nullable": col_info[2],
                        "default": col_info[3],
                        "key": col_info[4],
                        "extra": col_info[5],
                        "comment": col_info[6],
                        "max_length": col_info[7],
                        "precision": col_info[8],
                        "scale": col_info[9],
                    }

                # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT index_name, column_name, seq_in_index, non_unique,
                           index_type, comment
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table_name,),
                )

                indexes_info = cursor.fetchall()
                current_index = None
                for idx_info in indexes_info:
                    idx_name = idx_info[0]
                    if current_index != idx_name:
                        schema_analysis["tables"][table_name]["indexes"].append(
                            {
                                "name": idx_name,
                                "columns": [idx_info[1]],
                                "unique": idx_info[3] == 0,
                                "type": idx_info[4],
                                "comment": idx_info[5],
                            }
                        )
                        current_index = idx_name
                    else:
                        # ë³µí•© ì¸ë±ìŠ¤ì˜ ì¶”ê°€ ì»¬ëŸ¼
                        schema_analysis["tables"][table_name]["indexes"][-1][
                            "columns"
                        ].append(idx_info[1])

                # ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘
                cursor.execute(
                    """
                    SELECT kcu.constraint_name, kcu.column_name, kcu.referenced_table_name,
                           kcu.referenced_column_name, rc.update_rule, rc.delete_rule
                    FROM information_schema.key_column_usage kcu
                    JOIN information_schema.referential_constraints rc
                    ON kcu.constraint_name = rc.constraint_name 
                    AND kcu.table_schema = rc.constraint_schema
                    WHERE kcu.table_schema = DATABASE() AND kcu.table_name = %s
                    AND kcu.referenced_table_name IS NOT NULL
                """,
                    (table_name,),
                )

                fk_info = cursor.fetchall()
                for fk in fk_info:
                    schema_analysis["tables"][table_name]["foreign_keys"].append(
                        {
                            "constraint_name": fk[0],
                            "column": fk[1],
                            "referenced_table": fk[2],
                            "referenced_column": fk[3],
                            "update_rule": fk[4],
                            "delete_rule": fk[5],
                        }
                    )

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return {"success": True, "schema_analysis": schema_analysis}

        except Exception as e:
            return {"success": False, "error": f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"}

    async def check_ddl_conflicts(
        self, ddl_content: str, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """DDL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì  ì‚¬ì „ ê²€ì‚¬"""
        try:
            # í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë¶„ì„
            schema_result = await self.analyze_current_schema(
                database_secret, use_ssh_tunnel
            )
            if not schema_result["success"]:
                return schema_result

            schema = schema_result["schema_analysis"]
            ddl_type = self.extract_ddl_type(ddl_content)

            conflicts = []
            warnings = []
            recommendations = []

            if ddl_type == "CREATE_TABLE":
                conflicts.extend(
                    await self._check_create_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "ALTER_TABLE":
                conflicts.extend(
                    await self._check_alter_table_conflicts(ddl_content, schema)
                )
            elif ddl_type == "CREATE_INDEX":
                conflicts.extend(
                    await self._check_create_index_conflicts(ddl_content, schema)
                )
            elif ddl_type == "DROP":
                conflicts.extend(await self._check_drop_conflicts(ddl_content, schema))

            return {
                "success": True,
                "ddl_type": ddl_type,
                "conflicts": conflicts,
                "warnings": warnings,
                "recommendations": recommendations,
                "current_schema": schema,
            }

        except Exception as e:
            return {"success": False, "error": f"DDL ì¶©ëŒ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}"}

    async def _check_create_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?`?(\w+)`?",
            ddl_content,
            re.IGNORECASE,
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name in schema["tables"]:
                if "IF NOT EXISTS" not in ddl_content.upper():
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”."
                    )
                else:
                    conflicts.append(
                        f"í…Œì´ë¸” '{table_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. IF NOT EXISTSê°€ ìˆì–´ì„œ ì‹¤í–‰ì€ ë˜ì§€ë§Œ ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_alter_table_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """ALTER TABLE ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # í…Œì´ë¸”ëª… ì¶”ì¶œ
        table_match = re.search(
            r"ALTER\s+TABLE\s+`?(\w+)`?", ddl_content, re.IGNORECASE
        )
        if table_match:
            table_name = table_match.group(1)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ADD COLUMN ê²€ì‚¬
            add_column_matches = re.findall(
                r"ADD\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in add_column_matches:
                if col_name in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                    )

            # DROP COLUMN ê²€ì‚¬
            drop_column_matches = re.findall(
                r"DROP\s+(?:COLUMN\s+)?`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            for col_name in drop_column_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸
                    for fk in table_info["foreign_keys"]:
                        if fk["column"] == col_name:
                            conflicts.append(
                                f"ì»¬ëŸ¼ '{col_name}'ì€ ì™¸ë˜í‚¤ë¡œ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            )

            # MODIFY/CHANGE COLUMN ê²€ì‚¬
            modify_matches = re.findall(
                r"(?:MODIFY|CHANGE)\s+(?:COLUMN\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            for col_name in modify_matches:
                if col_name not in table_info["columns"]:
                    conflicts.append(
                        f"ìˆ˜ì •í•˜ë ¤ëŠ” ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    )

        return conflicts

    async def _check_create_index_conflicts(
        self, ddl_content: str, schema: Dict
    ) -> List[str]:
        """CREATE INDEX ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        # ì¸ë±ìŠ¤ëª…ê³¼ í…Œì´ë¸”ëª… ì¶”ì¶œ
        index_match = re.search(
            r"CREATE\s+(?:UNIQUE\s+)?INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?\s*\((.*?)\)",
            ddl_content,
            re.IGNORECASE,
        )
        if index_match:
            index_name = index_match.group(1)
            table_name = index_match.group(2)
            columns_str = index_match.group(3)

            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if table_name not in schema["tables"]:
                conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return conflicts

            table_info = schema["tables"][table_name]

            # ì¸ë±ìŠ¤ ì¤‘ë³µ í™•ì¸
            for existing_index in table_info["indexes"]:
                if existing_index["name"] == index_name:
                    conflicts.append(f"ì¸ë±ìŠ¤ '{index_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            columns = [col.strip().strip("`") for col in columns_str.split(",")]
            for col_name in columns:
                # í•¨ìˆ˜ë‚˜ í‘œí˜„ì‹ì´ ì•„ë‹Œ ë‹¨ìˆœ ì»¬ëŸ¼ëª…ë§Œ ê²€ì‚¬
                if col_name and not re.search(r"[()]", col_name):
                    if col_name not in table_info["columns"]:
                        conflicts.append(
                            f"ì»¬ëŸ¼ '{col_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def _check_drop_conflicts(self, ddl_content: str, schema: Dict) -> List[str]:
        """DROP ë¬¸ ì¶©ëŒ ê²€ì‚¬"""
        conflicts = []

        if "DROP TABLE" in ddl_content.upper():
            # í…Œì´ë¸” ì‚­ì œ ê²€ì‚¬
            table_match = re.search(
                r"DROP\s+TABLE\s+(?:IF\s+EXISTS\s+)?`?(\w+)`?",
                ddl_content,
                re.IGNORECASE,
            )
            if table_match:
                table_name = table_match.group(1)
                if table_name not in schema["tables"]:
                    if "IF EXISTS" not in ddl_content.upper():
                        conflicts.append(
                            f"ì‚­ì œí•˜ë ¤ëŠ” í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )
                else:
                    # ì™¸ë˜í‚¤ ì°¸ì¡° í™•ì¸ (ë‹¤ë¥¸ í…Œì´ë¸”ì—ì„œ ì´ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ëŠ”ì§€)
                    for other_table, table_info in schema["tables"].items():
                        for fk in table_info["foreign_keys"]:
                            if fk["referenced_table"] == table_name:
                                conflicts.append(
                                    f"í…Œì´ë¸” '{table_name}'ì€ í…Œì´ë¸” '{other_table}'ì—ì„œ ì™¸ë˜í‚¤ë¡œ ì°¸ì¡°ë˜ê³  ìˆì–´ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                )

        elif "DROP INDEX" in ddl_content.upper():
            # ì¸ë±ìŠ¤ ì‚­ì œ ê²€ì‚¬
            index_match = re.search(
                r"DROP\s+INDEX\s+`?(\w+)`?\s+ON\s+`?(\w+)`?", ddl_content, re.IGNORECASE
            )
            if index_match:
                index_name = index_match.group(1)
                table_name = index_match.group(2)

                if table_name not in schema["tables"]:
                    conflicts.append(f"í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    table_info = schema["tables"][table_name]
                    index_exists = any(
                        idx["name"] == index_name for idx in table_info["indexes"]
                    )
                    if not index_exists:
                        conflicts.append(
                            f"ì¸ë±ìŠ¤ '{index_name}'ì´ í…Œì´ë¸” '{table_name}'ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

        return conflicts

    async def get_aurora_mysql_parameters(
        self,
        cluster_identifier: str,
        region: str = "ap-northeast-2",
        filter_type: str = "important",
        category: str = "all",
    ) -> str:
        """Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„° ì¡°íšŒ

        Args:
            cluster_identifier: í´ëŸ¬ìŠ¤í„° ì‹ë³„ì
            region: AWS ë¦¬ì „
            filter_type: í•„í„° íƒ€ì… (important, custom, all)
            category: íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)
        """
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
            clusters_response = rds_client.describe_db_clusters(
                DBClusterIdentifier=cluster_identifier
            )

            if not clusters_response["DBClusters"]:
                return f"âŒ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cluster_identifier}"

            cluster = clusters_response["DBClusters"][0]
            cluster_param_group = cluster.get(
                "DBClusterParameterGroup", "default.aurora-mysql8.0"
            )

            # ì¹´í…Œê³ ë¦¬ë³„ ì œëª© ì„¤ì •
            category_titles = {
                "all": "ì „ì²´",
                "security": "ë³´ì•ˆ ë° ì¸ì¦",
                "performance": "ì„±ëŠ¥ ìµœì í™”",
                "memory": "ë©”ëª¨ë¦¬ ê´€ë¦¬",
                "io": "I/O ë° ìŠ¤í† ë¦¬ì§€",
                "connection": "ì—°ê²° ê´€ë¦¬",
                "logging": "ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                "replication": "ë³µì œ ë° ë°±ì—…",
                "aurora": "Aurora íŠ¹í™” ê¸°ëŠ¥",
            }

            category_title = category_titles.get(category, category)

            result = f"""ğŸ“Š Aurora MySQL íŒŒë¼ë¯¸í„° ì •ë³´ ({category_title})

ğŸ”§ **í´ëŸ¬ìŠ¤í„° ì •ë³´:**
- í´ëŸ¬ìŠ¤í„° ID: {cluster_identifier}
- í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ê·¸ë£¹: {cluster_param_group}
- ì—”ì§„ ë²„ì „: {cluster.get('EngineVersion', 'N/A')}"""

            # í´ëŸ¬ìŠ¤í„° íŒŒë¼ë¯¸í„° ì¡°íšŒ
            cluster_params = await self._get_parameters(
                rds_client, cluster_param_group, "cluster", filter_type, category
            )
            if cluster_params:
                result += f"\n\nğŸ—ï¸ **í´ëŸ¬ìŠ¤í„° ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{cluster_params}"

            # ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if cluster.get("DBClusterMembers"):
                instance_id = cluster["DBClusterMembers"][0]["DBInstanceIdentifier"]
                instance_response = rds_client.describe_db_instances(
                    DBInstanceIdentifier=instance_id
                )
                if instance_response["DBInstances"]:
                    instance_param_group = instance_response["DBInstances"][0][
                        "DBParameterGroups"
                    ][0]["DBParameterGroupName"]
                    result += f"\n- ì¸ìŠ¤í„´ìŠ¤ íŒŒë¼ë¯¸í„° ê·¸ë£¹: {instance_param_group}"

                    instance_params = await self._get_parameters(
                        rds_client,
                        instance_param_group,
                        "instance",
                        filter_type,
                        category,
                    )
                    if instance_params:
                        result += (
                            f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:**\n{instance_params}"
                        )
                    else:
                        if category == "all":
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— í‘œì‹œí•  íŒŒë¼ë¯¸í„° ì—†ìŒ"
                        else:
                            result += f"\n\nğŸ–¥ï¸ **ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ íŒŒë¼ë¯¸í„°:** {category_title} ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„° ì—†ìŒ"

            return result

        except Exception as e:
            return f"âŒ Aurora íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_parameters(
        self,
        rds_client,
        param_group_name: str,
        level: str,
        filter_type: str,
        category: str,
    ) -> str:
        """íŒŒë¼ë¯¸í„° ì¡°íšŒ ë° í•„í„°ë§"""
        try:
            # íŒŒë¼ë¯¸í„° ì¡°íšŒ
            if level == "cluster":
                response = rds_client.describe_db_cluster_parameters(
                    DBClusterParameterGroupName=param_group_name
                )
            else:
                response = rds_client.describe_db_parameters(
                    DBParameterGroupName=param_group_name
                )

            parameters = response["Parameters"]

            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒë¼ë¯¸í„° ì •ì˜ (í™•ì¥ëœ ë¶„ë¥˜)
            category_params = {
                "security": [
                    "activate_all_roles_on_login",
                    "authentication_kerberos_caseins_cmp",
                    "default_authentication_plugin",
                    "default_password_lifetime",
                    "check_proxy_users",
                    "mysql_native_password_proxy_users",
                    "sha256_password_proxy_users",
                    "validate_password_policy",
                ],
                "performance": [
                    "innodb_thread_concurrency",
                    "innodb_read_io_threads",
                    "innodb_write_io_threads",
                    "thread_cache_size",
                    "thread_stack",
                    "innodb_purge_threads",
                    "innodb_adaptive_flushing",
                    "innodb_adaptive_max_sleep_delay",
                    "innodb_concurrency_tickets",
                    "innodb_flushing_avg_loops",
                    "innodb_lru_scan_depth",
                    "innodb_max_dirty_pages_pct",
                    "innodb_max_purge_lag",
                    "innodb_max_purge_lag_delay",
                    "innodb_old_blocks_pct",
                    "innodb_old_blocks_time",
                    "innodb_parallel_read_threads",
                    "query_cache_size",
                    "query_cache_type",
                ],
                "memory": [
                    "innodb_buffer_pool_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "sort_buffer_size",
                    "read_buffer_size",
                    "read_rnd_buffer_size",
                    "join_buffer_size",
                    "innodb_log_buffer_size",
                    "key_buffer_size",
                    "innodb_buffer_pool_dump_at_shutdown",
                    "innodb_buffer_pool_load_at_startup",
                    "innodb_buffer_pool_dump_now",
                    "innodb_buffer_pool_load_now",
                    "innodb_change_buffer_max_size",
                    "bulk_insert_buffer_size",
                ],
                "io": [
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "innodb_log_file_size",
                    "innodb_io_capacity",
                    "innodb_io_capacity_max",
                    "innodb_flush_method",
                    "innodb_file_per_table",
                    "innodb_doublewrite",
                    "innodb_flush_neighbors",
                    "innodb_flush_log_at_timeout",
                    "innodb_log_compressed_pages",
                    "innodb_open_files",
                    "innodb_read_only",
                    "innodb_sort_buffer_size",
                ],
                "connection": [
                    "max_connections",
                    "max_user_connections",
                    "connect_timeout",
                    "interactive_timeout",
                    "wait_timeout",
                    "max_connect_errors",
                    "back_log",
                    "host_cache_size",
                    "max_allowed_packet",
                ],
                "logging": [
                    "general_log",
                    "slow_query_log",
                    "log_queries_not_using_indexes",
                    "long_query_time",
                    "log_slow_admin_statements",
                    "log_slow_slave_statements",
                    "general_log_file",
                    "slow_query_log_file",
                    "log_error",
                    "log_warnings",
                    "innodb_print_all_deadlocks",
                ],
                "replication": [
                    "binlog_format",
                    "expire_logs_days",
                    "max_binlog_size",
                    "binlog_cache_size",
                    "slave_net_timeout",
                    "slave_parallel_workers",
                    "binlog_checksum",
                    "binlog_group_commit_sync_delay",
                    "binlog_group_commit_sync_no_delay_count",
                    "binlog_order_commits",
                    "binlog_row_image",
                    "binlog_rows_query_log_events",
                    "binlog_stmt_cache_size",
                    "binlog_transaction_compression",
                    "binlog_backup",
                    "binlog_replication_globaldb",
                ],
                "aurora": [
                    "aurora_binlog_replication_max_yield_seconds",
                    "aurora_binlog_replication_sec_index_parallel_workers",
                    "aurora_enable_staggered_replica_restart",
                    "aurora_enhanced_binlog",
                    "aurora_full_double_precision_in_json",
                    "aurora_fwd_writer_idle_timeout",
                    "aurora_fwd_writer_max_connections_pct",
                    "aurora_in_memory_relaylog",
                    "aurora_jemalloc_background_thread",
                    "aurora_jemalloc_dirty_decay_ms",
                    "aurora_jemalloc_tcache_enabled",
                    "aurora_ml_inference_timeout",
                    "aurora_oom_response",
                    "aurora_parallel_query",
                    "aurora_read_replica_read_committed",
                    "aurora_replica_read_consistency",
                    "aurora_tmptable_enable_per_table_limit",
                    "aurora_use_vector_instructions",
                    "aurora_aurora_max_partitions_for_range",
                ],
            }

            # í•„í„°ë§ ì ìš©
            filtered_params = []

            if filter_type == "all":
                filtered_params = parameters
            elif filter_type == "custom":
                filtered_params = [p for p in parameters if p.get("Source") == "user"]
            elif filter_type == "important":
                important_params = [
                    "innodb_buffer_pool_size",
                    "max_connections",
                    "innodb_log_file_size",
                    "query_cache_size",
                    "tmp_table_size",
                    "max_heap_table_size",
                    "innodb_flush_log_at_trx_commit",
                    "sync_binlog",
                    "binlog_format",
                    "character_set_server",
                    "collation_server",
                    "time_zone",
                    "aurora_parallel_query",
                    "aurora_oom_response",
                    "aws_default_s3_role",
                ]
                filtered_params = [
                    p for p in parameters if p["ParameterName"] in important_params
                ]

            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            if category != "all" and category in category_params:
                category_param_names = category_params[category]
                filtered_params = [
                    p
                    for p in filtered_params
                    if p["ParameterName"] in category_param_names
                ]

            if not filtered_params:
                return ""

            # ê²°ê³¼ í¬ë§·íŒ… (ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”)
            result = ""

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”
            if category == "all":
                # ì „ì²´ ì¡°íšŒ ì‹œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ í‘œì‹œ
                categorized_params = {}
                uncategorized_params = []

                for param in filtered_params:
                    param_name = param["ParameterName"]
                    found_category = None

                    for cat, param_list in category_params.items():
                        if param_name in param_list:
                            if cat not in categorized_params:
                                categorized_params[cat] = []
                            categorized_params[cat].append(param)
                            found_category = cat
                            break

                    if not found_category:
                        uncategorized_params.append(param)

                # ì¹´í…Œê³ ë¦¬ë³„ ì¶œë ¥
                category_titles = {
                    "security": "ğŸ” ë³´ì•ˆ ë° ì¸ì¦",
                    "performance": "âš¡ ì„±ëŠ¥ ìµœì í™”",
                    "memory": "ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬",
                    "io": "ğŸ’¿ I/O ë° ìŠ¤í† ë¦¬ì§€",
                    "connection": "ğŸ”— ì—°ê²° ê´€ë¦¬",
                    "logging": "ğŸ“ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§",
                    "replication": "ğŸ”„ ë³µì œ ë° ë°±ì—…",
                    "aurora": "â˜ï¸ Aurora íŠ¹í™” ê¸°ëŠ¥",
                }

                for cat in [
                    "security",
                    "performance",
                    "memory",
                    "io",
                    "connection",
                    "logging",
                    "replication",
                    "aurora",
                ]:
                    if cat in categorized_params:
                        result += f"\n{category_titles[cat]}:\n"
                        for param in categorized_params[cat]:
                            name = param["ParameterName"]
                            value = param.get("ParameterValue", "N/A")
                            source = param.get("Source", "N/A")
                            description = (
                                param.get("Description", "")[:50] + "..."
                                if len(param.get("Description", "")) > 50
                                else param.get("Description", "")
                            )

                            result += f"â€¢ {name}: {value} (Source: {source})\n"
                            if description and filter_type == "all":
                                result += f"  â””â”€ {description}\n"

                # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°
                if uncategorized_params:
                    result += f"\nğŸ”§ ê¸°íƒ€ íŒŒë¼ë¯¸í„°:\n"
                    for param in uncategorized_params:
                        name = param["ParameterName"]
                        value = param.get("ParameterValue", "N/A")
                        source = param.get("Source", "N/A")
                        description = (
                            param.get("Description", "")[:50] + "..."
                            if len(param.get("Description", "")) > 50
                            else param.get("Description", "")
                        )

                        result += f"â€¢ {name}: {value} (Source: {source})\n"
                        if description and filter_type == "all":
                            result += f"  â””â”€ {description}\n"
            else:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œ
                for param in filtered_params:
                    name = param["ParameterName"]
                    value = param.get("ParameterValue", "N/A")
                    source = param.get("Source", "N/A")
                    description = (
                        param.get("Description", "")[:50] + "..."
                        if len(param.get("Description", "")) > 50
                        else param.get("Description", "")
                    )

                    result += f"â€¢ {name}: {value} (Source: {source})\n"
                    if description and filter_type == "all":
                        result += f"  â””â”€ {description}\n"

            return result.rstrip()

        except Exception as e:
            return f"âŒ {level} íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def _get_default_aurora_parameters(
        self, parameter_group_name: str, region: str
    ) -> str:
        """ê¸°ë³¸ Aurora íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´ ì¡°íšŒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ë“¤ ì¡°íšŒ
            param_groups_response = rds_client.describe_db_cluster_parameter_groups()

            default_group = None
            for group in param_groups_response["DBClusterParameterGroups"]:
                if "default.aurora-mysql" in group["DBClusterParameterGroupName"]:
                    default_group = group
                    break

            if not default_group:
                return (
                    f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parameter_group_name}"
                )

            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë“¤ ì¡°íšŒ
            parameters_response = rds_client.describe_db_cluster_parameters(
                DBClusterParameterGroupName=default_group["DBClusterParameterGroupName"]
            )

            parameters = parameters_response["Parameters"]

            result = f"""ğŸ“Š Aurora MySQL ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì •ë³´

ğŸ”§ **íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì •ë³´:**
- ê·¸ë£¹ëª…: {default_group['DBClusterParameterGroupName']}
- íŒ¨ë°€ë¦¬: {default_group['DBParameterGroupFamily']}
- ì„¤ëª…: {default_group['Description']}

ğŸ“‹ **ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì¼ë¶€):**"""

            # ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ë§Œ í‘œì‹œ
            important_params = [
                "innodb_buffer_pool_size",
                "max_connections",
                "innodb_log_file_size",
                "query_cache_size",
                "tmp_table_size",
                "max_heap_table_size",
            ]

            for param in parameters[:20]:  # ì²˜ìŒ 20ê°œë§Œ
                param_name = param["ParameterName"]
                if param_name in important_params:
                    value = param.get("ParameterValue", "N/A")
                    result += f"\nâ€¢ {param_name}: {value}"

            return result

        except Exception as e:
            return f"âŒ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def create_validation_plan(
        self, filename: str, database_secret: Optional[str] = None
    ) -> Dict[str, Any]:
        """DDL ê²€ì¦ ì‹¤í–‰ ê³„íš ìƒì„±"""
        try:
            # SQL íŒŒì¼ ì¡´ì¬ í™•ì¸
            sql_file_path = SQL_DIR / filename
            if not sql_file_path.exists():
                return {
                    "operation": "validate_sql_file",
                    "filename": filename,
                    "database_secret": database_secret,
                    "status": "error",
                    "error": f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}",
                    "steps": [],
                }

            # DDL ë‚´ìš© ë¯¸ë¦¬ ì½ê¸°
            with open(sql_file_path, "r", encoding="utf-8") as f:
                ddl_content = f.read()

            ddl_type = self.extract_ddl_type(ddl_content)

            # ê²€ì¦ ë‹¨ê³„ ì •ì˜
            steps = [
                {
                    "step": 1,
                    "name": "ë¬¸ë²• ê²€ì¦",
                    "description": "DDL ê¸°ë³¸ ë¬¸ë²• ë° êµ¬ì¡° ê²€ì¦",
                    "details": [
                        "ì„¸ë¯¸ì½œë¡  ëˆ„ë½ í™•ì¸",
                        "DDL êµ¬ë¬¸ êµ¬ì¡° ê²€ì¦",
                        "Claude AIë¥¼ í†µí•œ ê³ ê¸‰ ë¬¸ë²• ê²€ì¦",
                    ],
                },
                {
                    "step": 2,
                    "name": "í‘œì¤€ ê·œì¹™ ê²€ì¦",
                    "description": "ìŠ¤í‚¤ë§ˆ ëª…ëª… ê·œì¹™ ë° í‘œì¤€ ì¤€ìˆ˜ í™•ì¸",
                    "details": [
                        "í…Œì´ë¸”/ì»¬ëŸ¼ ëª…ëª… ê·œì¹™ ê²€ì¦",
                        "ë°ì´í„° íƒ€ì… í‘œì¤€ ì¤€ìˆ˜ í™•ì¸",
                    ],
                },
            ]

            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€ ë‹¨ê³„
            if database_secret:
                steps.extend(
                    [
                        {
                            "step": 3,
                            "name": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                            "description": f"'{database_secret}' ì‹œí¬ë¦¿ìœ¼ë¡œ DB ì—°ê²° í™•ì¸",
                            "details": [
                                "AWS Secrets Managerì—ì„œ ì—°ê²° ì •ë³´ ì¡°íšŒ",
                                "SSH í„°ë„ì„ í†µí•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                                "ì—°ê²° ìƒíƒœ ë° ê¶Œí•œ í™•ì¸",
                            ],
                        },
                        {
                            "step": 4,
                            "name": "ìŠ¤í‚¤ë§ˆ ê²€ì¦",
                            "description": "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ DDL í˜¸í™˜ì„± ê²€ì¦",
                            "details": [
                                f"{ddl_type} ì‘ì—… ëŒ€ìƒ í™•ì¸",
                                "í…Œì´ë¸”/ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦",
                                "ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± í™•ì¸",
                            ],
                        },
                        {
                            "step": 5,
                            "name": "ì œì•½ì¡°ê±´ ê²€ì¦",
                            "description": "ì™¸ë˜í‚¤, ì¸ë±ìŠ¤ ë“± ì œì•½ì¡°ê±´ ê²€ì¦",
                            "details": [
                                "ì™¸ë˜í‚¤ ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ í™•ì¸",
                                "ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€ ê²€ì¦",
                                "ì œì•½ì¡°ê±´ ì¶©ëŒ ê²€ì‚¬",
                            ],
                        },
                    ]
                )

            steps.append(
                {
                    "step": len(steps) + 1,
                    "name": "ìµœì¢… ë³´ê³ ì„œ ìƒì„±",
                    "description": "HTML í˜•ì‹ì˜ ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œ ìƒì„±",
                    "details": [
                        "ê²€ì¦ ê²°ê³¼ ì¢…í•©",
                        "ë¬¸ì œì  ë° ê¶Œì¥ì‚¬í•­ ì •ë¦¬",
                        "output ë””ë ‰í† ë¦¬ì— HTML ë³´ê³ ì„œ ì €ì¥",
                    ],
                }
            )

            return {
                "operation": "validate_sql_file",
                "filename": filename,
                "database_secret": database_secret,
                "ddl_type": ddl_type,
                "ddl_content": ddl_content,  # ì‹¤ì œ DDL ë‚´ìš© í¬í•¨
                "ddl_preview": (
                    ddl_content[:200] + "..." if len(ddl_content) > 200 else ddl_content
                ),
                "steps": steps,
                "created_at": datetime.now().isoformat(),
                "status": "created",
            }

        except Exception as e:
            return {
                "operation": "validate_sql_file",
                "filename": filename,
                "database_secret": database_secret,
                "status": "error",
                "error": f"ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "steps": [],
            }

    def _format_validation_plan_display(self, plan: Dict[str, Any]) -> str:
        """ê²€ì¦ ê³„íš í‘œì‹œ í˜•ì‹ ìƒì„±"""
        if plan["status"] == "error":
            return f"âŒ **ê³„íš ìƒì„± ì‹¤íŒ¨:** {plan['error']}"

        result = f"""ğŸ¯ **ê²€ì¦ ëŒ€ìƒ:** {plan['filename']}
ğŸ“… **ê³„íš ìƒì„± ì‹œê°„:** {plan['created_at']}
ğŸ”§ **DDL íƒ€ì…:** {plan['ddl_type']}
ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤:** {plan['database_secret'] or 'ì—°ê²° ì—†ìŒ (ê¸°ë³¸ ê²€ì¦ë§Œ)'}

ğŸ“ **DDL ë¯¸ë¦¬ë³´ê¸°:**
```sql
{plan['ddl_preview']}
```

ğŸ”„ **ê²€ì¦ ë‹¨ê³„ ({len(plan['steps'])}ë‹¨ê³„):**"""

        for step in plan["steps"]:
            result += f"\n\n   **{step['step']}. {step['name']}**"
            result += f"\n   â””â”€ {step['description']}"

            if step.get("details"):
                for detail in step["details"]:
                    result += f"\n      â€¢ {detail}"

        # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ë° ì£¼ì˜ì‚¬í•­
        estimated_time = "30ì´ˆ ~ 1ë¶„" if plan["database_secret"] else "10 ~ 20ì´ˆ"
        result += f"\n\nâ±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** {estimated_time}"

        if plan["database_secret"]:
            result += f"\nâš ï¸ **ì£¼ì˜ì‚¬í•­:** ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."

        return result

    def _create_validate_all_sql_plan(self, operation: str, **kwargs) -> Dict[str, Any]:
        """validate_all_sql ì‘ì—… ê³„íš ìƒì„±"""
        if operation == "validate_all_sql":
            database_secret = kwargs.get("database_secret")
            tool_name = "validate_all_sql"

            plan_steps = [
                {
                    "step": 1,
                    "action": "SQL íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
                    "target": "sql ë””ë ‰í† ë¦¬",
                    "tool": "list_sql_files",
                },
                {
                    "step": 2,
                    "action": "íŒŒì¼ ê°œìˆ˜ í™•ì¸",
                    "target": "ìµœëŒ€ 5ê°œ ì œí•œ",
                    "tool": "internal_check",
                },
                {
                    "step": 3,
                    "action": "ê° íŒŒì¼ ìˆœì°¨ ê²€ì¦",
                    "target": "ê°œë³„ íŒŒì¼",
                    "tool": "validate_sql_file",
                },
                {
                    "step": 4,
                    "action": "ì¢…í•© ê²°ê³¼ ìƒì„±",
                    "target": "ì „ì²´ ìš”ì•½",
                    "tool": "fs_write",
                },
            ]

        elif operation == "analyze_current_schema":
            database_secret = kwargs.get("database_secret")
            tool_name = "analyze_current_schema"

            plan_steps = [
                {
                    "step": 1,
                    "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                    "target": database_secret,
                    "tool": "test_database_connection",
                },
                {
                    "step": 2,
                    "action": "í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ",
                    "target": "information_schema",
                    "tool": "mysql_query",
                },
                {
                    "step": 3,
                    "action": "ì»¬ëŸ¼ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 4,
                    "action": "ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 5,
                    "action": "ì™¸ë˜í‚¤ ì •ë³´ ìˆ˜ì§‘",
                    "target": "ê° í…Œì´ë¸”",
                    "tool": "mysql_query",
                },
                {
                    "step": 6,
                    "action": "ìŠ¤í‚¤ë§ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±",
                    "target": "ì¢…í•© ì •ë³´",
                    "tool": "internal_analysis",
                },
            ]

        elif operation == "get_aurora_mysql_parameters":
            cluster_identifier = kwargs.get("cluster_identifier")
            tool_name = "get_aurora_mysql_parameters"

            plan_steps = [
                {
                    "step": 1,
                    "action": "í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ",
                    "target": cluster_identifier,
                    "tool": "use_aws",
                },
                {
                    "step": 2,
                    "action": "íŒŒë¼ë¯¸í„° ê·¸ë£¹ í™•ì¸",
                    "target": "ì ìš©ëœ ê·¸ë£¹",
                    "tool": "use_aws",
                },
                {
                    "step": 3,
                    "action": "íŒŒë¼ë¯¸í„° ê°’ ì¡°íšŒ",
                    "target": "ì£¼ìš” ì„¤ì •",
                    "tool": "use_aws",
                },
                {
                    "step": 4,
                    "action": "ì»¤ìŠ¤í…€ ì„¤ì • í•„í„°ë§",
                    "target": "ì‚¬ìš©ì ì •ì˜ ê°’",
                    "tool": "internal_filter",
                },
            ]

        else:
            plan_steps = [
                {
                    "step": 1,
                    "action": f"{operation} ì‹¤í–‰",
                    "target": "ê¸°ë³¸ ë™ì‘",
                    "tool": operation,
                }
            ]

        plan = {
            "operation": operation,
            "tool_name": tool_name,
            "parameters": kwargs,
            "steps": plan_steps,
            "created_at": datetime.now().isoformat(),
            "status": "created",
        }

        self.current_plan = plan
        return plan

    async def execute_with_auto_plan(self, operation: str, **kwargs) -> str:
        """ìë™ ì‹¤í–‰ ê³„íš ìƒì„± ë° í‘œì‹œ í›„ í™•ì¸"""
        try:
            # 1. ì‹¤í–‰ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)
            plan_display = self._format_plan_display(plan)

            # 2. ì‹¤í–‰ ê³„íš í‘œì‹œ ë° í™•ì¸ ìš”ì²­
            confirmation_message = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** 
   â€¢ 'y' ë˜ëŠ” 'yes': ì‹¤í–‰ ì§„í–‰
   â€¢ 'n' ë˜ëŠ” 'no': ì‹¤í–‰ ì·¨ì†Œ

ğŸ’¡ **ì°¸ê³ :** confirm_and_execute ë„êµ¬ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

            return confirmation_message

        except Exception as e:
            return f"âŒ ì‹¤í–‰ ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def execute_with_plan(self, operation: str, **kwargs) -> str:
        """ê³„íšì— ë”°ë¥¸ ì‹¤í–‰"""
        # ê¸°ì¡´ ê³„íšì´ ìˆëŠ”ì§€ í™•ì¸
        if self.current_plan and self.current_plan["operation"] == operation:
            plan = self.current_plan
            plan_display = self._format_plan_display(plan)

            # ì‚¬ìš©ì í™•ì¸ ìš”ì²­
            confirmation = f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)
"""
            return confirmation
        else:
            # ìƒˆ ê³„íš ìƒì„±
            plan = await self.create_execution_plan(operation, **kwargs)
            plan_display = self._format_plan_display(plan)

            return f"""ğŸ“‹ **ì‹¤í–‰ ê³„íšì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?** (y/n)

ğŸ’¡ **ì°¸ê³ :** 'y' ë˜ëŠ” 'yes'ë¡œ ì‘ë‹µí•˜ë©´ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.
"""

    async def create_execution_plan(self, operation: str, **kwargs) -> Dict[str, Any]:
        """ì‘ì—… ì‹¤í–‰ ê³„íš ìƒì„±"""
        try:
            if operation == "validate_sql_file":
                filename = kwargs.get("filename", "")
                database_secret = kwargs.get("database_secret", "")

                # SQL íŒŒì¼ ì½ê¸°
                sql_file_path = SQL_DIR / filename
                if not sql_file_path.exists():
                    return {"error": f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"}

                with open(sql_file_path, "r", encoding="utf-8") as f:
                    ddl_content = f.read()

                ddl_type = self.extract_ddl_type(ddl_content)

                # DDL ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)
                preview = (
                    ddl_content.strip()[:100] + "..."
                    if len(ddl_content.strip()) > 100
                    else ddl_content.strip()
                )

                plan = {
                    "operation": operation,
                    "filename": filename,
                    "database_secret": database_secret,
                    "ddl_type": ddl_type,
                    "preview": preview,
                    "steps": [
                        "ë¬¸ë²• ê²€ì¦",
                        "í‘œì¤€ ê·œì¹™ ê²€ì¦",
                        "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                        "ìŠ¤í‚¤ë§ˆ ê²€ì¦",
                        "ì œì•½ì¡°ê±´ ê²€ì¦",
                        "ìµœì¢… ë³´ê³ ì„œ ìƒì„±",
                    ],
                    "status": "created",
                }

                return plan

            elif operation == "test_database_connection":
                database_secret = kwargs.get("database_secret", "")

                plan = {
                    "operation": operation,
                    "database_secret": database_secret,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "tool_name": "test_database_connection",
                    "steps": [
                        {
                            "step": 1,
                            "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸",
                            "target": database_secret,
                            "tool": "test_database_connection",
                        }
                    ],
                    "status": "created",
                    "parameters": kwargs,
                }

                return plan

            elif operation == "get_schema_summary":
                database_secret = kwargs.get("database_secret", "")

                plan = {
                    "operation": operation,
                    "database_secret": database_secret,
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "tool_name": "get_schema_summary",
                    "steps": [
                        {
                            "step": 1,
                            "action": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
                            "target": database_secret,
                            "tool": "get_schema_summary",
                        },
                        {
                            "step": 2,
                            "action": "ìŠ¤í‚¤ë§ˆ ì •ë³´ ìˆ˜ì§‘",
                            "target": "ì „ì²´ ìŠ¤í‚¤ë§ˆ",
                            "tool": "get_schema_summary",
                        },
                    ],
                    "status": "created",
                    "parameters": kwargs,
                }

                return plan

            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì…ë‹ˆë‹¤: {operation}"}

        except Exception as e:
            return {"error": f"ì‹¤í–‰ ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}"}

    def _format_plan_display(self, plan: Dict[str, Any]) -> str:
        """ê³„íš í‘œì‹œ í˜•ì‹ ìƒì„±"""
        result = f"""ğŸ¯ **ì‘ì—…:** {plan['operation']}
ğŸ“… **ìƒì„± ì‹œê°„:** {plan['created_at']}
ğŸ”§ **ë§¤ì¹­ íˆ´:** {plan.get('tool_name', plan['operation'])}

ğŸ“ **ì‹¤í–‰ ë‹¨ê³„:**"""

        for step in plan["steps"]:
            tool_info = f" [{step.get('tool', 'internal')}]" if step.get("tool") else ""
            result += (
                f"\n   {step['step']}. {step['action']} â†’ {step['target']}{tool_info}"
            )

        if plan["parameters"]:
            result += f"\n\nâš™ï¸ **ë§¤ê°œë³€ìˆ˜:**"
            for key, value in plan["parameters"].items():
                if value:
                    result += f"\n   â€¢ {key}: {value}"

        return result

    async def confirm_and_execute(self, confirmation: str) -> str:
        """í™•ì¸ í›„ ì‹¤í–‰"""
        # ë¡œì»¬ ê²€ì¦ ìš”ì²­ ì²˜ë¦¬
        if confirmation.lower() in ["local", "2", "ë¡œì»¬"]:
            if hasattr(self, "pending_validation"):
                ddl_content = self.pending_validation["ddl_content"]
                filename = self.pending_validation["filename"]
                result = await self.execute_local_validation_only(ddl_content, filename)
                delattr(self, "pending_validation")
                return result
            else:
                return "âŒ ë¡œì»¬ ê²€ì¦í•  íŒŒì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        if not self.current_plan:
            return "âŒ ì‹¤í–‰í•  ê³„íšì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‘ì—…ì„ ìš”ì²­í•´ì£¼ì„¸ìš”."

        if confirmation.lower() not in ["y", "yes", "ì˜ˆ", "ã…‡"]:
            self.current_plan = None
            return "âŒ ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."

        # ê³„íšì— ë”°ë¼ ì‹¤ì œ ì‹¤í–‰
        plan = self.current_plan
        operation = plan["operation"]

        try:
            result = f"ğŸš€ **ì‹¤í–‰ ì‹œì‘:** {operation}\n\n"

            # DDL ê²€ì¦ ì‹¤í–‰
            if operation == "validate_sql_file":
                if plan["status"] == "error":
                    result += f"âŒ ê³„íš ì˜¤ë¥˜: {plan['error']}"
                else:
                    validation_result = await self.execute_validation_workflow(
                        plan.get("ddl_content", ""),
                        plan.get("database_secret"),
                        plan["filename"],
                    )
                    result += validation_result

            # ê¸°ì¡´ ë‹¤ë¥¸ ì‘ì—…ë“¤...
            elif operation == "test_database_connection":
                connection_result = await self.test_connection_only(
                    plan["parameters"]["database_secret"]
                )
                result += connection_result

            elif operation == "validate_all_sql":
                validation_result = await self.validate_all_sql_files(
                    plan["parameters"].get("database_secret")
                )
                result += validation_result

            elif operation == "analyze_current_schema":
                schema_result = await self.analyze_current_schema(
                    plan["parameters"]["database_secret"]
                )
                if schema_result["success"]:
                    schema = schema_result["schema_analysis"]
                    result += f"""âœ… ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ (DB: {schema['current_database']})

ğŸ“Š **ë¶„ì„ ê²°ê³¼:**
- ì´ í…Œì´ë¸” ìˆ˜: {len(schema['tables'])}ê°œ

ğŸ“‹ **í…Œì´ë¸” ìƒì„¸:**"""
                    for table_name, table_info in schema["tables"].items():
                        result += f"""
ğŸ”¹ **{table_name}** ({table_info['engine']})
   - ì»¬ëŸ¼: {len(table_info['columns'])}ê°œ
   - ì¸ë±ìŠ¤: {len(table_info['indexes'])}ê°œ  
   - ì™¸ë˜í‚¤: {len(table_info['foreign_keys'])}ê°œ
   - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""
                        if table_info["comment"]:
                            result += f"\n   - ì„¤ëª…: {table_info['comment']}"

                        # ì»¬ëŸ¼ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ“‹ **ì»¬ëŸ¼ ì •ë³´:**"
                        if table_info["columns"]:
                            for col_name, col_info in table_info["columns"].items():
                                data_type = col_info["data_type"]
                                if col_info["max_length"]:
                                    data_type += f"({col_info['max_length']})"
                                elif col_info["precision"] and col_info["scale"]:
                                    data_type += (
                                        f"({col_info['precision']},{col_info['scale']})"
                                    )
                                elif col_info["precision"]:
                                    data_type += f"({col_info['precision']})"

                                nullable = (
                                    "NULL"
                                    if col_info["is_nullable"] == "YES"
                                    else "NOT NULL"
                                )
                                key_info = (
                                    f" [{col_info['key']}]" if col_info["key"] else ""
                                )
                                extra_info = (
                                    f" {col_info['extra']}" if col_info["extra"] else ""
                                )
                                default_info = (
                                    f" DEFAULT {col_info['default']}"
                                    if col_info["default"]
                                    else ""
                                )

                                result += f"\n      â€¢ {col_name}: {data_type} {nullable}{key_info}{extra_info}{default_info}"
                                if col_info["comment"]:
                                    result += f" -- {col_info['comment']}"
                        else:
                            result += f"\n      ì»¬ëŸ¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        # ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ” **ì¸ë±ìŠ¤ ì •ë³´:**"
                        if table_info["indexes"]:
                            for idx in table_info["indexes"]:
                                unique_info = "UNIQUE " if idx["unique"] else ""
                                columns_str = ", ".join(idx["columns"])
                                result += f"\n      â€¢ {unique_info}{idx['name']} ({columns_str}) [{idx['type']}]"
                                if idx["comment"]:
                                    result += f" -- {idx['comment']}"
                        else:
                            result += f"\n      ì¸ë±ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # ì™¸ë˜í‚¤ ìƒì„¸ ì •ë³´ ì¶”ê°€ (í•­ìƒ í‘œì‹œ)
                        result += f"\n\n   ğŸ”— **ì™¸ë˜í‚¤ ì •ë³´:**"
                        if table_info["foreign_keys"]:
                            for fk in table_info["foreign_keys"]:
                                result += f"\n      â€¢ {fk['constraint_name']}: {fk['column']} â†’ {fk['referenced_table']}.{fk['referenced_column']}"
                                result += f" (UPDATE: {fk['update_rule']}, DELETE: {fk['delete_rule']})"
                        else:
                            result += f"\n      ì™¸ë˜í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."

                        # í…Œì´ë¸” í†µê³„ ì •ë³´ ì¶”ê°€
                        if table_info["data_length"] or table_info["index_length"]:
                            result += f"\n\n   ğŸ“Š **í…Œì´ë¸” í†µê³„:**"
                            if table_info["data_length"]:
                                data_size = (
                                    table_info["data_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ë°ì´í„° í¬ê¸°: {data_size:.2f} MB"
                            if table_info["index_length"]:
                                index_size = (
                                    table_info["index_length"] / 1024 / 1024
                                )  # MB ë³€í™˜
                                result += f"\n      â€¢ ì¸ë±ìŠ¤ í¬ê¸°: {index_size:.2f} MB"
                else:
                    result += f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            else:
                result += f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—…: {operation}"

            # ê³„íš ì™„ë£Œ
            self.current_plan = None
            result += f"\n\nâœ… **ì‹¤í–‰ ì™„ë£Œ:** {operation}"

            return result

        except Exception as e:
            self.current_plan = None
            return f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_schema_summary(self, database_secret: str) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            schema_result = await self.analyze_current_schema(database_secret)
            if not schema_result["success"]:
                return f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {schema_result['error']}"

            schema = schema_result["schema_analysis"]

            summary = f"""ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìš”ì•½ (DB: {schema['current_database']})

ğŸ“‹ **í…Œì´ë¸” ëª©ë¡** ({len(schema['tables'])}ê°œ):"""

            for table_name, table_info in schema["tables"].items():
                column_count = len(table_info["columns"])
                index_count = len(table_info["indexes"])
                fk_count = len(table_info["foreign_keys"])

                summary += f"""
  ğŸ”¹ **{table_name}** ({table_info['engine']})
     - ì»¬ëŸ¼: {column_count}ê°œ, ì¸ë±ìŠ¤: {index_count}ê°œ, ì™¸ë˜í‚¤: {fk_count}ê°œ
     - ì˜ˆìƒ í–‰ ìˆ˜: {table_info['rows']:,}"""

                if table_info["comment"]:
                    summary += f"\n     - ì„¤ëª…: {table_info['comment']}"

            return summary

        except Exception as e:
            return f"ìŠ¤í‚¤ë§ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def validate_all_sql_files(
        self, database_secret: Optional[str] = None
    ) -> str:
        """ëª¨ë“  SQL íŒŒì¼ ê²€ì¦ (ìµœëŒ€ 5ê°œ) - í†µí•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            sql_files = list(SQL_DIR.glob("*.sql"))
            if not sql_files:
                return "sql ë””ë ‰í† ë¦¬ì— SQL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # ìµœëŒ€ 5ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
            files_to_process = sql_files[:5]
            if len(sql_files) > 5:
                logger.warning(
                    f"SQL íŒŒì¼ì´ {len(sql_files)}ê°œ ìˆì§€ë§Œ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
                )

            validation_results = []
            summary_results = []

            for sql_file in files_to_process:
                try:
                    # ê°œë³„ íŒŒì¼ ê²€ì¦ì„ execute_validation_workflowë¡œ ì‹¤í–‰ (ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ë°©ì‹)
                    with open(sql_file, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ê°œë³„ ê²€ì¦ê³¼ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                    result = await self.execute_validation_workflow(
                        ddl_content, database_secret, sql_file.name
                    )

                    # ê²°ê³¼ì—ì„œ ìƒíƒœ íŒŒì•… (ê°„ë‹¨í•œ ë°©ì‹)
                    if "âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤" in result:
                        status = "PASS"
                        issues = []
                    else:
                        status = "FAIL"
                        # ê²°ê³¼ì—ì„œ ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ
                        import re

                        issue_match = re.search(r"(\d+)ê°œ ë¬¸ì œ", result)
                        issue_count = int(issue_match.group(1)) if issue_match else 1
                        issues = [f"ê²€ì¦ ì‹¤íŒ¨ ({issue_count}ê°œ ë¬¸ì œ ë°œê²¬)"]

                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": ddl_content,
                            "ddl_type": self.extract_ddl_type(ddl_content),
                            "status": status,
                            "issues": issues,
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": status == "PASS",
                            "full_result": result,
                        }
                    )

                    # ìš”ì•½ ê²°ê³¼
                    if status == "PASS":
                        summary_results.append(f"âœ… **{sql_file.name}**: í†µê³¼")
                    else:
                        summary_results.append(f"âŒ **{sql_file.name}**: ì‹¤íŒ¨")

                except Exception as e:
                    validation_results.append(
                        {
                            "filename": sql_file.name,
                            "ddl_content": "",
                            "ddl_type": "ERROR",
                            "status": "ERROR",
                            "issues": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                            "warnings": [],
                            "db_connection_info": None,
                            "syntax_valid": False,
                            "full_result": f"ì˜¤ë¥˜: {str(e)}",
                        }
                    )
                    summary_results.append(f"âŒ **{sql_file.name}**: ì˜¤ë¥˜ - {str(e)}")

            # í†µí•© HTML ë³´ê³ ì„œ ìƒì„±
            consolidated_report_path = await self.generate_consolidated_html_report(
                validation_results, database_secret
            )

            # ìš”ì•½ í†µê³„
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            summary = f"""ğŸ“Š ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì™„ë£Œ

ğŸ“‹ ìš”ì•½:
â€¢ ì´ íŒŒì¼: {total_files}ê°œ
â€¢ í†µê³¼: {passed_files}ê°œ ({passed_files/total_files*100:.1f}%)
â€¢ ì‹¤íŒ¨: {failed_files}ê°œ ({failed_files/total_files*100:.1f}%)

ğŸ“„ ì¢…í•© ë³´ê³ ì„œ: {consolidated_report_path}

ğŸ“Š ê°œë³„ ê²°ê³¼:
{chr(10).join(summary_results)}"""

            if len(sql_files) > 5:
                summary += (
                    f"\n\nâš ï¸ ì „ì²´ {len(sql_files)}ê°œ íŒŒì¼ ì¤‘ ì²˜ìŒ 5ê°œë§Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                )

            return summary

        except Exception as e:
            return f"ì „ì²´ SQL íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"

    async def generate_consolidated_html_report(
        self, validation_results: List[Dict], database_secret: str
    ) -> str:
        """ì—¬ëŸ¬ SQL íŒŒì¼ì˜ í†µí•© HTML ë³´ê³ ì„œ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"consolidated_validation_report_{timestamp}.html"
            report_path = OUTPUT_DIR / report_filename

            # ì „ì²´ í†µê³„ ê³„ì‚°
            total_files = len(validation_results)
            passed_files = sum(1 for r in validation_results if r["status"] == "PASS")
            failed_files = total_files - passed_files

            # íŒŒì¼ë³„ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
            file_sections = ""
            for i, result in enumerate(validation_results, 1):
                status_color = "#28a745" if result["status"] == "PASS" else "#dc3545"
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"

                # Claude ê²€ì¦ê³¼ ê¸°íƒ€ ê²€ì¦ ë¶„ë¦¬
                claude_issues = []
                other_issues = []

                for issue in result["issues"]:
                    if issue.startswith("Claude ê²€ì¦:"):
                        claude_issues.append(issue[12:].strip())
                    else:
                        other_issues.append(issue)

                # ê¸°íƒ€ ë¬¸ì œ ì„¹ì…˜
                other_issues_html = ""
                if other_issues:
                    other_issues_html = f"""
                    <div class="issues-subsection">
                        <h5>ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ</h5>
                        <ul>
                            {''.join(f'<li>{issue}</li>' for issue in other_issues)}
                        </ul>
                    </div>
                    """

                # Claude ê²€ì¦ ê²°ê³¼ ì„¹ì…˜
                claude_section_html = ""
                if claude_issues:
                    claude_section_html = f"""
                    <div class="claude-subsection">
                        <h5>ğŸ¤– Claude AI ê²€ì¦</h5>
                        {''.join(f'<div class="claude-result-small"><pre>{claude_result}</pre></div>' for claude_result in claude_issues)}
                    </div>
                    """

                # ì„±ê³µ ì„¹ì…˜
                success_html = ""
                if not result["issues"]:
                    success_html = '<div class="success-message">âœ… ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.</div>'

                file_sections += f"""
                <div class="file-section">
                    <div class="file-header">
                        <h3>{status_icon} {i}. {result['filename']}</h3>
                        <span class="status-badge" style="background-color: {status_color};">{result['status']}</span>
                    </div>
                    
                    <div class="file-details">
                        <div class="detail-item">
                            <strong>DDL íƒ€ì…:</strong> {result['ddl_type']}
                        </div>
                        <div class="detail-item">
                            <strong>ë¬¸ì œ ìˆ˜:</strong> {len(result['issues'])}ê°œ
                        </div>
                    </div>
                    
                    <div class="sql-code-small">
                        <h4>ğŸ“ DDL ë‚´ìš©</h4>
                        <pre>{result['ddl_content']}</pre>
                    </div>
                    
                    <div class="validation-result">
                        <h4>ğŸ“Š ê²€ì¦ ê²°ê³¼</h4>
                        <pre>{result.get('full_result', 'ê²°ê³¼ ì—†ìŒ')}</pre>
                    </div>
                    
                    {claude_section_html}
                    {other_issues_html}
                    {success_html}
                </div>
                """

            # HTML ë³´ê³ ì„œ ìƒì„±
            html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í†µí•© DDL ê²€ì¦ ë³´ê³ ì„œ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .content {{
            padding: 30px;
        }}
        .summary-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .stat-item {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        .stat-number {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        .file-section {{
            margin: 30px 0;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
        }}
        .file-header {{
            background: #f8f9fa;
            padding: 20px;
            border-bottom: 1px solid #e9ecef;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .file-header h3 {{
            margin: 0;
            color: #333;
        }}
        .status-badge {{
            padding: 6px 12px;
            border-radius: 15px;
            color: white;
            font-weight: bold;
            font-size: 0.9em;
        }}
        .file-details {{
            padding: 15px 20px;
            background: #fafafa;
            border-bottom: 1px solid #e9ecef;
        }}
        .detail-item {{
            display: inline-block;
            margin-right: 30px;
            color: #666;
        }}
        .sql-code-small {{
            padding: 20px;
        }}
        .sql-code-small h4, .validation-result h4 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .sql-code-small pre, .validation-result pre {{
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            max-height: 200px;
            overflow-y: auto;
        }}
        .validation-result {{
            padding: 20px;
            border-top: 1px solid #e9ecef;
        }}
        .claude-subsection {{
            padding: 15px 20px;
            background: #f8f9ff;
            border-top: 1px solid #e9ecef;
        }}
        .claude-subsection h5 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .claude-result-small {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            margin: 10px 0;
        }}
        .claude-result-small pre {{
            padding: 15px;
            margin: 0;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 0.9em;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 300px;
            overflow-y: auto;
        }}
        .issues-subsection {{
            padding: 15px 20px;
            border-top: 1px solid #e9ecef;
        }}
        .issues-subsection h5 {{
            margin: 0 0 15px 0;
            color: #495057;
        }}
        .issues-subsection ul {{
            margin: 0;
            padding-left: 20px;
        }}
        .issues-subsection li {{
            margin: 5px 0;
            color: #dc3545;
        }}
        .success-message {{
            padding: 15px 20px;
            background: #d4edda;
            color: #155724;
            border-top: 1px solid #c3e6cb;
            font-weight: 500;
        }}
        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }}
        @media (max-width: 768px) {{
            .summary-stats {{
                grid-template-columns: 1fr;
            }}
            .file-header {{
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š í†µí•© DDL ê²€ì¦ ë³´ê³ ì„œ</h1>
            <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="content">
            <div class="summary-stats">
                <div class="stat-item">
                    <div class="stat-number">{total_files}</div>
                    <div class="stat-label">ì´ íŒŒì¼ ìˆ˜</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number" style="color: #28a745;">{passed_files}</div>
                    <div class="stat-label">í†µê³¼</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number" style="color: #dc3545;">{failed_files}</div>
                    <div class="stat-label">ì‹¤íŒ¨</div>
                </div>
                <div class="stat-item">
                    <div class="stat-number">{passed_files/total_files*100:.1f}%</div>
                    <div class="stat-label">ì„±ê³µë¥ </div>
                </div>
            </div>
            
            <div class="database-info">
                <h2>ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´</h2>
                <p><strong>ì‹œí¬ë¦¿:</strong> {database_secret or 'N/A'}</p>
            </div>
            
            {file_sections}
        </div>
        
        <div class="footer">
            <p>Generated by DB Assistant MCP Server</p>
            <p>Report generated at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>"""

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            return str(report_path)

        except Exception as e:
            logger.error(f"í†µí•© HTML ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    async def list_aurora_mysql_clusters(self, region: str = "ap-northeast-2") -> str:
        """í˜„ì¬ ë¦¬ì „ì˜ Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)

            # Aurora MySQL í´ëŸ¬ìŠ¤í„° ì¡°íšŒ
            response = rds_client.describe_db_clusters()

            aurora_mysql_clusters = []
            for cluster in response["DBClusters"]:
                if (
                    cluster["Engine"] == "aurora-mysql"
                    and cluster["Status"] == "available"
                ):
                    aurora_mysql_clusters.append(
                        {
                            "identifier": cluster["DBClusterIdentifier"],
                            "engine_version": cluster["EngineVersion"],
                            "endpoint": cluster.get("Endpoint", "N/A"),
                            "status": cluster["Status"],
                            "database_name": cluster.get("DatabaseName", "N/A"),
                        }
                    )

            if not aurora_mysql_clusters:
                return f"âŒ {region} ë¦¬ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            result = f"ğŸ—„ï¸ **Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡** ({region} ë¦¬ì „)\n\n"

            for i, cluster in enumerate(aurora_mysql_clusters, 1):
                result += f"**{i}. {cluster['identifier']}**\n"
                result += f"   â€¢ ì—”ì§„ ë²„ì „: {cluster['engine_version']}\n"
                result += f"   â€¢ ì—”ë“œí¬ì¸íŠ¸: {cluster['endpoint']}\n"
                result += f"   â€¢ ê¸°ë³¸ DB: {cluster['database_name']}\n"
                result += f"   â€¢ ìƒíƒœ: {cluster['status']}\n\n"

            result += "ğŸ’¡ **ì‚¬ìš©ë²•:** í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ Aurora MySQL í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_aurora_cluster(
        self, cluster_selection: str, region: str = "ap-northeast-2"
    ) -> str:
        """Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ"""
        try:
            rds_client = boto3.client("rds", region_name=region, verify=False)
            response = rds_client.describe_db_clusters()

            aurora_mysql_clusters = []
            for cluster in response["DBClusters"]:
                if (
                    cluster["Engine"] == "aurora-mysql"
                    and cluster["Status"] == "available"
                ):
                    aurora_mysql_clusters.append(cluster)

            if not aurora_mysql_clusters:
                return f"âŒ {region} ë¦¬ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            selected_cluster = None

            # ë²ˆí˜¸ë¡œ ì„ íƒ
            if cluster_selection.isdigit():
                cluster_index = int(cluster_selection) - 1
                if 0 <= cluster_index < len(aurora_mysql_clusters):
                    selected_cluster = aurora_mysql_clusters[cluster_index]
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒ
                for cluster in aurora_mysql_clusters:
                    if (
                        cluster_selection.lower()
                        in cluster["DBClusterIdentifier"].lower()
                    ):
                        selected_cluster = cluster
                        break

            if not selected_cluster:
                return f"âŒ ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cluster_selection}"

            # ì„ íƒëœ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥ (ì„ì‹œë¡œ í´ë˜ìŠ¤ ë³€ìˆ˜ì— ì €ì¥)
            self.selected_cluster = {
                "identifier": selected_cluster["DBClusterIdentifier"],
                "endpoint": selected_cluster.get("Endpoint"),
                "database_name": selected_cluster.get("DatabaseName"),
                "region": region,
            }

            return f"""âœ… **Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ ì™„ë£Œ**

ğŸ—„ï¸ **ì„ íƒëœ í´ëŸ¬ìŠ¤í„°:** {selected_cluster['DBClusterIdentifier']}
ğŸŒ **ì—”ë“œí¬ì¸íŠ¸:** {selected_cluster.get('Endpoint', 'N/A')}
ğŸ“Š **ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤:** {selected_cluster.get('DatabaseName', 'N/A')}
ğŸ“ **ë¦¬ì „:** {region}

ğŸ’¡ ì´ì œ ì´ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ê²€ì¦ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        except Exception as e:
            return f"âŒ Aurora í´ëŸ¬ìŠ¤í„° ì„ íƒ ì‹¤íŒ¨: {str(e)}"

    async def list_databases(self, database_secret: str) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = cursor.fetchall()

            # ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ì œì™¸
            system_dbs = {"information_schema", "performance_schema", "mysql", "sys"}
            user_databases = [db[0] for db in databases if db[0] not in system_dbs]

            cursor.close()
            connection.close()

            if not user_databases:
                return "âŒ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            result = "ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡**\n\n"

            for i, db_name in enumerate(user_databases, 1):
                result += f"**{i}. {db_name}**\n"

            result += "\nğŸ’¡ **ì‚¬ìš©ë²•:** ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸ë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."

            return result

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def select_database(
        self, database_secret: str, database_selection: str
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ë° ë³€ê²½"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
            cursor.execute("SHOW DATABASES")
            databases = cursor.fetchall()

            # ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ì œì™¸
            system_dbs = {"information_schema", "performance_schema", "mysql", "sys"}
            user_databases = [db[0] for db in databases if db[0] not in system_dbs]

            if not user_databases:
                cursor.close()
                connection.close()
                return "âŒ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            selected_db = None

            # ë²ˆí˜¸ë¡œ ì„ íƒ
            if database_selection.isdigit():
                db_index = int(database_selection) - 1
                if 0 <= db_index < len(user_databases):
                    selected_db = user_databases[db_index]
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒ
                for db_name in user_databases:
                    if database_selection.lower() in db_name.lower():
                        selected_db = db_name
                        break

            if not selected_db:
                cursor.close()
                connection.close()
                return (
                    f"âŒ ì„ íƒí•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {database_selection}"
                )

            # USE ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½
            cursor.execute(f"USE `{selected_db}`")

            # ë³€ê²½ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]

            # ì„ íƒëœ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            self.selected_database = selected_db

            cursor.close()
            connection.close()

            return f"""âœ… **ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ì™„ë£Œ**

ğŸ—„ï¸ **í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤:** {current_db}
ğŸ”„ **ë³€ê²½ ëª…ë ¹:** USE `{selected_db}`

ğŸ’¡ ì´ì œ ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        except Exception as e:
            return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹¤íŒ¨: {str(e)}"

    async def copy_sql_file(
        self, source_path: str, target_name: Optional[str] = None
    ) -> str:
        """SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
        try:
            source = Path(source_path)
            if not source.exists():
                return f"ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}"

            if not source.suffix.lower() == ".sql":
                return f"SQL íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {source_path}"

            # ëŒ€ìƒ íŒŒì¼ëª… ê²°ì •
            if target_name:
                if not target_name.endswith(".sql"):
                    target_name += ".sql"
                target_path = SQL_DIR / target_name
            else:
                target_path = SQL_DIR / source.name

            # íŒŒì¼ ë³µì‚¬
            import shutil

            shutil.copy2(source, target_path)

            return f"âœ… SQL íŒŒì¼ì´ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤: {source.name} -> {target_path.name}"

        except Exception as e:
            return f"SQL íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}"

    async def test_connection_only(self, database_secret: str) -> str:
        """ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"""
        try:
            connection_result = await self.test_database_connection(
                database_secret, use_ssh_tunnel=True
            )

            if connection_result["success"]:
                databases_list = "\n".join(
                    [f"   - {db}" for db in connection_result.get("databases", [])]
                )
                tables_list = "\n".join(
                    [f"   - {table}" for table in connection_result.get("tables", [])]
                )

                return f"""âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!

**ì—°ê²° ì •ë³´:**
- í˜¸ìŠ¤íŠ¸: {connection_result.get('host', 'N/A')}
- í¬íŠ¸: {connection_result.get('port', 'N/A')}
- ì—°ê²° ë°©ì‹: {connection_result.get('connection_method', 'N/A')}
- í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {connection_result.get('current_database', 'N/A')}
- ì„œë²„ ë²„ì „: {connection_result.get('server_version', 'N/A')}

**ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡:**
{databases_list if databases_list else '   (ì—†ìŒ)'}

**í˜„ì¬ DB í…Œì´ë¸” ëª©ë¡:**
{tables_list if tables_list else '   (ì—†ìŒ)'}"""
            else:
                return f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {connection_result['error']}"

        except Exception as e:
            return f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def get_performance_metrics(
        self, database_secret: str, metric_type: str = "all"
    ) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“Š **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­**\n\n"

            if metric_type in ["all", "query"]:
                # ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        DIGEST_TEXT as query_pattern,
                        COUNT_STAR as exec_count,
                        ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                        ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                        ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec
                    FROM performance_schema.events_statements_summary_by_digest 
                    WHERE DIGEST_TEXT IS NOT NULL
                    ORDER BY AVG_TIMER_WAIT DESC 
                    LIMIT 5
                """
                )

                query_stats = cursor.fetchall()
                if query_stats:
                    result += "ğŸ” **ëŠë¦° ì¿¼ë¦¬ TOP 5:**\n"
                    for i, (
                        pattern,
                        count,
                        avg_time,
                        max_time,
                        total_time,
                    ) in enumerate(query_stats, 1):
                        pattern_short = (
                            (pattern[:60] + "...") if len(pattern) > 60 else pattern
                        )
                        result += f"{i}. {pattern_short}\n"
                        result += f"   - ì‹¤í–‰íšŸìˆ˜: {count:,}, í‰ê· ì‹œê°„: {avg_time:.3f}ì´ˆ, ìµœëŒ€ì‹œê°„: {max_time:.3f}ì´ˆ\n\n"

            if metric_type in ["all", "connection"]:
                # ì—°ê²° í†µê³„
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total_connections,
                        SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections
                    FROM information_schema.processlist
                """
                )

                conn_stats = cursor.fetchone()
                if conn_stats:
                    result += f"ğŸ”— **ì—°ê²° í†µê³„:**\n"
                    result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                    result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def analyze_slow_queries(self, database_secret: str, limit: int = 10) -> str:
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸŒ **ëŠë¦° ì¿¼ë¦¬ ë¶„ì„** (ìƒìœ„ {limit}ê°œ)\n\n"

            # Performance Schemaì—ì„œ ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
            cursor.execute(
                f"""
                SELECT 
                    DIGEST_TEXT as query_pattern,
                    COUNT_STAR as exec_count,
                    ROUND(AVG_TIMER_WAIT/1000000000000, 6) as avg_time_sec,
                    ROUND(MAX_TIMER_WAIT/1000000000000, 6) as max_time_sec,
                    ROUND(SUM_TIMER_WAIT/1000000000000, 6) as total_time_sec,
                    ROUND(SUM_ROWS_EXAMINED/COUNT_STAR, 0) as avg_rows_examined,
                    ROUND(SUM_ROWS_SENT/COUNT_STAR, 0) as avg_rows_sent
                FROM performance_schema.events_statements_summary_by_digest 
                WHERE DIGEST_TEXT IS NOT NULL 
                AND AVG_TIMER_WAIT > 1000000000
                ORDER BY AVG_TIMER_WAIT DESC 
                LIMIT {limit}
            """
            )

            slow_queries = cursor.fetchall()

            if not slow_queries:
                result += "âœ… ëŠë¦° ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    pattern,
                    count,
                    avg_time,
                    max_time,
                    total_time,
                    avg_examined,
                    avg_sent,
                ) in enumerate(slow_queries, 1):
                    pattern_short = (
                        (pattern[:80] + "...") if len(pattern) > 80 else pattern
                    )
                    result += f"**{i}. ì¿¼ë¦¬ íŒ¨í„´:**\n```sql\n{pattern_short}\n```\n"
                    result += f"ğŸ“ˆ **í†µê³„:**\n"
                    result += f"- ì‹¤í–‰ íšŸìˆ˜: {count:,}íšŒ\n"
                    result += f"- í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.3f}ì´ˆ\n"
                    result += f"- ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {max_time:.3f}ì´ˆ\n"
                    result += f"- ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.3f}ì´ˆ\n"
                    result += f"- í‰ê·  ê²€ì‚¬ í–‰ ìˆ˜: {avg_examined:,.0f}í–‰\n"
                    result += f"- í‰ê·  ë°˜í™˜ í–‰ ìˆ˜: {avg_sent:,.0f}í–‰\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ëŠë¦° ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_table_io_stats(
        self, database_secret: str, schema_name: Optional[str] = None
    ) -> str:
        """í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            # ìŠ¤í‚¤ë§ˆ ì´ë¦„ ì„¤ì •
            if not schema_name:
                cursor.execute("SELECT DATABASE()")
                schema_name = cursor.fetchone()[0]

            result = f"ğŸ’¿ **í…Œì´ë¸” I/O í†µê³„** (ìŠ¤í‚¤ë§ˆ: {schema_name})\n\n"

            # í…Œì´ë¸”ë³„ I/O í†µê³„ ì¡°íšŒ
            cursor.execute(
                """
                SELECT 
                    object_name as table_name,
                    count_read,
                    count_write,
                    count_read + count_write as total_io,
                    sum_timer_read/1000000000000 as read_time_sec,
                    sum_timer_write/1000000000000 as write_time_sec,
                    (sum_timer_read + sum_timer_write)/1000000000000 as total_time_sec
                FROM performance_schema.table_io_waits_summary_by_table 
                WHERE object_schema = %s
                AND count_read + count_write > 0
                ORDER BY count_read + count_write DESC 
                LIMIT 10
            """,
                (schema_name,),
            )

            io_stats = cursor.fetchall()

            if not io_stats:
                result += "ğŸ“Š I/O í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                result += "ğŸ“Š **ìƒìœ„ 10ê°œ í…Œì´ë¸” (I/O ê¸°ì¤€):**\n\n"
                for i, (
                    table,
                    read_count,
                    write_count,
                    total_io,
                    read_time,
                    write_time,
                    total_time,
                ) in enumerate(io_stats, 1):
                    result += f"**{i}. {table}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ I/O: {total_io:,}íšŒ ({total_time:.3f}ì´ˆ)\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ í…Œì´ë¸” I/O í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_index_usage_stats(
        self, database_secret: str, table_name: Optional[str] = None
    ) -> str:
        """ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ“‡ **ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„**"
            if table_name:
                result += f" (í…Œì´ë¸”: {table_name})"
            result += "\n\n"

            # ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ
            if table_name:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND object_name = %s
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC
                """,
                    (table_name,),
                )
            else:
                cursor.execute(
                    """
                    SELECT 
                        object_name as table_name,
                        index_name,
                        count_read,
                        count_write,
                        count_read + count_write as total_usage,
                        sum_timer_read/1000000000000 as read_time_sec,
                        sum_timer_write/1000000000000 as write_time_sec
                    FROM performance_schema.table_io_waits_summary_by_index_usage 
                    WHERE object_schema = DATABASE()
                    AND count_read + count_write > 0
                    ORDER BY count_read + count_write DESC 
                    LIMIT 15
                """
                )

            index_stats = cursor.fetchall()

            if not index_stats:
                result += "ğŸ“Š ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            else:
                for i, (
                    table,
                    index,
                    read_count,
                    write_count,
                    total_usage,
                    read_time,
                    write_time,
                ) in enumerate(index_stats, 1):
                    index_display = index if index else "PRIMARY"
                    result += f"**{i}. {table}.{index_display}**\n"
                    result += f"- ì½ê¸°: {read_count:,}íšŒ ({read_time:.3f}ì´ˆ)\n"
                    result += f"- ì“°ê¸°: {write_count:,}íšŒ ({write_time:.3f}ì´ˆ)\n"
                    result += f"- ì´ ì‚¬ìš©: {total_usage:,}íšŒ\n\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_connection_stats(self, database_secret: str) -> str:
        """ì—°ê²° ë° ì„¸ì…˜ í†µê³„ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”— **ì—°ê²° ë° ì„¸ì…˜ í†µê³„**\n\n"

            # í˜„ì¬ ì—°ê²° ìƒíƒœ
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total_connections,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_connections,
                    SUM(CASE WHEN COMMAND = 'Sleep' THEN 1 ELSE 0 END) as idle_connections
                FROM information_schema.processlist
            """
            )

            conn_stats = cursor.fetchone()
            if conn_stats:
                result += f"ğŸ“Š **í˜„ì¬ ì—°ê²° ìƒíƒœ:**\n"
                result += f"- ì´ ì—°ê²°: {conn_stats[0]}ê°œ\n"
                result += f"- í™œì„± ì—°ê²°: {conn_stats[1]}ê°œ\n"
                result += f"- ìœ íœ´ ì—°ê²°: {conn_stats[2]}ê°œ\n\n"

            # ì‚¬ìš©ìë³„ ì—°ê²° í†µê³„
            cursor.execute(
                """
                SELECT 
                    USER as username,
                    COUNT(*) as connection_count,
                    SUM(CASE WHEN COMMAND != 'Sleep' THEN 1 ELSE 0 END) as active_count
                FROM information_schema.processlist
                GROUP BY USER
                ORDER BY connection_count DESC
            """
            )

            user_stats = cursor.fetchall()
            if user_stats:
                result += f"ğŸ‘¥ **ì‚¬ìš©ìë³„ ì—°ê²°:**\n"
                for user, total, active in user_stats:
                    result += f"- {user}: {total}ê°œ ì—°ê²° (í™œì„±: {active}ê°œ)\n"
                result += "\n"

            # Performance Schema ìŠ¤ë ˆë“œ ì •ë³´
            cursor.execute(
                """
                SELECT 
                    type,
                    COUNT(*) as thread_count
                FROM performance_schema.threads
                GROUP BY type
            """
            )

            thread_stats = cursor.fetchall()
            if thread_stats:
                result += f"ğŸ§µ **ìŠ¤ë ˆë“œ í†µê³„:**\n"
                for thread_type, count in thread_stats:
                    result += f"- {thread_type}: {count}ê°œ\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ì—°ê²° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_memory_usage(self, database_secret: str) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„**\n\n"

            # ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ)
            cursor.execute(
                """
                SELECT 
                    event_name,
                    ROUND(sum_number_of_bytes_alloc/1024/1024, 2) as allocated_mb,
                    ROUND(sum_number_of_bytes_free/1024/1024, 2) as freed_mb,
                    ROUND((sum_number_of_bytes_alloc - sum_number_of_bytes_free)/1024/1024, 2) as current_mb
                FROM performance_schema.memory_summary_global_by_event_name
                WHERE sum_number_of_bytes_alloc > 0
                ORDER BY (sum_number_of_bytes_alloc - sum_number_of_bytes_free) DESC
                LIMIT 10
            """
            )

            memory_stats = cursor.fetchall()
            if memory_stats:
                result += f"ğŸ“Š **ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ):**\n"
                for event, allocated, freed, current in memory_stats:
                    event_short = event.replace("memory/", "").replace("sql/", "")
                    result += f"- {event_short}: {current:.2f}MB (í• ë‹¹: {allocated:.2f}MB, í•´ì œ: {freed:.2f}MB)\n"
                result += "\n"

            # ì£¼ìš” ë©”ëª¨ë¦¬ ê´€ë ¨ ì‹œìŠ¤í…œ ë³€ìˆ˜
            cursor.execute(
                """
                SHOW VARIABLES WHERE Variable_name IN (
                    'innodb_buffer_pool_size',
                    'key_buffer_size',
                    'query_cache_size',
                    'tmp_table_size',
                    'max_heap_table_size',
                    'sort_buffer_size',
                    'read_buffer_size',
                    'join_buffer_size'
                )
            """
            )

            variables = cursor.fetchall()
            if variables:
                result += f"âš™ï¸ **ì£¼ìš” ë©”ëª¨ë¦¬ ì„¤ì •:**\n"
                for var_name, var_value in variables:
                    # ë°”ì´íŠ¸ ë‹¨ìœ„ë¥¼ MBë¡œ ë³€í™˜
                    if var_value.isdigit():
                        mb_value = int(var_value) / 1024 / 1024
                        result += f"- {var_name}: {mb_value:.2f}MB\n"
                    else:
                        result += f"- {var_name}: {var_value}\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def get_lock_analysis(self, database_secret: str) -> str:
        """ë½ ìƒíƒœ ë¶„ì„"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”’ **ë½ ìƒíƒœ ë¶„ì„**\n\n"

            # í˜„ì¬ ë½ ìƒíƒœ (MySQL 8.0+)
            try:
                cursor.execute(
                    """
                    SELECT 
                        object_schema,
                        object_name,
                        lock_type,
                        lock_mode,
                        lock_status,
                        thread_id
                    FROM performance_schema.data_locks
                    LIMIT 10
                """
                )

                current_locks = cursor.fetchall()
                if current_locks:
                    result += f"ğŸ” **í˜„ì¬ í™œì„± ë½ (ìƒìœ„ 10ê°œ):**\n"
                    for (
                        schema,
                        table,
                        lock_type,
                        lock_mode,
                        status,
                        thread_id,
                    ) in current_locks:
                        result += f"- {schema}.{table}: {lock_type} ({lock_mode}) - {status} [Thread: {thread_id}]\n"
                    result += "\n"
                else:
                    result += f"âœ… **í˜„ì¬ í™œì„± ë½:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **í˜„ì¬ ë½ ì •ë³´:** Performance Schema data_locks í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ë½ ëŒ€ê¸° ìƒí™©
            try:
                cursor.execute(
                    """
                    SELECT 
                        requesting_thread_id,
                        blocking_thread_id,
                        object_schema,
                        object_name,
                        lock_type
                    FROM performance_schema.data_lock_waits
                    LIMIT 10
                """
                )

                lock_waits = cursor.fetchall()
                if lock_waits:
                    result += f"â³ **ë½ ëŒ€ê¸° ìƒí™©:**\n"
                    for (
                        req_thread,
                        block_thread,
                        schema,
                        table,
                        lock_type,
                    ) in lock_waits:
                        result += f"- Thread {req_thread}ì´ Thread {block_thread}ì— ì˜í•´ ëŒ€ê¸° ì¤‘\n"
                        result += f"  ëŒ€ìƒ: {schema}.{table} ({lock_type})\n"
                    result += "\n"
                else:
                    result += f"âœ… **ë½ ëŒ€ê¸°:** ì—†ìŒ\n\n"
            except Exception:
                result += f"â„¹ï¸ **ë½ ëŒ€ê¸° ì •ë³´:** Performance Schema data_lock_waits í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

            # ëŒ€ê¸° ì´ë²¤íŠ¸ í†µê³„
            cursor.execute(
                """
                SELECT 
                    event_name,
                    count_star as event_count,
                    ROUND(sum_timer_wait/1000000000000, 6) as total_wait_sec,
                    ROUND(avg_timer_wait/1000000000000, 6) as avg_wait_sec
                FROM performance_schema.events_waits_summary_global_by_event_name
                WHERE event_name LIKE '%lock%' 
                AND count_star > 0
                ORDER BY sum_timer_wait DESC
                LIMIT 5
            """
            )

            wait_events = cursor.fetchall()
            if wait_events:
                result += f"â±ï¸ **ë½ ê´€ë ¨ ëŒ€ê¸° ì´ë²¤íŠ¸ (ìƒìœ„ 5ê°œ):**\n"
                for event, count, total_wait, avg_wait in wait_events:
                    event_short = event.replace("wait/synch/", "").replace(
                        "wait/io/", ""
                    )
                    result += f"- {event_short}: {count:,}íšŒ (ì´ {total_wait:.3f}ì´ˆ, í‰ê·  {avg_wait:.6f}ì´ˆ)\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    async def get_replication_status(self, database_secret: str) -> str:
        """ë³µì œ ìƒíƒœ ì¡°íšŒ"""
        try:
            connection, tunnel_used = await self.get_db_connection(database_secret)
            cursor = connection.cursor()

            result = f"ğŸ”„ **ë³µì œ ìƒíƒœ ë¶„ì„**\n\n"

            # ë³µì œ ì—°ê²° ìƒíƒœ (Performance Schema)
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        host,
                        port,
                        user,
                        source_connection_auto_failover,
                        connection_retry_interval,
                        connection_retry_count
                    FROM performance_schema.replication_connection_configuration
                """
                )

                repl_config = cursor.fetchall()
                if repl_config:
                    result += f"âš™ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:**\n"
                    for (
                        channel,
                        host,
                        port,
                        user,
                        auto_failover,
                        retry_interval,
                        retry_count,
                    ) in repl_config:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ì†ŒìŠ¤: {host}:{port} (ì‚¬ìš©ì: {user})\n"
                        result += f"  ìë™ ì¥ì• ì¡°ì¹˜: {auto_failover}\n"
                        result += (
                            f"  ì¬ì‹œë„: {retry_count}íšŒ, ê°„ê²©: {retry_interval}ì´ˆ\n\n"
                        )
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ì„¤ì •:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ìƒíƒœ
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        service_state,
                        received_transaction_set,
                        last_error_message,
                        last_error_timestamp
                    FROM performance_schema.replication_connection_status
                """
                )

                repl_status = cursor.fetchall()
                if repl_status:
                    result += f"ğŸ“Š **ë³µì œ ì—°ê²° ìƒíƒœ:**\n"
                    for channel, state, trans_set, error_msg, error_time in repl_status:
                        result += f"- ì±„ë„: {channel}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if trans_set:
                            result += f"  ìˆ˜ì‹  íŠ¸ëœì­ì…˜: {trans_set[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ë§ˆì§€ë§‰ ì˜¤ë¥˜: {error_msg}\n"
                            result += f"  ì˜¤ë¥˜ ì‹œê°„: {error_time}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì—°ê²° ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë³µì œ ì§€ì—° ì •ë³´
            try:
                cursor.execute(
                    """
                    SELECT 
                        channel_name,
                        worker_id,
                        service_state,
                        last_error_message,
                        last_applied_transaction
                    FROM performance_schema.replication_applier_status_by_worker
                """
                )

                worker_status = cursor.fetchall()
                if worker_status:
                    result += f"ğŸ‘· **ë³µì œ ì›Œì»¤ ìƒíƒœ:**\n"
                    for (
                        channel,
                        worker_id,
                        state,
                        error_msg,
                        last_trans,
                    ) in worker_status:
                        result += f"- ì±„ë„: {channel}, ì›Œì»¤: {worker_id}\n"
                        result += f"  ìƒíƒœ: {state}\n"
                        if last_trans:
                            result += f"  ë§ˆì§€ë§‰ ì ìš©: {last_trans[:50]}...\n"
                        if error_msg:
                            result += f"  âŒ ì˜¤ë¥˜: {error_msg}\n"
                        result += "\n"
                else:
                    result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì—†ìŒ\n\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë³µì œ ì›Œì»¤ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n\n"

            # ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ
            try:
                cursor.execute("SHOW MASTER STATUS")
                master_status = cursor.fetchone()
                if master_status:
                    result += f"ğŸ“ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:**\n"
                    result += f"- íŒŒì¼: {master_status[0]}\n"
                    result += f"- ìœ„ì¹˜: {master_status[1]}\n"
                    if len(master_status) > 2 and master_status[2]:
                        result += f"- ë°”ì¸ë”© DB: {master_status[2]}\n"
                    if len(master_status) > 3 and master_status[3]:
                        result += f"- ì œì™¸ DB: {master_status[3]}\n"
                else:
                    result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸:** ë¹„í™œì„±í™”\n"
            except Exception as e:
                result += f"â„¹ï¸ **ë°”ì´ë„ˆë¦¬ ë¡œê·¸ ìƒíƒœ:** ì¡°íšŒ ë¶ˆê°€ ({str(e)})\n"

            cursor.close()
            connection.close()

            if tunnel_used:
                self.cleanup_ssh_tunnel()

            return result

        except Exception as e:
            return f"âŒ ë³µì œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    async def validate_with_claude(
        self,
        ddl_content: str,
        database_secret: str = None,
        schema_info: dict = None,
        existing_analysis: dict = None,
    ) -> str:
        """
        Claude cross-region í”„ë¡œíŒŒì¼ì„ í™œìš©í•œ DDL ê²€ì¦ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ë° ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í¬í•¨)
        """

        # ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆœì„œ ê³ ë ¤)

        if schema_info:
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            schema_text = []
            if isinstance(schema_info, dict):
                for key, value in schema_info.items():
                    schema_text.append(f"{key}: {value}")
            else:
                schema_text.append(str(schema_info))
            
            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if existing_analysis:
                schema_text.append(f"ê¸°ì¡´ ë¶„ì„ ê²°ê³¼: {existing_analysis}")
            
            schema_context = f"""
ê´€ë ¨ ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì‹¤í–‰ ìˆœì„œë³„):
{chr(10).join(schema_text)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ DDLì˜ ì ì ˆì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. íŒŒì¼ ë‚´ì—ì„œ ë¨¼ì € ìƒì„±ëœ í…Œì´ë¸”ì€ ì´í›„ ALTER/INDEX ì‘ì—…ì—ì„œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
2. ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì„±ì˜ ì¸ë±ìŠ¤ ì¤‘ë³µ ì—¬ë¶€
3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”/ì¸ë±ìŠ¤ì— ëŒ€í•œ DROP ì‹œë„
4. ì‹¤í–‰ ìˆœì„œìƒ ë…¼ë¦¬ì  ì˜¤ë¥˜
"""
        else:
            schema_context = """
ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë¬¸ë²• ê²€ì¦ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

        prompt = f"""
        ë‹¤ìŒ DDL ë¬¸ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:

        {ddl_content}

        {schema_context}

        ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
        1. ë¬¸ë²• ì˜¤ë¥˜
        2. í‘œì¤€ ê·œì¹™ ìœ„ë°˜
        3. ì„±ëŠ¥ ë¬¸ì œ
        4. ìŠ¤í‚¤ë§ˆ ì¶©ëŒ (í…Œì´ë¸”/ì»¬ëŸ¼/ì¸ë±ìŠ¤ ì¤‘ë³µ ë“±)
        5. ë°ì´í„° íƒ€ì… í˜¸í™˜ì„± ë¬¸ì œ
        
        ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì°¸ê³ í•˜ë˜, ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì¶”ê°€ ê²€ì¦ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        ë¬¸ì œê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•´ì£¼ì„¸ìš”. ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ê²€ì¦ í†µê³¼"ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """

        claude_input = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,  # í† í° ìˆ˜ë¥¼ 4ë°°ë¡œ ì¦ê°€
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt}]}
                ],
                "temperature": 0.3,
            }
        )

        sonnet_4_model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0"
        sonnet_3_7_model_id = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"

        # Claude Sonnet 4 inference profile í˜¸ì¶œ
        try:
            response = self.bedrock_client.invoke_model(
                modelId=sonnet_4_model_id, body=claude_input
            )
            # responseê°€ dictì¸ì§€ í™•ì¸í•˜ê³  body ì¶”ì¶œ
            if isinstance(response, dict) and "body" in response:
                response_body = json.loads(response["body"].read())
            else:
                logger.error(f"Unexpected response format: {type(response)}")
                return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ - ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ êµ¬ì¡°"

            content = response_body.get("content", [])
            if isinstance(content, list) and len(content) > 0:
                first_content = content[0]
                if isinstance(first_content, dict):
                    return first_content.get("text", "")
                else:
                    return str(first_content)
            else:
                return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"
        except Exception as e:
            logger.warning(
                f"Claude Sonnet 4 í˜¸ì¶œ ì‹¤íŒ¨ â†’ Claude 3.7 Sonnet cross-region profileë¡œ fallback: {e}"
            )
            # Claude 3.7 Sonnet inference profile í˜¸ì¶œ (fallback)
            try:
                response = self.bedrock_client.invoke_model(
                    modelId=sonnet_3_7_model_id, body=claude_input
                )
                # responseê°€ dictì¸ì§€ í™•ì¸í•˜ê³  body ì¶”ì¶œ
                if isinstance(response, dict) and "body" in response:
                    response_body = json.loads(response["body"].read())
                else:
                    logger.error(f"Unexpected response format: {type(response)}")
                    return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ - ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ êµ¬ì¡°"

                content = response_body.get("content", [])
                if isinstance(content, list) and len(content) > 0:
                    first_content = content[0]
                    if isinstance(first_content, dict):
                        return first_content.get("text", "")
                    else:
                        return str(first_content)
                else:
                    return "Claude ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"
            except Exception as e:
                logger.error(f"Claude 3.7 Sonnet í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return f"Claude í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def extract_current_schema_info(
        self, database_secret: str, use_ssh_tunnel: bool = True
    ) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘: database_secret={database_secret}")
            connection, tunnel_used = await self.get_db_connection(
                database_secret, None, use_ssh_tunnel
            )
            cursor = connection.cursor()

            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            cursor.execute("SELECT DATABASE()")
            current_db = cursor.fetchone()[0]
            logger.info(f"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤: {current_db}")

            schema_info = {"tables": [], "columns": {}, "indexes": {}}

            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            cursor.execute(
                """
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = DATABASE() AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """
            )
            tables = [row[0] for row in cursor.fetchall()]
            schema_info["tables"] = tables
            logger.info(f"ë°œê²¬ëœ í…Œì´ë¸”: {tables}")

            # ê° í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT column_name, data_type, character_maximum_length, 
                           numeric_precision, numeric_scale, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY ordinal_position
                """,
                    (table,),
                )

                columns = []
                for col_row in cursor.fetchall():
                    col_info = {
                        "name": col_row[0],
                        "data_type": col_row[1],
                        "max_length": col_row[2],
                        "precision": col_row[3],
                        "scale": col_row[4],
                        "nullable": col_row[5] == "YES",
                        "default": col_row[6],
                    }
                    columns.append(col_info)

                schema_info["columns"][table] = columns

            # ê° í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            for table in tables:
                cursor.execute(
                    """
                    SELECT index_name, column_name, seq_in_index, non_unique
                    FROM information_schema.statistics 
                    WHERE table_schema = DATABASE() AND table_name = %s
                    ORDER BY index_name, seq_in_index
                """,
                    (table,),
                )

                indexes = {}
                for idx_row in cursor.fetchall():
                    idx_name = idx_row[0]
                    col_name = idx_row[1]
                    seq = idx_row[2]
                    non_unique = idx_row[3]

                    if idx_name not in indexes:
                        indexes[idx_name] = {"columns": [], "unique": non_unique == 0}

                    indexes[idx_name]["columns"].append(col_name)

                schema_info["indexes"][table] = indexes

            cursor.close()
            connection.close()

            return schema_info

        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}


# MCP ì„œë²„ ì„¤ì •
server = Server("ddl-qcli-validator")
ddl_validator = DDLValidationQCLIServer()


@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        types.Tool(
            name="list_sql_files",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={"type": "object", "properties": {}},
        ),
        types.Tool(
            name="list_database_secrets",
            description="AWS Secrets Managerì˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "keyword": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="validate_sql_file",
            description="íŠ¹ì • SQL íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    },
                },
                "required": ["filename"],
            },
        ),
        types.Tool(
            name="test_database_connection",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="validate_all_sql",
            description="sql ë””ë ‰í† ë¦¬ì˜ SQL íŒŒì¼ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤ (ìµœëŒ€ 5ê°œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                    }
                },
            },
        ),
        types.Tool(
            name="copy_sql_to_directory",
            description="SQL íŒŒì¼ì„ sql ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_path": {
                        "type": "string",
                        "description": "ë³µì‚¬í•  SQL íŒŒì¼ì˜ ê²½ë¡œ",
                    },
                    "target_name": {
                        "type": "string",
                        "description": "ëŒ€ìƒ íŒŒì¼ëª… (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ ì›ë³¸ íŒŒì¼ëª…)",
                    },
                },
                "required": ["source_path"],
            },
        ),
        types.Tool(
            name="analyze_current_schema",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="check_ddl_conflicts",
            description="DDL ì‹¤í–‰ ì „ ì¶©ëŒ ë° ë¬¸ì œì ì„ ì‚¬ì „ ê²€ì‚¬í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "ddl_content": {"type": "string", "description": "ê²€ì‚¬í•  DDL ë¬¸"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["ddl_content", "database_secret"],
            },
        ),
        types.Tool(
            name="get_aurora_mysql_parameters",
            description="Aurora MySQL í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_identifier": {
                        "type": "string",
                        "description": "Aurora í´ëŸ¬ìŠ¤í„° ì‹ë³„ì",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                    },
                    "filter_type": {
                        "type": "string",
                        "description": "í•„í„° íƒ€ì… (important: ì£¼ìš” íŒŒë¼ë¯¸í„°ë§Œ, custom: ì‚¬ìš©ì ì •ì˜ë§Œ, all: ì „ì²´)",
                        "enum": ["important", "custom", "all"],
                        "default": "important",
                    },
                    "category": {
                        "type": "string",
                        "description": "íŒŒë¼ë¯¸í„° ì¹´í…Œê³ ë¦¬ (all, security, performance, memory, io, connection, logging, replication, aurora)",
                        "enum": [
                            "all",
                            "security",
                            "performance",
                            "memory",
                            "io",
                            "connection",
                            "logging",
                            "replication",
                            "aurora",
                        ],
                        "default": "all",
                    },
                },
                "required": ["cluster_identifier"],
            },
        ),
        types.Tool(
            name="create_execution_plan",
            description="ì‘ì—… ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "operation": {"type": "string", "description": "ì‹¤í–‰í•  ì‘ì—…ëª…"},
                    "parameters": {"type": "object", "description": "ì‘ì—… ë§¤ê°œë³€ìˆ˜"},
                },
                "required": ["operation"],
            },
        ),
        types.Tool(
            name="get_schema_summary",
            description="í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì˜ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="confirm_and_execute",
            description="ê³„íš í™•ì¸ í›„ ì‹¤í–‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "confirmation": {
                        "type": "string",
                        "description": "ì‹¤í–‰ í™•ì¸ (y/yes/n/no)",
                    }
                },
                "required": ["confirmation"],
            },
        ),
        types.Tool(
            name="get_performance_metrics",
            description="ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "ë©”íŠ¸ë¦­ íƒ€ì… (all, query, io, memory, connection)",
                        "enum": ["all", "query", "io", "memory", "connection"],
                        "default": "all",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="analyze_slow_queries",
            description="ëŠë¦° ì¿¼ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "limit": {
                        "type": "integer",
                        "description": "ì¡°íšŒí•  ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)",
                        "default": 10,
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_table_io_stats",
            description="í…Œì´ë¸”ë³„ I/O í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "schema_name": {
                        "type": "string",
                        "description": "ìŠ¤í‚¤ë§ˆ ì´ë¦„ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: í˜„ì¬ DB)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_index_usage_stats",
            description="ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "table_name": {
                        "type": "string",
                        "description": "í…Œì´ë¸” ì´ë¦„ (ì„ íƒì‚¬í•­, ì „ì²´ ì¡°íšŒì‹œ ìƒëµ)",
                    },
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_connection_stats",
            description="ì—°ê²° ë° ì„¸ì…˜ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_memory_usage",
            description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_lock_analysis",
            description="ë½ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="get_replication_status",
            description="ë³µì œ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="list_aurora_mysql_clusters",
            description="í˜„ì¬ ë¦¬ì „ì˜ Aurora MySQL í´ëŸ¬ìŠ¤í„° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    }
                },
            },
        ),
        types.Tool(
            name="select_aurora_cluster",
            description="Aurora MySQL í´ëŸ¬ìŠ¤í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "cluster_selection": {
                        "type": "string",
                        "description": "í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„",
                    },
                    "region": {
                        "type": "string",
                        "description": "AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)",
                        "default": "ap-northeast-2",
                    },
                },
                "required": ["cluster_selection"],
            },
        ),
        types.Tool(
            name="list_databases",
            description="ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    }
                },
                "required": ["database_secret"],
            },
        ),
        types.Tool(
            name="select_database",
            description="ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒí•˜ê³  ë³€ê²½í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                    "database_selection": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„",
                    },
                },
                "required": ["database_secret", "database_selection"],
            },
        ),
        types.Tool(
            name="validate_sql_with_database",
            description="ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì§€ì •í•˜ì—¬ SQL íŒŒì¼ì„ ì™„ì „ ê²€ì¦í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "filename": {"type": "string", "description": "ê²€ì¦í•  SQL íŒŒì¼ëª…"},
                    "database_secret": {
                        "type": "string",
                        "description": "ë°ì´í„°ë² ì´ìŠ¤ ì‹œí¬ë¦¿ ì´ë¦„",
                    },
                },
                "required": ["filename", "database_secret"],
            },
        ),
    ]


@server.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    try:
        if name == "list_sql_files":
            result = await ddl_validator.list_sql_files()
        elif name == "list_database_secrets":
            result = await ddl_validator.list_database_secrets(
                arguments.get("keyword", "")
            )
        elif name == "validate_sql_file":
            # database_secretì´ ì—†ìœ¼ë©´ ì„ íƒ ì˜µì…˜ ì œê³µ
            filename = arguments["filename"]
            database_secret = arguments.get("database_secret")

            if not database_secret:
                # SQL íŒŒì¼ ì½ê¸°
                sql_file_path = SQL_DIR / filename
                if not sql_file_path.exists():
                    result = f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
                else:
                    with open(sql_file_path, "r", encoding="utf-8") as f:
                        ddl_content = f.read()

                    # ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ í”„ë¡¬í”„íŠ¸ í‘œì‹œ
                    result = await ddl_validator.prompt_for_database_selection(
                        ddl_content, filename
                    )
            else:
                # ê¸°ì¡´ ê³„íš ìƒì„± ë°©ì‹
                plan = await ddl_validator.create_validation_plan(
                    filename, database_secret
                )
                plan_display = ddl_validator._format_validation_plan_display(plan)

                result = f"""ğŸ“‹ **DDL ê²€ì¦ ì‹¤í–‰ ê³„íš:**

{plan_display}

â“ **ì´ ê³„íšëŒ€ë¡œ ê²€ì¦ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**
   â€¢ 'y' ë˜ëŠ” 'yes': ê²€ì¦ ì‹¤í–‰
   â€¢ 'n' ë˜ëŠ” 'no': ê²€ì¦ ì·¨ì†Œ

ğŸ’¡ **ì°¸ê³ :** confirm_and_execute ë„êµ¬ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

                ddl_validator.current_plan = plan
        elif name == "test_database_connection":
            result = await ddl_validator.execute_with_auto_plan(
                "test_database_connection", database_secret=arguments["database_secret"]
            )
        elif name == "validate_all_sql":
            # ìë™ ê³„íš ìƒì„± ë° ì‹¤í–‰
            result = await ddl_validator.execute_with_auto_plan(
                "validate_all_sql", database_secret=arguments.get("database_secret")
            )
        elif name == "copy_sql_to_directory":
            result = await ddl_validator.copy_sql_file(
                arguments["source_path"], arguments.get("target_name")
            )
        elif name == "analyze_current_schema":
            result = await ddl_validator.execute_with_auto_plan(
                "analyze_current_schema", database_secret=arguments["database_secret"]
            )
        elif name == "check_ddl_conflicts":
            result = await ddl_validator.execute_with_auto_plan(
                "check_ddl_conflicts",
                ddl_content=arguments["ddl_content"],
                database_secret=arguments["database_secret"],
            )
        elif name == "get_schema_summary":
            result = await ddl_validator.execute_with_auto_plan(
                "get_schema_summary", database_secret=arguments["database_secret"]
            )
        elif name == "get_aurora_mysql_parameters":
            result = await ddl_validator.execute_with_auto_plan(
                "get_aurora_mysql_parameters",
                cluster_identifier=arguments["cluster_identifier"],
                region=arguments.get("region", "ap-northeast-2"),
                filter_type=arguments.get("filter_type", "important"),
                category=arguments.get("category", "all"),
            )
        elif name == "create_execution_plan":
            plan = await ddl_validator.create_execution_plan(
                arguments["operation"], **arguments.get("parameters", {})
            )
            result = ddl_validator._format_plan_display(plan)
        elif name == "confirm_and_execute":
            result = await ddl_validator.confirm_and_execute(arguments["confirmation"])
        elif name == "get_performance_metrics":
            result = await ddl_validator.get_performance_metrics(
                arguments["database_secret"], arguments.get("metric_type", "all")
            )
        elif name == "analyze_slow_queries":
            result = await ddl_validator.analyze_slow_queries(
                arguments["database_secret"], arguments.get("limit", 10)
            )
        elif name == "get_table_io_stats":
            result = await ddl_validator.get_table_io_stats(
                arguments["database_secret"], arguments.get("schema_name")
            )
        elif name == "get_index_usage_stats":
            result = await ddl_validator.get_index_usage_stats(
                arguments["database_secret"], arguments.get("table_name")
            )
        elif name == "get_connection_stats":
            result = await ddl_validator.get_connection_stats(
                arguments["database_secret"]
            )
        elif name == "get_memory_usage":
            result = await ddl_validator.get_memory_usage(arguments["database_secret"])
        elif name == "get_lock_analysis":
            result = await ddl_validator.get_lock_analysis(arguments["database_secret"])
        elif name == "get_replication_status":
            result = await ddl_validator.get_replication_status(
                arguments["database_secret"]
            )
        elif name == "list_aurora_mysql_clusters":
            result = await ddl_validator.list_aurora_mysql_clusters(
                arguments.get("region", "ap-northeast-2")
            )
        elif name == "select_aurora_cluster":
            result = await ddl_validator.select_aurora_cluster(
                arguments["cluster_selection"],
                arguments.get("region", "ap-northeast-2"),
            )
        elif name == "list_databases":
            result = await ddl_validator.list_databases(arguments["database_secret"])
        elif name == "select_database":
            result = await ddl_validator.select_database(
                arguments["database_secret"], arguments["database_selection"]
            )
        elif name == "validate_sql_with_database":
            result = await ddl_validator.execute_validation_workflow(
                ddl_content="",  # íŒŒì¼ì—ì„œ ì½ì–´ì˜¬ ì˜ˆì •
                database_secret=arguments["database_secret"],
                filename=arguments["filename"],
            )
            # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì½ì–´ì„œ ì²˜ë¦¬í•´ì•¼ í•¨
            sql_file_path = SQL_DIR / arguments["filename"]
            if sql_file_path.exists():
                with open(sql_file_path, "r", encoding="utf-8") as f:
                    ddl_content = f.read()
                result = await ddl_validator.execute_validation_workflow(
                    ddl_content, arguments["database_secret"], arguments["filename"]
                )
            else:
                result = f"SQL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {arguments['filename']}"
        else:
            result = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"

        return [types.TextContent(type="text", text=result)]

    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return [types.TextContent(type="text", text=f"ì˜¤ë¥˜: {str(e)}")]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="ddl-qcli-validator",
                    server_version="1.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={},
                    ),
                ),
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise e


if __name__ == "__main__":
    asyncio.run(main())
